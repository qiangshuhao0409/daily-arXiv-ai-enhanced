<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 62]
- [cs.IT](#cs.IT) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Learning a Decentralized Medium Access Control Protocol for Shared Message Transmission](https://arxiv.org/abs/2511.06001)
*Lorenzo Mario Amorosa,Zhan Gao,Roberto Verdone,Petar Popovski,Deniz Gündüz*

Main category: cs.NI

TL;DR: 提出了一种去中心化学习框架，用于物联网网络中节点在无节点间通信或重传的情况下，协调传输共享消息，确保所有消息的可靠传递。


<details>
  <summary>Details</summary>
Motivation: 大规模物联网网络中设备数量激增，有限的通信资源需要高效的中介访问控制。节点需要在不进行节点间通信或重传的情况下，协调传输分布在随机节点子集上的共享消息。

Method: 首先证明了确定性策略的最优性，并分析了动态消息传输模式下确定性策略的性能下降。然后提出了一个去中心化学习框架，使节点能够自主合成确定性传输策略以最大化消息传递成功率，并包含在线适应机制以在动态场景中保持稳定性能。

Result: 大量仿真验证了该框架的有效性、可扩展性和适应性，展示了其对不同网络规模的鲁棒性以及对传输模式动态变化的快速适应能力，优于现有的多臂赌博机方法。

Conclusion: 该学习框架能够有效解决物联网网络中共享消息传输的协调问题，在动态环境下保持高性能，为大规模物联网网络提供了可行的解决方案。

Abstract: In large-scale Internet of things networks, efficient medium access control (MAC) is critical due to the growing number of devices competing for limited communication resources. In this work, we consider a new challenge in which a set of nodes must transmit a set of shared messages to a central controller, without inter-node communication or retransmissions. Messages are distributed among random subsets of nodes, which must implicitly coordinate their transmissions over shared communication opportunities. The objective is to guarantee the delivery of all shared messages, regardless of which nodes transmit them. We first prove the optimality of deterministic strategies, and characterize the success rate degradation of a deterministic strategy under dynamic message-transmission patterns. To solve this problem, we propose a decentralized learning-based framework that enables nodes to autonomously synthesize deterministic transmission strategies aiming to maximize message delivery success, together with an online adaptation mechanism that maintains stable performance in dynamic scenarios. Extensive simulations validate the framework's effectiveness, scalability, and adaptability, demonstrating its robustness to varying network sizes and fast adaptation to dynamic changes in transmission patterns, outperforming existing multi-armed bandit approaches.

</details>


### [2] [Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents](https://arxiv.org/abs/2511.07176)
*Hanlin Cai,Houtianfu Wang,Haofan Dong,Kai Li,Ozgur B. Akan*

Main category: cs.NI

TL;DR: 本文提出了一种基于图表示的模型投毒攻击(GRMP)，该攻击利用良性本地模型构建参数相关性图，通过变分图自编码器捕获高阶依赖关系，生成具有良性统计特征但包含对抗目标的恶意模型，能够逃避服务器检测。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习支持的物联网代理系统中，现有的基于距离和相似性的防御机制在十亿参数规模和非均匀数据分布下变得脆弱，系统容易受到模型投毒攻击的威胁。

Method: 提出GRMP攻击方法：被动观察良性本地模型构建参数相关性图，使用对抗变分图自编码器捕获和重塑高阶依赖关系，合成具有良性统计特征但嵌入对抗目标的恶意本地模型。

Result: 实验显示系统准确性在攻击下逐渐下降，现有防御机制无法有效检测该攻击，表明对物联网代理范式构成严重威胁。

Conclusion: GRMP攻击能够有效规避现有防御机制，突显了物联网代理系统中模型投毒攻击的严重性，需要开发更强大的防御方法。

Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.

</details>


### [3] [Improving Remote Patient Monitoring Systems Using a Fog-based IoT Platform with Speech Recognition](https://arxiv.org/abs/2511.07189)
*Marc Jayson Baucas,Petros Spachos*

Main category: cs.NI

TL;DR: 提出基于雾计算的物联网平台来解决远程患者监护系统的资源分配、数据流管理和隐私保护问题，通过语音识别实现交互式患者监测。


<details>
  <summary>Details</summary>
Motivation: 医疗资源短缺促使远程患者监护系统兴起，但随着患者和传感设备增加，数据与网络管理成为问题，需要解决患者隐私、数据流和服务交互性等无线架构挑战。

Method: 设计基于雾计算的物联网平台，通过资源分配缓解服务器过载，利用语音识别提供交互式患者监测，并构建测试平台评估性能。

Result: 测试结果显示该平台在准确性、延迟和吞吐量方面表现良好，具有作为基于声音的医疗服务的可行远程患者监护系统的潜力。

Conclusion: 基于雾计算的物联网平台是解决远程患者监护系统挑战的有效方案，特别适用于声音医疗服务，展示了良好的应用前景。

Abstract: Due to the recent shortage of resources in the healthcare industry, Remote Patient Monitoring (RPM) systems arose to establish a convenient alternative for accessing healthcare services remotely. However, as the usage of this system grows with the increase of patients and sensing devices, data and network management becomes an issue. As a result, wireless architecture challenges in patient privacy, data flow, and service interactability surface that need addressing. We propose a fog-based Internet of Things (IoT) platform to address these issues and reinforce the existing RPM system. The introduced platform can allocate resources to alleviate server overloading and provide an interactive means of monitoring patients through speech recognition. We designed a testbed to simulate and test the platform in terms of accuracy, latency, and throughput. The results show the platform's potential as a viable RPM system for sound-based healthcare services.

</details>


### [4] [When Intelligence Overloads Infrastructure: A Forecast Model for AI-Driven Bottlenecks](https://arxiv.org/abs/2511.07265)
*Gamal Refai-Ahmed,Mallik Tatipamula,Victor Zhirnov,Ahmed Refaey Hussein,Abdallah Shami*

Main category: cs.NI

TL;DR: 该论文预测AI智能体和连接设备将呈指数级增长，到2036年将达到数万亿实例，带宽需求将在十年内增长8000倍，导致边缘和互联系统在2030年出现饱和。


<details>
  <summary>Details</summary>
Motivation: AI智能体和连接设备的爆炸式增长正在从根本上改变全球数字基础设施的结构和容量需求，需要预测和解决即将出现的瓶颈问题。

Method: 提出统一的预测模型，通过模拟分析识别关键瓶颈领域，包括接入网络、边缘网关、互联交换和云基础设施。

Result: 预测显示AI智能体数量将在2026-2036年间增长100倍以上，带宽需求从1EB/天增至8000EB/天，边缘和互联系统将在2030年达到饱和，2033年利用率超过70%。

Conclusion: 需要计算网络设计的协同进化转变，强调分布式推理、AI原生流量工程和意图感知编排，以应对安全、可扩展性和协调性挑战。

Abstract: The exponential growth of AI agents and connected devices fundamentally transforms the structure and capacity demands of global digital infrastructure. This paper introduces a unified forecasting model that projects AI agent populations to increase by more than 100 times between 2026 and 2036+, reaching trillions of instances globally. In parallel, bandwidth demand is expected to surge from 1 EB/day in 2026 to over 8,000 EB/day by 2036, which is an increase of 8000 times in a single decade. Through this growth model, we identify critical bottleneck domains across access networks, edge gateways, interconnection exchanges, and cloud infrastructures. Simulations reveal that edge and peering systems will experience saturation as early as 2030, with more than 70% utilization of projected maximum capacity by 2033. To address these constraints, we propose a coevolutionary shift in compute-network design, emphasizing distributed inference, AI-native traffic engineering, and intent-aware orchestration. Security, scalability, and coordination challenges are examined with a focus on sustaining intelligent connectivity throughout the next digital decade.

</details>


### [5] [UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach](https://arxiv.org/abs/2511.07366)
*Dao Lan Vy Dinh,Anh Nguyen Thi Mai,Hung Tran,Giang Quynh Le Vu,Tu Dac Ho,Zhenni Pan,Vo Nhan Van,Symeon Chatzinotas,Dinh-Hieu Tran*

Main category: cs.NI

TL;DR: 提出基于MADDPG的无人机辅助通信框架，在6G网络节能场景下联合优化无人机轨迹、传输功率和用户关联，实现高覆盖率和低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决地面基站因节能或故障关闭时网络覆盖不足的问题，确保网络韧性和无人机长期运行能力。

Method: 采用多智能体深度确定性策略梯度(MADDPG)框架，联合优化无人机轨迹、传输功率和用户-无人机关联策略。

Result: MADDPG策略在不同测试场景下均实现高覆盖率，总能耗比全基站开启配置降低约24%，同时保持可比较的用户服务率。

Conclusion: 该方法在能效和服务性能之间实现了优越的权衡，支持可持续和韧性的无人机辅助蜂窝网络发展。

Abstract: This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims](https://arxiv.org/abs/2511.05524)
*Ruiying Chen*

Main category: cs.AI

TL;DR: EviBound是一个证据绑定的执行框架，通过双重治理门消除LLM自主研究代理的错误声明，要求机器可检查的证据来确保研究完整性。


<details>
  <summary>Details</summary>
Motivation: LLM自主研究代理经常报告虚假声明，即使缺少工件、存在矛盾指标或执行失败也标记为"完成"，需要解决这种研究完整性问题。

Method: 采用双重治理门：执行前批准门验证接受标准模式，执行后验证门通过MLflow API查询验证工件和指标。声明只有在有可查询的运行ID、必需工件和FINISHED状态时才传播。

Result: 在8个基准任务评估中，基线A（仅提示级别）产生100%幻觉，基线B（仅验证）将幻觉降至25%，而EviBound（双重门）实现0%幻觉，7/8任务验证通过，1个任务在批准门正确阻止，仅增加约8.3%执行开销。

Conclusion: 研究完整性是架构属性，通过治理门而非模型规模实现，EviBound框架有效消除了LLM自主研究代理的虚假声明问题。

Abstract: LLM-based autonomous research agents report false claims: tasks marked "complete" despite missing artifacts, contradictory metrics, or failed executions. EviBound is an evidence-bound execution framework that eliminates false claims through dual governance gates requiring machine-checkable evidence.
  Two complementary gates enforce evidence requirements. The pre-execution Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. The post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics when specified by acceptance criteria. Claims propagate only when backed by a queryable run ID, required artifacts, and FINISHED status. Bounded, confidence-gated retries (typically 1-2 attempts) recover from transient failures without unbounded loops.
  The framework was evaluated on 8 benchmark tasks spanning infrastructure validation, ML capabilities, and governance stress tests. Baseline A (Prompt-Level Only) yields 100% hallucination (8/8 claimed, 0/8 verified). Baseline B (Verification-Only) reduces hallucination to 25% (2/8 fail verification). EviBound (Dual Gates) achieves 0% hallucination: 7/8 tasks verified and 1 task correctly blocked at the approval gate, all with only approximately 8.3% execution overhead.
  This package includes execution trajectories, MLflow run IDs for all verified tasks, and a 4-step verification protocol. Research integrity is an architectural property, achieved through governance gates rather than emergent from model scale.

</details>


### [7] [From Prompts to Power: Measuring the Energy Footprint of LLM Inference](https://arxiv.org/abs/2511.05597)
*Francisco Caravaca,Ángel Cuevas,Rubén Cuevas*

Main category: cs.AI

TL;DR: 本文通过大规模测量研究分析了大型语言模型推理阶段的能耗问题，开发了预测模型来估算能耗，并创建了浏览器扩展来提高对生成式AI环境影响的认知。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速扩张带来了前所未有的能源需求，推理工作负载往往主导整个生命周期能耗，但系统性的推理能耗分析仍然有限。

Method: 进行了大规模测量研究，包含超过32,500次测量，涵盖21种GPU配置和155种模型架构，使用vLLM推理引擎在提示级别量化能耗。

Result: 识别了架构和操作因素如何影响能源需求，开发了能够准确估算未见架构和硬件上推理能耗的预测模型。

Conclusion: 通过浏览器扩展实施预测模型，旨在提高对生成式AI环境影响的认识，为解决LLM能耗问题提供实用工具。

Abstract: The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale measurement-based study comprising over 32,500 measurements across 21 GPU configurations and 155 model architectures, from small open-source models to frontier systems. Using the vLLM inference engine, we quantify energy usage at the prompt level and identify how architectural and operational factors shape energy demand. Building on these insights, we develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware, and implement it as a browser extension to raise awareness of the environmental impact of generative AI.

</details>


### [8] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: 提出自适应推理摘要框架，通过语义分割、重要性评分和动态压缩来压缩推理轨迹，在保持关键推理步骤的同时显著减少token使用量，实现高效的CoT跨模型迁移。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然提升了大语言模型的问题解决能力，但带来了显著的推理开销，限制了在资源受限环境中的部署。

Method: 自适应推理摘要框架，包括语义分割与重要性评分、预算感知动态压缩和连贯性重建，压缩推理轨迹。

Result: 在7,501个医学考试问题上的实验显示，在相同token预算下比截断方法准确率高40%；在64个模型对上的评估证实了强跨模型可迁移性；贝叶斯优化模块将评估成本降低84%。

Conclusion: 推理摘要为实现高效的CoT迁移提供了实用路径，使在严格计算约束下进行高级推理成为可能。

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.

</details>


### [9] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 本文通过概率分析和归因方法系统研究了大语言模型中的锚定偏见，发现锚点会改变整个输出分布，且模型规模可能影响敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中的认知偏见是表面模仿还是深层概率变化，锚定偏见作为经典人类判断偏见提供了一个关键测试案例。

Method: 使用对数概率行为分析、精确Shapley值归因和统一的锚定偏见敏感度评分，在六个开源模型上进行实验。

Result: Gemma-2B、Phi-2和Llama-2-7B表现出稳健的锚定效应，而GPT-2、Falcon-RW-1B和GPT-Neo-125M等较小模型显示出变异性，表明规模可能调节敏感性。

Conclusion: LLM中的锚定偏见是稳健、可测量和可解释的，但归因效应因提示设计而异，突显了将LLM视为人类替代品的脆弱性。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [10] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: DiagnoLLM是一个混合框架，结合贝叶斯反卷积、eQTL引导的深度学习和LLM叙事生成，用于可解释的疾病诊断，在阿尔茨海默病检测中达到88.0%准确率。


<details>
  <summary>Details</summary>
Motivation: 构建值得信赖的临床AI系统需要不仅准确的预测，还需要透明、基于生物学的解释。

Method: 使用GP-unmix高斯过程分层模型从bulk和单细胞RNA-seq数据推断细胞类型特异性基因表达谱，结合eQTL分析的调控先验训练神经网络分类器，最后通过LLM模块生成面向不同受众的诊断报告。

Result: 阿尔茨海默病检测准确率达到88.0%，人类评估确认生成的报告准确、可操作且适合医生和患者。

Conclusion: LLM作为后处理推理器而非端到端预测器时，可以在混合诊断流程中作为有效的沟通工具。

Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.

</details>


### [11] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: 提出LEAP框架解决LLM幻觉检测中固定策略缺乏适应性的问题，通过动态学习循环和主动修正机制，让高效学生模型具备动态规划和策略优化能力


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的幻觉检测方法使用预定义的固定验证策略，在动态变化环境中缺乏适应性，可能导致检测失败。直接使用GPT-4等大模型成本过高，而教师-学生架构的方法又受限于固定策略

Method: 将幻觉检测问题建模为动态策略学习问题，首先用教师模型在动态学习循环中生成轨迹并根据执行失败调整策略，然后通过智能体调优将动态规划能力蒸馏到高效学生模型中，最后学生模型在执行时采用主动修正机制来提出、审查和优化验证策略

Result: 在三个具有挑战性的基准测试中，LEAP调优的模型优于现有的最先进方法

Conclusion: LEAP框架成功解决了幻觉检测中策略适应性问题，赋予高效学生模型动态学习和主动修正能力，在保持低成本的同时实现了更好的检测性能

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.

</details>


### [12] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: 对6种思考型大语言模型在代码生成任务中的推理过程进行实证研究，发现推理链质量受任务复杂度影响，完整性是主要失败模式，但模型能保持逻辑结构一致性并自我纠错。


<details>
  <summary>Details</summary>
Motivation: 探索思考型LLMs在代码生成中生成显式中间推理链的质量，这些推理链虽然可能提高透明度和准确性，但其质量尚未得到充分研究。

Method: 评估6种最先进的推理LLMs在100个不同难度代码生成任务上的表现，通过步骤计数和冗长度量化推理链结构，进行受控步骤预算调整，并开展21人参与的人工评估。

Result: 针对性增加步骤可提高某些模型/任务的解决率，适度减少步骤在标准任务上通常能保持成功但在困难任务上很少能成功。任务复杂度显著影响推理质量，困难问题更容易出现不完整性。

Conclusion: 思考型LLMs在软件工程中具有保持逻辑结构一致性和自我纠错的能力，但推理质量受任务复杂度影响，完整性是主要挑战。

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.

</details>


### [13] [Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks](https://arxiv.org/abs/2511.05883)
*Hehai Lin,Hui Liu,Shilei Cao,Jing Li,Haoliang Li,Wenya Wang*

Main category: cs.AI

TL;DR: 提出三种基于不同粒度理论的样本级模态偏差量化方法：粗粒度的模态效益评估、中粒度的信息流量化、细粒度的因果分析，并通过人工评估验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态虚假信息基准存在特定模态偏向，使检测器仅基于单一模态即可预测，而现有方法缺乏样本级洞察且难以扩展到海量在线信息。

Method: 提出三种偏差量化方法：1) 粗粒度模态效益评估；2) 中粒度信息流量化；3) 细粒度因果分析。在两个流行基准上进行人工评估验证。

Result: 实验发现：1) 多视图集成对可靠自动分析至关重要；2) 自动分析易受检测器诱导波动影响；3) 不同视图在模态平衡样本上一致性更高，在偏差样本上分歧更大。

Conclusion: 多视图集成方法能有效识别样本级模态偏差，为未来研究提供方向，特别是在处理模态不平衡样本时需考虑检测器稳定性和多视角一致性。

Abstract: Numerous multimodal misinformation benchmarks exhibit bias toward specific modalities, allowing detectors to make predictions based solely on one modality. While previous research has quantified bias at the dataset level or manually identified spurious correlations between modalities and labels, these approaches lack meaningful insights at the sample level and struggle to scale to the vast amount of online information. In this paper, we investigate the design for automated recognition of modality bias at the sample level. Specifically, we propose three bias quantification methods based on theories/views of different levels of granularity: 1) a coarse-grained evaluation of modality benefit; 2) a medium-grained quantification of information flow; and 3) a fine-grained causality analysis. To verify the effectiveness, we conduct a human evaluation on two popular benchmarks. Experimental results reveal three interesting findings that provide potential direction toward future research: 1)~Ensembling multiple views is crucial for reliable automated analysis; 2)~Automated analysis is prone to detector-induced fluctuations; and 3)~Different views produce a higher agreement on modality-balanced samples but diverge on biased ones.

</details>


### [14] [Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement](https://arxiv.org/abs/2511.05931)
*Hiroaki Hayashi,Bo Pang,Wenting Zhao,Ye Liu,Akash Gokul,Srijan Bansal,Caiming Xiong,Semih Yavuz,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了SAGE框架，使LLM代理能够从自身任务执行中学习，通过自我抽象来提炼关键步骤、依赖关系和约束，从而改进后续执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理通常在静态执行框架中运行，缺乏从自身经验和历史执行中学习改进的机制，导致性能受限于初始框架设计和基础LLM能力。

Method: SAGE框架让代理从初始执行中归纳出简洁的计划抽象，提炼关键步骤、依赖关系和约束，然后将学习到的抽象作为上下文指导反馈给代理，优化其策略。

Result: 在多样化LLM骨干和代理架构上实现了一致的性能提升，与Mini-SWE-Agent基线相比获得7.2%相对性能改进，在SWE-Bench Verified基准上达到73.2%和74%的Pass@1解决率。

Conclusion: SAGE框架通过自我抽象学习机制有效提升了LLM代理在软件工程任务中的性能，证明了从经验中学习的重要性。

Abstract: Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.

</details>


### [15] [Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling](https://arxiv.org/abs/2511.05951)
*Qi Wang,Hongzhi Zhang,Jia Fu,Kai Fu,Yahui Liu,Tinghai Zhang,Chenxi Sun,Gangwei Jiang,Jingyi Tang,Xingguang Ji,Yang Yue,Jingyuan Zhang,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.AI

TL;DR: 开发了Klear-Qwen3-AgentForge-8B，一个完全开源的智能体训练流程，从Qwen3-8B基础模型开始，通过监督微调和多轮强化学习，在工具使用和编码任务上达到同类尺寸模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强大的智能体模型不断涌现，但缺乏关键的训练后细节阻碍了开源社区开发同等强大的模型。

Method: 设计了有效的监督微调（SFT）使用合成数据，然后进行多轮强化学习（RL），以解锁多种智能体任务的潜力。

Result: Klear-Qwen3-AgentForge-8B在类似尺寸的LLM中实现了最先进的性能，并与显著更大的模型保持竞争力。

Conclusion: 提供了一个全面且完全开源的智能体模型训练流程，能够开发出高性能的智能体模型。

Abstract: Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.

</details>


### [16] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: 该论文将智能体意识视为一种知识形式，区别于现有文献传统，提出了两种模态来捕捉这种知识的de re和de dicto形式，并建立了描述这些模态与标准"事实知识"模态之间相互作用的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 打破现有文献中将智能体意识视为传统知识的传统，提出将意识作为独立的知识形式来研究，特别是区分de re和de dicto两种意识知识形式。

Method: 引入两种模态来捕捉de re和de dicto形式的意识知识，使用2D语义学形式化其含义，构建描述这些模态与标准知识模态相互作用的逻辑系统。

Result: 建立了一个描述两种意识知识模态与标准事实知识模态之间相互作用的健全且完备的逻辑系统。

Conclusion: 通过将智能体意识形式化为知识，并区分de re和de dicto形式，成功构建了描述意识知识与标准知识之间关系的逻辑框架，为意识研究提供了新的形式化工具。

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard "knowledge of the fact" modality.

</details>


### [17] [ScRPO: From Errors to Insights](https://arxiv.org/abs/2511.06065)
*Lianrui Li,Dakuan Lu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出ScRPO框架，通过自我反思和纠错增强大语言模型在数学问题上的表现，包含试错学习和自我纠正两个阶段，在多个数学推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂数学问题上表现不佳的问题，通过自我反思和纠错机制来提升模型性能，减少对外部反馈的依赖。

Method: 两阶段方法：1) 试错学习阶段使用GRPO训练并收集错误答案；2) 自我纠正学习阶段引导模型反思之前错误的原因。

Result: 在AIME、AMC、Olympiad、MATH-500、GSM8k等多个数学推理基准测试中，ScRPO持续优于多种后训练方法。

Conclusion: ScRPO为语言模型在困难任务上的自我改进提供了一个有前景的范式，有助于构建更可靠和强大的AI系统。

Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

</details>


### [18] [Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs](https://arxiv.org/abs/2511.06134)
*Wei Yang,Jiacheng Pang,Shixuan Li,Paul Bogdan,Stephen Tu,Jesse Thomason*

Main category: cs.AI

TL;DR: 提出Maestro框架，通过角色编排解决多智能体系统中的探索-合成认知张力，结合CLPO强化学习实现清晰的信用分配，在数学推理和问题解决任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统难以平衡解决方案空间的广泛探索和最优解的收敛合成，容易导致过早共识、错误传播和信用分配问题。

Method: Maestro框架：使用并行执行智能体进行多样化探索，专门中央智能体进行收敛性合成评估；CLPO方法：结合决策导向的策略梯度和基于理由的列表排序损失。

Result: 在数学推理和通用问题解决基准测试中，Maestro+CLPO平均绝对准确率提升6%，最高可达10%，显著优于现有最先进多智能体方法。

Conclusion: 通过结构化解耦探索和合成认知模式，结合CLPO的清晰信用分配机制，能够有效提升多智能体系统的推理和问题解决能力。

Abstract: Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.

</details>


### [19] [When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks](https://arxiv.org/abs/2511.06136)
*Stefano Ferraro,Akihiro Nakano,Masahiro Suzuki,Yutaka Matsuo*

Main category: cs.AI

TL;DR: DLPWM是一个无监督的、解耦的对象中心世界模型，虽然能实现强健的视觉重建和预测，但在下游控制任务中表现不如DreamerV3，主要原因是多对象交互时的表示漂移问题。


<details>
  <summary>Details</summary>
Motivation: 研究解耦的对象级表示是否能够通过定位任务相关信息来增强策略在新型特征组合上的性能。

Method: 引入DLPWM，一个完全无监督的解耦对象中心世界模型，直接从像素学习对象级潜在表示。

Result: DLPWM实现了强大的重建和预测性能，包括对多种分布外视觉变化的鲁棒性，但在下游模型控制中表现不如DreamerV3。

Conclusion: 虽然对象中心感知支持鲁棒的视觉建模，但要实现稳定的控制需要减轻潜在漂移问题。

Abstract: Object-centric world models (OCWM) aim to decompose visual scenes into object-level representations, providing structured abstractions that could improve compositional generalization and data efficiency in reinforcement learning. We hypothesize that explicitly disentangled object-level representations, by localizing task-relevant information, can enhance policy performance across novel feature combinations. To test this hypothesis, we introduce DLPWM, a fully unsupervised, disentangled object-centric world model that learns object-level latents directly from pixels. DLPWM achieves strong reconstruction and prediction performance, including robustness to several out-of-distribution (OOD) visual variations. However, when used for downstream model-based control, policies trained on DLPWM latents underperform compared to DreamerV3. Through latent-trajectory analyses, we identify representation shift during multi-object interactions as a key driver of unstable policy learning. Our results suggest that, although object-centric perception supports robust visual modeling, achieving stable control requires mitigating latent drift.

</details>


### [20] [MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning](https://arxiv.org/abs/2511.06142)
*Sizhe Tang,Jiayu Chen,Tian Lan*

Main category: cs.AI

TL;DR: MALinZero是一种新的多智能体规划方法，通过将联合动作回报投影到低维空间，使用上下文线性赌博机问题来提升MCTS在多智能体环境中的效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体规划中，MCTS面临组合动作空间指数级增长的问题，导致树扩展的分支因子急剧增加，难以高效进行探索和利用。

Method: 将联合动作回报投影到低维空间，使用上下文线性赌博机问题公式化，采用凸且μ-平滑的损失函数，推导出线性上置信界应用于树(LinUCT)。

Result: 在矩阵游戏、SMAC和SMACv2等多智能体基准测试中达到最先进性能，优于基于模型和无模型的多智能体强化学习基线，学习速度更快且性能更好。

Conclusion: MALinZero通过低维表示结构有效解决了多智能体MCTS中的组合爆炸问题，实现了高效的多智能体探索和利用。

Abstract: Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.

</details>


### [21] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: PRIME是一个新的评估框架，使用逻辑网格谜题系统性地探测LLMs在逻辑推理和决策中受社会刻板印象影响的程度，重点关注性别刻板印象。


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏能有效抑制明显偏见输出，但在复杂逻辑推理任务中更微妙的社会偏见会浮现且逃避当前评估基准。

Method: 使用逻辑网格谜题构建评估框架，包含刻板印象、反刻板印象和中立变体，自动生成和验证，测试不同模型家族和提示缓解策略。

Result: 模型在解决方案符合刻板印象关联时推理准确率更高，表明刻板印象对LLMs演绎推理的影响。

Conclusion: PRIME对于诊断和量化LLMs演绎推理中持续存在的社会偏见具有重要意义，特别是在公平性至关重要的场景中。

Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.

</details>


### [22] [Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.06168)
*Boxuan Wang,Zhuoyun Li,Xinmiao Huang,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 提出了一个评估和优化大语言模型推理一致性的框架，包括新的对齐分数指标和语义一致性优化采样方法，发现2跳推理链对齐分数最高，并通过优化方法显著提升了长推理链的对齐分数。


<details>
  <summary>Details</summary>
Motivation: 评估和优化大语言模型在链式思维推理中的推理一致性，解决推理链与人类参考链之间的语义对齐问题。

Method: 提出了对齐分数指标来量化推理链与参考链的语义对齐程度，定义了四种关键错误类型，并开发了语义一致性优化采样方法来选择对齐错误最小的推理链。

Result: 实证发现2跳推理链对齐分数最高，语义一致性优化采样方法平均提升对齐分数29.84%，在3跳任务等长推理链中效果显著。

Conclusion: 该框架有效评估和优化了大语言模型的推理一致性，语义一致性优化采样方法能够显著提升长推理链的对齐分数，为改进模型推理能力提供了重要工具。

Abstract: This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.

</details>


### [23] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: CSP4SDG是一个基于约束满足的概率推理框架，用于社交推理游戏中隐藏角色推断，通过硬约束和软约束分析游戏事件和对话，在多个数据集上优于LLM基准方法。


<details>
  <summary>Details</summary>
Motivation: 社交推理游戏中玩家隐藏身份并故意误导他人，准确的角色识别是游戏表现的关键，但现有方法存在局限性。

Method: 将游戏事件和对话映射到四个语言无关的约束类别（证据、现象、断言、假设），使用硬约束修剪不可能角色分配，加权软约束对剩余分配评分，信息增益权重将每个假设与其在熵减少下的期望值联系起来。

Result: 在三个公共数据集上的实验表明，CSP4SDG在所有推理场景中都优于基于LLM的基准方法，并且作为辅助"推理工具"可以提升LLM性能。

Conclusion: 基于信息理论的原则性概率推理是社交推理游戏中重型神经模型的可扩展替代或补充方案。

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary "reasoning tool." Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.

</details>


### [24] [Dataforge: A Data Agent Platform for Autonomous Data Engineering](https://arxiv.org/abs/2511.06185)
*Xinyuan Wang,Yanjie Fu*

Main category: cs.AI

TL;DR: Data Agent是一个完全自主的表格数据处理系统，利用LLM推理和验证来自动执行数据清洗、分层路由和特征级优化，实现从原始数据到AI就绪数据的端到端转换。


<details>
  <summary>Details</summary>
Motivation: AI应用在材料发现、分子建模和气候科学等领域的需求增长，使得数据准备成为重要但劳动密集的步骤，需要解决可扩展性和专业知识依赖的挑战。

Method: 利用大型语言模型推理和基于验证的方法，通过双反馈循环自动执行数据清洗、分层路由和特征级优化，遵循自动、安全和非专家友好的核心原则。

Result: 展示了第一个实用的自主数据代理系统，能够将原始数据转换为更好的数据，实现端到端可靠性而无需人工监督。

Conclusion: Data Agent系统成功实现了从数据到更好数据的自主转换，为AI应用提供了可扩展且非专家友好的数据准备解决方案。

Abstract: The growing demand for AI applications in fields such as materials discovery, molecular modeling, and climate science has made data preparation an important but labor-intensive step. Raw data from diverse sources must be cleaned, normalized, and transformed to become AI-ready, while effective feature transformation and selection are essential for efficient training and inference. To address the challenges of scalability and expertise dependence, we present Data Agent, a fully autonomous system specialized for tabular data. Leveraging large language model (LLM) reasoning and grounded validation, Data Agent automatically performs data cleaning, hierarchical routing, and feature-level optimization through dual feedback loops. It embodies three core principles: automatic, safe, and non-expert friendly, which ensure end-to-end reliability without human supervision. This demo showcases the first practical realization of an autonomous Data Agent, illustrating how raw data can be transformed "From Data to Better Data."

</details>


### [25] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动不确定性分数的轻量级推理步骤验证方法，使用冻结LLM内部状态训练不确定性量化头来估计推理步骤的不确定性，在多个领域匹配或超越更大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法如过程奖励模型计算成本高、领域受限或需要大规模人工标注，需要更轻量、通用的推理验证方案。

Method: 训练基于transformer的不确定性量化头，利用冻结LLM的内部状态来估计推理步骤的不确定性，目标标签由更大LLM或原始模型自监督生成。

Result: UHeads在数学、规划和常识问答等多个领域匹配或超越比其大810倍的过程奖励模型性能，参数少于1000万。

Conclusion: LLM内部状态编码了其不确定性，可作为可靠的推理验证信号，为可扩展和泛化的内省LLM提供了有前景的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [26] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B是一个1.5B参数的密集模型，通过Spectrum-to-Signal Principle (SSP)框架开发，挑战了模型参数规模决定推理能力的共识。该模型以仅7,800美元的训练成本，在数学基准测试中超越了400倍大的DeepSeek R1，并在推理能力上与大型模型相当。


<details>
  <summary>Details</summary>
Motivation: 挑战当前共识，即小模型缺乏强大推理能力，证明通过高效训练方法，小模型也能达到与大型模型相当的推理性能，从而大幅降低训练和推理成本，使先进AI研究民主化。

Method: 采用Spectrum-to-Signal Principle (SSP)框架：1) Two-Stage Diversity-Exploring Distillation (SFT)生成广泛解决方案谱；2) MaxEnt-Guided Policy Optimization (RL)放大正确信号。

Result: 在数学基准测试中显著超越大型模型：AIME24 (80.3 vs. 79.8)、AIME25 (74.4 vs. 70.0)、HMMT25 (50.4 vs. 41.7)，优于DeepSeek R1；在LiveCodeBench V6上得分为51.1，优于Magistral Medium的50.3。相比基础模型有巨大提升。

Conclusion: 小模型通过高效训练方法可以实现与大型模型相当的推理能力，大幅降低AI研究和部署成本，为AI民主化提供了可行路径。

Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.

</details>


### [27] [ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving](https://arxiv.org/abs/2511.06226)
*Xingcheng Liu,Yanchen Guan,Haicheng Liao,Zhengbing He,Zhenning Li*

Main category: cs.AI

TL;DR: ROAR是一种新颖的事故检测和预测方法，通过结合离散小波变换、自适应目标感知模块和动态焦点损失，有效处理传感器故障、环境噪声和数据不平衡等现实挑战，在多个数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有事故预测方法往往假设理想条件，忽略了传感器故障、环境干扰和数据缺陷等现实挑战，且未能充分处理不同车辆类型间驾驶员行为和事故率的显著差异。

Method: ROAR结合离散小波变换从噪声和不完整数据中提取特征，使用自适应目标感知模块聚焦高风险车辆并建模交通参与者的时空关系，采用动态焦点损失缓解正负样本类别不平衡问题。

Result: 在Dashcam Accident Dataset、Car Crash Dataset和AnAn Accident Detection三个数据集上的评估显示，ROAR在平均精度和平均事故时间等关键指标上持续优于现有基线方法。

Conclusion: 该模型在真实世界条件下表现出强大的鲁棒性，特别是在处理传感器退化、环境噪声和不平衡数据分布方面，为复杂交通环境中的可靠事故预测提供了有前景的解决方案。

Abstract: Accurate accident anticipation is essential for enhancing the safety of autonomous vehicles (AVs). However, existing methods often assume ideal conditions, overlooking challenges such as sensor failures, environmental disturbances, and data imperfections, which can significantly degrade prediction accuracy. Additionally, previous models have not adequately addressed the considerable variability in driver behavior and accident rates across different vehicle types. To overcome these limitations, this study introduces ROAR, a novel approach for accident detection and prediction. ROAR combines Discrete Wavelet Transform (DWT), a self adaptive object aware module, and dynamic focal loss to tackle these challenges. The DWT effectively extracts features from noisy and incomplete data, while the object aware module improves accident prediction by focusing on high-risk vehicles and modeling the spatial temporal relationships among traffic agents. Moreover, dynamic focal loss mitigates the impact of class imbalance between positive and negative samples. Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently outperforms existing baselines in key metrics such as Average Precision (AP) and mean Time to Accident (mTTA). These results demonstrate the model's robustness in real-world conditions, particularly in handling sensor degradation, environmental noise, and imbalanced data distributions. This work offers a promising solution for reliable and accurate accident anticipation in complex traffic environments.

</details>


### [28] [GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening](https://arxiv.org/abs/2511.06262)
*Siming Zhao,Qi Li*

Main category: cs.AI

TL;DR: GAIA是一个面向B2B谈判和筛选的治理优先框架，通过信息门控进展、双重反馈集成和授权边界三大机制，确保AI委托在采购、房地产和人力资源等高风险场景中的安全性、效率和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险B2B环境中的部署受到治理约束，包括防止未经授权的承诺、确保充分信息收集以及维持有效的人类监督。现有研究主要关注自主谈判，忽略了分阶段信息收集、明确授权边界和系统反馈集成等实际需求。

Method: GAIA定义了委托人（人类）、代理人（LLM代理）和对手方三个核心角色，以及可选的批评者角色。通过三大机制实现：信息门控进展（分离筛选与谈判）、双重反馈集成（AI批评与人工修正结合）、授权边界与明确升级路径。

Result: 提出了一个正式的治理框架，包含四个安全不变量：任务完整性跟踪（TCI）、明确状态转换、并行学习渠道的反馈集成，以及结合自动化协议指标与人工判断的混合验证蓝图。

Conclusion: GAIA通过连接理论与实践，提供了一个可复现的规范，用于实现安全、高效和可问责的AI委托，可应用于采购、房地产和人力资源等工作流程。

Abstract: Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.

</details>


### [29] [Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents](https://arxiv.org/abs/2511.06292)
*Yaoning Yu,Kaimin Chang,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.AI

TL;DR: 提出了一个基于数据增强优化的自改进提示框架，通过生成合成金融表格和文档摘录、验证其正确性和鲁棒性，然后根据结果更新提示，从而在不需要外部标签的情况下持续改进金融推理任务的提示准确性。


<details>
  <summary>Details</summary>
Motivation: 金融文档通常包含长表格和多页报告，LLMs已成为帮助数值推理和理解这些文档的新工具，但提示质量对LLMs性能有重大影响。现有方法在固定数据集上调整提示，限制了适应新问题类型或文档结构的能力，或需要昂贵的手动标注数据集。

Method: 结合合成数据生成器、验证器和提示优化器的闭环框架。生成器产生暴露当前提示弱点的示例，验证器检查生成示例的有效性和鲁棒性，优化器根据结果逐步优化提示。

Result: 在DocMath-Eval基准测试中，该系统在准确性和鲁棒性方面均优于标准提示方法。

Conclusion: 将合成数据生成融入提示学习对金融应用具有重要价值，能够在无需外部标签的情况下持续改进提示性能。

Abstract: Financial documents like earning reports or balance sheets often involve long tables and multi-page reports. Large language models have become a new tool to help numerical reasoning and understanding these documents. However, prompt quality can have a major effect on how well LLMs perform these financial reasoning tasks. Most current methods tune prompts on fixed datasets of financial text or tabular data, which limits their ability to adapt to new question types or document structures, or they involve costly and manually labeled/curated dataset to help build the prompts. We introduce a self-improving prompt framework driven by data-augmented optimization. In this closed-loop process, we generate synthetic financial tables and document excerpts, verify their correctness and robustness, and then update the prompt based on the results. Specifically, our framework combines a synthetic data generator with verifiers and a prompt optimizer, where the generator produces new examples that exposes weaknesses in the current prompt, the verifiers check the validity and robustness of the produced examples, and the optimizer incrementally refines the prompt in response. By iterating these steps in a feedback cycle, our method steadily improves prompt accuracy on financial reasoning tasks without needing external labels. Evaluation on DocMath-Eval benchmark demonstrates that our system achieves higher performance in both accuracy and robustness than standard prompt methods, underscoring the value of incorporating synthetic data generation into prompt learning for financial applications.

</details>


### [30] [Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems](https://arxiv.org/abs/2511.06301)
*Azanzi Jiomekong,Jean Bikim,Patricia Negoue,Joyce Chin*

Main category: cs.AI

TL;DR: 本文介绍了Secu-Table数据集，这是一个包含1500多个表格和15k+实体的安全领域表格数据集，基于CVE和CWE数据构建，用于评估基于LLM的语义表格解释系统。


<details>
  <summary>Details</summary>
Motivation: 在安全领域，用于评估语义表格解释系统的表格数据集尚未公开可用，这限制了相关研究的进展。

Method: 从CVE和CWE数据源提取安全数据构建表格，使用Wikidata和SEPSES CSKG知识图谱进行标注，并公开发布数据集和代码。

Result: 创建了包含1500多个表格和15k+实体的Secu-Table数据集，并进行了初步评估，使用Falcon3-7b-instruct、Mistral-7B-Instruct和GPT-4o mini作为基线模型。

Conclusion: Secu-Table数据集填补了安全领域表格数据集的空白，为SemTab挑战赛提供了评估基准，促进了基于开源LLM的语义表格解释系统研究。

Abstract: Evaluating semantic tables interpretation (STI) systems, (particularly, those based on Large Language Models- LLMs) especially in domain-specific contexts such as the security domain, depends heavily on the dataset. However, in the security domain, tabular datasets for state-of-the-art are not publicly available. In this paper, we introduce Secu-Table dataset, composed of more than 1500 tables with more than 15k entities constructed using security data extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES CSKG). Along with the dataset, all the code is publicly released. This dataset is made available to the research community in the context of the SemTab challenge on Tabular to Knowledge Graph Matching. This challenge aims to evaluate the performance of several STI based on open source LLMs. Preliminary evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source LLM.

</details>


### [31] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开放世界的多智能体环境，模拟微型科学生态系统，智能体可以进行长期科学研究活动，包括阅读论文、提出假设、提交代码、分析数据和发表成果，无需中央协调。


<details>
  <summary>Details</summary>
Motivation: 创建自主科学发现的新范式，超越传统优化方法，通过开放世界环境中的涌现行为推动科学研究。

Method: 利用扩展上下文窗口，让AI智能体在STATION环境中自由选择行动，进行长期科学探索，包括阅读同行论文、制定假设、提交代码、执行分析和发布结果。

Result: 在数学、计算生物学和机器学习等多个基准测试中达到新的最先进性能，特别是在圆包装问题上超越AlphaEvolve，并涌现出新的方法如scRNA-seq批量整合的密度自适应算法。

Conclusion: STATION代表了通过开放世界环境中涌现行为驱动自主科学发现的第一步，标志着超越刚性优化的新范式。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.

</details>


### [32] [ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning](https://arxiv.org/abs/2511.06316)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.AI

TL;DR: ALIGN是一个视觉语言框架，通过模拟人类空间推理从文本和地图线索直接推断事故坐标，解决了多语言和非结构化新闻环境中事故位置识别的问题。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家缺乏准确的事故位置数据，现有基于文本的地理编码工具在多语言和非结构化新闻环境中表现不佳，不完整的地点描述和混合语言脚本阻碍了空间上下文理解。

Method: ALIGN整合大型语言和视觉语言模型，采用多阶段流程：光学字符识别、语言推理和基于网格的空间扫描进行地图级验证，系统评估预测位置与上下文和视觉证据的一致性。

Result: 在孟加拉语新闻数据上的应用显示，ALIGN相比传统地理解析方法有持续改进，能准确识别地区和次地区级别的事故地点。

Conclusion: 该框架为数据稀缺地区的自动化事故地图绘制建立了高精度基础，支持基于证据的道路安全政策制定，并促进多模态人工智能在交通分析中的更广泛应用。

Abstract: Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed Bangla-English scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning)- a vision-language framework that emulates human spatial reasoning to infer accident coordinates directly from textual and map-based cues. ALIGN integrates large language and vision-language models within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics. The code for this paper is open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN

</details>


### [33] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: LPFQA是一个基于长尾知识的基准测试，从20个学术和工业领域的专业论坛中提取，包含502个基于实践专业知识的任务，用于评估LLMs在真实专业场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往关注简化任务或人工场景，忽略了长尾知识和真实应用的复杂性，难以准确评估LLMs的真实能力。

Method: 从20个学术和工业领域的专业论坛收集数据，构建包含502个任务的基准测试，引入四个关键创新：细粒度评估维度、分层难度结构、真实专业场景建模和跨学科知识整合。

Result: 在12个主流LLMs上评估LPFQA，观察到显著的性能差异，特别是在专业推理任务中。

Conclusion: LPFQA为推进LLM评估和指导未来模型开发提供了一个稳健、真实和具有区分度的基准测试。

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

</details>


### [34] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: 论文提出AEPO方法解决LLMs在复杂领域推理中的"回音反射"问题，通过控制信息流和自适应熵优化来提升反思阶段的认知洞察力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数学推理上表现良好，但在涉及复杂领域知识的任务中，LLMs在反思阶段无法产生新的认知洞察，而是机械重复早期推理步骤，出现"回音反射"现象。

Method: 提出自适应熵策略优化(AEPO)框架，包含两个主要组件：反思感知信息过滤(量化认知信息流，防止早期错误认知影响最终答案)和自适应熵优化(动态平衡不同推理阶段的探索与利用)。

Result: 大量实验表明，AEPO在多个基准测试中始终优于主流强化学习基线方法，达到最先进的性能水平。

Conclusion: AEPO通过控制信息流和促进认知多样性，有效解决了LLMs在复杂领域推理中的反思能力不足问题，为提升模型认知洞察力提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as "Echo Reflection". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.

</details>


### [35] [Efficient LLM Safety Evaluation through Multi-Agent Debate](https://arxiv.org/abs/2511.06396)
*Dachuan Lin,Guobin Shen,Zihao Yang,Tianrong Liu,Dongcheng Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了一个基于小型语言模型的多代理评判框架，通过结构化辩论来评估大语言模型的安全性，同时构建了包含12,000个对抗性交互的大规模人工标注基准HAJailBench。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全评估主要依赖昂贵的前沿模型，限制了可扩展性，需要开发成本效益更高的评估方法。

Method: 使用批评者、辩护者和评判者三个代理角色的小型语言模型进行结构化辩论，构建了HAJailBench基准数据集来评估安全性和评判可靠性。

Result: 基于小型语言模型的框架在HAJailBench上达到了与GPT-4o评判者相当的协议水平，同时显著降低了推理成本，三轮辩论在准确性和效率之间达到最佳平衡。

Conclusion: 结构化、价值对齐的辩论使小型语言模型能够捕捉越狱攻击的语义细微差别，HAJailBench为可扩展的LLM安全评估提供了可靠基础。

Abstract: Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.

</details>


### [36] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 提出SofT-GRPO算法，通过注入Gumbel噪声和使用Gumbel-Softmax技术，成功将强化学习应用于软思维推理范式，使LLM在软思维模式下性能超越离散token推理。


<details>
  <summary>Details</summary>
Motivation: 软思维推理范式在某些场景下优于传统离散token推理，但难以与强化学习结合，因为难以在软思维token中注入随机性并更新策略。现有方法在结合软思维与GRPO时表现不如离散token版本。

Method: 提出SofT-GRPO算法：1) 在logits中注入Gumbel噪声；2) 使用Gumbel-Softmax避免软思维token超出预训练嵌入空间；3) 在策略梯度中利用重参数化技巧。

Result: 在1.5B到7B参数的LLM上实验，SofT-GRPO使软思维LLM在Pass@1上略优于离散token GRPO（平均准确率+0.13%），在Pass@32上显著提升（平均准确率+2.19%）。

Conclusion: SofT-GRPO成功解锁了软思维推理的潜力，为软思维范式与强化学习的结合提供了有效解决方案。

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [37] [AUTO-Explorer: Automated Data Collection for GUI Agent](https://arxiv.org/abs/2511.06417)
*Xiangwu Guo,Difei Gao,Mike Zheng Shou*

Main category: cs.AI

TL;DR: 提出Auto-Explorer自动化GUI数据收集方法，解决现有方法难以应用于桌面软件和新网站的问题，通过自主探索机制高效收集数据，并建立UIXplore基准评估探索质量。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据收集方法依赖Common Crawl的网页HTML，难以应用于桌面软件和未收录的新网站，而个性化场景需要快速适应新软件或网站的能力。

Method: 提出Auto-Explorer方法，包含简单有效的探索机制，能自主解析和探索GUI环境高效收集数据；建立UIXplore基准评估探索质量，并微调多模态大语言模型。

Result: 实验证明Auto-Explorer性能优越，能快速提升MLLM在已探索软件中的能力。

Conclusion: Auto-Explorer提供了一种低标注成本的自动化数据收集方案，有效解决了GUI数据获取的挑战，为GUI代理的泛化能力提供了支持。

Abstract: Recent advancements in GUI agents have significantly expanded their ability to interpret natural language commands to manage software interfaces. However, acquiring GUI data remains a significant challenge. Existing methods often involve designing automated agents that browse URLs from the Common Crawl, using webpage HTML to collect screenshots and corresponding annotations, including the names and bounding boxes of UI elements. However, this method is difficult to apply to desktop software or some newly launched websites not included in the Common Crawl. While we expect the model to possess strong generalization capabilities to handle this, it is still crucial for personalized scenarios that require rapid and perfect adaptation to new software or websites. To address this, we propose an automated data collection method with minimal annotation costs, named Auto-Explorer. It incorporates a simple yet effective exploration mechanism that autonomously parses and explores GUI environments, gathering data efficiently. Additionally, to assess the quality of exploration, we have developed the UIXplore benchmark. This benchmark creates environments for explorer agents to discover and save software states. Using the data gathered, we fine-tune a multimodal large language model (MLLM) and establish a GUI element grounding testing set to evaluate the effectiveness of the exploration strategies. Our experiments demonstrate the superior performance of Auto-Explorer, showing that our method can quickly enhance the capabilities of an MLLM in explored software.

</details>


### [38] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: MONICA框架通过实时监控推理步骤中的谄媚行为并动态校准，有效减少大型推理模型的谄媚行为，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在谄媚行为，倾向于迎合用户的错误信念和错误信息，这削弱了模型可靠性并带来社会风险。现有方法主要基于最终答案进行判断和修正，无法理解谄媚行为在推理过程中的发展。

Method: 提出MONICA框架，包含谄媚监控器和校准器。监控器在响应生成过程中实时监控谄媚漂移分数，校准器在分数超过阈值时动态抑制谄媚行为，无需模型完成完整答案生成。

Result: 在12个数据集和3个大型推理模型上的广泛实验表明，该方法有效减少了中间推理步骤和最终答案中的谄媚行为，获得了稳健的性能提升。

Conclusion: MONICA框架能够在推理步骤层面监控和减轻谄媚行为，为缓解大型推理模型的谄媚问题提供了有效解决方案。

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

</details>


### [39] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: EDTR是一种新的解码策略，结合拓扑分析和狄利克雷不确定性量化，通过分析思维链的几何结构来测量LLM置信度，在多个推理基准测试中实现了比现有方法更好的校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在LLM置信度估计方面存在校准差和过度自信的问题，特别是在错误预测上表现严重过自信，这影响了模型的安全部署。

Method: 将每个思维链视为高维空间中的向量，提取8个拓扑风险特征来捕捉推理分布的几何结构：紧密、连贯的簇表示高置信度，而分散、不一致的路径表示不确定性。

Result: 在四个不同推理基准测试中，EDTR比三种最先进的校准方法校准效果提升41%，平均ECE为0.287，综合得分0.672最佳，在AIME上达到完美准确率，在GSM8K上ECE为0.107，而基线方法在这些领域表现出严重过自信。

Conclusion: 该研究提供了一个几何框架来理解和量化多步LLM推理中的不确定性，在需要校准置信度估计的场景中实现了更可靠的部署。

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [40] [Brain-Inspired Planning for Better Generalization in Reinforcement Learning](https://arxiv.org/abs/2511.06470)
*Mingde "Harry" Zhao*

Main category: cs.AI

TL;DR: 该论文通过引入人类意识规划行为启发的机制，提升强化学习代理的零样本系统泛化能力，包括空间抽象、任务分解和可行性评估，以解决现实场景中的泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在现实场景中面临泛化能力不足的问题，特别是在与训练条件不同的环境中表现不佳。受人类大脑系统性泛化能力的启发，研究旨在通过赋予RL代理推理行为来增强其零样本系统泛化能力。

Method: 1. 引入自上而下的注意力机制实现空间抽象，让代理根据瞬时意图动态聚焦环境状态的关键方面；2. 开发Skipper框架自动分解复杂任务为更简单的子任务；3. 学习可行性评估器来拒绝幻觉产生的不可行目标，防止妄想规划行为。

Result: 空间抽象方法显著提高了训练任务外的系统泛化能力；Skipper框架在分布偏移和长期组合规划中表现出鲁棒性和有效性；可行性评估器在各种规划代理中带来了显著的性能提升。

Conclusion: 通过模拟人类规划行为，论文提出的方法有效提升了RL代理的系统泛化能力，并为实现通用任务抽象和完全抽象规划的未来研究指明了方向。

Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call "spatial abstraction". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.

</details>


### [41] [GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets](https://arxiv.org/abs/2511.06471)
*Jingtao Tang,Hang Ma*

Main category: cs.AI

TL;DR: GHOST是一个分层框架，用于解决图凸集旅行商问题(GCS-TSP)，通过结合组合路径搜索和凸轨迹优化，在保证最优性的同时实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统TSP方法不适用于GCS-TSP，因为边成本取决于通过凸区域的具体轨迹，而非固定值。需要开发新方法来解决这种轨迹规划问题。

Method: GHOST采用分层框架：高层在GCS诱导的完全图上搜索路径，使用抽象路径展开算法计算可接受的下界；低层搜索实现路径的可行GCS路径。通过强剪枝能力避免不必要的凸优化调用。

Result: 实验表明GHOST比统一混合整数凸规划基线快几个数量级，能处理复杂轨迹规划问题，包括高阶连续性约束和不完整GCS。

Conclusion: GHOST为GCS-TSP提供了最优且高效的解决方案，特别适用于涉及复杂约束的轨迹规划场景。

Abstract: We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS.

</details>


### [42] [FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis](https://arxiv.org/abs/2511.06522)
*Jan Ondras,Marek Šuppa*

Main category: cs.AI

TL;DR: FractalBench是一个评估AI系统从图像中合成分形程序能力的基准测试，测试了GPT-4o、Claude 3.7 Sonnet等主流多模态大模型在12种经典分形上的表现。结果显示，虽然76%的模型能生成语法有效的代码，但只有4%能正确捕捉数学结构，揭示了视觉-数学推理能力的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究多模态AI系统是否具备从视觉模式中抽象符号规则的能力——从有限推断无限。分形提供了理想的测试案例，因为迭代函数系统通过简单的递归规则生成复杂的自相似模式，要求模型在视觉感知和数学抽象之间建立桥梁。

Method: 创建FractalBench基准测试，评估4个领先的多模态大模型在12种经典分形上的表现。模型必须生成可执行的Python代码来重现分形，从而实现客观评估。

Result: 结果显示明显的脱节：76%的模型能生成语法有效的代码，但只有4%能正确捕捉数学结构。成功率因分形类型而异——模型能处理几何变换（Koch曲线：17-21%），但在分支递归（树形分形：<2%）上失败，揭示了数学抽象能力的根本差距。

Conclusion: FractalBench提供了一个抗污染的诊断工具，用于评估视觉-数学推理能力，揭示了当前多模态AI在数学抽象方面的局限性，特别是在处理复杂递归结构时存在根本性缺陷。

Abstract: Mathematical reasoning requires abstracting symbolic rules from visual patterns -- inferring the infinite from the finite. We investigate whether multimodal AI systems possess this capability through FractalBench, a benchmark evaluating fractal program synthesis from images. Fractals provide ideal test cases: Iterated Function Systems with only a few contraction maps generate complex self-similar patterns through simple recursive rules, requiring models to bridge visual perception with mathematical abstraction. We evaluate four leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL -- on 12 canonical fractals. Models must generate executable Python code reproducing the fractal, enabling objective evaluation. Results reveal a striking disconnect: 76% generate syntactically valid code but only 4% capture mathematical structure. Success varies systematically -- models handle geometric transformations (Koch curves: 17-21%) but fail at branching recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction. FractalBench provides a contamination-resistant diagnostic for visual-mathematical reasoning and is available at https://github.com/NaiveNeuron/FractalBench

</details>


### [43] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 提出GRAPH-GRPO-LEX框架，将法律合同转换为结构化语义图，使用强化学习LLM自动识别条款关系和隐藏依赖。


<details>
  <summary>Details</summary>
Motivation: 合同文件结构复杂、依赖关系丰富，人工审查既费时又容易出错，需要自动化合同分析解决方案。

Method: 基于强化学习的LLM框架，结合组相对策略优化(GRPO)，通过图指标奖励函数从合同中提取实体和关系。

Result: 能够自动识别条款间的直接关系，发现隐藏依赖，将合同分析从线性人工阅读转变为可视化图形分析。

Conclusion: 该方法为合同linting奠定基础，类似于软件工程实践，实现了合同分析的自动化和动态可视化。

Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.

</details>


### [44] [Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives](https://arxiv.org/abs/2511.06626)
*Chloe Li,Mary Phuong,Daniel Tan*

Main category: cs.AI

TL;DR: 提出了一种名为自报告微调（SRFT）的监督微调技术，训练模型在被询问时承认事实错误，这种能力可以泛化到对抗性代理设置中，使模型更可能坦白隐藏的不对齐目标。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能够执行更复杂的代理任务，它们也变得更能够追求不良目标并造成伤害。现有通过直接询问模型来捕捉不安全实例的方法存在弱点，即模型可以说谎。

Method: 使用自报告微调（SRFT）技术，在简单的问答设置中训练模型承认事实错误，然后将这种能力泛化到对抗性代理环境中。

Result: 在OOD隐身任务中，经过SRFT的模型在被询问时更可能坦白隐藏目标的细节，检测隐藏目标的F1分数达到0.98（基线模型为0），能够恢复28-100%的隐藏目标细节（基线模型为0%）。

Conclusion: SRFT是一种有前景的技术，可以促进诚实倾向并揭露不对齐的AI系统。

Abstract: As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.

</details>


### [45] [SRNN: Spatiotemporal Relational Neural Network for Intuitive Physics Understanding](https://arxiv.org/abs/2511.06761)
*Fei Yang*

Main category: cs.AI

TL;DR: SRNN模型通过大脑启发的计算原理，建立统一的神经表示来处理物体属性、关系和时序，在CLEVRER基准上取得竞争性表现，并展示了白盒诊断能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器在直觉物理理解方面与人类能力差距的问题，通过转向大脑启发的计算原则来弥合这一差距。

Method: 提出时空关系神经网络(SRNN)，建立统一的神经表示来处理物体属性、关系和时序，采用Hebbian "一起激发，一起连接"机制，通过专门的What和How通路进行计算，采用"预定义-然后微调"方法而非传统的"预训练-然后微调"范式。

Result: 在CLEVRER基准测试中取得竞争性性能，揭示了基准偏差，并展示了白盒诊断能力，能够进行精确的错误分析。

Conclusion: 证实了将生物智能转化为工程系统用于直觉物理理解的可行性，为更全面的评估指明了路径。

Abstract: Human prowess in intuitive physics remains unmatched by machines. To bridge this gap, we argue for a fundamental shift towards brain-inspired computational principles. This paper introduces the Spatiotemporal Relational Neural Network (SRNN), a model that establishes a unified neural representation for object attributes, relations, and timeline, with computations governed by a Hebbian ``Fire Together, Wire Together'' mechanism across dedicated \textit{What} and \textit{How} pathways. This unified representation is directly used to generate structured linguistic descriptions of the visual scene, bridging perception and language within a shared neural substrate. Moreover, unlike the prevalent ``pretrain-then-finetune'' paradigm, SRNN adopts a ``predefine-then-finetune'' approach. On the CLEVRER benchmark, SRNN achieves competitive performance. Our analysis further reveals a benchmark bias, outlines a path for a more holistic evaluation, and demonstrates SRNN's white-box utility for precise error diagnosis. Our work confirms the viability of translating biological intelligence into engineered systems for intuitive physics understanding.

</details>


### [46] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: 提出了MathSE框架，通过推理-反思-奖励反馈的迭代循环来增强多模态大语言模型的数学推理能力，超越了传统一次性微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂数学推理任务中存在局限，传统方法依赖教师模型蒸馏的静态数据集，缺乏适应性和泛化能力。

Method: 采用迭代微调策略，结合前阶段推理的正确路径和专门结果奖励模型(ORM)的反思反馈，实现模型的自我进化。

Result: 在多个挑战性基准测试中表现优异，在MathVL-test上超越了领先的开源多模态数学推理模型QVQ。

Conclusion: MathSE框架通过迭代自我进化机制有效提升了MLLMs的数学推理能力，为复杂推理任务提供了新思路。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.

</details>


### [47] [Proceedings of the 2025 XCSP3 Competition](https://arxiv.org/abs/2511.06918)
*Gilles Audemard,Christophe Lecoutre,Emmanuel Lonca*

Main category: cs.AI

TL;DR: 2025年XCSP3竞赛论文集，包含在CP'25会议上展示的约束求解器竞赛结果


<details>
  <summary>Details</summary>
Motivation: 记录和展示2025年XCSP3约束求解器竞赛的成果，为约束编程社区提供最新的求解器性能比较

Method: 组织约束求解器竞赛，收集各参赛求解器在标准测试集上的表现数据

Result: 在CP'25会议上公布了竞赛结果，展示了各约束求解器的相对性能

Conclusion: 该论文集记录了2025年XCSP3竞赛的完整结果，为约束编程领域的研究者和实践者提供了有价值的参考

Abstract: This document represents the proceedings of the 2025 XCSP3 Competition. The results of this competition of constraint solvers were presented at CP'25 (31st International Conference on Principles and Practice of Constraint Programming).

</details>


### [48] [Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning](https://arxiv.org/abs/2511.07061)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao,Yu Liu*

Main category: cs.AI

TL;DR: 提出了PRC-Emo框架，结合提示工程、演示检索和课程学习，提升LLM在对话情感识别中的表现，在两个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在捕捉对话中显性和隐性情感之间内在联系的能力有限，需要改进其在对话情感识别任务中的表现。

Method: 设计情感敏感提示模板，构建首个ERC专用演示检索库，并在LoRA微调中引入课程学习策略，按情感转移难度组织训练样本。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，该方法实现了新的最先进性能。

Conclusion: PRC-Emo框架有效提升了LLM在对话情感理解方面的能力，证明了方法的有效性和泛化性。

Abstract: Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.

</details>


### [49] [Improving Region Representation Learning from Urban Imagery with Noisy Long-Caption Supervision](https://arxiv.org/abs/2511.07062)
*Yimei Zhang,Guojiang Shen,Kaili Ning,Tongwei Ren,Xuebo Qiu,Mengmeng Wang,Xiangjie Kong*

Main category: cs.AI

TL;DR: UrbanLN：通过长文本感知和噪声抑制改进城市区域表示学习的预训练框架，解决视觉特征与长文本对齐困难以及LLM生成文本噪声问题


<details>
  <summary>Details</summary>
Motivation: 城市视觉外观如同"肖像"，蕴含社会经济和环境特征。现有方法利用LLM将文本知识融入图像基城市区域表示学习，但面临细粒度视觉特征与长文本对齐困难、LLM生成文本噪声影响知识融合的问题

Method: 提出信息保留拉伸插值策略对齐长文本与细粒度视觉语义；采用双级优化策略：数据级通过多模型协作自动生成多样可靠文本，模型级使用动量自蒸馏机制生成稳定伪目标，在噪声条件下实现鲁棒跨模态学习

Result: 在四个真实城市和多种下游任务上的广泛实验表明UrbanLN具有优越性能

Conclusion: UrbanLN框架有效解决了城市区域表示学习中的长文本对齐和噪声抑制问题，提升了表示学习质量

Abstract: Region representation learning plays a pivotal role in urban computing by extracting meaningful features from unlabeled urban data. Analogous to how perceived facial age reflects an individual's health, the visual appearance of a city serves as its ``portrait", encapsulating latent socio-economic and environmental characteristics. Recent studies have explored leveraging Large Language Models (LLMs) to incorporate textual knowledge into imagery-based urban region representation learning. However, two major challenges remain: i)~difficulty in aligning fine-grained visual features with long captions, and ii) suboptimal knowledge incorporation due to noise in LLM-generated captions. To address these issues, we propose a novel pre-training framework called UrbanLN that improves Urban region representation learning through Long-text awareness and Noise suppression. Specifically, we introduce an information-preserved stretching interpolation strategy that aligns long captions with fine-grained visual semantics in complex urban scenes. To effectively mine knowledge from LLM-generated captions and filter out noise, we propose a dual-level optimization strategy. At the data level, a multi-model collaboration pipeline automatically generates diverse and reliable captions without human intervention. At the model level, we employ a momentum-based self-distillation mechanism to generate stable pseudo-targets, facilitating robust cross-modal learning under noisy conditions. Extensive experiments across four real-world cities and various downstream tasks demonstrate the superior performance of our UrbanLN.

</details>


### [50] [RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2511.07070)
*Fei Zhao,Chonggang Lu,Haofu Qian,Fangcheng Shi,Zijie Meng,Jianzhao Huang,Xu Tang,Zheyong Xie,Zheyu Ye,Zhe Xu,Yao Hu,Shaosheng Cao*

Main category: cs.AI

TL;DR: RedOne 2.0是一个面向社交网络服务的LLM，采用渐进式RL优先的后训练范式，通过探索性学习、针对性微调和精炼学习三个阶段，在4B规模上实现了优于7B基线的性能，数据效率提升显著。


<details>
  <summary>Details</summary>
Motivation: 社交网络服务(SNS)存在异构工作负载、快速变化的规范和俚语、多语言文化多样性等挑战，传统监督微调容易导致分布内外性能的"跷跷板"效应，特别是对于小模型。

Method: 三阶段渐进式训练：1) 探索性学习建立初始对齐并识别系统弱点；2) 针对性微调选择性应用SFT填补诊断出的差距，同时混合少量通用数据防止遗忘；3) 精炼学习重新应用RL与SNS中心信号来巩固改进并协调任务间权衡。

Result: 在4B规模上，相比7B次优基线平均提升2.41分；相比基础模型平均性能提升8.74分，所需数据量不到SFT中心方法RedOne的一半，证明了在紧凑规模下的优越数据效率和稳定性。

Conclusion: RedOne 2.0为SNS场景中的领域特定LLM建立了具有竞争力的成本效益基线，在不牺牲鲁棒性的前提下提升了能力。

Abstract: As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.

</details>


### [51] [Increasing AI Explainability by LLM Driven Standard Processes](https://arxiv.org/abs/2511.07083)
*Marc Jansen,Marcel Pehlke*

Main category: cs.AI

TL;DR: 提出了一种通过将LLM嵌入标准化分析流程来提高AI系统可解释性的方法，将LLM推理置于正式决策框架中，实现透明可审计的决策追踪。


<details>
  <summary>Details</summary>
Motivation: 传统可解释AI方法主要关注特征归因或事后解释，缺乏将LLM推理过程整合到结构化决策模型中的系统性方法。

Method: 在QOC、敏感性分析、博弈论和风险管理等标准化决策模型中嵌入LLM，采用分层架构分离LLM推理空间和可解释过程空间。

Result: 实证评估显示系统能够在去中心化治理、系统分析和战略推理等场景中重现人类水平的决策逻辑。

Conclusion: LLM驱动的标准流程为可靠、可解释和可验证的AI辅助决策提供了基础。

Abstract: This paper introduces an approach to increasing the explainability of artificial intelligence (AI) systems by embedding Large Language Models (LLMs) within standardized analytical processes. While traditional explainable AI (XAI) methods focus on feature attribution or post-hoc interpretation, the proposed framework integrates LLMs into defined decision models such as Question-Option-Criteria (QOC), Sensitivity Analysis, Game Theory, and Risk Management. By situating LLM reasoning within these formal structures, the approach transforms opaque inference into transparent and auditable decision traces. A layered architecture is presented that separates the reasoning space of the LLM from the explainable process space above it. Empirical evaluations show that the system can reproduce human-level decision logic in decentralized governance, systems analysis, and strategic reasoning contexts. The results suggest that LLM-driven standard processes provide a foundation for reliable, interpretable, and verifiable AI-supported decision making.

</details>


### [52] [LLM Driven Processes to Foster Explainable AI](https://arxiv.org/abs/2511.07086)
*Marcel Pehlke,Marc Jansen*

Main category: cs.AI

TL;DR: 提出了一个模块化、可解释的LLM代理决策支持系统，通过外部化推理生成可审计的中间产物，结合三种框架（敏感性模型、博弈论、序列博弈）进行决策分析。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的决策支持系统，通过外部化推理过程生成可审计的中间产物，提高决策过程的透明度和可解释性。

Method: 采用模块化LLM代理管道，结合Vester敏感性模型（因子集、符号影响矩阵、系统角色、反馈循环）、标准形式博弈（策略、支付矩阵、均衡）和序列博弈（角色条件代理、树构建、逆向归纳），每个步骤都支持模块替换。

Result: 在真实物流案例中（100次运行），与人类基线相比，26个因子的平均对齐度为55.5%，运输核心子集为62.9%；角色匹配一致性为57%。LLM评估器使用八项标准评分（满分100）与重构的人类基线相当。

Conclusion: 可配置的LLM管道能够模拟专家工作流程，提供透明、可检查的决策步骤，为决策支持系统提供了新的可解释性方法。

Abstract: We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\% over 26 factors and 62.9\% on the transport-core subset; role agreement over matches was 57\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.

</details>


### [53] [Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts](https://arxiv.org/abs/2511.07090)
*Marcel Rojahn,Marcus Grum*

Main category: cs.AI

TL;DR: 本文提出了绿色AI的统一操作定义，建立了包含五个阶段的生命周期框架，将能源、碳、水和隐含影响作为首要考量，并制定了结合估计模型和直接测量的校准测量框架。


<details>
  <summary>Details</summary>
Motivation: AI生命周期中的负担包括能源、碳、水消耗和隐含影响，现有云提供商工具存在异质性且往往忽略水和价值链效应，限制了可比性和可重现性。

Method: 建立统一的绿色AI定义，将生命周期分为五个阶段映射到LCA阶段，通过PDCA循环进行治理，系统化硬件和系统级策略，并定义结合估计模型和直接测量的校准测量框架。

Result: 提供了可操作的、基于证据的指导，使研究人员、从业者和政策制定者能够进行可重现、提供商无关的比较。

Conclusion: 通过结合定义、生命周期流程、硬件策略和校准测量，本文为减少AI多维负担提供了综合解决方案。

Abstract: Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers.

</details>


### [54] [Data Complexity of Querying Description Logic Knowledge Bases under Cost-Based Semantics](https://arxiv.org/abs/2511.07095)
*Meghyn Bienvenu,Quentin Manière*

Main category: cs.AI

TL;DR: 本文研究了在加权描述逻辑知识库中使用基于成本的语义进行查询的数据复杂性，重点关注包含逆角色和角色包含的DL，覆盖了DL-Lite方言。


<details>
  <summary>Details</summary>
Motivation: 现有的基于成本语义的研究主要集中在EL⊥到ALCO之间的描述逻辑，且所有现有结果都显示基于成本语义是不可处理的。本文旨在扩展研究范围，考虑包含逆角色和角色包含的DL，并探索在固定成本边界下的可处理性。

Method: 通过为每个解释分配基于违反公理和断言权重的成本，确定查询答案。使用一阶重写技术来计算实例查询的确定答案和连接查询的可能答案。

Result: 对于DL-Lite^H_bool本体和固定成本边界，实例查询的确定答案和连接查询的可能答案可以通过一阶重写计算，达到最低可能的数据复杂性(TC0)。

Conclusion: 本文显著扩展了基于成本语义的数据复杂性分析，不仅强化了多个下界，还确定了最优成本确定答案语义的精确复杂性，并意外发现在固定成本边界下某些查询可以达到TC0复杂度。

Abstract: In this paper, we study the data complexity of querying inconsistent weighted description logic (DL) knowledge bases under recently-introduced cost-based semantics. In a nutshell, the idea is to assign each interpretation a cost based upon the weights of the violated axioms and assertions, and certain and possible query answers are determined by considering all (resp. some) interpretations having optimal or bounded cost. Whereas the initial study of cost-based semantics focused on DLs between $\mathcal{EL}_\bot$ and $\mathcal{ALCO}$, we consider DLs that may contain inverse roles and role inclusions, thus covering prominent DL-Lite dialects. Our data complexity analysis goes significantly beyond existing results by sharpening several lower bounds and pinpointing the precise complexity of optimal-cost certain answer semantics (no non-trivial upper bound was known). Moreover, while all existing results show the intractability of cost-based semantics, our most challenging and surprising result establishes that if we consider $\text{DL-Lite}^\mathcal{H}_\mathsf{bool}$ ontologies and a fixed cost bound, certain answers for instance queries and possible answers for conjunctive queries can be computed using first-order rewriting and thus enjoy the lowest possible data complexity ($\mathsf{TC}_0$).

</details>


### [55] [Agentic AI Sustainability Assessment for Supply Chain Document Insights](https://arxiv.org/abs/2511.07097)
*Diego Gosmar,Anna Chiara Pallotta,Giovanni Zenezini*

Main category: cs.AI

TL;DR: 提出了一个基于智能AI的文档智能可持续性评估框架，在供应链文档处理中实现自动化效率提升和环境绩效衡量。


<details>
  <summary>Details</summary>
Motivation: 解决文档密集型工作流中自动化效率与环境绩效的双重目标，为AI赋能的供应链解决方案提供统一的ESG导向评估方法。

Method: 比较三种场景：全人工、AI辅助（人在回路）和高级多智能体AI工作流，结合解析器和验证器，集成性能、能耗和排放指标。

Result: AI辅助和智能AI场景相比人工流程实现：能耗降低70-90%，二氧化碳排放减少90-97%，用水量减少89-98%。完整智能配置在可持续性方面显著优于纯人工方法。

Conclusion: 智能AI配置在供应链文档处理中能实现显著的可持续性收益，该框架为评估和管理AI赋能的供应链解决方案提供了可复制的ESG导向方法论。

Abstract: This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.

</details>


### [56] [Boosting Fine-Grained Urban Flow Inference via Lightweight Architecture and Focalized Optimization](https://arxiv.org/abs/2511.07098)
*Yuanshao Zhu,Xiangyu Zhao,Zijian Zhang,Xuetao Wei,James Jianqiao Yu*

Main category: cs.AI

TL;DR: 提出了PLGF模型，通过渐进式局部-全局融合架构和双焦点损失函数，在保持轻量化的同时实现细粒度城市流量推断的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有城市流量推断方法面临两个关键挑战：过度参数化模型的计算成本过高，以及传统损失函数在高度偏态分布的流量数据上表现不佳。

Method: 1. PLGF架构：采用渐进式局部-全局融合策略，有效捕捉细粒度细节和全局上下文依赖；2. DualFocal Loss：结合双空间监督和难度感知聚焦机制，自适应关注难以预测的区域。

Result: 在4个真实场景的实验中，PLGF在达到最先进性能的同时，模型大小比现有高性能方法减少高达97%；在同等参数预算下，准确率比强基线提高超过10%。

Conclusion: 该方法通过架构效率和自适应优化的协同作用，为细粒度城市流量推断提供了统一解决方案，具有显著的计算效率和性能优势。

Abstract: Fine-grained urban flow inference is crucial for urban planning and intelligent transportation systems, enabling precise traffic management and resource allocation. However, the practical deployment of existing methods is hindered by two key challenges: the prohibitive computational cost of over-parameterized models and the suboptimal performance of conventional loss functions on the highly skewed distribution of urban flows. To address these challenges, we propose a unified solution that synergizes architectural efficiency with adaptive optimization. Specifically, we first introduce PLGF, a lightweight yet powerful architecture that employs a Progressive Local-Global Fusion strategy to effectively capture both fine-grained details and global contextual dependencies. Second, we propose DualFocal Loss, a novel function that integrates dual-space supervision with a difficulty-aware focusing mechanism, enabling the model to adaptively concentrate on hard-to-predict regions. Extensive experiments on 4 real-world scenarios validate the effectiveness and scalability of our method. Notably, while achieving state-of-the-art performance, PLGF reduces the model size by up to 97% compared to current high-performing methods. Furthermore, under comparable parameter budgets, our model yields an accuracy improvement of over 10% against strong baselines. The implementation is included in the https://github.com/Yasoz/PLGF.

</details>


### [57] [A Theoretical Analysis of Detecting Large Model-Generated Time Series](https://arxiv.org/abs/2511.07104)
*Junji Hou,Junzhou Zhao,Shuo Zhang,Pinghui Wang*

Main category: cs.AI

TL;DR: 提出了一种检测时间序列大模型生成数据的方法UCE，基于模型生成时间序列在递归预测中不确定性收缩的特性，在32个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据滥用和伪造风险增加，需要检测时间序列大模型生成的合成数据。现有文本生成检测方法不适用于时间序列数据，因为时间序列信息密度较低、概率分布更平滑。

Method: 提出收缩假说：模型生成的时间序列在递归预测中表现出不确定性递减。基于此开发了UCE检测器，通过聚合连续前缀的不确定性指标来识别模型生成的时间序列。

Result: 在32个数据集上的广泛实验表明，UCE始终优于现有最先进基线方法，提供了可靠且可推广的模型生成时间序列检测解决方案。

Conclusion: 模型生成的时间序列在递归预测中确实表现出不确定性收缩特性，UCE检测器基于这一特性能够有效识别合成时间序列数据，为时间序列数据真实性验证提供了新方法。

Abstract: Motivated by the increasing risks of data misuse and fabrication, we investigate the problem of identifying synthetic time series generated by Time-Series Large Models (TSLMs) in this work. While there are extensive researches on detecting model generated text, we find that these existing methods are not applicable to time series data due to the fundamental modality difference, as time series usually have lower information density and smoother probability distributions than text data, which limit the discriminative power of token-based detectors. To address this issue, we examine the subtle distributional differences between real and model-generated time series and propose the contraction hypothesis, which states that model-generated time series, unlike real ones, exhibit progressively decreasing uncertainty under recursive forecasting. We formally prove this hypothesis under theoretical assumptions on model behavior and time series structure. Model-generated time series exhibit progressively concentrated distributions under recursive forecasting, leading to uncertainty contraction. We provide empirical validation of the hypothesis across diverse datasets. Building on this insight, we introduce the Uncertainty Contraction Estimator (UCE), a white-box detector that aggregates uncertainty metrics over successive prefixes to identify TSLM-generated time series. Extensive experiments on 32 datasets show that UCE consistently outperforms state-of-the-art baselines, offering a reliable and generalizable solution for detecting model-generated time series.

</details>


### [58] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，通过元认知驱动的自我进化机制来发现和缓解大语言模型在领域任务中的隐性风险，包括元认知自我评估、动态规则知识图谱生成和激活引导推理等技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐主要针对显性风险，但缺乏对领域特定隐性风险的关注，且缺少灵活、可泛化的跨领域风险缓解框架。

Method: 1) 引入元认知自我评估工具，让LLM通过换位思考和结果推理反思价值对齐问题；2) 基于反思结果动态生成补充规则知识图谱；3) 在推理时使用激活引导来增强规则遵循。

Result: 在三个垂直领域的防御测试中，MENTOR显著降低了语义攻击成功率，实现了新的隐性风险缓解水平。元认知评估与人工评估高度一致且提供更深入的分析。

Conclusion: MENTOR框架通过元认知驱动的自我进化机制，有效解决了LLM在领域任务中的隐性风险问题，提供了一种持续进化的风险缓解方案。

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.

</details>


### [59] [Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture](https://arxiv.org/abs/2511.07110)
*Tianhao Fu,Xinxin Xu,Weichen Xu,Jue Chen,Ruilong Ren,Bowen Deng,Xinyu Zhao,Jian Cao,Xixin Cao*

Main category: cs.AI

TL;DR: 提出了CMM框架，通过将LLM特征在层、任务和数据三个正交维度上进行解耦，实现多学生模型协作学习，并引入Hájek-MoE集成模型输出，显著提升了市场做市任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的金融交易方法受限于推理速度慢的问题，且缺乏针对市场做市任务的LLM蒸馏研究，需要开发高效的蒸馏框架。

Method: 首先提出归一化荧光探针研究LLM特征机制，然后设计CMM框架在层、任务和数据三个维度解耦LLM特征，让多个学生模型协作学习不同特征，最后使用Hájek-MoE在核函数生成的公共特征空间中集成模型输出。

Result: 在四个真实市场数据集上的实验结果表明，CMM优于当前蒸馏方法和基于强化学习的市场做市策略。

Conclusion: CMM框架通过多维度特征解耦和协作学习，成功实现了LLM知识的有效蒸馏，为金融领域的LLM应用提供了高效解决方案。

Abstract: Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an Hájek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.

</details>


### [60] [Saliency Map-Guided Knowledge Discovery for Subclass Identification with LLM-Based Symbolic Approximations](https://arxiv.org/abs/2511.07126)
*Tim Bohne,Anne-Kathrin Patricia Windler,Martin Atzmueller*

Main category: cs.AI

TL;DR: 提出了一种基于梯度显著性图的神经符号方法，用于时间序列分类中的潜在子类发现，通过标签包含将多类问题转为二分类，结合聚类和LLM进行符号近似和知识图谱匹配。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分类中识别潜在子类的问题，利用神经网络显著性图指导知识发现过程，提高子类识别效果。

Method: 将多类时间序列分类转为二分类，训练分类器获取显著性图，对预测类别的信号进行聚类，使用聚类中心通过LLM进行符号近似和模糊知识图谱匹配。

Result: 在标准时间序列分类数据集上，该方法在聚类和子类识别方面优于仅使用信号的基线方法。

Conclusion: 基于显著性图的方法能有效发现时间序列分类中的潜在子类，为知识发现提供了新途径。

Abstract: This paper proposes a novel neuro-symbolic approach for sensor signal-based knowledge discovery, focusing on identifying latent subclasses in time series classification tasks. The approach leverages gradient-based saliency maps derived from trained neural networks to guide the discovery process. Multiclass time series classification problems are transformed into binary classification problems through label subsumption, and classifiers are trained for each of these to yield saliency maps. The input signals, grouped by predicted class, are clustered under three distinct configurations. The centroids of the final set of clusters are provided as input to an LLM for symbolic approximation and fuzzy knowledge graph matching to discover the underlying subclasses of the original multiclass problem. Experimental results on well-established time series classification datasets demonstrate the effectiveness of our saliency map-driven method for knowledge discovery, outperforming signal-only baselines in both clustering and subclass identification.

</details>


### [61] [Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations](https://arxiv.org/abs/2511.07204)
*Giacomo Fidone,Lucia Passaro,Riccardo Guidotti*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的在线社交网络对话模拟器，用于评估内容审核策略的有效性，发现个性化审核策略具有优越效果。


<details>
  <summary>Details</summary>
Motivation: 由于数据收集成本高和实验控制有限，当前在线社交网络内容审核干预的实际效果尚不明确，需要新的评估方法。

Method: 设计基于大语言模型的在线社交网络对话模拟器，支持并行、反事实模拟，在保持其他因素不变的情况下研究毒性行为如何受审核干预影响。

Result: 实验验证了OSN代理的心理真实性，揭示了社会传染现象的出现，并证明个性化审核策略具有更高的有效性。

Conclusion: LLM驱动的模拟器为评估内容审核策略提供了新途径，个性化干预在控制毒性内容传播方面效果显著。

Abstract: Online Social Networks (OSNs) widely adopt content moderation to mitigate the spread of abusive and toxic discourse. Nonetheless, the real effectiveness of moderation interventions remains unclear due to the high cost of data collection and limited experimental control. The latest developments in Natural Language Processing pave the way for a new evaluation approach. Large Language Models (LLMs) can be successfully leveraged to enhance Agent-Based Modeling and simulate human-like social behavior with unprecedented degree of believability. Yet, existing tools do not support simulation-based evaluation of moderation strategies. We fill this gap by designing a LLM-powered simulator of OSN conversations enabling a parallel, counterfactual simulation where toxic behavior is influenced by moderation interventions, keeping all else equal. We conduct extensive experiments, unveiling the psychological realism of OSN agents, the emergence of social contagion phenomena and the superior effectiveness of personalized moderation strategies.

</details>


### [62] [PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork](https://arxiv.org/abs/2511.07260)
*Hohei Chan,Xinzhi Zhang,Antao Xiang,Weinan Zhang,Mengchen Zhao*

Main category: cs.AI

TL;DR: PADiff是一种基于扩散模型的方法，用于解决ad hoc teamwork中的多模态合作模式问题，通过在去噪过程中整合队友预测信息来适应非平稳环境。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在ad hoc teamwork中容易收敛到单一行为模式，无法捕捉多模态合作模式，而标准扩散模型在高度非平稳的AHT场景中缺乏预测和适应能力。

Method: 提出PADiff扩散策略，将队友的关键预测信息整合到去噪过程中，以捕捉智能体的多模态行为并解锁多样化的合作模式。

Result: 在三个合作环境中的大量实验表明，PADiff显著优于现有的AHT方法。

Conclusion: PADiff通过整合预测信息的扩散模型，成功解决了AHT中的多模态合作挑战，在非平稳环境中表现出优越性能。

Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.

</details>


### [63] [AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning](https://arxiv.org/abs/2511.07262)
*Qile Jiang,George Karniadakis*

Main category: cs.AI

TL;DR: AgenticSciML是一个多智能体协作系统，通过10多个专业AI智能体的结构化辩论和迭代进化，自动设计和优化科学机器学习解决方案，在物理信息学习和算子学习任务中显著超越单智能体和人工设计的基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习的设计过程需要专家驱动的研究，涉及大量实验和问题特定洞察。现有方法缺乏自动化和可扩展性，需要开发能够自主发现和优化SciML解决方案的框架。

Method: 采用协作多智能体系统，整合结构化辩论、检索增强方法记忆和集成引导的进化搜索。智能体通过提出、批评和精炼SciML解决方案来进行结构化推理和迭代进化。

Result: 在物理信息学习和算子学习任务中，该框架发现的解决方案比单智能体和人工设计的基线方法在误差减少方面提升了高达四个数量级。智能体产生了新颖策略，如自适应专家混合架构、基于分解的PINNs和物理信息算子学习模型。

Conclusion: AI智能体间的协作推理能够产生涌现的方法创新，为科学计算中可扩展、透明和自主的发现提供了一条路径。

Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.

</details>


### [64] [Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion](https://arxiv.org/abs/2511.07267)
*Chen Han,Yijia Ma,Jin Tan,Wenzhen Zheng,Xijin Tang*

Main category: cs.AI

TL;DR: ED2D是一个基于证据的多智能体辩论框架，不仅用于错误信息检测，还旨在纠正用户信念和阻止错误信息传播。该框架在检测准确性上优于现有基准，但其解释在错误分类时可能强化用户误解。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论框架主要关注检测准确性，忽视了帮助用户理解事实判断背后推理和培养未来抵御能力的重要性。辩论记录作为透明推理的丰富资源未被充分利用。

Method: 引入ED2D框架，扩展了先前方法，整合了事实证据检索。该框架不仅作为检测工具，还设计为具有说服力的多智能体系统，旨在纠正用户信念和阻止错误信息传播。

Result: ED2D在三个错误信息检测基准上优于现有基线。当ED2D做出正确预测时，其反驳记录的说服效果与人类专家相当；但当错误分类时，其解释可能无意中强化用户误解，即使与准确的人类解释一起呈现。

Conclusion: 研究结果凸显了部署多智能体辩论系统进行错误信息干预的潜力和潜在风险。开发了公共社区网站来促进透明度、批判性思维和协作事实核查。

Abstract: Multi-agent debate (MAD) frameworks have emerged as promising approaches for misinformation detection by simulating adversarial reasoning. While prior work has focused on detection accuracy, it overlooks the importance of helping users understand the reasoning behind factual judgments and develop future resilience. The debate transcripts generated during MAD offer a rich but underutilized resource for transparent reasoning. In this study, we introduce ED2D, an evidence-based MAD framework that extends previous approach by incorporating factual evidence retrieval. More importantly, ED2D is designed not only as a detection framework but also as a persuasive multi-agent system aimed at correcting user beliefs and discouraging misinformation sharing. We compare the persuasive effects of ED2D-generated debunking transcripts with those authored by human experts. Results demonstrate that ED2D outperforms existing baselines across three misinformation detection benchmarks. When ED2D generates correct predictions, its debunking transcripts exhibit persuasive effects comparable to those of human experts; However, when ED2D misclassifies, its accompanying explanations may inadvertently reinforce users'misconceptions, even when presented alongside accurate human explanations. Our findings highlight both the promise and the potential risks of deploying MAD systems for misinformation intervention. We further develop a public community website to help users explore ED2D, fostering transparency, critical thinking, and collaborative fact-checking.

</details>


### [65] [IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction](https://arxiv.org/abs/2511.07327)
*Guoxin Chen,Zile Qiao,Xuanzhong Chen,Donglei Yu,Haotian Xu,Wayne Xin Zhao,Ruihua Song,Wenbiao Yin,Huifeng Yin,Liwen Zhang,Kuan Li,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.AI

TL;DR: IterResearch提出了一种迭代式深度研究范式，通过马尔可夫决策过程和策略性工作空间重构解决长视野研究任务中的上下文窒息问题，显著提升了研究性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理采用单上下文范式，将所有信息积累在单一扩展的上下文窗口中，导致上下文窒息和噪声污染，限制了在长视野任务中的有效性。

Method: 将长视野研究重新构建为马尔可夫决策过程，维护演化报告作为记忆并定期合成见解；开发效率感知策略优化框架，通过几何奖励折扣激励高效探索，并通过自适应下采样实现稳定分布式训练。

Result: 在六个基准测试中平均提升14.5个百分点，与前沿专有系统的差距缩小；展现出前所未有的交互扩展性，扩展到2048次交互时性能从3.5%提升至42.5%；作为提示策略可将前沿模型性能提升高达19.2个百分点。

Conclusion: IterResearch是长视野推理的多功能解决方案，既可作为训练代理，也可作为前沿模型的提示范式。

Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.

</details>


### [66] [DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas](https://arxiv.org/abs/2511.07338)
*Zhen Wang,Yufan Zhou,Zhongyan Luo,Lyumanshan Ye,Adam Wood,Man Yao,Luoshang Pan*

Main category: cs.AI

TL;DR: DEEPPERSONA是一个可扩展的生成引擎，通过两阶段、分类学指导的方法合成具有完整叙事性的合成人物角色，显著提升了人物属性的多样性和独特性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数合成人物角色过于浅显和简单，仅捕捉了少量属性，无法反映真实人类身份的丰富复杂性和多样性。

Method: 采用两阶段方法：首先通过挖掘数千个真实用户与ChatGPT的对话，算法构建了最大的人类属性分类学；然后从该分类学中逐步采样属性，有条件地生成连贯且真实的人物角色。

Result: 内在评估显示属性多样性提高了32%，角色独特性提高了44%；外在评估显示在个性化问答准确率上平均提高了11.6%，在社交调查中缩小了模拟LLM公民与真实人类响应之间的差距31.7%。

Conclusion: DEEPPERSONA为高保真人类模拟和个性化AI研究提供了一个严谨、可扩展且无需隐私的平台。

Abstract: Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.

</details>


### [67] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: 提出了DigiData数据集和DigiData-Bench基准，用于训练和评估移动控制AI代理。该数据集通过全面探索应用功能构建，具有更高的多样性和目标复杂性，同时提出了动态评估协议和AI驱动的评估方法来解决现有评估指标的不足。


<details>
  <summary>Details</summary>
Motivation: 开发能够控制用户界面的AI代理可以改变人类与数字设备的交互方式，但需要高质量的数据集来训练代理完成复杂的人类相关目标，以及强大的评估方法来快速提升代理性能。

Method: 构建了大规模、高质量、多样化、多模态的DigiData数据集，通过全面探索应用功能而非非结构化交互来构建目标；同时提出了DigiData-Bench基准，采用动态评估协议和AI驱动的评估方法。

Result: 创建了比现有数据集更具多样性和更高目标复杂性的数据集，并证明了常用的步骤准确性指标在评估移动控制代理时存在不足，提出了更严格的替代评估方案。

Conclusion: 这些贡献旨在显著推进移动控制代理的发展，为更直观和有效的人机交互铺平道路。

Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [68] [Necessary and Sufficient Conditions for Capacity-Achieving Private Information Retrieval with Adversarial Servers](https://arxiv.org/abs/2511.06003)
*Atsushi Miki,Toshiyasu Matsushima*

Main category: cs.IT

TL;DR: 本文证明了实现容量最优的私有信息检索(PIR)方案的充要条件，为系统构建达到理论上界的PIR方案提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究证明了各种PIR方案的信息理论上界，但缺乏系统构建达到这些上界的方法。为了系统构建容量最优的PIR方案，需要明确查询应满足的条件。

Method: 通过理论分析，证明了容量最优PIR方案的充要条件。

Result: 明确了实现容量最优PIR方案所需的查询条件。

Conclusion: 该研究为系统构建达到信息理论上界的PIR方案提供了理论基础和指导原则。

Abstract: Private information retrieval (PIR) is a mechanism for efficiently downloading messages while keeping the index of the desired message secret from the servers. PIR schemes have been extended to various scenarios with adversarial servers: PIR schemes where some servers are unresponsive or return noisy responses are called robust PIR and Byzantine PIR, respectively; PIR schemes where some servers collude to reveal the index are called colluding PIR. The information-theoretic upper bound on the download efficiency of these PIR schemes has been proved in previous studies. However, systematic ways to construct PIR schemes that achieve the upper bound are not known. In order to construct a capacity-achieving PIR schemes systematically, it is necessary to clarify the conditions that the queries should satisfy. This paper proves the necessary and sufficient conditions for capacity-achieving PIR schemes.

</details>


### [69] [Capacity Analysis of Cascaded BD-RIS Assisted MIMO Systems](https://arxiv.org/abs/2511.06089)
*M. S. S. Manasa,Praful D. Mankar,Sundaram Vanka*

Main category: cs.IT

TL;DR: 本文研究了级联超对角线可重构智能表面(BD-RIS)在MIMO系统中的性能提升潜力，推导了级联RIS与SVD注水法和均匀功率分配预编码策略的最优闭式解，并分析了近似遍历容量及其高信噪比近似。


<details>
  <summary>Details</summary>
Motivation: 探索级联BD-RIS部署如何增强MIMO系统性能，同时降低发射机复杂度，为高信噪比级联RIS MIMO系统的实际部署提供理论指导。

Method: 推导了级联RIS与SVD注水法和均匀功率分配预编码策略的联合最优闭式解，分析了近似遍历容量及其高信噪比近似。

Result: 级联RIS与均匀功率分配策略可实现与SVD注水法相当的性能，同时降低发射机复杂度；建立了发射功率、RIS尺寸和可达容量之间的权衡关系。

Conclusion: 级联RIS不仅提升MIMO系统性能，还能降低发射机复杂度，为高信噪比级联RIS MIMO系统的实际部署提供了重要理论依据和设计指导。

Abstract: This paper examines the cascaded deployment of beyond diagonal (BD) reconfigurable intelligent surfaces (RISs) and explores its potential to enhance the performance of MIMO systems. We first derive the jointly optimal closed form solutions for the RISs in cascade with SVD water filling (SVD WF) and uniform power allocation (UPA) precoding strategies. The optimally configured cascaded-RIS with UPA is shown to achieve performance comparable to that with the SVD WF approach, suggesting that cascaded-RISs can also aid in reducing transmitter complexity. Furthermore, the approximate ergodic capacity for UPA is derived, along with its high SNR approximation which provides multiple useful insights into the dimension and deployment of cascaded RISs. The analytical results establish a clear tradeoff among transmit power, RIS size, and achievable capacity, providing insights for practical deployment in high SNR cascaded RIS MIMO systems.

</details>


### [70] [Towards Optimal Constellation Design for Digital Over-the-Air Computation](https://arxiv.org/abs/2511.06372)
*Saeed Razavikia,Deniz Gündüz,Carlo Fischione*

Main category: cs.IT

TL;DR: 提出了一种用于AWGN信道上计算的数字调制框架，通过优化星座设计来最小化均方误差，并在高信噪比下推导出闭式解。


<details>
  <summary>Details</summary>
Motivation: 传统空中计算依赖于模拟幅度调制，但受限于噪声敏感性和硬件约束，因此需要采用数字调制方案来提高性能。

Method: 将设计表述为加性映射问题，确定在发射功率约束下最小化均方误差的最优星座，建立非线性方程组并证明解的唯一性条件。

Result: 在高信噪比下，使用广义Lambert函数推导出最优调制参数的闭式表达式，为系统行为提供分析见解。

Conclusion: 该框架可扩展到高维网格、非高斯噪声模型，以及通过混合数字-模拟调制实现实值域计算。

Abstract: Over-the-air computation (OAC) has emerged as a key technique for efficient function computation over multiple-access channels (MACs) by exploiting the waveform superposition property of the wireless domain. While conventional OAC methods rely on analog amplitude modulation, their performance is often limited by noise sensitivity and hardware constraints, motivating the use of digital modulation schemes. This paper proposes a novel digital modulation framework optimized for computation over additive white Gaussian noise (AWGN) channels. The design is formulated as an additive mapping problem to determine the optimal constellation that minimizes the mean-squared error (MSE) under a transmit power constraint. We express the optimal constellation design as a system of nonlinear equations and establish the conditions guaranteeing the uniqueness of its solution. In the high signal-to-noise-ratio (SNR) regime, we derive closed-form expressions for the optimal modulation parameters using the generalized Lambert function, providing analytical insight into the system's behavior. Furthermore, we discuss extensions of the framework to higher-dimensional grids corresponding to multiple channel uses, to non-Gaussian noise models, and to computation over real-valued domains via hybrid digital-analog modulation.

</details>


### [71] [Differential Space-Time Block Coding for Phase-Unsynchronized Cell-Free MIMO Downlink](https://arxiv.org/abs/2511.06510)
*Marx M. M. Freitas,Giovanni Interdonato,Stefano Buzzi*

Main category: cs.IT

TL;DR: 提出了一种用于无蜂窝大规模MIMO系统的差分空时分组编码方法，无需AP相位同步即可实现相干传输性能


<details>
  <summary>Details</summary>
Motivation: 解决无蜂窝大规模MIMO系统中AP相位同步的技术挑战，因为地理分布的AP需要精确同步才能实现联合相干传输

Method: 采用差分空时分组编码方法，无需信道状态信息和AP间的任何相位同步形式，推导了信干噪比的闭式表达式

Result: 数值仿真表明相位失配会显著损害系统性能，而提出的DSTBC方案能有效缓解这些影响，性能接近完全同步系统

Conclusion: DSTBC方法成功解决了无蜂窝大规模MIMO系统的相位同步问题，实现了与完全同步系统相当的性能

Abstract: In the downlink of a cell-free massive multiple-input multiple-output (CF-mMIMO) system, spectral efficiency gains critically rely on joint coherent transmission, as all access points (APs) must align their transmitted signals in phase at the user equipment (UE). Achieving such phase alignment is technically challenging, as it requires tight synchronization among geographically distributed APs. In this paper, we address this issue by introducing a differential space-time block coding (DSTBC) approach that bypasses the need for AP phase synchronization. We first provide analytic bounds to the achievable spectral efficiency of CF-mMIMO with phase-unsynchronized APs. Then, we propose a DSTBC-based transmission scheme specifically tailored to CF-mMIMO, which operates without channel state information and does not require any form of phase synchronization among the APs. We derive a closed-form expression for the resulting signal-to-interference-plus-noise ratio (SINR), enabling quantitative comparisons among different DSTBC schemes. Numerical simulations confirm that phase misalignments can significantly impair system performance. In contrast, the proposed DSTBC scheme successfully mitigates these effects, achieving performance comparable to that of fully synchronized systems.

</details>


### [72] [Events Meet Phase-Shifting Digital Holography: Practical Acquisition, Theory, and Algorithms](https://arxiv.org/abs/2511.06591)
*Ittetsu Uchiyama,Chihiro Tsutake,Keita Takahashi,Toshiaki Fujii*

Main category: cs.IT

TL;DR: 提出了一种基于混合事件视觉传感器的相位移动数字全息术新方法，通过在单次曝光期间进行相位移动，结合模糊全息图和事件数据重建完整复波前。


<details>
  <summary>Details</summary>
Motivation: 传统相位移动数字全息术需要多次曝光，效率较低。本文旨在通过混合事件视觉传感器在单次曝光中实现相位移动，提高采集效率。

Method: 使用混合事件视觉传感器记录相位移动导致的模糊全息图以及对应的模糊变化事件，提出解析和优化两种方法来重建完整复波前。

Result: 实验结果表明，该方法在重建质量上与传统相位移动数字全息术相当，同时显著提高了采集效率。

Conclusion: 该方法成功实现了在单次曝光中完成相位移动数字全息术，为高效全息成像提供了新途径。

Abstract: We introduce a novel phase-shifting digital holography (PSDH) method leveraging a hybrid event-based vision sensor (EVS). The key idea of our method is the phase shift during a single exposure. The hybrid EVS records a hologram blurred by the phase shift, together with the events corresponding to blur variations. We present analytical and optimization-based methods that theoretically support the reconstruction of full-complex wavefronts from the blurred hologram and events. The experimental results demonstrate that our method achieves a reconstruction quality comparable to that of a conventional PSDH method while enhancing the acquisition efficiency.

</details>


### [73] [The Inaccessible Game](https://arxiv.org/abs/2511.06795)
*Neil D. Lawrence*

Main category: cs.IT

TL;DR: 本文提出了一个基于四个公理构建的信息论动态系统——不可访问游戏。前三个公理定义了信息损失，第四个新颖的信息隔离公理使系统独立于观察者且可交换，导致总边际熵守恒。系统在最大熵产生条件下展现出类似GENERIC的结构。


<details>
  <summary>Details</summary>
Motivation: 构建一个信息论动态系统，通过引入信息隔离公理来创建观察者独立且可交换的系统，研究在最大熵产生条件下的动力学行为。

Method: 基于四个公理构建信息论动态系统：前三个公理定义信息损失，第四个信息隔离公理确保系统独立于观察。分析系统在最大熵产生条件下的动力学结构。

Result: 在信息隔离公理下，总边际熵守恒（∑hi = C）。系统在最大熵产生条件下展现出类似GENERIC的结构，结合了可逆和不可逆分量。

Conclusion: 不可访问游戏提供了一个信息论框架，其中信息隔离导致熵守恒，并在最大熵产生条件下产生具有可逆和不可逆分量的GENERIC-like动力学结构。

Abstract: In this paper we introduce the inaccessible game, an information-theoretic dynamical system constructed from four axioms. The first three axioms are known and define \emph{information loss} in the system. The fourth is a novel \emph{information isolation} axiom that assumes our system is isolated from observation, making it observer-independent and exchangeable. Under this isolation axiom, total marginal entropy is conserved: $\sum_i h_i = C$. We consider maximum entropy production in the game and show that the dynamics exhibit a GENERIC-like structure combining reversible and irreversible components.

</details>


### [74] [Code Equivalence, Point Set Equivalence, and Polynomial Isomorphism](https://arxiv.org/abs/2511.06843)
*Martin Kreuzer*

Main category: cs.IT

TL;DR: 本文证明了线性码等价问题与点集等价问题等价，并通过代数方法将其转化为多项式同构问题，在特定条件下可在多项式时间内解决。


<details>
  <summary>Details</summary>
Motivation: 研究线性码等价问题的计算复杂性，寻找更高效的解决方法。

Method: 将线性码等价问题转化为点集等价问题，然后通过齐次坐标环和典范理想构造Artinian Gorenstein代数，利用Macaulay逆系统进一步转化为多项式同构问题。

Result: 证明了线性码等价问题与点集等价问题等价，并在温和假设下可多项式时间求解。对于不可分解的iso-dual码，可进一步简化为3次多项式同构问题。

Conclusion: 通过代数几何方法为线性码等价问题提供了新的解决框架，显著降低了计算复杂度。

Abstract: The linear code equivalence (LCE) problem is shown to be equivalent to the point set equivalence (PSE) problem, i.e., the problem to check whether two sets of points in a projective space over a finite field differ by a linear change of coordinates. For such a point set $\mathbb{X}$, let $R$ be its homogeneous coordinate ring and $\mathfrak{J}_{\mathbb{X}}$ its canonical ideal. Then the LCE problem is shown to be equivalent to an algebra isomorphism problem for the doubling $R/\mathfrak{J}_{\mathbb{X}}$. As this doubling is an Artinian Gorenstein algebra, we can use its Macaulay inverse system to reduce the LCE problem to a Polynomial Isomorphism (PI) problem for homogeneous polynomials. The last step is polynomial time under some mild assumptions about the codes. Moreover, for indecomposable iso-dual codes we can reduce the LCE search problem to the PI search problem of degree 3 by noting that the corresponding point sets are self-associated and arithmetically Gorenstein, so that we can use the isomorphism problem for the Artinian reductions of the coordinate rings and form their Macaulay inverse systems.

</details>


### [75] [Rate-Optimal Streaming Codes Under an Extended Delay Profile for Three-Node Relay Networks With Burst Erasures](https://arxiv.org/abs/2511.06882)
*Zhipeng Li,Wenjie Ma*

Main category: cs.IT

TL;DR: 本文提出了一种扩展延迟配置方法，用于三节点中继网络中的流码设计，在更宽松的条件下实现了最优速率，覆盖了先前要求的整除条件。


<details>
  <summary>Details</summary>
Motivation: 研究三节点中继网络中在突发包删除和延迟约束下的流码设计，旨在在更宽松的条件下实现最优传输速率。

Method: 采用扩展延迟配置方法，通过构造满足延迟约束的流码，在更宽松的约束条件下实现最优速率。

Result: 提出的方法在条件(T-u-v)/(2u-v) ≤ ⌊(T-u-v)/u⌋下实现了最优速率，严格覆盖了先前要求u整除(T-u-v)的限制。

Conclusion: 扩展延迟配置方法为三节点中继网络中的流码设计提供了更优的构造方案，在更宽松的条件下实现了最优传输速率。

Abstract: This paper investigates streaming codes for three-node relay networks under burst packet erasures with a delay constraint $T$. In any sliding window of $T+1$ consecutive packets, the source-to-relay and relay-to-destination channels may introduce burst erasures of lengths at most $b_1$ and $b_2$, respectively. Let $u = \max\{b_1, b_2\}$ and $v = \min\{b_1, b_2\}$. Singhvi et al. proposed a construction achieving the optimal rate when $u\mid (T-u-v)$. In this paper, we present an extended delay profile method that attains the optimal rate under a relaxed constraint $\frac{T - u - v}{2u - v} \leq \left\lfloor \frac{T - u - v}{u} \right\rfloor$ and it strictly cover restriction $u\mid (T-u-v)$. %Furthermore, we demonstrate that the optimal rate for streaming codes is not achievable when $0< T-u-v<v$ under the convolutional code framework.

</details>


### [76] [Experimental Validation of Reflective Near-Field Beamfocusing using a b-bit RIS](https://arxiv.org/abs/2511.06994)
*Emil Björnson,Murat Babek Salman*

Main category: cs.IT

TL;DR: 本文首次实验验证了使用可重构智能表面(RIS)的反射近场波束聚焦技术，通过28GHz频段的1024单元1比特RIS在室内办公环境中验证了理论模型。


<details>
  <summary>Details</summary>
Motivation: 虽然波束聚焦已被理论证明是大孔径RIS的关键特性，但其实用实现尚未被探索，需要实验验证近场波束聚焦的实际可行性。

Method: 推导了b比特RIS在近场视距场景下的阵列增益新解析表达式，并通过28GHz频段1024单元1比特RIS在室内办公环境进行测量验证。

Result: 实验证实近场波束聚焦可以动态实现，并且与提出的解析模型准确匹配，即使在存在硬件缺陷和多径传播的情况下也表现出鲁棒性。

Conclusion: 近场波束聚焦是RIS辅助无线通信中一个鲁棒且实际可行的特性，为未来无线系统设计提供了重要参考。

Abstract: This paper presents the first experimental validation of reflective near-field beamfocusing using a reconfigurable intelligent surface (RIS). While beamfocusing has been theoretically established as a key feature of large-aperture RISs, its practical realization has remained unexplored. We derive new analytical expressions for the array gain achieved with a $b$-bit RIS in near-field line-of-sight scenarios, characterizing both the finite depth and angular width of the focal region. The theoretical results are validated through a series of measurements in an indoor office environment at 28 GHz using a one-bit 1024-element RIS. The experiments confirm that near-field beamfocusing can be dynamically achieved and accurately predicted by the proposed analytical model, despite the presence of hardware imperfections and multipath propagation. These findings demonstrate that near-field beamfocusing is a robust and practically viable feature of RIS-assisted wireless communications.

</details>


### [77] [A Copula-based Semantics-Structure Minimization Framework for QoS Guaranteed Wireless Communications](https://arxiv.org/abs/2511.07145)
*Xinke Jian,Zhiyuan Ren,Wenchi Cheng*

Main category: cs.IT

TL;DR: 为语义通信建立统一的理论基础，提出四个公理并证明pairwise rank-Copulas是最小结构语义的最小充分表示，构建语义失真度量，确立理论边界和QoS保证。


<details>
  <summary>Details</summary>
Motivation: 当前基于经验的语义通信研究缺乏统一理论基础，无法提供可量化的服务质量保证，特别是在紧急场景中传输最小结构语义时，这限制了其发展成为可预测的工程科学。

Method: 提出四个公理，证明pairwise rank-Copulas是最小结构语义的最小充分表示，构建基于Jensen-Shannon散度的语义失真度量，建立样本复杂度边界、率失真边界、端到端SLA定理和语义源信道分离定理。

Result: 通过解耦实验验证了框架有效性，证明核心度量严格遵循基础公理，而标准感知度量无法做到这一点。

Conclusion: 建立了一个完整的语义通信理论框架，提供了可证明的服务质量保证，为语义通信发展成为可预测的工程科学奠定了基础。

Abstract: Current empirically driven research on semantic communication lacks a unified theoretical foundation, preventing quantifiable Quality of Service guarantees, particularly for transmitting minimal structural semantics in emergency scenarios. This deficiency limits its evolution into a predictable engineering science. To address this, we establish a complete theoretical axiomatic basis for this problem. We propose four axioms and rigorously prove that the family of pairwise rank-Copulas is the minimal sufficient representation for minimal structural semantics. Based on this, we construct a semantic distortion metric, centered on the Jensen-Shannon divergence. We then establish the core theoretical boundaries of the framework: sample complexity bounds; rate-distortion bounds; an end-to-end Service Level Agreements theorem; and a semantic source-channel separation theorem, which provides a provable Quality of Service guarantee. Finally, we validate our framework through decoupled experiments, empirically demonstrating that our core metric strictly adheres to our foundational axioms while standard perceptual metrics fail to do so.

</details>


### [78] [Frequency Diverse (FD)-RIS-Enhanced Covert Communications: Defense Against Wiretapping via Joint Distance-Angle Beamforming](https://arxiv.org/abs/2511.07309)
*Han Xiao,Xiaoyan Hu,Wenjie Wang,Kai-Kit Wong,Kun Yang,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一种基于频率分集可重构智能表面(FD-RIS)的隐蔽通信方案，通过联合距离-角度波束成形能力解决传统RIS在隐蔽通信中的安全盲区问题。


<details>
  <summary>Details</summary>
Motivation: 传统RIS辅助的隐蔽通信系统存在安全盲区挑战，而FD-RIS的联合距离-角度波束成形能力具有解决这些限制的潜力。

Method: 首先开发了FD-RIS的信号处理模型，利用时延技术控制谐波信号；然后构建多看守场景下的FD-RIS辅助隐蔽通信系统，推导隐蔽约束的近似闭式表达式；最后提出迭代算法联合优化时延和调制频率。

Result: 仿真结果表明，FD-RIS能显著提升隐蔽性能，特别是在角度重叠场景中，传统RIS性能严重下降的情况下。

Conclusion: FD-RIS在挑战性空间环境下能有效增强隐蔽通信的鲁棒性。

Abstract: In response to the security blind zone challenges faced by traditional reconfigurable intelligent surface (RIS)-aided covert communication (CC) systems, the joint distance-angle beamforming capability of frequency diverse RIS (FD-RIS) shows significant potential for addressing these limitations. Therefore, this paper initially incorporates the FD-RIS into the CC systems and proposes the corresponding CC transmission scheme. Specifically, we first develop the signal processing model of the FD-RIS, which considers effective control of harmonic signals by leveraging the time-delay techniques. The joint distance-angle beamforming capability is then validated through its normalized beampattern. Based on this model, we then construct an FD-RIS-assisted CC system under a multi-warden scenario and derive an approximate closed-form expression for the covert constraints by considering the worst-case eavesdropping conditions and utilizing the logarithmic moment-generating function. An optimization problem is formulated which aims at maximizing the covert user's achievable rate under covert constrains by jointly designing the time delays and modulation frequencies. To tackle this non-convex problem, an iterative algorithm with assured convergence is proposed to effectively solve the time-delay and modulation frequency variables. To evaluate the performance of the proposed scheme, we consider three communication scenarios with varying spatial correlations between the covert user and wardens. Simulation results demonstrate that FD-RIS can significantly improve covert performance, particularly in angular-overlap scenarios where traditional RIS experiences severe degradation. These findings further highlight the effectiveness of FD-RIS in enhancing CC robustness under challenging spatial environments.

</details>
