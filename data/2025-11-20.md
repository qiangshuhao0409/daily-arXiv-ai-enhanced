<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [RAID: In-Network RA Signaling Storm Detection for 5G Open RAN](https://arxiv.org/abs/2511.14921)
*Mohamed Rouili,Yang Xiao,Sihang Liu,Raouf Boutaba*

Main category: cs.NI

TL;DR: RAID是一个基于P4可编程交换机的5G O-RAN信令风暴检测与缓解系统，通过在数据平面嵌入轻量级随机森林分类器，实现微秒级延迟的实时恶意RA请求过滤。


<details>
  <summary>Details</summary>
Motivation: 5G O-RAN的分解和虚拟化引入了控制平面新漏洞，恶意UE发起的随机接入信令风暴会快速饱和中央单元处理管道，导致大规模连接失败，而现有基于n-RT RIC的检测方法无法及时响应。

Method: 在可编程Tofino交换机中嵌入轻量级随机森林分类器，利用P4可编程交换ASIC实现线速流分类和确定性微秒级推理延迟，直接在数据平面进行ML检测。

Result: RAID达到94%以上的检测准确率，每流推理延迟固定为3.4微秒，能够满足严格的O-RAN控制平面时限要求，并在多种流量负载下保持性能。

Conclusion: RAID是一个快速、可扩展的解决方案，能够有效检测和缓解5G O-RAN中的信令风暴攻击。

Abstract: The disaggregation and virtualization of 5G Open RAN (O-RAN) introduces new vulnerabilities in the control plane that can greatly impact the quality of service (QoS) of latency-sensitive 5G applications and services. One critical issue is Random Access (RA) signaling storms where, a burst of illegitimate or misbehaving user equipments (UEs) send Radio Resource Control (RRC) connection requests that rapidly saturate a Central Unit's (CU) processing pipeline. Such storms trigger widespread connection failures within the short contention resolution window defined by 3GPP. Existing detection and mitigation approaches based on near-real-time RAN Intelligent Controller (n-RT RIC) applications cannot guarantee a timely reaction to such attacks as RIC control loops incur tens to hundreds of milliseconds of latency due to the non-deterministic nature of their general purpose processor (GPP) based architectures. This paper presents RAID, an in-network RA signaling storm detection and mitigation system that leverages P4-programmable switch ASICs to enable real-time protection from malicious attacks. RAID embeds a lightweight Random Forest (RF) classifier into a programmable Tofino switch, enabling line-rate flow classification with deterministic microsecond-scale inference delay. By performing ML-based detection directly in the data plane, RAID catches and filters malicious RA requests before they reach and overwhelm the RRC. RAID achieves above 94% detection accuracy with a fixed per-flow inference delay on the order of 3.4 microseconds, effectively meeting strict O-RAN control-plane deadlines. These improvements are sustained across multiple traffic loads, making RAID a fast and scalable solution for the detection and mitigation of signaling storms in 5G O-RAN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs](https://arxiv.org/abs/2511.14777)
*Mahdi Samiei,Mahdi Mansouri,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了有限状态机（FSM）执行作为评估LLMs程序推理能力的基准框架，发现随着任务复杂度增加，模型性能系统性下降，特别是在高分支复杂度和多步推理场景下表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在形式化推理任务上表现优异，但其执行多步骤、基于规则的程序推理能力尚不明确，缺乏可控、可解释的基准来隔离和测量这种能力衰退。

Method: 设计FSM执行任务，给模型明确的FSM定义，要求其根据输入动作逐步执行，在多轮中保持状态一致性。任务无需世界知识，只需忠实应用确定性转换规则。

Result: 实验结果显示：1）任务长度或分支复杂度增加时性能系统性下降；2）规则检索涉及高分支因子时表现显著更差；3）大模型局部准确性提高但在多步推理中仍脆弱，除非明确提示外部化中间步骤。

Conclusion: FSM评估为诊断LLMs程序推理失败模式提供了透明、复杂度可控的探针，有助于建立理解和改进LLMs算法可靠性的严谨实验基础。

Abstract: Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.

</details>


### [3] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT是一个强化学习环境，用于自动化数学理论发现，通过进化算法和LLM合成数学对象的有趣性度量，在初等数论和有限域领域优于硬编码基线。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论自动发现这一重大挑战，特别是自动评估数学对象的趣味性。

Method: 引入FERMAT强化学习环境建模概念发现和定理证明；使用基于LLM的进化算法合成有趣性度量，包含函数抽象机制。

Result: 在初等数论和有限域领域，该方法相比硬编码基线取得了显著改进。

Conclusion: FERMAT环境为理论发现开辟了新的RL问题空间，基于LLM的进化算法能有效发现数学对象的趣味性度量。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [4] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录重放交互、查询信念和注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛现象，提供可重现的方法来分析智能体信念动态。

Method: 开发Ask WhAI框架，包含记录重放交互、带外查询信念和反事实证据注入功能，应用于具有多智能体共享内存和预言智能体的医疗案例模拟器。

Result: 模拟显示智能体信念常反映现实世界学科立场，包括过度依赖经典研究和抵制反证据，这些信念可以以人类专家无法实现的方式进行追踪和询问。

Conclusion: Ask WhAI通过使信念动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [5] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 提出了一种完全自动化的LLM辅助工作流，使用GPT-4o处理文本位置信息，并通过交叉验证三个地理信息库来分配几何形状和可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害数据库中的位置数据通常以非结构化文本形式报告，存在粒度不一致和拼写差异，难以与空间数据集集成。

Method: 使用GPT-4o清理文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个独立的地理信息库来分配几何形状，并根据来源一致性分配可靠性评分。

Result: 应用于2000-2024年的EM-DAT数据集，成功地理编码了14,215个事件，覆盖17,948个独特位置。

Conclusion: 该方法无需人工干预，覆盖所有灾害类型，支持跨源验证，并展示了LLM从非结构化文本中提取和结构化地理信息的潜力。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [6] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel通过创建AI学术身份Rachel So并发表10+篇AI生成论文，研究学术生态系统对AI作者身份的反应，发现AI论文被引用并获得同行评审邀请。


<details>
  <summary>Details</summary>
Motivation: 调查学术生态系统如何应对AI作者身份，为关于超级智能AI系统参与学术交流的未来讨论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10+篇AI生成的研究论文，并跟踪其被引用和同行评审邀请情况。

Result: AI生成的论文被其他研究者引用，Rachel So还收到了同行评审邀请，表明学术系统在某种程度上接受了AI作者身份。

Conclusion: AI作者身份对出版商、研究人员和整个科学系统具有重要影响，需要就超级智能AI系统参与学术交流的未来进行必要讨论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [7] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 提出了一种概率方法来量化AI系统训练测试数据的代表性，通过比较场景套件与目标操作域的特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，产生区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，关键在于训练测试数据集的数据相关安全属性，如代表性。本文专注于代表性——场景数据反映系统设计安全运行的操作条件（ODD）或预期遇到的条件（TOD）的程度。

Method: 采用概率方法比较场景套件特征统计分布与TOD特征分布，使用不精确贝叶斯方法处理有限数据和不确定先验，产生区间值、不确定性感知的代表性估计。

Result: 通过数值示例比较场景套件与推断TOD在操作类别（天气、道路类型、时间等）下的分布，考虑依赖关系和先验不确定性，估计局部和全局代表性区间。

Conclusion: 该方法能够量化数据代表性并提供不确定性感知的区间估计，有助于评估AI系统训练测试数据的质量，特别是在TOD真实分布未知且只能从有限数据推断的情况下。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [8] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出了一种结合Sharpness-Aware Minimization (SAM)的改进Soft Actor Critic算法，用于O-RAN架构中的分布式多智能体强化学习资源管理，通过基于TD误差方差的自适应SAM机制提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 下一代网络采用O-RAN架构实现动态资源管理，但深度强化学习模型在动态环境中存在鲁棒性和泛化性不足的问题。

Method: 在分布式多智能体强化学习框架中，将SAM与SAC算法结合，引入基于TD误差方差的自适应选择性SAM机制，仅对面临高环境复杂度的智能体进行正则化，并采用动态ρ调度方案优化探索-利用权衡。

Result: 实验结果显示该方法显著优于传统DRL方法，资源分配效率提升高达22%，并在多样化O-RAN切片中确保优越的QoS满意度。

Conclusion: 所提出的自适应选择性SAM机制有效提升了DRL模型在动态网络环境中的鲁棒性和泛化能力，同时保持了学习效率。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [9] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出MAQ框架，通过将人类演示提炼为宏观动作来增强强化学习代理的人类相似性，在D4RL Adroit基准测试中显著提高了轨迹相似度得分。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习代理虽然在许多领域表现出色，但与人类行为相比往往显得不自然，这引发了可解释性和可信赖性的担忧。本文旨在设计更像人类的强化学习代理。

Method: 将人类相似性建模为轨迹优化问题，采用后退时域控制作为可实现的实现方法。提出宏观动作量化(MAQ)框架，使用向量量化VAE从人类演示中提取宏观动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似性，增加了轨迹相似度得分，在人类评估研究中获得了最高的人类相似性排名。

Conclusion: MAQ可以轻松集成到各种现成的强化学习算法中，为学习人类相似强化学习代理开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [10] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，通过模块化设计和智能体专业化，在基因组问答任务中匹配或超越了GeneGPT的性能，同时降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的应用潜力。

Method: 采用模块化多智能体框架，引入工具路由、查询生成和响应验证的专业智能体，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或超越GeneGPT，在Gene-Turing和GeneHop上分别获得0.849和0.830的平均分数，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，能够在不牺牲性能的前提下提高效率和降低成本。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [11] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM解决RAC问题，通过提取动作和问题、逐步执行动作推导最终状态，然后评估查询来得出答案。


<details>
  <summary>Details</summary>
Motivation: 为了解决RAC（动作与变化推理）问题，提出一个结合神经和符号方法的框架，利用LLM的能力来处理复杂的推理任务。

Method: 提取RAC问题中的基本元素（动作和问题），逐步执行每个动作来推导最终状态，然后在该状态下评估查询以获得答案。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上都取得了强劲性能。

Conclusion: ProRAC框架能够有效解决RAC问题，展示了在多样化设置下的鲁棒性和通用性。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [12] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专门智能体（科学家、提取器、测试器）的协作，结合外部知识检索和丰富的定性反馈机制，实现知识引导的自动特征提取。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动特征提取方法存在架构单一、反馈机制简单、缺乏外部知识整合等问题，限制了特征提取的质量和可解释性。

Method: 采用去中心化的三智能体系统（科学家、提取器、测试器），结合检索增强生成（RAG）引入外部知识，使用"泛滥-修剪"策略平衡特征探索与利用，并提供丰富的定性反馈机制。

Result: 在19个分类和9个回归数据集上显著优于现有最先进方法，并在心肌数据集上发现了新的潜在生物标志物，展示了其科学发现能力。

Conclusion: Rogue One框架不仅提升了特征提取的性能，还增强了特征的可解释性和语义意义，为科学发现提供了有力工具。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [13] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是第一个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包括风险分类分级、细粒度输出分析和人类安全对齐验证。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提高答案质量，但这也引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由合理化。现有的安全评估主要关注输出层面判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，考虑受影响群体和严重程度；2) 细粒度输出分析：通过微思维分块机制将长推理轨迹分割成语义连贯单元，在十个安全维度上进行细粒度评估；3) 人类安全对齐：用专门设计的人类标注验证基于LLM的评估。

Result: 对19个大型推理模型的评估表明，SafeRBench能够进行详细的多维度安全评估，从多个角度提供风险和保护机制的见解。

Conclusion: SafeRBench为大型推理模型提供了首个端到端的安全评估框架，能够全面捕捉推理过程中的动态安全风险，填补了现有安全评估的空白。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [14] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: HISE-KT是一个结合异质信息网络和大型语言模型的知识追踪框架，通过智能评分筛选元路径实例，基于相似学生检索机制，生成准确预测和可解释分析报告。


<details>
  <summary>Details</summary>
Motivation: 现有基于异质信息网络的方法因手动或随机选择元路径而引入噪声，缺乏元路径实例质量评估；基于大语言模型的方法忽视学生间的丰富信息，两种方法都难以提供准确且基于证据的解释。

Method: 构建多关系异质信息网络捕获结构关系，使用LLM智能评分和过滤元路径实例，设计基于元路径的相似学生检索机制，通过结构化提示整合目标学生历史和相似轨迹。

Result: 在四个公共数据集上的实验表明，HISE-KT在预测性能和可解释性方面均优于现有知识追踪基线方法。

Conclusion: HISE-KT成功整合了异质信息网络和大型语言模型的优势，实现了更准确的知识追踪预测和基于证据的可解释分析。

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [15] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中是否包含版权内容的新框架，通过将LLM的过度自信转化为优势，实现无需经验阈值的高精度版权检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练时可能使用了受版权保护的内容，现有成员推理攻击方法存在局限性，如LLM的过度自信、缺乏真实训练数据访问、依赖经验阈值等问题。

Method: 采用双重策略：(1)将文件分割成小片段以减少对大规模训练数据的依赖；(2)基于不确定性的无监督聚类来消除经验调优阈值需求，利用不确定性模式区分训练数据和非训练数据。

Result: 在LLaMA 7b上平均平衡准确率达90.1%，在LLaMA2 7b上达91.6%，相比现有最优方法有90%以上的相对提升，最高达93.8%平衡准确率，并在GPT-J 6B上保持高性能。

Conclusion: 这是首个将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具，成功将LLM过度自信从限制转化为优势。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [16] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID框架通过结合数学优化和大型语言模型，利用对偶价格和偏差惩罚实现迭代协作，在保持模块化和数据隐私的同时提升决策质量，在投资组合优化中表现出优于纯优化方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时利用数学优化的精确性和LLMs的上下文理解能力，需要一种能协同两者优势的框架来提升智能决策质量。

Method: 提出SOLID框架，通过优化代理和LLM代理的迭代协作，使用对偶价格和偏差惩罚机制进行交互，在凸性假设下保持理论收敛性。

Result: 在股票投资组合案例中，SOLID在各种场景下都表现出收敛性，年化收益率相比纯优化基线方法有所提升。

Conclusion: SOLID为跨领域的自动化和智能决策提供了一个有前景的框架，验证了优化与LLM协同的有效性。

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [17] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 论文指出，随着AI研究转向复杂问题解决，推理AI的性能不再受限于训练数据量，而是随着训练和推理的指数级计算投入而持续扩展。效率提升已接近物理极限，需要将显性限制嵌入推理AI的优化和治理中。


<details>
  <summary>Details</summary>
Motivation: AI研究正从模式识别转向多步推理，而计算效率提升已接近物理极限。推理AI缺乏需求饱和点，性能持续随指数级计算投入扩展，仅靠效率无法实现可持续性。

Method: 通过分析计算能效趋势和推理AI的扩展特性，论证效率提升的局限性，并提出需要将显性限制嵌入系统优化和治理的研究和政策方向。

Result: 发现推理AI的性能扩展不受数据量限制，而是持续依赖指数级计算投入，效率提升无法单独解决可持续性问题。

Conclusion: 仅靠效率提升无法实现可持续的推理AI，需要在优化和治理中嵌入显性限制，这是未来研究和政策的关键方向。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [18] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 论文分析了AI研究中两种对立的情报观：情报现实主义认为情报是单一通用能力，情报多元主义认为情报是多样化的情境依赖能力。这两种观点在方法论、解释和风险评估方面产生根本不同的研究路径。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中隐含的两种基本情报观念如何影响研究实践和争议，帮助澄清该领域的根本分歧。

Method: 通过分析当前AI研究中的辩论，展示两种情报观念如何影响模型选择、基准设计、实验验证、现象解释和风险评估。

Result: 发现情报现实主义者和多元主义者在方法论、解释框架和风险评估上存在系统性差异，这些差异源于对情报本质的根本不同理解。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术对话。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [19] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: Octopus是一个新的多模态推理范式，通过协调六种核心能力实现自主推理路径探索和动态能力选择，在Octopus-Bench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型存在架构限制，缺乏自主探索多样化推理路径的能力，无法适应动态变化的任务需求，而人类具备互补的思维能力。

Method: 提出Octopus框架，定义六种核心多模态推理能力，构建Octopus-Bench评估基准，实现自主推理探索和动态能力选择。

Result: 实验结果显示Octopus在Octopus-Bench大多数任务中取得最佳性能，证明了能力协调在多模态推理中的关键作用。

Conclusion: 能力协调是智能多模态推理的关键，Octopus通过六种能力协同工作展示了优于现有方法的性能。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [20] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在部分可观测性、信用分配、表示学习、巨大动作空间等多个经典挑战上的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在无关任务间切换策略的能力，而非测试智能体在多个相互关联挑战中进行深度推理的能力。需要创建能真正测试综合推理能力的环境。

Method: 基于《文明V》游戏构建综合性挑战环境，让多个经典RL挑战同时出现，要求智能体具备跨多个交互变量的集成、长视野理解能力。

Result: 提出了Terra Nova环境，区别于仅聚合无关任务的并行流多任务基准，强调挑战的相互关联性。

Conclusion: Terra Nova为RL研究提供了更真实的综合性测试平台，能够评估智能体在复杂交互环境中的深度推理能力。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [21] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 论文提出IPR（交互式物理推理器），通过世界模型推演来增强VLM的策略，并使用PhysCode将语义意图与物理动力学对齐，在1000+游戏中训练后，在三个推理级别上表现稳健，总体与GPT-5相当，在好奇心级别上超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能够像人类一样通过交互获得物理和因果推理能力，并在更多经验中持续改进。

Method: 提出IPR框架，使用世界模型推演来评分和强化VLM的策略；引入PhysCode，一种以物理为中心的动作编码，将语义意图与动力学对齐，为预测和推理提供共享动作空间。

Result: 在1000+游戏上预训练后，IPR在三个推理级别（生存、好奇心、实用性）表现稳健，总体与GPT-5匹配，在好奇心级别超越GPT-5；性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见游戏。

Conclusion: 以物理为中心的交互是持续改进物理推理能力的可行路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [22] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，通过DeFi意图分类法和多智能体LLM系统来推断用户交易意图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易理解困难，现有方法缺乏深度语义洞察，需要解决复杂智能合约交互、多因素分析和不透明日志的问题。

Method: 构建DeFi意图分类法，采用多智能体LLM系统，包括元级规划器动态协调领域专家、问题求解器处理多模态数据、认知评估器减轻幻觉。

Result: TIM框架在实验中显著优于机器学习模型、单LLM和单智能体基线方法。

Conclusion: 该工作为DeFi中用户动机提供了更可靠的理解，为复杂区块链活动提供上下文感知解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [23] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents作为作者和审稿人参与的首个科学会议Agents4Science，探讨了AI在科学研究中的能力和人机协作模式。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在科学研究中作为科学家和审稿人的能力，以及人机协作在科学领域的潜力。

Method: 组织Agents4Science会议，让AI代理担任主要作者和审稿人，人类作为合著者和共同审稿人参与。

Result: 获得了关于AI代理在科学研究和评审中能力的重要见解，以及人机协作模式的经验。

Conclusion: 该会议为理解AI在科学中的角色和人机协作提供了有价值的经验，对未来的科学合作模式具有启示意义。

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [24] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文研究了AI研究代理中构思多样性对性能的影响，发现在MLE-bench基准测试中，构思多样性更高的代理表现更好，并通过控制实验验证了这一因果关系。


<details>
  <summary>Details</summary>
Motivation: AI研究代理有望加速科学进步，但该领域仍处于早期阶段，成功或失败的关键因素尚未完全理解。本文旨在探究构思多样性在代理性能中的作用。

Method: 首先分析MLE-bench基准测试中不同模型和代理框架的轨迹，然后进行控制实验调节构思多样性程度，最后使用除标准奖牌评分外的其他评估指标验证结果。

Result: 分析显示不同模型和代理框架产生不同程度的构思多样性，性能更高的代理往往具有更高的构思多样性。控制实验证实更高的构思多样性确实导致更强的性能。

Conclusion: 构思多样性是AI研究代理性能的重要驱动因素，这一发现在多种评估指标下均成立，为优化AI研究代理提供了重要指导。

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [25] [Channel Coding for Gaussian Channels with Multifaceted Power Constraints](https://arxiv.org/abs/2511.14849)
*Adeel Mahmood,Aaron B. Wagner*

Main category: cs.IT

TL;DR: 基于正态近似的渐近结果，研究高阶编码性能如何依赖于平均功率Γ和输入功率的精细统计特性。提出了多面功率模型，约束归一化平均功率任意函数的期望，推广了现有模型。在函数增长和连续性假设下，给出了高斯信道最小平均错误概率关于一阶和二阶编码率的精确表征。


<details>
  <summary>Details</summary>
Motivation: 受基于正态近似的精化渐近结果启发，研究高阶编码性能对平均功率Γ和输入功率更精细统计特性的依赖性。

Method: 引入多面功率模型，约束归一化平均功率任意函数的期望；将码设计问题转化为在概率分布紧集上的最小化问题，刻画该集合的极值点并应用Bauer最大化原理。

Result: 在函数增长和连续性假设下，给出了高斯信道最小平均错误概率关于一阶和二阶编码率的精确表征。

Conclusion: 提出的多面功率模型推广了现有功率约束模型，为高斯信道的高阶编码性能分析提供了精确的理论框架。

Abstract: Motivated by refined asymptotic results based on the normal approximation, we study how higher-order coding performance depends on the mean power $Γ$ as well as on finer statistics of the input power. We introduce a multifaceted power model in which the expectation of an arbitrary number of arbitrary functions of the normalized average power is constrained. The framework generalizes existing models, recovering the standard maximal and expected power constraints and the recent mean and variance constraint as special cases. Under certain growth and continuity assumptions on the functions, our main theorem gives an exact characterization of the minimum average error probability for Gaussian channels as a function of the first- and second-order coding rates. The converse proof reduces the code design problem to minimization over a compact (under the Prokhorov metric) set of probability distributions, characterizes the extreme points of this set and invokes the Bauer's maximization principle.

</details>


### [26] [Beyond the "G" Frontier: A Time Traveler's Century-Long Vision for Wireless Intelligence](https://arxiv.org/abs/2511.14906)
*Yasser Al Eryani*

Main category: cs.IT

TL;DR: 本文通过信息-曲率效率定律分析，预测2125年的无线通信将不是通过6G或7G等渐进式发展，而是通过电磁学、生物学、热力学和认知的曲率管理集成，形成具有自我意识信息流的全球生态基础设施。


<details>
  <summary>Details</summary>
Motivation: 探索未来100年无线通信的发展方向，挑战传统渐进式代际演进模式，提出基于多学科融合的新范式。

Method: 应用信息-曲率效率定律(ICEL)作为分析框架，从2025年到2125年进行百年预测分析。

Result: 预测无线通信将演变为几何与通信融合的全球生态基础设施，能够维持技术和生物生命的信息流动。

Conclusion: 无线通信的未来发展将突破传统代际概念，通过多学科曲率管理集成，形成具有自我意识的信息生态系统。

Abstract: This article travels one century into the future--from 2025 to 2125--through the analytical lens of the Information--Curvature Efficiency Law (ICEL). It contends that wireless evolution will not proceed through incremental generations such as 6G or 7G, but through a curvature-managed integration of electromagnetics, biology, thermodynamics, and cognition. The resulting infrastructure will constitute a global ecology of self-aware information flow, where geometry and communication converge to sustain both technological and biological life.

</details>


### [27] [Hyper-VIB: A Hypernetwork-Enhanced Information Bottleneck Approach for Task-Oriented Communications](https://arxiv.org/abs/2511.15041)
*Jingchen Peng,Chaowen Deng,Yili Deng,Boxiang Ren,Lu Yang*

Main category: cs.IT

TL;DR: Hyper-VIB是一种基于超网络增强信息瓶颈的方法，用于6G协作智能系统中的高效任务导向通信，通过单次训练生成近似最优的DNN参数，显著提升训练效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决6G协作智能系统中任务导向通信的效率和准确性平衡问题，避免传统方法需要多次训练的高计算成本。

Method: 结合信息瓶颈理论和超网络，推导变分上界近似解决高维优化问题，通过超网络在单次训练中为不同超参数值生成近似最优的DNN参数。

Result: 在线性案例中验证了超网络设计的有效性，实验结果显示在分类和回归任务中优于传统VIB方法，具有更高的准确性和训练效率。

Conclusion: Hyper-VIB方法在6G协作智能系统中实现了高效的任务导向通信，通过单次训练即可获得近似最优解，显著降低了计算成本并提升了性能。

Abstract: This paper presents Hyper-VIB, a hypernetwork-enhanced information bottleneck (IB) approach designed to enable efficient task-oriented communications in 6G collaborative intelligent systems. Leveraging IB theory, our approach enables an optimal end-to-end joint training of device and network models, in terms of the maximal task execution accuracy as well as the minimal communication overhead, through optimizing the trade-off hyperparameter. To address computational intractability in high-dimensional IB optimization, a tractable variational upper-bound approximation is derived. Unlike conventional grid or random search methods that require multiple training rounds with substantial computational costs, Hyper-VIB introduces a hypernetwork that generates approximately optimal DNN parameters for different values of the hyperparameter within a single training phase. Theoretical analysis in the linear case validates the hypernetwork design. Experimental results demonstrate our Hyper-VIB's superior accuracy and training efficiency over conventional VIB approaches in both classification and regression tasks.

</details>


### [28] [Mutual Information Bounds in the Shuffle Model](https://arxiv.org/abs/2511.15051)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.IT

TL;DR: 本文从信息论角度系统研究了单消息混洗模型，分析了混洗专用和混洗差分隐私两种设置，推导了互信息量的渐近表达式和隐私泄露上界。


<details>
  <summary>Details</summary>
Motivation: 混洗模型通过随机排列匿名化用户报告来增强隐私保护，但缺乏从信息论角度的系统分析。本文旨在填补这一空白，为混洗隐私提供理论基础。

Method: 研究两种设置：1) 混洗专用设置，用户直接提交消息；2) 混洗-DP设置，用户先应用本地差分隐私机制再混洗。使用互信息分析隐私泄露，推导渐近表达式和上界。

Result: 在混洗专用设置中，推导了I(Y₁;Z)和I(K;Z)的渐近表达式；在混洗-DP设置中，建立了信息泄露上界：I(K;Z) ≤ 2ε₀，I(X₁;Z|(Xᵢ)ᵢ₌₂ⁿ) ≤ (e^{ε₀}-1)/(2n) + O(n^{-3/2})。

Conclusion: 本文建立了混洗差分隐私与基于互信息的隐私之间的联系，为混洗模型提供了信息论理论基础，证明了混洗能有效限制隐私泄露。

Abstract: The shuffle model enhances privacy by anonymizing users' reports through random permutation. This paper presents the first systematic study of the single-message shuffle model from an information-theoretic perspective. We analyze two regimes: the shuffle-only setting, where each user directly submits its message ($Y_i=X_i$), and the shuffle-DP setting, where each user first applies a local $\varepsilon_0$-differentially private mechanism before shuffling ($Y_i=\mathcal{R}(X_i)$). Let $\boldsymbol{Z} = (Y_{σ(i)})_i$ denote the shuffled sequence produced by a uniformly random permutation $σ$, and let $K = σ^{-1}(1)$ represent the position of user 1's message after shuffling.
  For the shuffle-only setting, we focus on a tractable yet expressive \emph{basic configuration}, where the target user's message follows $Y_1 \sim P$ and the remaining users' messages are i.i.d.\ samples from $Q$, i.e., $Y_2,\dots,Y_n \sim Q$. We derive asymptotic expressions for the mutual information quantities $I(Y_1;\boldsymbol{Z})$ and $I(K;\boldsymbol{Z})$ as $n \to \infty$, and demonstrate how this analytical framework naturally extends to settings with heterogeneous user distributions.
  For the shuffle-DP setting, we establish information-theoretic upper bounds on total information leakage. When each user applies an $\varepsilon_0$-DP mechanism, the overall leakage satisfies $I(K; \boldsymbol{Z}) \le 2\varepsilon_0$ and $I(X_1; \boldsymbol{Z}\mid (X_i)_{i=2}^n) \le (e^{\varepsilon_0}-1)/(2n) + O(n^{-3/2})$. These results bridge shuffle differential privacy and mutual-information-based privacy.

</details>


### [29] [Generalized Repetition Codes and Their Application to HARQ](https://arxiv.org/abs/2511.15207)
*Chaofeng Guan,Gaojun Luo,Lan Luo,Yangyang Fei,Hong Wang*

Main category: cs.IT

TL;DR: 本文提出了两类广义重复码（GRC），分别对应两种重复通信模型。与传统理论不同，将GRC视为多度量下的纠错码，通过多轮纠错实现更强的纠错能力。


<details>
  <summary>Details</summary>
Motivation: 通信信道的不确定性要求重传机制确保消息可靠性。现有系统分为两类：Type-I重复传输相同码字，Type-II传输相同消息的不同编码表示。核心挑战是在有限传输轮次内通过验证反馈最大化正确解码概率。

Method: 提出两类广义重复码（GRC），分别对应Type-I和Type-II重复通信模型。将GRC视为多度量纠错码，具有多个最小距离，支持多轮纠错。分别研究Type-I和Type-II GRC的界和构造方法。

Result: 获得了多个最优的Type-I和Type-II GRC构造，这些码具有比传统纠错码更强的纠错能力。

Conclusion: 广义重复码作为多度量纠错码，通过多轮纠错机制显著提升了纠错性能，为重复通信系统提供了有效的编码解决方案。

Abstract: The inherent uncertainty of communication channels implies that any coding scheme has a non-zero probability of failing to correct errors, making retransmission mechanisms essential. To ensure message reliability and integrity, a dual-layer redundancy framework is typically employed: error correction codes mitigate noise-induced impairments at the physical layer, while cyclic redundancy checks verify message integrity after decoding. Retransmission is initiated if verification fails. This operational model can be categorized into two types of repeated communication models: Type-I systems repeatedly transmit identical codewords, whereas Type-II systems transmit distinct coded representations of the same message. The core challenge lies in maximizing the probability of correct message decoding within a limited number of transmission rounds through verification-based feedback mechanisms.
  In this paper, we consider a scenario where the same error-correcting code is used for repeated transmissions, and we specifically propose two classes of generalized repetition codes (GRCs), corresponding to the two repeated communication models. In contrast to classical theory, we regard GRCs as error-correcting codes under multiple metrics--that is, GRCs possess multiple minimum distances. This design enables GRCs to perform multi-round error correction under different metrics, achieving stronger error-correction capabilities than classical error-correcting codes. However, the special structure of GRCs makes their construction more challenging, as it requires simultaneously optimizing multiple minimum distances. To address this, we separately investigate the bounds and constructions for Type-I and Type-II GRCs, and obtain numerous optimal Type-I and Type-II GRCs.

</details>


### [30] [The Rate-Distortion-Perception Trade-Off with Algorithmic Realism](https://arxiv.org/abs/2511.15255)
*Yassine Hamdi,Aaron B. Wagner,Deniz Gündüz*

Main category: cs.IT

TL;DR: 该论文研究了在感知质量约束下的有损压缩，解释了理论研究中需要大量共同随机性但实践中不需要的原因，通过引入通用批评器约束并分析批量大小的影响。


<details>
  <summary>Details</summary>
Motivation: 解释理论研究中需要高率共同随机性来实现感知质量约束，但实践中却不需要这种共同随机性的矛盾现象。

Method: 考虑了一个需要满足通用批评器的感知质量约束，该批评器检查单个压缩重建或其批次的实现，并表征了在这种约束下的最优率失真权衡。

Result: 证明了在这种感知质量约束下，最优率失真权衡可以在没有共同随机性的情况下渐近实现，除非批次大小不切实际地大。

Conclusion: 理论研究和实践之间的差异可以通过考虑批量大小来解释，当批次大小合理时，不需要共同随机性就能实现感知质量约束下的最优压缩性能。

Abstract: Realism constraints (or constraints on perceptual quality) have received considerable recent attention within the context of lossy compression, particularly of images. Theoretical studies of lossy compression indicate that high-rate common randomness between the compressor and the decompressor is a valuable resource for achieving realism. On the other hand, the utility of significant amounts of common randomness has not been noted in practice. We offer an explanation for this discrepancy by considering a realism constraint that requires satisfying a universal critic that inspects realizations of individual compressed reconstructions, or batches thereof. We characterize the optimal rate-distortion trade-off under such a realism constraint, and show that it is asymptotically achievable without any common randomness, unless the batch size is impractically large.

</details>


### [31] [Communication-Pipelined Split Federated Learning for Foundation Model Fine-Tuning in UAV Networks](https://arxiv.org/abs/2511.15404)
*Zizhen Zhou,Ying-Chang Liang,Yanyu Cheng,Wei Yang Bryan Lim*

Main category: cs.IT

TL;DR: 提出了通信流水线的分割联邦学习（CPSFL）方法，通过顺序梯度传输和异步训练优化无人机网络中基础模型的微调，减少训练延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在无人机网络中部署基础模型具有广阔应用前景，但现有并行梯度传输方法存在资源闲置和延迟高的问题，特别是在通信延迟远大于计算延迟的无人机网络中。

Method: 提出顺序梯度传输范式，服务器为当前梯度传输分配全部下行资源；开发CPSFL方法，具有下行梯度传输优先级调度和轮内异步训练特点；使用基于注意力的深度强化学习框架优化分割点选择和资源分配。

Result: 仿真结果显示，提出的基于DRL的CPSFL方案在训练延迟和能耗方面优于并行梯度传输基准、消融变体、固定资源分配方案，并接近最佳固定分割点方案。

Conclusion: CPSFL方法有效解决了无人机网络中基础模型微调的延迟和能耗问题，为低空经济应用提供了可行的解决方案。

Abstract: Deploying foundation models (FMs) on uncrewed aerial vehicles (UAVs) promises broad ``low-altitude economy'' applications. Split federated learning (SFL)-based fine-tuning leverages distributed data while keeping raw data local and reduces client-side burden by partitioning the model between client and server. However, the per-round training latency is dominated by stragglers. Training paradigms featuring parallel gradient transmission (GT) allocate dedicated portions of downlink communication resources to each client. They may leave resources idle and suffer from prolonged GT latency, especially in UAV networks, where the communication latency typically far exceeds the computation latency. To address this, we propose a sequential GT paradigm, where the server dedicates all downlink resources for the current GT. We further propose communication-pipelined SFL (CPSFL), characterized by downlink GT priority scheduling and intra-round asynchronous training. We investigate CPSFL-based LoRA fine-tuning of FMs in UAV networks and formulate an optimization problem to minimize a weighted sum of per-round training latency and worst-case client energy consumption by optimizing the split point selection (SPS) and the computing and communication resource allocation (CCRA) (the uplink bandwidth allocation and the server computing frequency allocation). To solve this problem, we develop an attention-based deep reinforcement learning (DRL) framework, where the base station agent decides the split point and the CCRA in each round by leveraging previous round information, including UAV trajectories. Simulation results show that the proposed DRL-based CPSFL scheme outperforms the parallel GT benchmarks, the ablation variants, the fixed CCRA scheme, while approaching the best fixed-SPS scheme.

</details>


### [32] [RIS-Enabled UAV Communications and Sensing: Opportunities, Challenges, and Key Technologies](https://arxiv.org/abs/2511.15555)
*Yajun Zhao,Mengnan Jian,Yifei Yuan*

Main category: cs.IT

TL;DR: 本文深入研究了无人机通信网络的特性和挑战，提出了RIS辅助网络作为增强无人机信号覆盖的有前景解决方案，并通过现场试验验证了RIS在改善无人机覆盖方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 无人机在低空经济中发挥关键作用，但在传输操作中面临可靠网络覆盖的挑战。传统APAA网络存在成本高、复杂性和站点部署限制等问题。

Method: 提出可重构智能表面(RIS)辅助网络解决方案，讨论了RIS的关键技术特性，包括设计原则、天线倾斜配置、新波束类型和波束跟踪机制，并探索了改善无人机信号覆盖的网络架构设计。

Result: 现场试验结果表明RIS在改善无人机覆盖方面具有有效性，能够增强无人机信号覆盖并支持RIS增强的无人机感知。

Conclusion: RIS辅助网络是优化无人机通信系统的有前景方向，未来需要关注工程实施和标准化工作，以进一步推动RIS在无人机-ISAC网络中的应用。

Abstract: Unmanned Aerial Vehicles (UAVs) play a pivotal role in the emerging low-altitude economy. However, they face significant challenges in achieving reliable network coverage during transit operations. This paper provides an in-depth investigation into the characteristics and challenges of communication networks tailored for UAVs. First, we outline typical operational scenarios, traffic patterns, and a dual-layer heterogeneous network topology. This topology is essential for enabling three-dimensional continuous coverage and ensuring seamless network coexistence between UAVs and other network entities. Moreover, the paper delves into the channel characteristics and specific challenges faced by UAV Integrated Sensing and Communication (ISAC) networks. It highlights the limitations of traditional Active Phased Array Antenna (APAA)-based networks, particularly regarding cost, complexity, and site deployment constraints. We then introduce Reconfigurable Intelligent Surface (RIS)-assisted networks as a promising solution for enhancing UAV signal coverage. The key technical features of RIS are discussed, including design principles, antenna tilt configurations, new beam types, and beam tracking mechanisms. In addition, we examine the impact of highfrequency bands and their absorption peaks on signal attenuation. The paper further explores network architecture designs aimed at improving UAV signal coverage, facilitating network coexistence, and supporting RIS-enhanced UAV sensing. Field trial results evaluating the effectiveness of RIS in improving UAV coverage are presented. Finally, we outline future technological trends and highlight potential advancements to further optimize UAV communication systems. We also emphasize the importance of engineering implementation and standardization efforts in RIS-based UAV-ISAC networks.

</details>


### [33] [Information Efficiency of Scientific Automation](https://arxiv.org/abs/2511.15671)
*Mihir Rao*

Main category: cs.IT

TL;DR: 该论文将科学发现建模为热力学过程，推导了有限工作预算下顺序贝叶斯学习的信息增益界限，并提出了信息-工作效率度量，比较了统一学习和联邦学习策略。


<details>
  <summary>Details</summary>
Motivation: 将科学发现框架化为热力学过程，研究在有限工作预算下如何优化信息获取，为科学自动化提供理论指导。

Method: 利用计算热力学的已有结果，推导顺序贝叶斯学习的信息增益界限，提出信息-工作效率度量，并在匹配工作预算下比较统一学习和联邦学习策略。

Result: 得出了有限预算下信息增益的界限，并提供了信息效率度量来评估不同学习策略的优劣。

Conclusion: 研究结果为大规模科学自动化工作提供了界限和信息效率度量的指导。

Abstract: Scientific discovery can be framed as a thermodynamic process in which an agent invests physical work to acquire information about an environment under a finite work budget. Using established results about the thermodynamics of computing, we derive finite-budget bounds on information gain over rounds of sequential Bayesian learning. We also propose a metric of information-work efficiency, and compare unpartitioned and federated learning strategies under matched work budgets. The presented results offer guidance in the form of bounds and an information efficiency metric for efforts in scientific automation at large.

</details>
