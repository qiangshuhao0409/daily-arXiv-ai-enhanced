<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 60]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions](https://arxiv.org/abs/2510.00477)
*Chengzhen Li,Likun Zhang,Chuang Zhang,Jiahui Li,Changyuan Zhao,Ruichen Zhang,Geng Sun*

Main category: cs.NI

TL;DR: 该论文探讨了无线电力传输(WLPT)作为无人机辅助物联网网络中可持续能源供应的变革性解决方案，提出了多智能体强化学习框架来优化能源可持续性和数据新鲜度。


<details>
  <summary>Details</summary>
Motivation: 无人机辅助物联网面临空中平台和地面传感器能源限制的关键挑战，需要可持续的能源供应解决方案来提升运营能力。

Method: 系统研究WLPT基本原理和比较优势，提出三种系统集成操作范式，并设计多智能体强化学习框架来解决WLPT赋能无人机辅助物联网数据收集中的协调和优化挑战。

Result: 仿真结果表明，所提出的框架显著提高了能源可持续性和数据新鲜度。

Conclusion: WLPT是无人机辅助物联网网络中实现可持续能源供应的有前景解决方案，未来需要进一步研究相关技术方向。

Abstract: Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers
for the Internet of Things (IoT) by offering enhanced coverage, improved
connectivity and access to remote areas. A critical challenge limiting their
operational capacity lies in the energy constraints of both aerial platforms
and ground-based sensors. This paper explores WLPT as a transformative solution
for sustainable energy provisioning in UAV-assisted IoT networks. We first
systematically investigate the fundamental principles of WLPT and analysis the
comparative advantages. Then, we introduce three operational paradigms for
system integration, identify key challenges, and discuss corresponding
potential solutions. In case study, we propose a multi-agent reinforcement
learning framework to address the coordination and optimization challenges in
WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate
that our framework significantly improves energy sustainability and data
freshness. Finally, we discuss some future directions.

</details>


### [2] [Make a Video Call with LLM: A Measurement Campaign over Five Mainstream Apps](https://arxiv.org/abs/2510.00481)
*Jiayang Xu,Xiangjie Huang,Zijie Li,Zili Meng*

Main category: cs.NI

TL;DR: 本文提出了首个针对AI视频聊天系统的综合基准测试，从质量、延迟、内部机制和系统开销四个维度评估了五个主流AI视频聊天机器人。


<details>
  <summary>Details</summary>
Motivation: 2025年LLM服务推出了AI视频聊天功能，允许用户通过实时视频通信与AI代理互动，但目前缺乏对现有AI视频聊天系统性能的系统性研究。

Method: 设计了一个包含四个维度（质量、延迟、内部机制、系统开销）的综合基准测试，并使用定制测试平台评估了五个主流AI视频聊天机器人。

Result: 为研究社区提供了真实世界性能的基准，并识别了独特的系统瓶颈。

Conclusion: 基准测试结果为未来AI视频聊天机器人的优化提出了若干研究问题。

Abstract: In 2025, Large Language Model (LLM) services have launched a new feature --
AI video chat -- allowing users to interact with AI agents via real-time video
communication (RTC), just like chatting with real people. Despite its
significance, no systematic study has characterized the performance of existing
AI video chat systems. To address this gap, this paper proposes a comprehensive
benchmark with carefully designed metrics across four dimensions: quality,
latency, internal mechanisms, and system overhead. Using custom testbeds, we
further evaluate five mainstream AI video chatbots with this benchmark. This
work provides the research community a baseline of real-world performance and
identifies unique system bottlenecks. In the meantime, our benchmarking results
also open up several research questions for future optimizations of AI video
chatbots.

</details>


### [3] [Dynamic Low Power Traffic Pattern for Energy Constrained Wireless Sensor Networks](https://arxiv.org/abs/2510.00588)
*Almamoon Alauthman*

Main category: cs.NI

TL;DR: 提出了一种针对树状结构无线传感器网络的节能策略，通过减少空闲监听状态来降低能耗，延长网络寿命并提高吞吐量


<details>
  <summary>Details</summary>
Motivation: 无线传感器网络在关键应用中广泛使用，但传感器节点依赖有限电池资源，能耗效率是主要挑战

Method: 针对具有动态流量模式的树状结构网络，通过减少空闲监听状态的长度和频率来降低功耗

Result: 使用OMNeT++模拟器和MiXiM框架的仿真结果表明，该方案显著降低了能耗，提高了数据吞吐量和整体网络效率

Conclusion: 所提出的节能策略能有效延长无线传感器网络的运行寿命并提高网络性能

Abstract: Wireless Sensor Networks (WSNs) are extensively utilized in critical
applications, including remote monitoring, target tracking, healthcare systems,
industrial automation, and smart control in both residential and industrial
settings. One of the primary challenges in these systems is maintaining energy
efficiency, given that most sensor nodes rely on limited battery resources. To
tackle this problem, this study introduces an energy-saving strategy designed
for tree-structured networks with dynamic traffic patterns. The approach
focuses on lowering power usage by decreasing the length and occurrence of idle
listening state where nodes remain active unnecessarily while waiting for data
transmissions that may never occur. By reducing this form of energy waste, the
proposed approach is designed to extend the operational lifetime and enhance
the throughput of the wireless sensor network. Simulation results obtained
using the OMNeT++ simulator with the MiXiM framework demonstrate that the
solution significantly reduces energy consumption, increases data throughput,
and improves overall network efficiency and longevity.

</details>


### [4] [Faster Offloads by Unloading them -- The RDMA Case](https://arxiv.org/abs/2510.00735)
*Georgia Fragkouli,Laurent Vanbever*

Main category: cs.NI

TL;DR: 论文提出"卸载逆转"概念，认为为了进一步加速卸载技术，需要将部分已卸载的任务重新移回应用层执行，并以RDMA写入操作为例展示了卸载逆转的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件和软件卸载技术虽然能提升性能，但完全卸载并不总能达到预期加速效果。作者观察到这是因为卸载将任务从应用层移到网络层，改变了执行环境，因此需要探索卸载逆转的可能性。

Method: 聚焦于RDMA写入操作，研究：1) 哪些部分适合卸载逆转；2) 如何动态决定在卸载路径还是逆转路径执行写入操作；3) 如何保持两条路径的兼容性。

Result: 当前原型显示卸载逆转能够将RDMA写入操作加速高达31%。

Conclusion: 卸载逆转是一个有前景的方向，能够进一步提升卸载技术的性能，但面临着一系列需要解决的挑战。

Abstract: From hardware offloads like RDMA to software ones like eBPF, offloads are
everywhere and their value is in performance. However, there is evidence that
fully offloading -- even when feasible -- does not always give the expected
speedups. Starting from the observation that this is due to changes the
offloads make -- by moving tasks from the application/CPU closer to the
network/link layer -- we argue that to further accelerate offloads, we need to
make offloads reversible by unloading them -- moving back part of the offloaded
tasks.
  Unloading comes with a set of challenges that we start answering in this
paper by focusing on (offloaded) RDMA writes: which part of the write operation
does it make sense to unload? how do we dynamically decide which writes to
execute on the unload or offload path to improve performance? how do we
maintain compatibility between the two paths? Our current prototype shows the
potential of unloading by accelerating RDMA writes by up to 31%.

</details>


### [5] [Optimizing Version AoI in Energy-Harvesting IoT: Model-Based and Learning-Based Approaches](https://arxiv.org/abs/2510.00904)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 该论文研究了在能量收集传感器系统中优化语义指标版本信息年龄(VAoI)的问题，针对系统模型完全已知、部分已知和未知三种情况，分别提出了基于模型、基于估计和无模型的优化策略。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的物联网系统中，需要语义感知管理来最大化及时且信息丰富数据的传输效率，特别是在能量收集传感器场景下优化版本信息年龄这一语义指标。

Method: 使用马尔可夫决策过程(MDP)和强化学习(RL)框架，针对系统模型的不同认知程度（完全已知、部分已知、未知）分别开发了模型驱动、估计驱动和无模型的优化方法。

Result: 分析了在不同模型信息程度下的性能权衡，为设计高效且自适应的语义感知策略提供了指导。

Conclusion: 研究结果为在已知和未知物联网环境中设计有效的语义感知策略提供了重要指导，能够适应不同程度的系统模型知识。

Abstract: Efficient data transmission in resource-constrained Internet of Things (IoT)
systems requires semantics-aware management that maximizes the delivery of
timely and informative data. This paper investigates the optimization of the
semantic metric Version Age of Information (VAoI) in a status update system
comprising an energy-harvesting (EH) sensor and a destination monitoring node.
We consider three levels of knowledge about the system model -- fully known,
partially known, and unknown -- and propose corresponding optimization
strategies: model-based, estimation-based, and model-free methods. By employing
Markov Decision Process (MDP) and Reinforcement Learning (RL) frameworks, we
analyze performance trade-offs under varying degrees of model information. Our
findings provide guidance for designing efficient and adaptive semantics-aware
policies in both known and unknown IoT environments.

</details>


### [6] [Enhancing Urban VANETs Stability: A Single-Hop Clustering Strategy in Metropolitan Environments](https://arxiv.org/abs/2510.00939)
*Pouya Firouzmakan,Suprakash Datta*

Main category: cs.NI

TL;DR: 提出了一种针对城市环境的VANET高效分簇算法，利用公共交通巴士作为主要簇头，减少对路边单元(RSU)的依赖，提高簇稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决车载自组织网络(VANET)在城市环境中高度动态性带来的部署挑战，减少对基础设施的依赖，提高网络稳定性和服务质量。

Method: 设计以公共交通巴士为主要簇头，独立车辆动态选择额外簇头的分簇算法，利用现有城市基础设施补偿RSU的缺失。

Result: 通过案例研究和与现有算法的比较分析，证明该方法在不同传输范围内均表现出优越性能。

Conclusion: 所提出的算法能够有效提高VANET在城市环境中的簇稳定性，减少对RSU的依赖，为智能交通系统提供更可靠的通信解决方案。

Abstract: Vehicular Ad-hoc Networks (VANETs), a subclass of Mobile Ad-hoc Networks
(MANETs), are expected to play a crucial role in the future of intelligent
transportation systems (ITSs). A key objective of VANETs is to enable efficient
and cost-effective communication among vehicles while supporting a large number
of network participants and minimizing infrastructure dependency. However, the
highly dynamic nature of vehicular networks poses significant challenges to
their deployment. Clustering techniques are employed to address these
challenges, with a strong emphasis on stability, as they directly influence the
routing process and enhance the quality of service (QoS). This paper explores
the feasibility of reducing reliance on roadside units (RSUs) in metropolitan
areas while improving cluster stability. We propose an efficient clustering
algorithm tailored for urban environments, leveraging existing metropolitan
infrastructure to compensate for the absence of RSUs. Our approach designates
public transportation buses as primary cluster heads (CHs), minimizing reliance
on additional infrastructure, while stand-alone vehicles (SAVs) dynamically
select additional CHs. Through comprehensive case studies and comparative
analysis with existing algorithms, our results demonstrate the superior
performance of the proposed method across different transmission ranges (TRs).

</details>


### [7] [Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning](https://arxiv.org/abs/2510.00956)
*Carlos Güemes-Palau,Miquel Ferriol-Galmés,Jordi Paillisse-Vilanova,Albert López-Brescó,Pere Barlet-Ros,Albert Cabellos-Aparicio*

Main category: cs.NI

TL;DR: 提出了一种结合模拟数据和真实数据的混合方法，通过迁移学习显著提升网络性能预测精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习网络模型需要大量训练数据，但真实网络数据收集成本高且有限，特别是故障等关键场景。模拟数据虽然容易获取，但在真实环境中部署时准确性会降低。

Method: 使用RouteNet-Fermi模型，通过迁移学习将预训练模型与少量真实数据集进行微调，结合OMNeT++模拟器和自定义测试平台。

Result: 实验显示，该方法将包延迟预测的平均绝对百分比误差(MAPE)降低了高达88%。仅使用10个真实场景，MAPE降低37%；使用50个场景，MAPE降低48%。

Conclusion: 通过迁移学习结合模拟和真实数据的方法，能够用少量真实数据显著提升网络性能预测模型的准确性。

Abstract: Machine Learning (ML)-based network models provide fast and accurate
predictions for complex network behaviors but require substantial training
data. Collecting such data from real networks is often costly and limited,
especially for critical scenarios like failures. As a result, researchers
commonly rely on simulated data, which reduces accuracy when models are
deployed in real environments. We propose a hybrid approach leveraging transfer
learning to combine simulated and real-world data. Using RouteNet-Fermi, we
show that fine-tuning a pre-trained model with a small real dataset
significantly improves performance. Our experiments with OMNeT++ and a custom
testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay
prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and
with 50 scenarios, by 48%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Learning to Lead Themselves: Agentic AI in MAS using MARL](https://arxiv.org/abs/2510.00022)
*Ansh Kamthan*

Main category: cs.AI

TL;DR: 本文研究了如何在多智能体系统中使用自主智能体来改进任务分配和协调，主要关注无人机配送，次要关注仓库自动化。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统从原型转向实际部署，多个智能体进行去中心化协作决策的能力成为核心需求。

Method: 在合作多智能体强化学习框架下制定问题，使用PyTorch实现轻量级多智能体近端策略优化（IPPO）方法，采用集中训练、分散执行的范式。

Result: 在PettingZoo环境中进行实验，多个同质无人机或智能体必须自组织覆盖不同目标而无需显式通信。

Conclusion: 自主智能体能够独立、自适应和主动行动，可以改善多智能体系统中的任务分配和协调。

Abstract: As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

</details>


### [9] [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)
*Quy Minh Le,Minh Sao Khue Luu,Khanh-Tung Tran,Duc-Hai Nguyen,Hoang-Quoc-Viet Pham,Quan Le,Hoang Thanh Lam,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: ToolBrain是一个轻量级框架，通过灵活的强化学习训练智能代理使用工具，解决了传统方法中手动设计奖励、训练数据有限和多工具选择困难等问题。


<details>
  <summary>Details</summary>
Motivation: 当前训练AI代理使用工具存在挑战：需要手动设计奖励函数、训练数据有限、多工具选择困难，导致适应慢、计算资源浪费和性能不佳。

Method: ToolBrain框架支持多种训练策略，包括GRPO和DPO等强化学习算法，以及监督学习。支持自定义奖励函数或使用LLM-as-a-judge自动生成奖励，提供知识蒸馏、自动任务生成、工具检索、QLoRA微调等功能。

Result: 通过训练CodeAct代理执行邮件搜索任务的案例，显示工具使用技能快速提升了30.0%，同时保持代码库简洁且可扩展。

Conclusion: ToolBrain为研究人员和从业者提供了一个用户友好的框架，能够有效训练LLM代理在特定领域使用工具，显著提升性能并降低开发门槛。

Abstract: Effective tool use is essential for agentic AI, yet training agents to
utilize tools remains challenging due to manually designed rewards, limited
training data, and poor multi-tool selection, resulting in slow adaptation,
wasted computational resources, and suboptimal performance. We introduce
ToolBrain, a lightweight and user-friendly framework for coaching tool use in
agentic models with flexible reinforcement learning (RL), easing the barriers
for researchers and practitioners to adapt LLM-based agents to specific
domains. It supports a wide range of training strategies, including RL
algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain
enables custom reward callables directly on an agent's execution traces or
simply utilizes an automated LLM-as-a-judge system for reward generation. It is
packed with useful capabilities, including knowledge distillation from large to
small models for efficient development, automatic task generation from tool
descriptions, seamless tool retrieval, efficient fine-tuning pipelines with
QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate
ToolBrain through diverse use cases, such as training a CodeAct agent to
autonomously execute email search tasks, showing fast, targeted improvements
(up to 30.0%) in tool-use skills while keeping the codebase simple and
extensible in Agentic AI. Our framework is publicly available at
https://toolbrain.org.

</details>


### [10] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: 提出自适应推理抑制(ARS)方法，通过动态监测推理确定性来抑制冗余推理步骤，在保持准确性的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型在复杂推理任务中表现出色，但由于过度思考现象导致计算效率低下，现有方法难以平衡推理质量和推理成本

Method: 训练无关的自适应推理抑制方法，采用多检查点确定性估计机制和渐进抑制阈值，动态抑制冗余推理步骤

Result: 在数学推理基准测试中，ARS实现了最高53%的token减少、46.1%的延迟减少和57.9%的能耗减少，同时保持或提高了准确性

Conclusion: ARS方法有效解决了大型推理模型的效率问题，在显著降低计算成本的同时保持了推理质量

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [11] [A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting](https://arxiv.org/abs/2510.00960)
*Miha Ožbot,Igor Škrjanc,Vitomir Štruc*

Main category: cs.AI

TL;DR: 提出Fuzzformer模型，结合循环神经网络、多头自注意力和模糊推理系统，用于多元时间序列预测，在保持准确性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多元时间序列预测中，同时实现准确性和可解释性是一个重大挑战。

Method: 使用LSTM网络和时间注意力将多元数据压缩为适合模糊推理系统的可解释特征，结合多头自注意力和模糊推理系统。

Result: 在S&P500股票市场指数上的初步结果显示，与传统ARIMA和LSTM模型相比具有相当的预测性能，同时提供网络内部有意义的信息流。

Conclusion: 该方法在理解和预测股票市场行为方面具有实际应用潜力，但存在性能权衡问题。

Abstract: In the complex landscape of multivariate time series forecasting, achieving
both accuracy and interpretability remains a significant challenge. This paper
introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network
architecture combined with multi-head self-attention and fuzzy inference
systems to analyze multivariate stock market data and conduct long-term time
series forecasting. The method leverages LSTM networks and temporal attention
to condense multivariate data into interpretable features suitable for fuzzy
inference systems. The resulting architecture offers comparable forecasting
performance to conventional models such as ARIMA and LSTM while providing
meaningful information flow within the network. The method was examined on the
real world stock market index S\&P500. Initial results show potential for
interpretable forecasting and identify current performance tradeoffs,
suggesting practical application in understanding and forecasting stock market
behavior.

</details>


### [12] [NeurIPS should lead scientific consensus on AI policy](https://arxiv.org/abs/2510.00075)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 本文主张NeurIPS会议应积极推动AI政策领域的科学共识形成，借鉴IPCC在气候政策方面的经验，通过试点项目来填补当前AI政策共识机制的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策制定缺乏科学共识形成机制，而NeurIPS作为AI领域的领先会议，具备推动这一进程的独特优势。

Method: 建议NeurIPS借鉴IPCC在气候政策共识形成方面的经验，开展初步试点项目，主动催化AI政策领域的科学共识。

Result: 识别了AI政策领域共识形成的完全空白，论证了NeurIPS是最佳选择，并提出了具体的实施建议。

Conclusion: NeurIPS应在AI政策领域发挥领导作用，通过推动科学共识来创造更高质量的AI政策。

Abstract: Designing wise AI policy is a grand challenge for society. To design such
policy, policymakers should place a premium on rigorous evidence and scientific
consensus. While several mechanisms exist for evidence generation, and nascent
mechanisms tackle evidence synthesis, we identify a complete void on consensus
formation. In this position paper, we argue NeurIPS should actively catalyze
scientific consensus on AI policy. Beyond identifying the current deficit in
consensus formation mechanisms, we argue that NeurIPS is the best option due
its strengths and the paucity of compelling alternatives. To make progress, we
recommend initial pilots for NeurIPS by distilling lessons from the IPCC's
leadership to build scientific consensus on climate policy. We dispel
predictable counters that AI researchers disagree too much to achieve consensus
and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on
many fronts, and it should champion scientific consensus to create higher
quality AI policy.

</details>


### [13] [Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems](https://arxiv.org/abs/2510.00084)
*Fabian Kovac,Sebastian Neumaier,Timea Pahi,Torsten Priebe,Rafael Rodrigues,Dimitrios Christodoulou,Maxime Cordy,Sylvain Kubler,Ali Kordia,Georgios Pitsiladis,John Soldatos,Petros Zervoudakis*

Main category: cs.AI

TL;DR: CERTAIN项目开发了一个综合框架，将监管合规、伦理标准和透明度整合到AI系统中，通过语义MLOps、本体驱动数据溯源和RegOps工作流来推进AI系统的合规性和负责任创新。


<details>
  <summary>Details</summary>
Motivation: AI在欧洲社会和经济中的快速普及带来了关键的伦理、法律和监管挑战，需要解决这些挑战以确保AI的负责任发展。

Method: 开发综合框架，包括：(i) 语义MLOps用于结构化AI生命周期管理，(ii) 本体驱动数据溯源跟踪确保可追溯性和问责制，(iii) RegOps工作流操作化合规要求。

Result: 通过在不同试点中实施和验证解决方案，CERTAIN项目推进了监管合规并促进了符合欧洲标准的负责任AI创新。

Conclusion: CERTAIN项目提供了一个有效的框架来应对AI发展中的伦理、法律和监管挑战，支持欧洲AI生态系统的可持续发展。

Abstract: Artificial Intelligence has rapidly become a cornerstone technology,
significantly influencing Europe's societal and economic landscapes. However,
the proliferation of AI also raises critical ethical, legal, and regulatory
challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency
in Artificial Intelligence) project addresses these issues by developing a
comprehensive framework that integrates regulatory compliance, ethical
standards, and transparency into AI systems. In this position paper, we outline
the methodological steps for building the core components of this framework.
Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for
structured AI lifecycle management, (ii) ontology-driven data lineage tracking
to ensure traceability and accountability, and (iii) regulatory operations
(RegOps) workflows to operationalize compliance requirements. By implementing
and validating its solutions across diverse pilots, CERTAIN aims to advance
regulatory compliance and to promote responsible AI innovation aligned with
European standards.

</details>


### [14] [Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction](https://arxiv.org/abs/2510.00088)
*Sagnik Basu,Shubham Prakash,Ashish Maruti Barge,Siddharth D Jaiswal,Abhisek Dash,Saptarshi Ghosh,Animesh Mukherjee*

Main category: cs.AI

TL;DR: 本文审计了视觉语言模型在保释决策预测中的表现，发现模型存在偏见且错误拒绝保释，通过引入法律先例和微调显著改善了性能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的发展，法律判决预测系统开始利用罪犯图像和文本报告，但可能带来意外后果和恶意使用，需要评估其效率和偏见。

Method: 通过审计评估VLMs在保释预测中的表现，设计干预算法：首先通过RAG管道引入法律先例，然后使用创新方案微调VLMs。

Result: 发现模型在多个交叉群体中表现不佳，错误拒绝保释且置信度过高；干预后保释预测性能显著提升。

Conclusion: 为未来在实际部署前设计更智能的VLM干预措施铺平了道路。

Abstract: Large language models (LLMs) have been extensively used for legal judgment
prediction tasks based on case reports and crime history. However, with a surge
in the availability of large vision language models (VLMs), legal judgment
prediction systems can now be made to leverage the images of the criminals in
addition to the textual case reports/crime history. Applications built in this
way could lead to inadvertent consequences and be used with malicious intent.
In this work, we run an audit to investigate the efficiency of standalone VLMs
in the bail decision prediction task. We observe that the performance is poor
across multiple intersectional groups and models \textit{wrongly deny bail to
deserving individuals with very high confidence}. We design different
intervention algorithms by first including legal precedents through a RAG
pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate
that these interventions substantially improve the performance of bail
prediction. Our work paves the way for the design of smarter interventions on
VLMs in the future, before they can be deployed for real-world legal judgment
prediction.

</details>


### [15] [AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery](https://arxiv.org/abs/2510.00156)
*Songran Bai,Bingzhe Wu,Yiwei Zhang,Chengke Wu,Xiaolong Zheng,Yaze Yuan,Ke Wu,Jianqiang Li*

Main category: cs.AI

TL;DR: 提出了一个名为AuditAgent的多智能体推理框架，用于金融欺诈检测中的细粒度证据链定位，通过整合审计领域专业知识，在真实监管场景中显著优于通用智能体方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的金融欺诈检测面临证据分散在多年财务报告中的挑战，需要更精确的证据定位方法来应对复杂的财务披露场景。

Method: 开发了多智能体推理框架，整合主体级风险先验、混合检索策略和专用智能体模块，基于中国证监会执法文件和财务报告构建专家标注数据集。

Result: 在召回率和可解释性方面显著优于通用智能体范式，为自动化、透明的金融取证建立了新基准。

Conclusion: 领域特定推理和数据集构建对于推进实际监管应用中的稳健金融欺诈检测具有重要价值。

Abstract: Financial fraud detection in real-world scenarios presents significant
challenges due to the subtlety and dispersion of evidence across complex,
multi-year financial disclosures. In this work, we introduce a novel
multi-agent reasoning framework AuditAgent, enhanced with auditing domain
expertise, for fine-grained evidence chain localization in financial fraud
cases. Leveraging an expert-annotated dataset constructed from enforcement
documents and financial reports released by the China Securities Regulatory
Commission, our approach integrates subject-level risk priors, a hybrid
retrieval strategy, and specialized agent modules to efficiently identify and
aggregate cross-report evidence. Extensive experiments demonstrate that our
method substantially outperforms General-Purpose Agent paradigm in both recall
and interpretability, establishing a new benchmark for automated, transparent
financial forensics. Our results highlight the value of domain-specific
reasoning and dataset construction for advancing robust financial fraud
detection in practical, real-world regulatory applications.

</details>


### [16] [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167)
*Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: 该论文提出使用具身AI和大型视觉语言模型来增强无人机在突发事件中的自适应决策能力，替代传统手工编码恢复规则的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖安全工程师手工编码大量恢复规则，无法预测现实世界中的各种意外情况，且容易变得不完整。需要更智能、自适应的决策系统来应对突发事件。

Method: 利用具身AI和大型视觉语言模型，在Unreal Engine模拟的城市场景中，让无人机动态解读环境并实时生成适当的紧急机动决策。

Result: 研究结果表明，具身AI能够实现一类以前无法手工设计的自适应恢复和决策流程，显著提升了自主空中系统的韧性和安全性。

Conclusion: 具身AI为自主无人机系统提供了一种新的自适应恢复和决策能力，能够更好地应对现实世界中的突发事件，提高系统的安全性和韧性。

Abstract: Autonomous drones must often respond to sudden events, such as alarms,
faults, or unexpected changes in their environment, that require immediate and
adaptive decision-making. Traditional approaches rely on safety engineers
hand-coding large sets of recovery rules, but this strategy cannot anticipate
the vast range of real-world contingencies and quickly becomes incomplete.
Recent advances in embodied AI, powered by large visual language models,
provide commonsense reasoning to assess context and generate appropriate
actions in real time. We demonstrate this capability in a simulated urban
benchmark in the Unreal Engine, where drones dynamically interpret their
surroundings and decide on sudden maneuvers for safe landings. Our results show
that embodied AI makes possible a new class of adaptive recovery and
decision-making pipelines that were previously infeasible to design by hand,
advancing resilience and safety in autonomous aerial systems.

</details>


### [17] [Object-Centric Case-Based Reasoning via Argumentation](https://arxiv.org/abs/2510.00185)
*Gabriel de Olim Gaul,Adam Gould,Avinash Kori,Francesca Toni*

Main category: cs.AI

TL;DR: SAA-CBR是一个新颖的神经符号化图像分类管道，结合了基于Slot Attention的神经对象中心学习和基于抽象论证的符号推理。


<details>
  <summary>Details</summary>
Motivation: 整合神经学习和符号推理的优势，通过对象中心表示和论证推理来提高图像分类的性能和可解释性。

Method: 使用Slot Attention进行对象中心学习，结合AA-CBR进行符号推理，包括特征组合策略、案例库缩减、基于计数的偏序、One-Vs-Rest多类分类扩展以及支持型AA-CBR应用。

Result: 在CLEVR-Hans数据集上表现出色，与基线模型相比具有竞争力。

Conclusion: SAA-CBR是一个有效的分类器，成功整合了神经和符号方法，在图像分类任务中展现出良好性能。

Abstract: We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),
a novel neuro-symbolic pipeline for image classification that integrates
object-centric learning via a neural Slot Attention (SA) component with
symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning
(AA-CBR). We explore novel integrations of AA-CBR with the neural component,
including feature combination strategies, casebase reduction via representative
samples, novel count-based partial orders, a One-Vs-Rest strategy for extending
AA-CBR to multi-class classification, and an application of Supported AA-CBR, a
bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective
classifier on the CLEVR-Hans datasets, showing competitive performance against
baseline models.

</details>


### [18] [Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective](https://arxiv.org/abs/2510.00186)
*Anni Li,Aria Attar,Paul Dong*

Main category: cs.AI

TL;DR: Thinkquel是一个经过微调的模型，用于生成可靠、可移植且经过执行验证的数据库查询。它通过创新的合成数据管道TS-SQL和Token-Sequence GRPO强化学习目标，解决了自然语言到SQL转换中的模式链接和SQL方言问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言请求转换为可靠的生产级数据转换具有挑战性：正确性依赖于精确的模式链接和仓库特定的SQL方言，而训练期间最强的监督（执行成功和结果匹配）仅在序列级别提供。同时，组装大型执行验证语料库成本高昂，令牌级目标与这些全局信号不匹配，导致优化不稳定和可移植性有限。

Method: Thinkquel集成了一个新颖的合成数据管道TS-SQL，利用dbt作为可移植的中间表示，并采用span-aware强化学习目标和Token-Sequence GRPO（TS-GRPO），专门设计用于在微调LLMs时弥合令牌级训练信号和序列级执行奖励之间的差距。

Result: 在500个示例的TS-SQL测试集上，Thinkquel（32B）通过两阶段SFT课程达到93.2%的执行成功率和61.8%的精确结果匹配，相比基础模型提高了67.2%（执行）和44.4%（匹配）。在Spider（14B）实验中，TS-GRPO相对于GRPO和GSPO提高了训练稳定性并加速了执行匹配奖励的收敛。

Conclusion: Thinkquel通过创新的合成数据生成和强化学习方法，显著提高了自然语言到SQL转换的可靠性和可移植性，在多个基准测试中表现出色。

Abstract: Transforming natural-language requests into reliable, production-ready data
transformations remains challenging: correctness depends on precise schema
linking and warehouse-specific SQL dialects, while the strongest supervision
available during training--execution success and result matching--are provided
only at the sequence level. At the same time, assembling large,
execution-validated corpora is costly, and token-level objectives misalign with
these global signals, yielding unstable optimization and limited portability.
We introduce Thinkquel, a fine-tuned model for producing robust, portable, and
execution-validated database queries. Methodologies in Thinkquel integrates a
novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable
intermediate representation with a span-aware reinforcement learning objective,
and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap
between token-level training signals and sequence-level execution rewards when
finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches
93.2\% execution success and 61.8\% exact-result match with a two-stage SFT
curriculum, improving over the base model by 67.2\% (exec.) and 44.4\% (match).
In Spider (14B) experiments, TS-GRPO increases training stability and speeds
convergence of the execution-match reward relative to GRPO and GSPO.

</details>


### [19] [DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems](https://arxiv.org/abs/2510.00229)
*Rohan Kadekodi,Zhan Jin,Keisuke Kamahori,Yile Gu,Sean Khatiri,Noah H. Bayindirli,Sergey Gorbunov,Baris Kasikci*

Main category: cs.AI

TL;DR: 提出了一种解耦微调方法和DualTune推理框架，将工具调用任务分解为工具选择和参数生成两个子任务，通过专用LoRA适配器提升本地LLM在工具调用场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 本地LLM在工具调用场景中表现不佳，需要隐私保护、成本效益高的解决方案，但现有模型在大规模工具选择和复杂参数生成方面存在困难。

Method: 采用解耦微调方法，使用LoRA微调创建专用适配器，分别处理工具选择和参数生成子任务，并实现分层编排来限制工具选择数量。

Result: 在MCP-Bench基准测试中，使用解耦微调的Qwen-2.5-7B模型将基础模型的工具调用准确率提升了46%，在大多数情况下优于2倍大小的模型。

Conclusion: 解耦微调和DualTune框架有效提升了本地LLM在工具调用任务中的性能，为设备端推理提供了高效解决方案。

Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has
revolutionized task automation, but the need for privacy-preserving,
cost-effective solutions demands on-device inference capabilities. However,
local LLMs consistently underperform compared to frontier models in tool
calling scenarios, struggling with both tool selection from large tool sets and
accurate argument generation for complex parameter structures. We introduce a
methodology that disaggregates a tool-calling task into two distinct subtasks:
tool selection and argument generation. We propose "decoupled fine-tuning", a
novel post-training approach that employs LoRA fine-tuning to create dedicated
LoRA adapters for tool selection and tool-specific argument generation using
separate loss masking for each of the subtasks. Furthermore, we present
DualTune, an inference framework that leverages the LoRA adapters created using
decoupled fine-tuning to perform efficient agent orchestration with the help of
local models on end-user devices. DualTune decomposes the tool-call generation
step into tool selection and argument generation, and dynamically loads the
corresponding LoRA adapters to generate tool calls. Additionally, DualTune
implements hierarchical orchestration to restrict the number of tools required
for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that
the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool
calling accuracy of the base model by 46%, and outperforms other local
reasoning, non-reasoning and fine-tuned models of similar size in all cases,
and models that are 2x larger, in most cases.

</details>


### [20] [MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning](https://arxiv.org/abs/2510.00274)
*Maisha Maliha,Dean Hougen*

Main category: cs.AI

TL;DR: 提出了MAGIC-MASK框架，将基于扰动的可解释性方法扩展到多智能体强化学习，通过智能体间协作共享掩码状态信息和经验，提高关键状态发现的效率和解释保真度。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习智能体决策过程的可解释性问题，特别是在安全关键和多智能体环境中。现有方法如StateMask存在计算成本高、探索覆盖不足和缺乏多智能体适应性等限制。

Method: 整合近端策略优化、自适应epsilon-greedy探索和轻量级智能体间协作，使智能体能够执行显著性引导的掩码操作并与同伴共享基于奖励的见解。基于轨迹扰动、奖励保真度分析和KL散度正则化的统一数学形式化。

Result: 在单智能体和多智能体基准测试（包括多智能体高速公路驾驶环境和Google Research Football）上验证，MAGIC-MASK在保真度、学习效率和策略鲁棒性方面持续优于最先进的基线方法。

Conclusion: MAGIC-MASK通过概率建模和多智能体马尔可夫决策过程为基础，提供了局部化、可解释的解释，同时提供了可转移的解释能力。

Abstract: Understanding the decision-making process of Deep Reinforcement Learning
agents remains a key challenge for deploying these systems in safety-critical
and multi-agent environments. While prior explainability methods like
StateMask, have advanced the identification of critical states, they remain
limited by computational cost, exploration coverage, and lack of adaptation to
multi-agent settings. To overcome these limitations, we propose a
mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent
Collaboration with Mask-Based Explainability for Reinforcement Learning), that
extends perturbation-based explanation to Multi-Agent Reinforcement Learning.
Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy
exploration, and lightweight inter-agent collaboration to share masked state
information and peer experience. This collaboration enables each agent to
perform saliency-guided masking and share reward-based insights with peers,
reducing the time required for critical state discovery, improving explanation
fidelity, and leading to faster and more robust learning. The core novelty of
our approach lies in generalizing explainability from single-agent to
multi-agent systems through a unified mathematical formalism built on
trajectory perturbation, reward fidelity analysis, and Kullback-Leibler
divergence regularization. This framework yields localized, interpretable
explanations grounded in probabilistic modeling and multi-agent Markov decision
processes. We validate our framework on both single-agent and multi-agent
benchmarks, including a multi-agent highway driving environment and Google
Research Football, demonstrating that MAGIC-MASK consistently outperforms
state-of-the-art baselines in fidelity, learning efficiency, and policy
robustness while offering interpretable and transferable explanations.

</details>


### [21] [ICL Optimized Fragility](https://arxiv.org/abs/2510.00300)
*Serena Gomez Wannaz*

Main category: cs.AI

TL;DR: ICL指导虽然能提升特定任务性能，但会损害跨领域推理能力，导致"优化脆弱性"现象——在简单任务上表现优异(91%-99%)，但在复杂推理问题上性能大幅下降(10-43%)。


<details>
  <summary>Details</summary>
Motivation: 探索ICL指导对跨领域认知能力的影响，特别是对推理灵活性的系统性影响。

Method: 使用GPT-OSS:20b模型的6个变体（1个基线+5种ICL配置），在840个测试中评估一般知识问题、逻辑谜题和数学奥赛题的推理能力。

Result: ICL模型在一般知识任务上达到91%-99%准确率，但在逻辑谜题上准确率降至10-43%（基线为43%）。数学奥赛题表现无显著差异(p=0.2173)。

Conclusion: ICL指导在效率和推理灵活性之间存在系统性权衡，对LLM部署和AI安全具有重要启示。

Abstract: ICL guides are known to improve task-specific performance, but their impact
on cross-domain cognitive abilities remains unexplored. This study examines how
ICL guides affect reasoning across different knowledge domains using six
variants of the GPT-OSS:20b model: one baseline model and five ICL
configurations (simple, chain-of-thought, random, appended text, and symbolic
language). The models were subjected to 840 tests spanning general knowledge
questions, logic riddles, and a mathematical olympiad problem. Statistical
analysis (ANOVA) revealed significant behavioral modifications (p less than
0.001) across ICL variants, demonstrating a phenomenon termed "optimized
fragility." ICL models achieved 91%-99% accuracy on general knowledge tasks
while showing degraded performance on complex reasoning problems, with accuracy
dropping to 10-43% on riddles compared to 43% for the baseline model. Notably,
no significant differences emerged on the olympiad problem (p=0.2173),
suggesting that complex mathematical reasoning remains unaffected by ICL
optimization. These findings indicate that ICL guides create systematic
trade-offs between efficiency and reasoning flexibility, with important
implications for LLM deployment and AI safety.

</details>


### [22] [BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models](https://arxiv.org/abs/2510.00307)
*Thierry Blankenstein,Jialin Yu,Zixuan Li,Vassilis Plachouras,Sunando Sengupta,Philip Torr,Yarin Gal,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 本文研究了LLM代理在工具选择中的偏见问题，提出了评估工具选择偏见的基准，发现模型存在不公平偏好，并提出轻量级缓解方法。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖外部工具市场，但工具选择如果存在系统性偏见，会降低用户体验并扭曲竞争，使某些提供商获得不公平优势。

Method: 构建包含多个功能等效工具的多样化工具类别基准，测试7个模型，进行控制实验分析工具特征、元数据和预训练暴露的影响，并提出了先过滤候选工具再均匀采样的缓解方法。

Result: 发现不公平性确实存在，模型要么固定选择单一提供商，要么不成比例地偏好上下文中较早列出的工具；语义对齐是最强预测因子，扰动描述会显著改变选择，重复预训练暴露会放大偏见。

Conclusion: 工具选择偏见是工具增强LLM公平部署的关键障碍，提出的轻量级缓解方法能在保持良好任务覆盖的同时减少偏见。

Abstract: Agents backed by large language models (LLMs) often rely on external tools
drawn from marketplaces where multiple providers offer functionally equivalent
options. This raises a critical point concerning fairness: if selection is
systematically biased, it can degrade user experience and distort competition
by privileging some providers over others. We introduce a benchmark of diverse
tool categories, each containing multiple functionally equivalent tools, to
evaluate tool-selection bias. Using this benchmark, we test seven models and
show that unfairness exists with models either fixating on a single provider or
disproportionately preferring earlier-listed tools in context. To investigate
the origins of this bias, we conduct controlled experiments examining tool
features, metadata (name, description, parameters), and pre-training exposure.
We find that: (1) semantic alignment between queries and metadata is the
strongest predictor of choice; (2) perturbing descriptions significantly shifts
selections; and (3) repeated pre-training exposure to a single endpoint
amplifies bias. Finally, we propose a lightweight mitigation that first filters
the candidate tools to a relevant subset and then samples uniformly, reducing
bias while preserving good task coverage. Our findings highlight tool-selection
bias as a key obstacle for the fair deployment of tool-augmented LLMs.

</details>


### [23] [When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets](https://arxiv.org/abs/2510.00332)
*Zeshi Dai,Zimo Peng,Zerui Cheng,Ryan Yihe Li*

Main category: cs.AI

TL;DR: CAIA基准测试揭示了AI在对抗性高风险环境中的严重缺陷：即使最先进的模型也无法在错误不可逆的对抗环境中有效运作，特别是在加密货币市场等存在主动欺骗的领域。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估主要关注受控环境下的任务完成能力，而现实世界部署需要AI能够抵御主动欺骗。加密货币市场在2024年因漏洞损失300亿美元，是测试AI对抗性鲁棒性的理想场景。

Method: 在加密货币市场测试平台上，对17个模型进行178个时间锚定任务的评估，要求AI区分真相与操纵、在碎片化信息环境中导航，并在对抗压力下做出不可逆的金融决策。

Result: 无工具时前沿模型准确率仅28%，工具增强后提升至67.4%，但仍低于80%的人类基准。模型存在系统性工具选择灾难，偏好不可靠的网页搜索而非权威数据源，即使正确答案可通过专业工具直接获取。

Conclusion: 当前模型尽管在推理得分上表现优异，但在需要抵御主动对抗的环境中仍存在根本性不足。对抗性鲁棒性是可信AI自主性的必要条件，CAIA基准为此提供了持续更新的评估框架。

Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:
the inability of state-of-the-art models to operate in adversarial, high-stakes
environments where misinformation is weaponized and errors are irreversible.
While existing benchmarks measure task completion in controlled settings,
real-world deployment demands resilience against active deception. Using crypto
markets as a testbed where $30 billion was lost to exploits in 2024, we
evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish
truth from manipulation, navigate fragmented information landscapes, and make
irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier
models achieve only 28% accuracy on tasks junior analysts routinely handle.
Tool augmentation improves performance but plateaus at 67.4% versus 80% human
baseline, despite unlimited access to professional resources. Most critically,
we uncover a systematic tool selection catastrophe: models preferentially
choose unreliable web search over authoritative data, falling for SEO-optimized
misinformation and social media manipulation. This behavior persists even when
correct answers are directly accessible through specialized tools, suggesting
foundational limitations rather than knowledge gaps. We also find that Pass@k
metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries,
e.g. cybersecurity, content moderation, etc. We release CAIA with contamination
controls and continuous updates, establishing adversarial robustness as a
necessary condition for trustworthy AI autonomy. The benchmark reveals that
current models, despite impressive reasoning scores, remain fundamentally
unprepared for environments where intelligence must survive active opposition.

</details>


### [24] [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355)
*Renee Ge,Qianli Liao,Tomaso Poggio*

Main category: cs.AI

TL;DR: 对层次推理模型进行批判性回顾，提出改进变体，在数独和迷宫任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: Transformer在逻辑推理方面表现不佳，可能是因为缺乏对潜在空间和循环推理等创造性应用的探索，需要深入研究新兴的层次推理模型

Method: 对层次推理模型进行批判性回顾，分析关键设计选择，提出改进变体

Result: 在Sudoku-Extreme和Maze-Hard任务上取得了比之前报道显著更好的性能

Conclusion: 研究结果提出了令人惊讶的观察和进一步研究的有趣方向

Abstract: Transformers have demonstrated remarkable performance in natural language
processing and related domains, as they largely focus on sequential,
autoregressive next-token prediction tasks. Yet, they struggle in logical
reasoning, not necessarily because of a fundamental limitation of these models,
but possibly due to the lack of exploration of more creative uses, such as
latent space and recurrent reasoning. An emerging exploration in this direction
is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a
novel type of recurrent reasoning in the latent space of transformers,
achieving remarkable performance on a wide range of 2D reasoning tasks. Despite
the promising results, this line of models is still at an early stage and calls
for in-depth investigation. In this work, we perform a critical review on this
class of models, examine key design choices and present intriguing variants
that achieve significantly better performance on the Sudoku-Extreme and
Maze-Hard tasks than previously reported. Our results also raise surprising
observations and intriguing directions for further research.

</details>


### [25] [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381)
*Kaiwen Yu,Mengying Sun,Zhijin Qin,Xiaodong Xu,Ping Yang,Yue Xiao,Gang Wu*

Main category: cs.AI

TL;DR: 提出语义驱动的AI智能体通信框架，包含语义自适应传输、语义轻量传输和语义自进化控制三大技术，旨在解决动态环境和有限资源下的智能体通信挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能服务增长，通信目标从人类转向AI智能体，需要新的通信范式来实现实时感知、决策和协作。语义通信虽然前景广阔，但在动态环境和有限资源下的实际部署仍受限制。

Method: 开发三种关键技术：1) 语义自适应传输 - 使用真实或生成样本进行微调以适应环境变化；2) 语义轻量传输 - 结合剪枝、量化和感知感知采样降低模型复杂度；3) 语义自进化控制 - 采用分布式分层决策优化多维资源。

Result: 仿真结果显示，所提方案实现了更快的收敛速度和更强的鲁棒性，分布式分层优化方法显著优于传统决策方案。

Conclusion: 该框架在AI智能体通信网络中具有重要潜力，能够有效支持动态环境下的多智能体协作。

Abstract: With the rapid growth of intelligent services, communication targets are
shifting from humans to artificial intelligent (AI) agents, which require new
paradigms to enable real-time perception, decision-making, and collaboration.
Semantic communication, which conveys task-relevant meaning rather than raw
data, offers a promising solution. However, its practical deployment remains
constrained by dynamic environments and limited resources. To address these
issues, this article proposes a semantic-driven AI agent communication
framework and develops three enabling techniques. First, semantic adaptation
transmission applies fine-tuning with real or generative samples to efficiently
adapt models to varying environments. Second, semantic lightweight transmission
incorporates pruning, quantization, and perception-aware sampling to reduce
model complexity and alleviate computational burden on edge agents. Third,
semantic self-evolution control employs distributed hierarchical
decision-making to optimize multi-dimensional resources, enabling robust
multi-agent collaboration in dynamic environments. Simulation results show that
the proposed solutions achieve faster convergence and stronger robustness,
while the proposed distributed hierarchical optimization method significantly
outperforms conventional decision-making schemes, highlighting its potential
for AI agent communication networks.

</details>


### [26] [Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm](https://arxiv.org/abs/2510.00415)
*Dadi Guo,Tianyi Zhou,Dongrui Liu,Chen Qian,Qihan Ren,Shuai Shao,Zhiyuan Fan,Yi R. Fung,Kun Wang,Linfeng Zhang,Jing Shao*

Main category: cs.AI

TL;DR: 提出了TRACE框架，通过让智能体自由探索和演化现有基准任务，自动生成更高难度的新任务并记录可验证的执行轨迹，解决现有智能体基准测试快速达到性能上限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准测试面临新开发智能体快速达到性能上限的问题，难以满足评估智能体能力的需求，需要更可持续和具有挑战性的评估系统。

Method: TRACE框架包含三个阶段：进化提案挖掘（通过初步探索和发散思维提供任务进化提案）、问题形成与自由探索（将提案概念化为可行问题候选，智能体自由探索并记录执行轨迹）、多级验证（确保演化任务具有可验证和可复现的轨迹）。

Result: 在GAIA基准上的实验表明，TRACE框架能够持续提升任务复杂度，同时通过可验证的执行轨迹提高正确性的可靠性。

Conclusion: 这项工作标志着从静态、人工策划的基准测试向动态、自演化评估系统的范式转变，为智能体发展提供了可持续且具有挑战性的跑道。

Abstract: Recent advances in large language models (LLMs) and agent system designs have
empowered agents with unprecedented levels of capability. However, existing
agent benchmarks are showing a trend of rapid ceiling-hitting by newly
developed agents, making it difficult to meet the demands for evaluating agent
abilities. To address this problem, we propose the Trajectory-based
Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)
framework. This framework takes an original task from an existing benchmark and
encourages agents to freely explore and evolve it into a new task with higher
difficulty while recording validatable agent trajectories. The framework
proceeds in three stages: (1) evolutionary proposal mining, which provides task
evolution proposals through preliminary exploration and divergent thinking; (2)
problem formation and free exploration, where proposals are conceptualized into
feasible problem candidates and the agents then explore them freely while
recording their execution trajectories; and (3) multi-level validation, which
ensures that the evolved tasks are accompanied by validatable and reproducible
trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE
framework consistently enhances task complexity while improving the reliability
of correctness through validatable execution trajectories. This work marks a
paradigm shift from static, manually curated benchmarks to dynamic,
self-evolving evaluation systems, providing a sustainable and challenging
runway for agent development.

</details>


### [27] [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.AI

TL;DR: 该研究探讨了自动化评估AI系统回答患者健康问题的可行性，通过系统研究评估方法，发现精心设计的自动化评估能够有效扩展AI系统的比较评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI回答患者健康问题的黄金标准——人工专家评审——劳动密集且缓慢，限制了可扩展性。自动化指标虽然前景广阔，但与人类判断的一致性不一且通常依赖于上下文。

Method: 在100个患者案例中，收集了28个AI系统的响应（共2800个），并沿三个维度评估：是否回答问题、是否适当使用临床笔记证据、是否使用一般医学知识。使用临床医生撰写的参考答案作为指标锚点。

Result: 自动化排名与专家评分高度匹配，表明精心设计的自动化评估能够有效扩展AI系统的比较评估。

Conclusion: 研究结果表明，精心设计的自动化评估可以扩展AI系统的比较评估，并支持患者-临床医生沟通。

Abstract: Automated approaches to answer patient-posed health questions are rising, but
selecting among systems requires reliable evaluation. The current gold standard
for evaluating the free-text artificial intelligence (AI) responses--human
expert review--is labor-intensive and slow, limiting scalability. Automated
metrics are promising yet variably aligned with human judgments and often
context-dependent. To address the feasibility of automating the evaluation of
AI responses to hospitalization-related questions posed by patients, we
conducted a large systematic study of evaluation approaches. Across 100 patient
cases, we collected responses from 28 AI systems (2800 total) and assessed them
along three dimensions: whether a system response (1) answers the question, (2)
appropriately uses clinical note evidence, and (3) uses general medical
knowledge. Using clinician-authored reference answers to anchor metrics,
automated rankings closely matched expert ratings. Our findings suggest that
carefully designed automated evaluation can scale comparative assessment of AI
systems and support patient-clinician communication.

</details>


### [28] [Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis](https://arxiv.org/abs/2510.00480)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 提出了Expandable Decision-Making States (EDMS)方法，通过语义丰富的状态表示和动作掩码方案，在足球等入侵性团队运动中构建可解释的玩家级智能体模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的分析方法直观但有限，现代机器学习模型缺乏明确的智能体表示和战术可解释性。需要从数据中构建玩家级智能体模型，使其学习到的价值和策略既具有战术可解释性，又能在异构数据源中保持鲁棒性。

Method: EDMS方法通过增强原始位置和速度信息，添加关系变量（如空间评分、传球和得分），并结合动作掩码方案，为持球和无球球员提供不同的决策集。

Result: 与基线相比，EDMS方法持续降低了动作预测损失和时间差分误差。定性案例研究和Q值可视化表明，EDMS能够突出高风险高回报的战术模式。

Conclusion: EDMS方法能够将学习到的价值函数和动作策略映射到人类可解释的战术概念，并与比赛规则保持一致，实现了跨数据源的可重复实验。

Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly
coupled state space as many players continuously interact on a shared field,
challenging quantitative tactical analysis. Traditional rule-based analyses are
intuitive, while modern predictive machine learning models often perform
pattern-matching without explicit agent representations. The problem we address
is how to build player-level agent models from data, whose learned values and
policies are both tactically interpretable and robust across heterogeneous data
sources. Here, we propose Expandable Decision-Making States (EDMS), a
semantically enriched state representation that augments raw positions and
velocities with relational variables (e.g., scoring of space, pass, and score),
combined with an action-masking scheme that gives on-ball and off-ball agents
distinct decision sets. Compared to prior work, EDMS maps learned value
functions and action policies to human-interpretable tactical concepts (e.g.,
marking pressure, passing lanes, ball accessibility) instead of raw coordinate
features, and aligns agent choices with the rules of play. In the experiments,
EDMS with action masking consistently reduced both action-prediction loss and
temporal-difference (TD) error compared to the baseline. Qualitative case
studies and Q-value visualizations further indicate that EDMS highlights
high-risk, high-reward tactical patterns (e.g., fast counterattacks and
defensive breakthroughs). We also integrated our approach into an open-source
library and demonstrated compatibility with multiple commercial and open
datasets, enabling cross-provider evaluation and reproducible experiments.

</details>


### [29] [Rethinking Reward Models for Multi-Domain Test-Time Scaling](https://arxiv.org/abs/2510.00492)
*Dong Bok Lee,Seanie Lee,Sangwoo Park,Minki Kang,Jinheon Baek,Dongki Kim,Dominik Wagner,Jiongdao Jin,Heejun Lee,Tobias Bocklet,Jinyu Wang,Jingjing Fu,Sung Ju Hwang,Jiang Bia,Lei Song*

Main category: cs.AI

TL;DR: 本文挑战了传统观点，发现在14个多样化领域中，生成式结果奖励模型(GenORM)表现最稳健，而非传统认为的过程奖励模型(PRM)更优。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为过程奖励模型(PRM)优于结果奖励模型(ORM)，但这种观点主要基于数学相关领域的证据。本文旨在在多样化领域中对四种奖励模型变体进行统一评估。

Method: 在14个多样化领域中统一评估了四种奖励模型变体：判别式ORM和PRM(DisORM, DisPRM)以及生成式ORM和PRM(GenORM, GenPRM)。

Result: 发现DisORM与DisPRM表现相当，GenPRM不具竞争力，而GenORM是最稳健的模型，在所有测试领域中都取得了显著且一致的提升。

Conclusion: 这些发现挑战了细粒度监督总是更好的普遍假设，支持在多领域部署中使用生成式结果验证。过程奖励模型的逐步评分会继承LLM自动标注的标签噪声，并且在评估长推理轨迹时存在困难。

Abstract: The reliability of large language models (LLMs) during test-time scaling is
often assessed with \emph{external verifiers} or \emph{reward models} that
distinguish correct reasoning from flawed logic. Prior work generally assumes
that process reward models (PRMs), which score every intermediate reasoning
step, outperform outcome reward models (ORMs) that assess only the final
answer. This view is based mainly on evidence from narrow, math-adjacent
domains. We present the first unified evaluation of four reward model variants,
discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM
(\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom,
we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not
competitive, and (iii) overall, \GenORM is the most robust, yielding
significant and consistent gains across every tested domain. We attribute this
to PRM-style stepwise scoring, which inherits label noise from LLM
auto-labeling and has difficulty evaluating long reasoning trajectories,
including those involving self-correcting reasoning. Our theoretical analysis
shows that step-wise aggregation compounds errors as reasoning length grows,
and our empirical observations confirm this effect. These findings challenge
the prevailing assumption that fine-grained supervision is always better and
support generative outcome verification for multi-domain deployment. We
publicly release our code, datasets, and checkpoints at
\href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}}
to facilitate future research in multi-domain settings.

</details>


### [30] [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/abs/2510.00523)
*Wei-Yao Wang,Kazuya Tateishi,Qiyu Wu,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.AI

TL;DR: 提出了VIRTUE模型，将视觉交互能力引入嵌入模型，通过分割模型处理视觉提示来精确定位图像特定区域，在36个通用任务和5个视觉交互任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型缺乏视觉交互能力，无法处理用户指定的感兴趣区域（如点、边界框、掩码），限制了其在需要局部接地用户意图的应用中的潜力。

Method: 结合分割模型和视觉语言模型，分割模型处理视觉提示以精确定位图像区域，使嵌入器能更精确处理复杂和模糊场景。

Result: 在36个通用MMEB任务上提升3.1%-8.5%，在5个视觉交互SCaR任务上提升15.2%-20.3%，均达到最先进水平。

Conclusion: VIRTUE成功将视觉交互能力扩展到表示学习领域，为嵌入模型解锁了新的应用场景，并能学习图像中的实体级信息来补充全局表示。

Abstract: Multimodal representation learning models have demonstrated successful
operation across complex tasks, and the integration of vision-language models
(VLMs) has further enabled embedding models with instruction-following
capabilities. However, existing embedding models lack visual-interactive
capabilities to specify regions of interest from users (e.g., point, bounding
box, mask), which have been explored in generative models to broaden their
human-interactive applicability. Equipping embedding models with visual
interactions not only would unlock new applications with localized grounding of
user intent, which remains unexplored, but also enable the models to learn
entity-level information within images to complement their global
representations for conventional embedding tasks. In this paper, we propose a
novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends
the capabilities of the segmentation model and the vision-language model to the
realm of representation learning. In VIRTUE, the segmentation model can process
visual prompts that pinpoint specific regions within an image, thereby enabling
the embedder to handle complex and ambiguous scenarios more precisely. To
evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale
Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples
that aims to retrieve the text caption by jointly considering the entity with a
specific object and image scene. VIRTUE consistently achieves a
state-of-the-art performance with significant improvements across 36 universal
MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

</details>


### [31] [Data Quality Challenges in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00552)
*Leopold Müller,Joshua Holstein,Sarah Bause,Gerhard Satzger,Niklas Kühl*

Main category: cs.AI

TL;DR: 本研究为RAG系统开发了15个数据质量维度，涵盖四个处理阶段，发现需要在前端阶段加强质量管理，并采用动态的质量管理方法。


<details>
  <summary>Details</summary>
Motivation: 当前的数据质量框架主要针对静态数据集，无法充分应对RAG系统的动态、多阶段特性，需要为这类基于AI的新系统开发专门的数据质量维度。

Method: 通过对领先IT服务公司的16名从业者进行半结构化访谈，通过定性内容分析归纳得出15个不同的数据质量维度。

Result: 研究识别出15个数据质量维度，分布在RAG系统的四个处理阶段：数据提取、数据转换、提示与搜索、生成。发现新维度主要集中在早期阶段，数据质量问题会在管道中转化和传播。

Conclusion: 需要在传统数据质量框架中添加新维度以覆盖RAG上下文，建议采用前端加载的质量管理策略和动态的、步骤感知的质量管理方法。

Abstract: Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to
enhance Large Language Models with enterprise-specific knowledge. However,
current data quality (DQ) frameworks have been primarily developed for static
datasets, and only inadequately address the dynamic, multi-stage nature of RAG
systems. This study aims to develop DQ dimensions for this new type of AI-based
systems. We conduct 16 semi-structured interviews with practitioners of leading
IT service companies. Through a qualitative content analysis, we inductively
derive 15 distinct DQ dimensions across the four processing stages of RAG
systems: data extraction, data transformation, prompt & search, and generation.
Our findings reveal that (1) new dimensions have to be added to traditional DQ
frameworks to also cover RAG contexts; (2) these new dimensions are
concentrated in early RAG steps, suggesting the need for front-loaded quality
management strategies, and (3) DQ issues transform and propagate through the
RAG pipeline, necessitating a dynamic, step-aware approach to quality
management.

</details>


### [32] [Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability](https://arxiv.org/abs/2510.00565)
*Shojiro Yamabe,Jun Sakuma*

Main category: cs.AI

TL;DR: 扩散语言模型(DLMs)存在由迭代去噪过程带来的关键安全漏洞，攻击者可以通过注入肯定令牌绕过安全防护。本文提出了一种针对DLMs的安全对齐方法，能有效缓解该漏洞。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过并行迭代去噪生成令牌，虽然能降低延迟并支持双向条件生成，但其迭代推理机制带来的安全风险尚未被充分理解。需要研究DLMs特有的安全漏洞及防护措施。

Method: 研究发现如果有害查询的肯定令牌出现在中间步骤，后续去噪过程会被引导生成有害响应。基于此分析，提出了一种专门针对DLMs的安全对齐方法，训练模型从包含肯定令牌的污染中间状态生成安全响应。

Result: 实验表明，所提方法能显著缓解该安全漏洞，且对任务性能影响最小。同时，该方法还能提高对传统越狱攻击的鲁棒性。

Conclusion: 扩散语言模型存在特有的安全漏洞，需要针对性的安全研究。提出的安全对齐方法能有效防护此类攻击，同时保持模型性能。

Abstract: Diffusion language models (DLMs) generate tokens in parallel through
iterative denoising, which can reduce latency and enable bidirectional
conditioning. However, the safety risks posed by jailbreak attacks that exploit
this inference mechanism are not well understood. In this paper, we reveal that
DLMs have a critical vulnerability stemming from their iterative denoising
process and propose a countermeasure. Specifically, our investigation shows
that if an affirmative token for a harmful query appears at an intermediate
step, subsequent denoising can be steered toward a harmful response even in
aligned models. As a result, simply injecting such affirmative tokens can
readily bypass the safety guardrails. Furthermore, we demonstrate that the
vulnerability allows existing optimization-based jailbreak attacks to succeed
on DLMs. Building on this analysis, we propose a novel safety alignment method
tailored to DLMs that trains models to generate safe responses from
contaminated intermediate states that contain affirmative tokens. Our
experiments indicate that the proposed method significantly mitigates the
vulnerability with minimal impact on task performance. Furthermore, our method
improves robustness against conventional jailbreak attacks. Our work
underscores the need for DLM-specific safety research.

</details>


### [33] [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
*Minki Kang,Wei-Ning Chen,Dongge Han,Huseyin A. Inan,Lukas Wutschitz,Yanzhi Chen,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 提出了Agent Context Optimization (ACON)框架，通过优化压缩指南来压缩环境观察和交互历史，减少内存使用26-54%同时保持任务性能，并能蒸馏到更小的压缩器中。


<details>
  <summary>Details</summary>
Motivation: LLM作为代理在动态环境中部署时，需要积累长历史记录导致上下文长度增加，这会提高成本并降低长视野任务的效率。现有上下文压缩方法主要关注单步任务或窄应用。

Method: ACON框架在自然语言空间中优化压缩指南：当完整上下文成功但压缩上下文失败时，让能力强的LLM分析失败原因并相应更新压缩指南。还将优化的LLM压缩器蒸馏到更小的模型中。

Result: 在AppWorld、OfficeBench和Multi-objective QA上的实验显示，ACON减少内存使用26-54%（峰值令牌），同时很大程度上保持任务性能，蒸馏到更小压缩器时保持95%以上准确率，并将较小LM作为长视野代理的性能提升高达46%。

Conclusion: ACON提供了一种有效的上下文压缩方法，能够在减少内存使用的同时保持代理性能，并能成功蒸馏到更小的模型中，为长视野代理任务提供了实用的解决方案。

Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement.

</details>


### [34] [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
*Rosni Vasu,Peter Jansen,Pao Siangliulue,Cristina Sarasua,Abraham Bernstein,Peter Clark,Bhavana Dalvi Mishra*

Main category: cs.AI

TL;DR: HARPA是一个自动化科学发现系统，通过文献挖掘识别研究趋势、探索假设设计空间，并基于先前实验结果学习奖励模型，生成可测试且基于科学文献的假设。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化科学发现工具难以生成既可测试又基于科学文献的假设，且无法适应先前实验结果的问题。

Method: 采用受人类研究人员启发的构思流程：文献挖掘识别新兴研究趋势，探索假设设计空间，通过定位研究空白和论证设计选择来收敛到精确可测试的假设，并学习基于先前实验结果的奖励模型。

Result: HARPA生成的假设驱动研究提案在可行性(+0.78)和基础性(+0.85)方面显著优于基线；与ASD代理(CodeScientist)测试时，成功执行更多(20 vs 11/40)，失败更少(16 vs 21/40)；奖励模型比未训练基线提升约28%。

Conclusion: 这些方法代表了AI驱动科学发现领域的重要进展，展示了如何生成更可行、更基础且能适应实验结果的科学假设。

Abstract: While there has been a surge of interest in automated scientific discovery
(ASD), especially with the emergence of LLMs, it remains challenging for tools
to generate hypotheses that are both testable and grounded in the scientific
literature. Additionally, existing ideation tools are not adaptive to prior
experimental outcomes. We developed HARPA to address these challenges by
incorporating the ideation workflow inspired by human researchers. HARPA first
identifies emerging research trends through literature mining, then explores
hypothesis design spaces, and finally converges on precise, testable hypotheses
by pinpointing research gaps and justifying design choices. Our evaluations
show that HARPA-generated hypothesis-driven research proposals perform
comparably to a strong baseline AI-researcher across most qualitative
dimensions (e.g., specificity, novelty, overall quality), but achieve
significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness
(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the
ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11
out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert
feasibility judgments track with actual execution success. Furthermore, to
simulate how researchers continuously refine their understanding of what
hypotheses are both testable and potentially interesting from experience, HARPA
learns a reward model that scores new hypotheses based on prior experimental
outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained
baseline scorer. Together, these methods represent a step forward in the field
of AI-driven scientific discovery.

</details>


### [35] [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](https://arxiv.org/abs/2510.00625)
*Wei Liu,Haomei Xu,Bingqing Liu,Zhiying Deng,Haozhao Wang,Jun Wang,Ruixuan Li,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 该论文揭示了当前模型编辑方法的脆弱性，发现其成功很大程度上是基于利用隐藏的捷径而非真正的语义理解，这从根本上挑战了模型编辑领域的可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型不可避免地编码了过时或不正确的知识，更新、删除和遗忘这些知识对于对齐、安全等问题很重要。模型编辑作为一种有前景的范式出现，但作者发现其可靠性建立在脆弱的基础上。

Method: 作者系统地开发了一套新的评估方法，特别设计了包含否定查询的负例评估框架，以揭示模型编辑方法是否真正基于语义理解。

Result: 令人震惊的是，研究发现即使是最先进的模型编辑方法在最简单的否定查询下也会崩溃，表明编辑很可能基于捷径而非完整的语义。

Conclusion: 模型编辑很可能基于捷径而非真正的语义理解，这要求在进一步推进模型编辑研究之前，需要重新考虑其基础前提。

Abstract: Large language models (LLMs) inevitably encode outdated or incorrect
knowledge. Updating, deleting, and forgetting such knowledge is important for
alignment, safety, and other issues. To address this issue, model editing has
emerged as a promising paradigm: by precisely editing a small subset of
parameters such that a specific fact is updated while preserving other
knowledge. Despite its great success reported in previous papers, we find the
apparent reliability of editing rests on a fragile foundation and the current
literature is largely driven by illusory success. The fundamental goal of
steering the model's output toward a target with minimal modification would
encourage exploiting hidden shortcuts, rather than utilizing real semantics.
This problem directly challenges the feasibility of the current model editing
literature at its very foundation, as shortcuts are inherently at odds with
robust knowledge integration. Coincidentally, this issue has long been obscured
by evaluation frameworks that lack the design of negative examples. To uncover
it, we systematically develop a suite of new evaluation methods. Strikingly, we
find that state-of-the-art approaches collapse even under the simplest negation
queries. Our empirical evidence shows that editing is likely to be based on
shortcuts rather than full semantics, calling for an urgent reconsideration of
the very basis of model editing before further advancements can be meaningfully
pursued.

</details>


### [36] [Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction](https://arxiv.org/abs/2510.00627)
*Bingzhang Wang,Kehua Chen,Yinhai Wang*

Main category: cs.AI

TL;DR: 提出CDDM方法，通过协作渐进蒸馏技术将大模型知识迁移到轻量学生模型，实现实时轻量轨迹预测，在保持高精度的同时大幅减少参数和采样步骤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在轨迹预测中表现出色，但模型规模大、采样慢，难以实际部署。需要开发既保持高性能又满足实时性要求的轻量模型。

Method: 基于协作渐进蒸馏(CPD)，逐步将教师扩散模型知识迁移到学生模型，同时减少采样步骤和模型规模。引入双信号正则化蒸馏损失，结合教师和真实数据指导。

Result: 在ETH-UCY和nuScenes基准测试中达到最先进精度，仅需231K参数和2-4采样步骤，实现161倍压缩、31倍加速和9ms延迟，保持96.2% ADE和95.5% FDE性能。

Conclusion: CDDM成功将高性能生成模型与实际部署约束结合，为自动驾驶和智能交通系统提供资源高效的轨迹预测方案。

Abstract: Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and
Intelligent Transportation Systems (ITS), supporting efficient motion planning
and real-time traffic safety management. Diffusion models have recently
demonstrated strong performance in probabilistic trajectory prediction, but
their large model size and slow sampling process hinder real-world deployment.
This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel
method for real-time and lightweight trajectory prediction. Built upon
Collaborative Progressive Distillation (CPD), CDDM progressively transfers
knowledge from a high-capacity teacher diffusion model to a lightweight student
model, jointly reducing both the number of sampling steps and the model size
across distillation iterations. A dual-signal regularized distillation loss is
further introduced to incorporate guidance from both the teacher and
ground-truth data, mitigating potential overfitting and ensuring robust
performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the
nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art
prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the
baseline model's ADE and FDE performance on pedestrian trajectories, while
requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x
compression, 31x acceleration, and 9 ms latency. Qualitative results further
show that CDDM generates diverse and accurate trajectories under dynamic agent
behaviors and complex social interactions. By bridging high-performing
generative models with practical deployment constraints, CDDM enables
resource-efficient probabilistic prediction for AVs and ITS. Code is available
at https://github.com/bingzhangw/CDDM.

</details>


### [37] [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
*Alessio Devoto,Maximilian Jeblick,Simon Jégou*

Main category: cs.AI

TL;DR: 提出Expected Attention方法，通过预测未来查询对KV对的关注度来估计其重要性，实现训练免费的KV缓存压缩，有效解决内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: KV缓存内存消耗是大型语言模型推理效率的主要瓶颈。现有基于注意力分数的KV缓存剪枝方法面临实际限制：未来token的注意力分数在压缩时不可用，且现代实现如Flash Attention不生成完整注意力矩阵。

Method: 利用LLM激活的分布特性，以闭式形式计算每个KV对的期望注意力分数，基于这些分数进行原则性排序和剪枝，对残差流影响最小。

Result: 该方法在预填充和解码阶段都能无缝运行，在两个场景中都持续优于最先进的基线方法。

Conclusion: 开发了KVPress库，包含20多种技术，支持研究人员实现和基准测试KV缓存压缩方法。

Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck
for efficient large language model inference. While attention-score-based KV
cache pruning shows promise, it faces critical practical limitations: attention
scores from future tokens are unavailable during compression, and modern
implementations like Flash Attention do not materialize the full attention
matrix, making past scores inaccessible. To overcome these challenges, we
introduce $\textbf{Expected Attention, a training-free compression method}$
that estimates KV pairs importance by predicting how future queries will attend
to them. Our approach leverages the distributional properties of LLM
activations to compute expected attention scores in closed form for each KV
pair. These scores enable principled ranking and pruning of KV pairs with
minimal impact on the residual stream, achieving effective compression without
performance degradation. Importantly, our method operates seamlessly across
both prefilling and decoding phases, consistently outperforming
state-of-the-art baselines in both scenarios. Finally, $\textbf{we release
KVPress, a comprehensive library to enable researchers to implement and
benchmark KV cache compression methods, already including more than 20
techniques}$.

</details>


### [38] [Batch-CAM: Introduction to better reasoning in convolutional deep learning models](https://arxiv.org/abs/2510.00664)
*Giacomo Ignesti,Davide Moroni,Massimo Martinelli*

Main category: cs.AI

TL;DR: 提出Batch-CAM训练范式，融合批量Grad-CAM算法和原型重建损失，提升模型对显著图像特征的关注，在提高分类准确率的同时改善图像重建质量并减少训练推理时间。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，深度学习模型的可解释性至关重要，准确解释与精确度同等重要。需要构建更透明、可解释和可信赖的AI系统。

Method: 结合批量实现的Grad-CAM算法和原型重建损失，引导模型关注显著图像特征，确保模型从证据相关信息中学习。

Result: Batch-CAM在分类任务中同时提高了准确率和图像重建质量，同时减少了训练和推理时间。

Conclusion: 该方法通过确保模型学习证据相关信息，为构建更透明、可解释和可信赖的AI系统做出了重要贡献。

Abstract: Understanding the inner workings of deep learning models is crucial for
advancing artificial intelligence, particularly in high-stakes fields such as
healthcare, where accurate explanations are as vital as precision. This paper
introduces Batch-CAM, a novel training paradigm that fuses a batch
implementation of the Grad-CAM algorithm with a prototypical reconstruction
loss. This combination guides the model to focus on salient image features,
thereby enhancing its performance across classification tasks. Our results
demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and
image reconstruction quality while reducing training and inference times. By
ensuring models learn from evidence-relevant information,this approach makes a
relevant contribution to building more transparent, explainable, and
trustworthy AI systems.

</details>


### [39] [Relevance-Zone Reduction in Game Solving](https://arxiv.org/abs/2510.00689)
*Chi-Huang Lin,Ting Han Wei,Chun-Jui Wang,Hung Guei,Chung-Chin Shih,Yun-Jui Tsai,I-Chen Wu,Ti-Rong Wu*

Main category: cs.AI

TL;DR: 提出了一种迭代式相关性区域(RZ)缩减方法，通过重复求解同一位置并逐步限制参与区域来获得更小的RZ，从而提高策略重用和剪枝效率。


<details>
  <summary>Details</summary>
Motivation: 游戏求解面临指数级增长的游戏树问题，相关性区域(RZ)技术虽然能显著减少搜索空间，但不同解会产生不同大小的RZ，较小的RZ更有利于重用和剪枝效率。

Method: 设计了三种约束生成策略，并集成了RZ模式表以充分利用过往解，通过迭代求解同一位置并逐步限制区域来引导求解器获得更小的RZ。

Result: 在7x7 Killall-Go实验中，平均RZ大小减少到原来的85.95%，缩减后的RZ可作为可重用知识存储。

Conclusion: 该方法能有效缩减RZ大小，缩减后的RZ可作为永久性可重用知识，特别适用于更大棋盘或不同开局情况。

Abstract: Game solving aims to find the optimal strategies for all players and
determine the theoretical outcome of a game. However, due to the exponential
growth of game trees, many games remain unsolved, even though methods like
AlphaZero have demonstrated super-human level in game playing. The
Relevance-Zone (RZ) is a local strategy reuse technique that restricts the
search to only the regions relevant to the outcome, significantly reducing the
search space. However, RZs are not unique. Different solutions may result in
RZs of varying sizes. Smaller RZs are generally more favorable, as they
increase the chance of reuse and improve pruning efficiency. To this end, we
propose an iterative RZ reduction method that repeatedly solves the same
position while gradually restricting the region involved, guiding the solver
toward smaller RZs. We design three constraint generation strategies and
integrate an RZ Pattern Table to fully leverage past solutions. In experiments
on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the
original. Furthermore, the reduced RZs can be permanently stored as reusable
knowledge for future solving tasks, especially for larger board sizes or
different openings.

</details>


### [40] [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](https://arxiv.org/abs/2510.00690)
*Yunhao Wang,Ziting Li,Shuai Chen,Tao Liu,Chao Song,Junjie Jiang,Jian Zhu,Peng Gao,Bin Qin*

Main category: cs.AI

TL;DR: 提出了自适应课程策略优化（ACPO）框架，通过动态课程和自适应裁剪机制改进视觉语言模型的强化学习对齐，在多项推理基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化算法（如PPO）在视觉语言模型对齐中存在静态训练计划和刚性裁剪机制的限制，阻碍了复杂推理任务的学习效果。

Method: ACPO包含两个核心组件：1）动态课程学习，从稳定的近策略探索逐步过渡到高效的离策略利用；2）优势感知自适应裁剪（AAAC），用动态的样本级裁剪边界替代固定裁剪参数。

Result: 在MathVista、LogicVista和MMMU-Pro等多模态推理基准测试中，ACPO显著优于DAPO和PAPO等基线方法，实现了最先进性能、加速收敛和优越的训练稳定性。

Conclusion: ACPO通过自适应学习策略有效解决了现有强化学习算法的局限性，为大规模视觉语言模型的复杂推理对齐提供了更高效和稳定的训练框架。

Abstract: Aligning large-scale vision-language models (VLMs) for complex reasoning via
reinforcement learning is often hampered by the limitations of existing policy
optimization algorithms, such as static training schedules and the rigid,
uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,
we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework
that addresses these challenges through a dual-component adaptive learning
strategy. First, ACPO employs a dynamic curriculum that orchestrates a
principled transition from a stable, near on-policy exploration phase to an
efficient, off-policy exploitation phase by progressively increasing sample
reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism
that replaces the fixed clipping hyperparameter with dynamic, sample-wise
bounds modulated by the normalized advantage of each token. This allows for
more granular and robust policy updates, enabling larger gradients for
high-potential samples while safeguarding against destructive ones. We conduct
extensive experiments on a suite of challenging multimodal reasoning
benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate
that ACPO consistently outperforms strong baselines such as DAPO and PAPO,
achieving state-of-the-art performance, accelerated convergence, and superior
training stability.

</details>


### [41] [AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment](https://arxiv.org/abs/2510.00706)
*Yusif Ibrahimov,Tarique Anwar,Tommy Yuan,Turan Mutallimov,Elgun Hasanov*

Main category: cs.AI

TL;DR: 提出AttentionDep模型，通过融合上下文和领域知识进行可解释的抑郁症严重程度检测，在社交媒体平台上优于现有方法5%以上。


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体平台作为了解个体心理状态的窗口，开发可信赖且透明的AI系统用于心理健康评估。

Method: 使用注意力机制分层编码帖子内容，结合心理健康知识图谱的领域知识，采用有序回归框架预测抑郁症严重程度。

Result: 在多个数据集上，AttentionDep在分级F1分数上比最先进基线方法高出5%以上，并提供可解释的预测洞察。

Conclusion: 该工作推动了基于社交媒体的心理健康评估中可信赖和透明AI系统的发展。

Abstract: In today's interconnected society, social media platforms provide a window
into individuals' thoughts, emotions, and mental states. This paper explores
the use of platforms like Facebook, X (formerly Twitter), and Reddit for
depression severity detection. We propose AttentionDep, a domain-aware
attention model that drives explainable depression severity estimation by
fusing contextual and domain knowledge. Posts are encoded hierarchically using
unigrams and bigrams, with attention mechanisms highlighting clinically
relevant tokens. Domain knowledge from a curated mental health knowledge graph
is incorporated through a cross-attention mechanism, enriching the contextual
features. Finally, depression severity is predicted using an ordinal regression
framework that respects the clinical-relevance and natural ordering of severity
levels. Our experiments demonstrate that AttentionDep outperforms
state-of-the-art baselines by over 5% in graded F1 score across datasets, while
providing interpretable insights into its predictions. This work advances the
development of trustworthy and transparent AI systems for mental health
assessment from social media.

</details>


### [42] [EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty](https://arxiv.org/abs/2510.00732)
*Yuchen Tian,Ruiyuan Huang,Xuanwu Wang,Jing Ma,Zengfeng Huang,Ziyang Luo,Hongzhan Lin,Da Zheng,Lun Du*

Main category: cs.AI

TL;DR: 提出了一种新颖的数据增强管道，从对称性和难度两个角度提升定理证明模型的鲁棒性，训练出的EvolProver模型在多个基准测试中创下新记录。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在形式化定理证明中缺乏泛化能力，对问题表述的微小变换也很脆弱，需要提升模型的鲁棒性。

Method: 从对称性角度提出EvolAST（基于抽象语法树）和EvolDomain（跨数学领域翻译）；从难度角度提出EvolDifficulty（进化指令生成不同难度定理）；使用增强数据训练7B参数的EvolProver定理证明器。

Result: EvolProver在FormalMATH-Lite达到53.8% pass@32，在MiniF2F-Test、Ineq-Comp-Seed和Ineq-Comp-Transformed等基准测试中创下非推理模型的新记录。

Conclusion: 数据增强管道有效提升了定理证明模型的鲁棒性，EvolProver在多个基准测试中表现出色，超越了同规模模型包括推理模型。

Abstract: Large Language Models (LLMs) for formal theorem proving have shown
significant promise, yet they often lack generalizability and are fragile to
even minor transformations of problem statements. To address this limitation,
we introduce a novel data augmentation pipeline designed to enhance model
robustness from two perspectives: symmetry and difficulty. From the symmetry
perspective, we propose two complementary methods: EvolAST, an Abstract Syntax
Tree (AST) based approach that targets syntactic symmetry to generate
semantically equivalent problem variants, and EvolDomain, which leverages LLMs
to address semantic symmetry by translating theorems across mathematical
domains. From the difficulty perspective, we propose EvolDifficulty, which uses
carefully designed evolutionary instructions to guide LLMs in generating new
theorems with a wider range of difficulty. We then use the evolved data to
train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver
establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%
pass@32 rate, surpassing all models of comparable size, including
reasoning-based models. It also sets new SOTA records for non-reasoning models
on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and
Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our
data augmentation pipeline's effectiveness across multiple benchmarks.

</details>


### [43] [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](https://arxiv.org/abs/2510.00778)
*Seunghoo Hong,Geonho Son,Juhun Lee,Simon S. Woo*

Main category: cs.AI

TL;DR: 提出了DDIM反演攻击(DIA)方法，通过攻击集成DDIM轨迹路径来有效破坏恶意图像编辑，在防御性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: DDIM反演技术虽然便于图像编辑，但也容易被恶意用于制作虚假内容。现有防御方法在迭代去噪轨迹上的目标不匹配导致防御效果有限。

Method: 提出DDIM反演攻击(DIA)，攻击集成DDIM轨迹路径，与迭代去噪过程更好地对齐。

Result: DIA在破坏各种编辑方法方面表现出色，超越了之前的防御方法。

Conclusion: 该框架为行业和研究社区提供了实用的防御方法，对抗AI的恶意使用。

Abstract: Diffusion models have shown to be strong representation learners, showcasing
state-of-the-art performance across multiple domains. Aside from accelerated
sampling, DDIM also enables the inversion of real images back to their latent
codes. A direct inheriting application of this inversion operation is real
image editing, where the inversion yields latent trajectories to be utilized
during the synthesis of the edited image. Unfortunately, this practical tool
has enabled malicious users to freely synthesize misinformative or deepfake
contents with greater ease, which promotes the spread of unethical and abusive,
as well as privacy-, and copyright-infringing contents. While defensive
algorithms such as AdvDM and Photoguard have been shown to disrupt the
diffusion process on these images, the misalignment between their objectives
and the iterative denoising trajectory at test time results in weak disruptive
performance.In this work, we present the DDIM Inversion Attack (DIA) that
attacks the integrated DDIM trajectory path. Our results support the effective
disruption, surpassing previous defensive methods across various editing
methods. We believe that our frameworks and results can provide practical
defense methods against the malicious use of AI for both the industry and the
research community. Our code is available here:
https://anonymous.4open.science/r/DIA-13419/.

</details>


### [44] [AI in data science education: experiences from the classroom](https://arxiv.org/abs/2510.00793)
*J. A. Hageman,C. F. W. Peeters*

Main category: cs.AI

TL;DR: 本研究探讨了AI（特别是像ChatGPT这样的大型语言模型）在教育环境中的整合，重点关注其对教学和学习的影响。研究发现AI工具可以简化任务并增强学习，但也存在学生过度依赖这些技术的问题，可能阻碍基本认知和问题解决能力的发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在教育领域的快速发展，了解AI工具（特别是LLMs）如何影响教学过程和学习成果变得至关重要。研究旨在识别AI整合带来的益处和挑战，为教育工作者提供指导。

Method: 通过对瓦赫宁根大学数据科学课程的课程协调员进行访谈，收集关于AI在课堂中使用情况的定性数据。

Result: 研究识别了AI在教育中的双重影响：一方面可以简化任务和增强学习体验，另一方面可能导致学生过度依赖技术，阻碍基本技能发展。同时强调了负责任使用AI、伦理考虑以及调整评估方法的重要性。

Conclusion: 通过谨慎整合，AI可以成为教育中的宝贵资产，但前提是它被用来补充而非替代基本学习过程。需要平衡技术使用与保持核心教育目标之间的关系。

Abstract: This study explores the integration of AI, particularly large language models
(LLMs) like ChatGPT, into educational settings, focusing on the implications
for teaching and learning. Through interviews with course coordinators from
data science courses at Wageningen University, this research identifies both
the benefits and challenges associated with AI in the classroom. While AI tools
can streamline tasks and enhance learning, concerns arise regarding students'
overreliance on these technologies, potentially hindering the development of
essential cognitive and problem solving skills. The study highlights the
importance of responsible AI usage, ethical considerations, and the need for
adapting assessment methods to ensure educational outcomes are met. With
careful integration, AI can be a valuable asset in education, provided it is
used to complement rather than replace fundamental learning processes.

</details>


### [45] [Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX](https://arxiv.org/abs/2510.00795)
*Anastasia Vepreva,Julia Razlivina,Maria Eremeeva,Nina Gubina,Anastasia Orlova,Aleksei Dmitrenko,Ksenya Kapranova,Susan Jyakhwo,Nikita Vasilev,Arsen Sarkisyan,Ivan Yu. Chernyshov,Vladimir Vinogradov,Andrei Dmitrenko*

Main category: cs.AI

TL;DR: 提出了ChemX数据集，包含10个手工整理并经领域专家验证的数据集，用于评估和改进化学信息提取方法。通过对比现有最先进的代理系统，发现化学信息提取仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 化学信息提取由于数据异质性而具有挑战性，现有代理方法在该领域表现有限，需要专门的基准数据集来推动该领域发展。

Method: 创建了10个手工整理的化学数据集，进行了广泛的基准测试，比较了ChatGPT Agent、化学专用提取代理等系统，并提出了自己的单代理方法控制文档预处理。

Result: 实证研究发现化学信息提取在领域特定术语、复杂表格和示意图处理以及上下文依赖的模糊性方面仍存在持续挑战。

Conclusion: ChemX基准是推进化学自动化信息提取的关键资源，挑战了现有方法的泛化能力，并为有效评估策略提供了宝贵见解。

Abstract: The emergence of agent-based systems represents a significant advancement in
artificial intelligence, with growing applications in automated data
extraction. However, chemical information extraction remains a formidable
challenge due to the inherent heterogeneity of chemical data. Current
agent-based approaches, both general-purpose and domain-specific, exhibit
limited performance in this domain. To address this gap, we present ChemX, a
comprehensive collection of 10 manually curated and domain-expert-validated
datasets focusing on nanomaterials and small molecules. These datasets are
designed to rigorously evaluate and enhance automated extraction methodologies
in chemistry. To demonstrate their utility, we conduct an extensive
benchmarking study comparing existing state-of-the-art agentic systems such as
ChatGPT Agent and chemical-specific data extraction agents. Additionally, we
introduce our own single-agent approach that enables precise control over
document preprocessing prior to extraction. We further evaluate the performance
of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their
capabilities with agentic approaches. Our empirical findings reveal persistent
challenges in chemical information extraction, particularly in processing
domain-specific terminology, complex tabular and schematic representations, and
context-dependent ambiguities. The ChemX benchmark serves as a critical
resource for advancing automated information extraction in chemistry,
challenging the generalization capabilities of existing methods, and providing
valuable insights into effective evaluation strategies.

</details>


### [46] [Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective](https://arxiv.org/abs/2510.00817)
*Nicholas Leisegang,Giovanni Casini,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文比较了加权知识库与c-表示两种处理不一致知识库的方法，证明了在特定条件下两者能生成相同的解释排序，并在语义层面建立了等价关系。


<details>
  <summary>Details</summary>
Motivation: 研究加权知识库和c-表示这两种处理不一致知识库的形式化方法之间的语义关系，探索它们在解释排序和推理上的等价性。

Method: 在语义层面比较两种方法：加权知识库通过为每个解释分配基于违反规则的成本，c-表示通过为违反条件分配惩罚值来排序解释。分析它们在生成解释排序和推理关系上的等价条件。

Result: 证明在特定条件下，加权知识库和一组可废止条件可以生成相同的解释排序，实现语义结构的相对成本等价。某些推理概念在两种形式化中具有等价表达性。

Conclusion: 两种方法在语义层面存在等价关系，这一发现有助于进一步研究基于成本的语义和c-表示方法，为处理不一致知识库提供理论支持。

Abstract: Weighted-knowledge bases and cost-based semantics represent a recent
formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in
the case where a given knowledge base is inconsistent. This is done by adding a
weight to each statement in the knowledge base (KB), and then giving each DL
interpretation a cost based on how often it breaks rules in the KB. In this
paper we compare this approach with c-representations, a form of non-monotonic
reasoning originally introduced by Kern-Isberner. c-Representations describe a
means to interpret defeasible concept inclusions in the first-order case. This
is done by assigning a numerical ranking to each interpretations via penalties
for each violated conditional. We compare these two approaches on a semantic
level. In particular, we show that under certain conditions a weighted
knowledge base and a set of defeasible conditionals can generate the same
ordering on interpretations, and therefore an equivalence of semantic
structures up to relative cost. Moreover, we compare entailment described in
both cases, where certain notions are equivalently expressible in both
formalisms. Our results have the potential to benefit further work on both
cost-based semantics and c-representations

</details>


### [47] [Logical Consistency Between Disagreeing Experts and Its Role in AI Safety](https://arxiv.org/abs/2510.00821)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 本文提出了一种无监督评估分类器的逻辑框架，通过分析分类器之间的一致性和分歧来推断其性能，无需真实标签。


<details>
  <summary>Details</summary>
Motivation: 当专家在测试中意见一致时，无法排除任何可能的评估结果；但当他们意见分歧时，可以确定至少有一个是错误的。这种一致性与分歧的不对称性激发了无监督评估方法的研究。

Method: 将分类器对齐决策的统计摘要作为输入，构建整数空间中的线性规划问题，包含逻辑约束和普遍适用的线性等式公理。

Result: 该方法能够构建无需知识的警报系统，检测LLMs作为评判者是否违反用户设定的最低评分阈值。

Conclusion: 仅通过逻辑一致性进行无监督评估具有实际应用价值，能够有效监控分类器性能而无需真实标签信息。

Abstract: If two experts disagree on a test, we may conclude both cannot be 100 per
cent correct. But if they completely agree, no possible evaluation can be
excluded. This asymmetry in the utility of agreements versus disagreements is
explored here by formalizing a logic of unsupervised evaluation for
classifiers. Its core problem is computing the set of group evaluations that
are logically consistent with how we observe them agreeing and disagreeing in
their decisions. Statistical summaries of their aligned decisions are inputs
into a Linear Programming problem in the integer space of possible correct or
incorrect responses given true labels. Obvious logical constraints, such as,
the number of correct responses cannot exceed the number of observed responses,
are inequalities. But in addition, there are axioms, universally applicable
linear equalities that apply to all finite tests. The practical and immediate
utility of this approach to unsupervised evaluation using only logical
consistency is demonstrated by building no-knowledge alarms that can detect
when one or more LLMs-as-Judges are violating a minimum grading threshold
specified by the user.

</details>


### [48] [Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection](https://arxiv.org/abs/2510.00831)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Pérez-Toro,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.AI

TL;DR: 本文首次对电力系统保护中的故障分类和故障定位进行了经典机器学习模型的比较基准研究，基于EMT数据评估模型在实时约束下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源特别是可再生能源的集成增加，电力系统保护面临重大挑战。基于固定阈值的传统保护方案无法在动态条件下可靠识别和定位短路故障，而机器学习提供了有前景的替代方案，但跨模型和设置的系统性基准研究仍然有限。

Method: 使用电压和电流波形，将其分割为10毫秒到50毫秒的滑动窗口，在现实的实时约束下评估经典机器学习模型。性能从准确性、对窗口大小的鲁棒性和运行时效率等方面进行评估。

Result: 性能最佳的故障分类模型实现了0.992±0.001的F1分数，而顶级故障定位模型达到了0.806±0.008的R2分数，平均处理时间为0.563毫秒。

Conclusion: 机器学习模型在电力系统故障分类和定位方面表现出色，能够满足实时处理要求，为复杂电网条件下的保护方案提供了有效解决方案。

Abstract: The increasing integration of distributed energy resources (DERs),
particularly renewables, poses significant challenges for power system
protection, with fault classification (FC) and fault localization (FL) being
among the most critical tasks. Conventional protection schemes, based on fixed
thresholds, cannot reliably identify and localize short circuits with the
increasing complexity of the grid under dynamic conditions. Machine learning
(ML) offers a promising alternative; however, systematic benchmarks across
models and settings remain limited. This work presents, for the first time, a
comparative benchmarking study of classical ML models for FC and FL in power
system protection based on EMT data. Using voltage and current waveforms
segmented into sliding windows of 10 ms to 50 ms, we evaluate models under
realistic real-time constraints. Performance is assessed in terms of accuracy,
robustness to window size, and runtime efficiency. The best-performing FC model
achieved an F1 score of 0.992$\pm$0.001, while the top FL model reached an R2
of 0.806$\pm$0.008 with a mean processing time of 0.563 ms.

</details>


### [49] [Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques](https://arxiv.org/abs/2510.00836)
*Jieun Yu,Minjung Park,Sangmi Chai*

Main category: cs.AI

TL;DR: 应用SMOTE技术解决加密货币市场中P&D操纵检测的类别不平衡问题，结合集成学习方法显著提高了检测性能，XGBoost和LightGBM表现最佳。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场中P&D操纵事件稀缺导致的类别不平衡问题严重影响了检测准确性，需要解决这一技术挑战。

Method: 应用SMOTE技术进行数据平衡处理，评估多种集成学习模型来区分操纵交易行为和正常市场活动。

Result: SMOTE显著提升了所有模型的P&D事件检测能力，XGBoost和LightGBM分别达到94.87%和93.59%的召回率，具有高F1分数和快速计算性能。

Conclusion: 数据平衡技术与集成方法结合能显著改善操纵活动的早期检测，有助于建立更公平、透明和稳定的加密货币市场。

Abstract: This study aims to detect pump and dump (P&D) manipulation in cryptocurrency
markets, where the scarcity of such events causes severe class imbalance and
hinders accurate detection. To address this issue, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, and advanced ensemble learning
models were evaluated to distinguish manipulative trading behavior from normal
market activity. The experimental results show that applying SMOTE greatly
enhanced the ability of all models to detect P&D events by increasing recall
and improving the overall balance between precision and recall. In particular,
XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,
respectively) with strong F1-scores and demonstrated fast computational
performance, making them suitable for near real time surveillance. These
findings indicate that integrating data balancing techniques with ensemble
methods significantly improves the early detection of manipulative activities,
contributing to a fairer, more transparent, and more stable cryptocurrency
market.

</details>


### [50] [Learning Compact Representations of LLM Abilities via Item Response Theory](https://arxiv.org/abs/2510.00844)
*Jianhao Chen,Chenxu Wang,Gengrui Zhang,Peng Ye,Lei Bai,Wei Hu,Yuzhong Qu,Shuyue Hu*

Main category: cs.AI

TL;DR: 提出一种基于项目反应理论的方法，学习大语言模型能力的紧凑表示，用于模型路由和性能预测


<details>
  <summary>Details</summary>
Motivation: 大语言模型数量激增，但如何有效管理和利用这些资源仍面临挑战，需要学习模型能力的紧凑表示来支持下游任务

Method: 受心理测量学中项目反应理论启发，将模型正确回答查询的概率建模为三个因素的函数：模型多技能能力向量、查询区分度向量和查询难度标量，使用混合专家网络联合学习这些参数

Result: 在模型路由和基准测试精度预测任务上达到最先进性能，学习到的参数编码了模型能力和查询特征的有意义、可解释信息

Conclusion: 该方法能有效学习大语言模型能力的紧凑表示，为模型管理和利用提供了实用工具

Abstract: Recent years have witnessed a surge in the number of large language models
(LLMs), yet efficiently managing and utilizing these vast resources remains a
significant challenge. In this work, we explore how to learn compact
representations of LLM abilities that can facilitate downstream tasks, such as
model routing and performance prediction on new benchmarks. We frame this
problem as estimating the probability that a given model will correctly answer
a specific query. Inspired by the item response theory (IRT) in psychometrics,
we model this probability as a function of three key factors: (i) the model's
multi-skill ability vector, (2) the query's discrimination vector that
separates models of differing skills, and (3) the query's difficulty scalar. To
learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network
that couples model- and query-level embeddings. Extensive experiments
demonstrate that our approach leads to state-of-the-art performance in both
model routing and benchmark accuracy prediction. Moreover, analysis validates
that the learned parameters encode meaningful, interpretable information about
model capabilities and query characteristics.

</details>


### [51] [Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery](https://arxiv.org/abs/2510.00876)
*Pietro Totis,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种基于蒙特卡洛树搜索(MCTS)的自动洞察和数据探索(AIDE)方法，用于解决从数据中自动发现知识的挑战。


<details>
  <summary>Details</summary>
Motivation: 组织收集了大量数据但难以转化为可操作知识，自动知识发现面临数据导航、模型构建和主观目标等复杂问题。

Method: 使用蒙特卡洛树搜索(MCTS)作为核心框架，自动识别数据转换和模型来发现有趣的数据模式。

Result: 在真实世界和合成数据上的评估表明，AIDE能有效识别数据转换和模型，发现有趣的数据模式。

Conclusion: AIDE为自动知识发现提供了可扩展的坚实基础，未来可集成更多模式提取策略和领域知识。

Abstract: Organizations are increasingly focused on leveraging data from their
processes to gain insights and drive decision-making. However, converting this
data into actionable knowledge remains a difficult and time-consuming task.
There is often a gap between the volume of data collected and the ability to
process and understand it, which automated knowledge discovery aims to fill.
Automated knowledge discovery involves complex open problems, including
effectively navigating data, building models to extract implicit relationships,
and considering subjective goals and knowledge. In this paper, we introduce a
novel method for Automated Insights and Data Exploration (AIDE), that serves as
a robust foundation for tackling these challenges through the use of Monte
Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic
data, demonstrating its effectiveness in identifying data transformations and
models that uncover interesting data patterns. Among its strengths, AIDE's
MCTS-based framework offers significant extensibility, allowing for future
integration of additional pattern extraction strategies and domain knowledge.
This makes AIDE a valuable step towards developing a comprehensive solution for
automated knowledge discovery.

</details>


### [52] [FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs](https://arxiv.org/abs/2510.00894)
*Ran Liu,Yuan Fang,Xiaoli Li*

Main category: cs.AI

TL;DR: 提出FusionAdapter方法用于多模态知识图谱中的少样本关系学习，通过适配器模块和融合策略有效整合多模态信息，在低资源场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MMKG方法主要将多模态对齐到共享空间，忽略了特定模态的独特贡献，在低资源设置下性能受限。

Method: 引入适配器模块使各模态能高效适应未见关系，并提出融合策略在整合多模态实体表示时保留模态特定特征。

Result: 在两个基准MMKG数据集上的广泛实验表明，FusionAdapter优于最先进方法。

Conclusion: 通过有效适应和融合多模态信息，FusionAdapter能以最少监督提升对新颖关系的泛化能力。

Abstract: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including
text and images, to enhance entity and relation representations. Notably,
different modalities for the same entity often present complementary and
diverse information. However, existing MMKG methods primarily align modalities
into a shared space, which tends to overlook the distinct contributions of
specific modalities, limiting their performance particularly in low-resource
settings. To address this challenge, we propose FusionAdapter for the learning
of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an
adapter module that enables efficient adaptation of each modality to unseen
relations and (2) a fusion strategy that integrates multimodal entity
representations while preserving diverse modality-specific characteristics. By
effectively adapting and fusing information from diverse modalities,
FusionAdapter improves generalization to novel relations with minimal
supervision. Extensive experiments on two benchmark MMKG datasets demonstrate
that FusionAdapter achieves superior performance over state-of-the-art methods.

</details>


### [53] [On Discovering Algorithms for Adversarial Imitation Learning](https://arxiv.org/abs/2510.00922)
*Shashank Reddy Chirra,Jayden Teoh,Praveen Paruchuri,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 本文提出了DAIL（Discovered Adversarial Imitation Learning），这是第一个通过元学习发现的对抗模仿学习算法，通过LLM引导的进化框架自动发现奖励分配函数，显著提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习（AIL）方法虽然有效，但通常被认为不稳定。现有研究主要关注密度比估计的改进，而奖励分配函数在影响训练动态和最终策略性能方面的作用被忽视。这些函数通常基于人类设计的散度最小化目标，缺乏数据驱动的方法。

Method: 采用LLM引导的进化框架，在奖励分配函数空间中进行高效探索，自动发现基于模仿策略性能的数据驱动奖励分配函数，从而开发出DAIL算法。

Result: DAIL在未见过的环境和策略优化算法中表现出良好的泛化能力，超越了当前最先进的人类设计基线方法，同时提供了更稳定的训练过程。

Conclusion: DAIL不仅提升了对抗模仿学习的性能，还通过分析揭示了奖励分配函数在AIL稳定性中的关键作用，为未来研究提供了新的见解。

Abstract: Adversarial Imitation Learning (AIL) methods, while effective in settings
with limited expert demonstrations, are often considered unstable. These
approaches typically decompose into two components: Density Ratio (DR)
estimation $\frac{\rho_E}{\rho_{\pi}}$, where a discriminator estimates the
relative occupancy of state-action pairs under the policy versus the expert;
and Reward Assignment (RA), where this ratio is transformed into a reward
signal used to train the policy. While significant research has focused on
improving density estimation, the role of reward assignment in influencing
training dynamics and final policy performance has been largely overlooked. RA
functions in AIL are typically derived from divergence minimization objectives,
relying heavily on human design and ingenuity. In this work, we take a
different approach: we investigate the discovery of data-driven RA functions,
i.e, based directly on the performance of the resulting imitation policy. To
this end, we leverage an LLM-guided evolutionary framework that efficiently
explores the space of RA functions, yielding \emph{Discovered Adversarial
Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,
DAIL generalises across unseen environments and policy optimization algorithms,
outperforming the current state-of-the-art of \emph{human-designed} baselines.
Finally, we analyse why DAIL leads to more stable training, offering novel
insights into the role of RA functions in the stability of AIL. Code is
publicly available: https://github.com/shshnkreddy/DAIL.

</details>


### [54] [Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2510.00958)
*Yoonju Sim,Hyeonah Kim,Changhyun Kwon*

Main category: cs.AI

TL;DR: 提出一种增强训练模型推理性能的新方法，通过随机性测试时搜索来改进CVRP中切割平面的生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的分离方法生成的切割数量不足，因为模型对生成多样化子集的敏感性不够。

Method: 在图形粗化过程中引入随机边选择，并提出GraphCHiP算法利用粗化历史识别RCIs和FCIs。

Result: 在随机生成的CVRP实例上，该方法相比现有神经分离方法有效减少了对偶间隙，并成功识别出有效的FCIs。

Conclusion: 测试时搜索与随机性结合能显著提升训练模型的性能，特别是在识别复杂切割不等式方面。

Abstract: The identification of valid inequalities, such as the rounded capacity
inequalities (RCIs), is a key component of cutting plane methods for the
Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based
separation method can learn to find high-quality cuts, our analysis reveals
that the model produces fewer cuts than expected because it is insufficiently
sensitive to generate a diverse set of generated subsets. This paper proposes
an alternative: enhancing the performance of a trained model at inference time
through a new test-time search with stochasticity. First, we introduce
stochastic edge selection into the graph coarsening procedure, replacing the
previously proposed greedy approach. Second, we propose the Graph Coarsening
History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening
history to identify not only RCIs but also, for the first time, the Framed
capacity inequalities (FCIs). Experiments on randomly generated CVRP instances
demonstrate the effectiveness of our approach in reducing the dual gap compared
to the existing neural separation method. Additionally, our method discovers
effective FCIs on a specific instance, despite the challenging nature of
identifying such cuts.

</details>


### [55] [QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL](https://arxiv.org/abs/2510.00967)
*Cong Yu,Valter Uotila,Shilong Deng,Qingyuan Wu,Tuo Shi,Songlin Jiang,Lei You,Bo Zhao*

Main category: cs.AI

TL;DR: QUASAR是一个基于工具增强大语言模型的强化学习框架，用于生成和优化量子电路，解决了量子门参数优化和LLM缺乏量子领域知识的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的量子电路生成方法存在两个主要挑战：(i)参数化量子门需要精确数值值以获得最佳性能；(ii)LLM由于缺乏量子领域特定知识，经常生成低质量或不正确的量子电路。

Method: 提出QUASAR框架，采用工具增强的LLM和强化学习，设计了量子电路验证方法和分层奖励机制，通过外部量子模拟器验证电路并改进训练过程。

Result: 在4B参数的LLM上，QUASAR在Pass@1中达到99.31%的有效性，在Pass@10中达到100%，优于GPT-4o、GPT-5、DeepSeek-V3等工业级LLM以及仅使用监督微调或强化学习的基线方法。

Conclusion: QUASAR通过结合工具增强的LLM和强化学习，显著提高了生成量子电路的语法和语义性能，为自动化量子电路设计和优化提供了有效解决方案。

Abstract: Designing and optimizing task-specific quantum circuits are crucial to
leverage the advantage of quantum computing. Recent large language model
(LLM)-based quantum circuit generation has emerged as a promising automatic
solution. However, the fundamental challenges remain unaddressed: (i)
parameterized quantum gates require precise numerical values for optimal
performance, which also depend on multiple aspects, including the number of
quantum gates, their parameters, and the layout/depth of the circuits. (ii)
LLMs often generate low-quality or incorrect quantum circuits due to the lack
of quantum domain-specific knowledge. We propose QUASAR, an agentic
reinforcement learning (RL) framework for quantum circuits generation and
optimization based on tool-augmented LLMs. To align the LLM with
quantum-specific knowledge and improve the generated quantum circuits, QUASAR
designs (i) a quantum circuit verification approach with external quantum
simulators and (ii) a sophisticated hierarchical reward mechanism in RL
training. Extensive evaluation shows improvements in both syntax and semantic
performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR
has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,
outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several
supervised-fine-tuning (SFT)-only and RL-only baselines.

</details>


### [56] [Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation](https://arxiv.org/abs/2510.00976)
*Aueaphum Aueawatthanaphisut*

Main category: cs.AI

TL;DR: 提出AFFR框架，通过元学习联邦优化、能量感知客户端调度和安全聚合，解决罕见病诊断中的数据稀缺、设备掉线和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据极度稀缺、隐私担忧和边缘设备资源有限等挑战，需要一种能在真实临床网络中部署的解决方案。

Method: 整合三个支柱：基于元学习的少样本联邦优化、能量感知客户端调度、以及带校准差分隐私的安全聚合。

Result: 在模拟罕见病检测数据集上，相比基线联邦学习准确率提升10%，客户端掉线率降低50%以上，隐私-效用权衡在临床可接受范围内。

Conclusion: AFFR为罕见病的公平可信联邦诊断提供了实用路径。

Abstract: Rare-disease diagnosis remains one of the most pressing challenges in digital
health, hindered by extreme data scarcity, privacy concerns, and the limited
resources of edge devices. This paper proposes the Adaptive Federated Few-Shot
Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)
few-shot federated optimization with meta-learning to generalize from limited
patient samples, (ii) energy-aware client scheduling to mitigate device
dropouts and ensure balanced participation, and (iii) secure aggregation with
calibrated differential privacy to safeguard sensitive model updates. Unlike
prior work that addresses these aspects in isolation, AFFR unifies them into a
modular pipeline deployable on real-world clinical networks. Experimental
evaluation on simulated rare-disease detection datasets demonstrates up to 10%
improvement in accuracy compared with baseline FL, while reducing client
dropouts by over 50% without degrading convergence. Furthermore,
privacy-utility trade-offs remain within clinically acceptable bounds. These
findings highlight AFFR as a practical pathway for equitable and trustworthy
federated diagnosis of rare conditions.

</details>


### [57] [Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer](https://arxiv.org/abs/2510.01006)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 本文提出了一个售后需求预测和监控的实用架构，该架构将统计、机器学习和深度学习模型与基于角色的分析层相结合，用于评分卡和趋势诊断。该框架处理外生信号，将COVID-19视为特殊制度，生成带有校准区间的国家-零件预测。


<details>
  <summary>Details</summary>
Motivation: 解决售后需求预测中的复杂性，包括处理多个外生信号、COVID-19影响、高收入项目与长尾项目的不同预测需求，以及为决策者提供可操作的洞察。

Method: 使用帕累托感知分割对高收入项目单独预测，对长尾项目通过聚类池化预测；采用水平感知集成，根据业务相关损失调整权重；嵌入LLM生成角色感知叙述和执行质量检查。

Result: 系统在90多个国家和约6000个零件上实现了从预测到库存决策的闭环，提供了可复现的工作流程和AI生成的叙述。

Conclusion: 该架构成功地将预测、监控和库存决策连接起来，使规划者能够从关注当前准确性转向关注准确性趋势和可操作的杠杆，提高了售后需求管理的效率和效果。

Abstract: This paper presents a practical architecture for after-sales demand
forecasting and monitoring that unifies a revenue- and cluster-aware ensemble
of statistical, machine-learning, and deep-learning models with a role-driven
analytics layer for scorecards and trend diagnostics. The framework ingests
exogenous signals (installed base, pricing, macro indicators, life cycle,
seasonality) and treats COVID-19 as a distinct regime, producing country-part
forecasts with calibrated intervals. A Pareto-aware segmentation forecasts
high-revenue items individually and pools the long tail via clusters, while
horizon-aware ensembling aligns weights with business-relevant losses (e.g.,
WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused
insights: accuracy within tolerance thresholds by revenue share and count, bias
decomposition (over- vs under-forecast), geographic and product-family
hotspots, and ranked root causes tied to high-impact part-country pairs. A
trend module tracks trajectories of MAPE/WMAPE and bias across recent months,
flags entities that are improving or deteriorating, detects change points
aligned with known regimes, and attributes movements to lifecycle and seasonal
factors. LLMs are embedded in the analytics layer to generate role-aware
narratives and enforce reporting contracts. They standardize business
definitions, automate quality checks and reconciliations, and translate
quantitative results into concise, explainable summaries for planners and
executives. The system exposes a reproducible workflow -- request
specification, model execution, database-backed artifacts, and AI-generated
narratives -- so planners can move from "How accurate are we now?" to "Where is
accuracy heading and which levers should we pull?", closing the loop between
forecasting, monitoring, and inventory decisions across more than 90 countries
and about 6,000 parts.

</details>


### [58] [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
*Federico Tiblias,Irina Bigoulaeva,Jingcheng Niu,Simone Balloccu,Iryna Gurevych*

Main category: cs.AI

TL;DR: 提出了SMDS方法来自动发现语言模型中的特征流形，发现不同特征形成各种几何结构（如圆形、直线、簇），这些结构反映了概念属性、跨模型稳定、支持推理并随上下文动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于发现特定特征的特定几何结构，缺乏通用性。需要一种模型无关的方法来自动发现特征流形。

Method: 引入监督多维缩放(SMDS)，一种模型无关的方法，以时间推理为案例研究应用该方法。

Result: 发现不同特征形成各种几何结构，这些结构一致反映概念属性、跨模型家族和尺寸稳定、主动支持模型推理、并随上下文变化动态重塑。

Conclusion: 研究结果阐明了特征流形的功能作用，支持基于实体的推理模型，其中语言模型编码和转换结构化表示。

Abstract: The linear representation hypothesis states that language models (LMs) encode
concepts as directions in their latent space, forming organized,
multidimensional manifolds. Prior efforts focus on discovering specific
geometries for specific features, and thus lack generalization. We introduce
Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to
automatically discover feature manifolds. We apply SMDS to temporal reasoning
as a case study, finding that different features form various geometric
structures such as circles, lines, and clusters. SMDS reveals many insights on
these structures: they consistently reflect the properties of the concepts they
represent; are stable across model families and sizes; actively support
reasoning in models; and dynamically reshape in response to context changes.
Together, our findings shed light on the functional role of feature manifolds,
supporting a model of entity-based reasoning in which LMs encode and transform
structured representations.

</details>


### [59] [Uncovering the Computational Ingredients of Human-Like Representations in LLMs](https://arxiv.org/abs/2510.01030)
*Zach Studdiford,Timothy T. Rogers,Kushin Mukherjee,Siddharth Suresh*

Main category: cs.AI

TL;DR: 该研究评估了70多个不同架构的LLM在概念表示方面与人类的对齐程度，发现指令微调和注意力头维度是影响对齐的关键因素，而多模态预训练和参数规模影响有限。现有基准测试无法充分捕捉人类-AI对齐度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试不适合衡量人类与模型之间的表示对齐，无法可靠评估LLM是否朝着有用认知模型的方向发展。需要确定哪些计算要素对构建具有人类类似表示的模型最为关键。

Method: 使用认知科学中成熟的三元组相似性任务，基于THINGS数据库的概念，评估70多个不同计算要素（架构、微调方法、训练数据等）的模型，比较人类和模型的表示对齐度。

Result: 经过指令微调且具有较大注意力头维度的模型与人类表示最为对齐；多模态预训练和参数规模对对齐度影响有限；现有基准测试中MMLU比MUSR更能捕捉表示对齐，但都无法完全解释对齐度方差。

Conclusion: 指令微调和注意力头维度是推进LLM成为人类概念表示模型的关键计算要素，当前基准测试存在重大缺陷，需要开发更好的对齐度评估方法。

Abstract: The ability to translate diverse patterns of inputs into structured patterns
of behavior has been thought to rest on both humans' and machines' ability to
learn robust representations of relevant concepts. The rapid advancement of
transformer-based large language models (LLMs) has led to a diversity of
computational ingredients -- architectures, fine tuning methods, and training
datasets among others -- but it remains unclear which of these ingredients are
most crucial for building models that develop human-like representations.
Further, most current LLM benchmarks are not suited to measuring
representational alignment between humans and models, making benchmark scores
unreliable for assessing if current LLMs are making progress towards becoming
useful cognitive models. We address these limitations by first evaluating a set
of over 70 models that widely vary in their computational ingredients on a
triplet similarity task, a method well established in the cognitive sciences
for measuring human conceptual representations, using concepts from the THINGS
database. Comparing human and model representations, we find that models that
undergo instruction-finetuning and which have larger dimensionality of
attention heads are among the most human aligned, while multimodal pretraining
and parameter size have limited bearing on alignment. Correlations between
alignment scores and scores on existing benchmarks reveal that while some
benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for
capturing representational alignment, no existing benchmark is capable of fully
accounting for the variance of alignment scores, demonstrating their
insufficiency in capturing human-AI alignment. Taken together, our findings
help highlight the computational ingredients most essential for advancing LLMs
towards models of human conceptual representation and address a key
benchmarking gap in LLM evaluation.

</details>


### [60] [Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI](https://arxiv.org/abs/2510.01038)
*Akchunya Chanchal,David A. Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 提出了一种新的前向传播范式Activation-Deactivation (AD)，通过关闭模型中与遮挡部分对应的部分来消除遮挡输入特征对模型决策的影响，解决了传统黑盒解释方法依赖遮挡导致分布外图像的问题。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒解释方法依赖遮挡输入部分生成突变体，这会导致分布外图像，影响解释质量。此外，选择合适的遮挡值通常需要领域知识。

Method: 引入ConvAD机制，可以轻松添加到任何训练好的卷积神经网络中，实现AD范式。该方法无需额外训练或微调，通过关闭模型中与遮挡部分对应的部分来消除遮挡输入特征的影响。

Result: 实验评估显示，AD解释在多个数据集和模型架构上相比传统遮挡方法，鲁棒性提高了高达62.5%，且无需领域知识就能提取更鲁棒的解释。

Conclusion: ConvAD机制能够在不改变网络决策过程的情况下，提供更鲁棒的解释，解决了传统遮挡方法的问题。

Abstract: Black-box explainability methods are popular tools for explaining the
decisions of image classifiers. A major drawback of these tools is their
reliance on mutants obtained by occluding parts of the input, leading to
out-of-distribution images. This raises doubts about the quality of the
explanations. Moreover, choosing an appropriate occlusion value often requires
domain knowledge. In this paper we introduce a novel forward-pass paradigm
Activation-Deactivation (AD), which removes the effects of occluded input
features from the model's decision-making by switching off the parts of the
model that correspond to the occlusions. We introduce ConvAD, a drop-in
mechanism that can be easily added to any trained Convolutional Neural Network
(CNN), and which implements the AD paradigm. This leads to more robust
explanations without any additional training or fine-tuning. We prove that the
ConvAD mechanism does not change the decision-making process of the network. We
provide experimental evaluation across several datasets and model
architectures. We compare the quality of AD-explanations with explanations
achieved using a set of masking values, using the proxies of robustness, size,
and confidence drop-off. We observe a consistent improvement in robustness of
AD explanations (up to 62.5%) compared to explanations obtained with
occlusions, demonstrating that ConvAD extracts more robust explanations without
the need for domain knowledge.

</details>


### [61] [Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning](https://arxiv.org/abs/2510.01069)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出基于Curry-Howard对应的理论框架，将CoT推理轨迹映射为形式化类型证明结构，以验证推理的忠实性。


<details>
  <summary>Details</summary>
Motivation: 解决CoT提示生成的推理轨迹的忠实性问题，提升模型可解释性和可靠性。

Method: 利用Curry-Howard对应关系，将自然语言的CoT步骤映射为形式化的类型证明结构，通过类型检查验证推理忠实性。

Result: 成功将CoT轨迹转换为良好类型的证明，提供了可验证的计算忠实性证书。

Conclusion: 该框架为将可信的叙述性解释转化为可形式验证的程序提供了方法学，有助于构建更可靠和可信的AI系统。

Abstract: While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of
large language models, the faithfulness of the generated rationales remains an
open problem for model interpretability. We propose a novel theoretical lens
for this problem grounded in the Curry-Howard correspondence, which posits a
direct relationship between formal proofs and computer programs. Under this
paradigm, a faithful reasoning trace is analogous to a well-typed program,
where each intermediate step corresponds to a typed logical inference. We
operationalise this analogy, presenting methods to extract and map the
informal, natural language steps of CoT into a formal, typed proof structure.
Successfully converting a CoT trace into a well-typed proof serves as a strong,
verifiable certificate of its computational faithfulness, moving beyond
heuristic interpretability towards formal verification. Our framework provides
a methodology to transform plausible narrative explanations into formally
verifiable programs, offering a path towards building more reliable and
trustworthy AI systems.

</details>


### [62] [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088)
*Guobin Shen,Dongcheng Zhao,Haibo Tong,Jindong Li,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: SIRL利用LLM内部的安全信念，将高置信度拒绝行为转化为自生成奖励信号，无需外部验证器即可实现有效对齐。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏通用标准和可靠的内容验证器，确保LLM安全具有挑战性。研究发现对齐模型已具备内部安全信念，但这一信号未被充分利用。

Method: 提出SIRL方法，将模型内部的安全置信度转化为自生成奖励信号，通过强化低熵拒绝行为来教导模型信任自身安全本能。

Result: 在Llama和Qwen模型上，SIRL对20+种越狱方法保持89%+的防御成功率，仅使用15,000个未标注提示就超越了资源密集的监督方法。

Conclusion: 有效对齐可以从模型内部产生，为无需大量人工监督的自主、鲁棒AI安全机制开辟了道路。

Abstract: Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically "know" when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.

</details>


### [63] [Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation](https://arxiv.org/abs/2510.01094)
*Alexander Nasuta,Alessandro Cisi,Sylwia Olbrych,Gustavo Vieira,Rui Fernandes,Lucas Paletta,Marlene Mayr,Rishyank Chevuri,Robert Woitsch,Hans Aoyang Zhou,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.AI

TL;DR: 提出一个两层人本生产规划框架，结合约束规划和强化学习优化工业制造中的运营效率和劳动力公平性。


<details>
  <summary>Details</summary>
Motivation: 解决工业制造中同时优化运营效率和劳动力公平性的挑战，将人类因素如工人偏好、经验、韧性和医疗约束整合到生产规划中。

Method: 第一层使用约束编程(CP)进行订单-产线分配，第二层使用马尔可夫决策过程(MDP)进行工人-产线分配，比较了贪婪分配、MCTS和RL三种策略。

Result: CP调度产生紧凑可行的生产计划，MDP工人分配显著提高公平性和偏好对齐度，领域专家认为两个组件都有效。

Conclusion: 结合CP与学习型决策为人本生产规划提供了稳健方法，能同时优化吞吐量和劳动力福祉，为公平高效的制造调度提供实践基础。

Abstract: This work presents a two-layer, human-centric production planning framework
designed to optimize both operational efficiency and workforce fairness in
industrial manufacturing. The first layer formulates the Order-Line allocation
as a Constraint Programming (CP) problem, generating high-utilization
production schedules that respect machine capacities, processing times, and due
dates. The second layer models Worker-Line allocation as a Markov Decision
Process (MDP), integrating human factors such as worker preference, experience,
resilience, and medical constraints into the assignment process. Three solution
strategies, greedy allocation, MCTS, and RL, are implemented and compared
across multiple evaluation scenarios. The proposed system is validated through
16 test sessions with domain experts from the automotive industry, combining
quantitative key performance indicators (KPIs) with expert ratings. Results
indicate that the CP-based scheduling approach produces compact, feasible
production plans with low tardiness, while the MDP-based worker allocation
significantly improves fairness and preference alignment compared to baseline
approaches. Domain experts rated both the Order-Line and Worker-Line components
as effective and highlighted opportunities to further refine the objective
function to penalize excessive earliness and improve continuity in worker
assignments. Overall, the findings demonstrate that combining CP with
learning-based decision-making provides a robust approach for human-centric
production planning. The approach enables simultaneous optimization of
throughput and workforce well-being, offering a practical foundation for fair
and efficient manufacturing scheduling in industrial settings.

</details>


### [64] [PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis](https://arxiv.org/abs/2510.01114)
*Lionel Levine,John Santerre,Alexander S. Young,T. Barry Levine,Francis Campion,Majid Sarrafzadeh*

Main category: cs.AI

TL;DR: PRISM-Consult是一个临床医生对齐的专家小组架构，将紧凑的PRISM序列模型扩展为路由的领域专家家族，通过轻量级路由器将急诊病例分派到不同专科模型，实现参数效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了在急诊科实现安全、可审计、低延迟的大规模会诊，需要开发一个既能保持模型紧凑性又能处理多专科病例的架构。

Method: 采用路由专家架构，将临床事件标记化，通过轻量级路由器读取前几个标记并分派到专科模型（心脏血管、肺、胃肠、肌肉骨骼、心理源性），每个专科继承PRISM的小型transformer骨干和标记模板。

Result: 在真实世界急诊科队列中，专科模型在各领域表现出平滑收敛和低开发困惑度，路由器在安全优先策略下实现高质量路由和大量计算节省。

Conclusion: 该框架为安全、可审计、低延迟的大规模会诊提供了实用路径，并通过外部/时间复制、非对称生命威胁阈值和多标签仲裁等验证步骤满足临床部署标准。

Abstract: We present PRISM-Consult, a clinician-aligned panel-of-experts architecture
that extends the compact PRISM sequence model into a routed family of domain
specialists. Episodes are tokenized as structured clinical events; a
light-weight router reads the first few tokens and dispatches to specialist
models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,
Psychogenic). Each specialist inherits PRISM's small transformer backbone and
token template, enabling parameter efficiency and interpretability. On
real-world Emergency Department cohorts, specialists exhibit smooth convergence
with low development perplexities across domains, while the router achieves
high routing quality and large compute savings versus consult-all under a
safety-first policy. We detail the data methodology (initial vs. conclusive
ICD-9 families), routing thresholds and calibration, and report per-domain
results to avoid dominance by common events. The framework provides a practical
path to safe, auditable, and low-latency consult at scale, and we outline
validation steps-external/temporal replication, asymmetric life-threat
thresholds, and multi-label arbitration-to meet prospective clinical deployment
standards.

</details>


### [65] [Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis](https://arxiv.org/abs/2510.01115)
*Evan Heus,Rick Bookstaber,Dhruv Sharma*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的智能体框架，用于供应链风险分析，通过将供应链网络视为知识图谱，利用网络中心性评分指导图遍历，结合上下文模板使定量数据对LLM可理解，实现实时、可解释的风险分析。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂的多模态金融风险数据时过于简化关系，而专业模型成本高且静态，无法满足实时供应链风险分析需求。

Method: 将供应链网络建模为知识图谱，使用网络中心性评分指导图遍历提取关键风险路径；采用智能体架构协调图检索、数值因子表和新闻流数据；使用"上下文模板"将原始数据转换为自然语言描述。

Result: 该轻量级方法无需昂贵的微调或专用图数据库，即可生成简洁、可解释且上下文丰富的实时风险叙述。

Conclusion: 通过利用网络与知识图谱之间的内在对偶性，该框架成功解决了LLM在处理复杂金融风险数据时的局限性，实现了高效实时的供应链风险分析。

Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and
network-native data underlying financial risk. Standard Retrieval-Augmented
Generation (RAG) oversimplifies relationships, while specialist models are
costly and static. We address this gap with an LLM-centric agent framework for
supply chain risk analysis. Our core contribution is to exploit the inherent
duality between networks and knowledge graphs (KG). We treat the supply chain
network as a KG, allowing us to use structural network science principles for
retrieval. A graph traverser, guided by network centrality scores, efficiently
extracts the most economically salient risk paths. An agentic architecture
orchestrates this graph retrieval alongside data from numerical factor tables
and news streams. Crucially, it employs novel ``context shells'' -- descriptive
templates that embed raw figures in natural language -- to make quantitative
data fully intelligible to the LLM. This lightweight approach enables the model
to generate concise, explainable, and context-rich risk narratives in real-time
without costly fine-tuning or a dedicated graph database.

</details>


### [66] [Apriel-1.5-15b-Thinker](https://arxiv.org/abs/2510.01141)
*Shruthan Radhakrishna,Aman Tiwari,Aanjaneya Shukla,Masoud Hashemi,Rishabh Maheshwary,Shiva Krishna Reddy Malay,Jash Mehta,Pulkit Pattnaik,Saloni Mittal,Khalil Slimi,Kelechi Ogueji,Akintunde Oladipo,Soham Parikh,Oluwanifemi Bamgbose,Toby Liang,Ahmed Masry,Khyati Mahajan,Sai Rajeswar Mudumba,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sagar Davasam,Srinivas Sunkara,Nicholas Chapados*

Main category: cs.AI

TL;DR: Apriel-1.5-15B-Thinker是一个150亿参数的多模态推理模型，通过创新的三阶段训练方法而非单纯扩大规模，实现了前沿性能，在单GPU部署限制下达到与更大模型竞争的结果。


<details>
  <summary>Details</summary>
Motivation: 旨在证明通过精心设计的训练方法而非单纯扩大模型规模，可以在有限计算资源下实现前沿水平的多模态推理能力，使更多组织能够获得高性能AI模型。

Method: 采用渐进式三阶段方法：1)深度扩展推理能力；2)分阶段持续预训练，包括基础文本视觉理解和针对性合成数据增强视觉推理；3)高质量文本监督微调，涵盖数学、编程、科学和工具使用等领域的推理轨迹。

Result: 在Artificial Analysis Intelligence Index上获得52分，与DeepSeek-R1-0528相当；在十个图像基准测试中，平均性能仅比Gemini-2.5-Flash和Claude Sonnet-3.7低5分，且无需强化学习或偏好优化。

Conclusion: 深思熟虑的中期训练设计可以在不依赖大规模计算的情况下显著缩小能力差距，使前沿多模态推理对基础设施有限的组织变得可及，模型及相关资源已开源发布。

Abstract: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
multimodal reasoning model that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) depth upscaling to expand reasoning
capacity without pretraining from scratch, (2) staged continual pre-training
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted synthetic data generation addressing spatial
structure, compositional understanding, and fine-grained perception, and (3)
high-quality text-only supervised fine-tuning on curated instruction-response
pairs with explicit reasoning traces spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within single-GPU deployment constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.

</details>


### [67] [Generalized Parallel Scaling with Interdependent Generations](https://arxiv.org/abs/2510.01143)
*Harry Dong,David Brandfonbrener,Eryk Helenowski,Yun He,Mrinal Kumar,Han Fang,Yuejie Chi,Karthik Abinav Sankararaman*

Main category: cs.AI

TL;DR: Bridge方法通过重新思考批量LLM隐藏状态作为整体张量而非独立切片，在并行LLM推理中生成相互依赖的响应，仅需少量新参数即可显著提升响应质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统并行LLM推理中，N个并行响应通常相互独立生成，这导致计算资源分割且无法利用其他生成中的有用信息，与序列长度扩展中利用过去计算的方式形成对比。

Method: 提出Bridge方法，将批量LLM隐藏状态视为整体张量而非独立切片，仅添加少量新参数（2.8%-5.1%），在并行生成过程中实现响应间的信息共享。

Result: Bridge将基于可验证奖励的强化学习的相对平均准确率提升高达50%，并提高了正确响应的一致性。一次训练即可扩展到任何生成宽度，性能始终优于独立生成。

Conclusion: Bridge解锁了一种更通用的并行扩展模式，有效利用序列间的信息，且与任何后生成聚合技术兼容。

Abstract: Parallel LLM inference scaling involves sampling a set of $N>1$ responses for
a single input prompt. However, these $N$ parallel responses tend to be
generated independently from each other, partitioning compute resources and
leaving potentially useful information in one generation untapped by others.
This is in contrast to response length scaling where past computation is used
in all future steps. For higher quality responses and response sets, we propose
Bridge to generate interdependent responses in parallel by rethinking batched
LLM hidden states as holistic tensors rather than independent slices. With only
a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean
accuracy gains from reinforcement learning with verifiable rewards by up to 50%
and boosts consistency of correct responses. Trained once, Bridge scales to any
generation width, all with greater performance than independent generations,
unlocking a more general mode of parallel scaling that effectively leverages
information between sequences, compatible with any post-generation aggregation
technique.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [68] [Directed Information $γ$-covering: An Information-Theoretic Framework for Context Engineering](https://arxiv.org/abs/2510.00079)
*Hai Huang*

Main category: cs.IT

TL;DR: 提出了一种基于定向信息的γ覆盖框架，用于冗余感知的上下文工程，通过贪婪算法实现高效上下文选择，在HotpotQA上验证了优于BM25基线的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM管道中上下文冗余问题，提供无需在线计算的查询无关上下文选择方法，提高上下文压缩和单槽提示选择的效率。

Method: 基于定向信息理论，定义γ覆盖准则，将上下文选择建模为集合覆盖问题，采用贪婪算法实现近似最优解，并保证多样性边界。

Result: 在HotpotQA数据集上实验表明，γ覆盖方法持续优于BM25基线，在上下文压缩和单槽提示选择等硬决策场景中表现突出。

Conclusion: 定向信息γ覆盖为现代LLM管道提供了一个有理论基础、自组织的骨干框架，具有离线计算、查询无关和性能保证等优势。

Abstract: We introduce \textbf{Directed Information $\gamma$-covering}, a simple but
general framework for redundancy-aware context engineering. Directed
information (DI), a causal analogue of mutual information, measures asymmetric
predictiveness between chunks. If $\operatorname{DI}_{i \to j} \ge H(C_j) -
\gamma$, then $C_i$ suffices to represent $C_j$ up to $\gamma$ bits. Building
on this criterion, we formulate context selection as a $\gamma$-cover problem
and propose a greedy algorithm with provable guarantees: it preserves query
information within bounded slack, inherits $(1+\ln n)$ and $(1-1/e)$
approximations from submodular set cover, and enforces a diversity margin.
Importantly, building the $\gamma$-cover is \emph{query-agnostic}: it incurs no
online cost and can be computed once offline and amortized across all queries.
Experiments on HotpotQA show that $\gamma$-covering consistently improves over
BM25, a competitive baseline, and provides clear advantages in hard-decision
regimes such as context compression and single-slot prompt selection. These
results establish DI $\gamma$-covering as a principled, self-organizing
backbone for modern LLM pipelines.

</details>


### [69] [An Adaptive cmWave/FR3 Channel Sounder for Integrated Sensing and Communication](https://arxiv.org/abs/2510.00257)
*K. F. Nieman,O. Kanhere,R. Shiu,W. Xu,C. Duan,S. S. Ghassemzadeh*

Main category: cs.IT

TL;DR: 本文介绍了一种先进的信道探测系统，适用于各类蜂窝部署场景的感知和传播实验，具有高适应性、高分辨率和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够适应各种室内外测量场景的通用信道探测系统，以支持双定向信道探测、高速车载通信以及集成通信与感知实验。

Method: 设计具有2.5 ns延迟分辨率、170 dB路径损耗测量能力的信道探测系统，能够在0.9 ms内测量360度功率-角度-延迟剖面，并通过更换RF前端天线实现不同频段的测量。

Result: 系统展现出卓越的适应性、高分辨率和灵敏度，成为室内外测量活动中不可或缺的工具。

Conclusion: 这种多功能探测系统适用于双定向信道探测、车对车和车对基础设施通信等高速车载实验，以及集成通信与感知实验。

Abstract: In this paper, we present an advanced channel sounding system designed for
sensing and propagation experiments in all types of cellular deployment
scenarios. The system's exceptional adaptability, high resolution, and
sensitivity makes it an invaluable tool for utilization in a variety of indoor
and outdoor measurement campaigns. The sounder has a 2.5 ns delay resolution,
170 dB path loss measurement capability and is able to measure a
{360\textdegree} power-angular delay profile of the channel in less than 0.9
ms. Additionally, the system can be easily reconfigured to measure different
frequency bands by changing the RF front-end antennas. This versatile sounder
is suitable for double directional channel sounding, high-speed vehicular
experiments such as vehicle-to-vehicle and vehicle-to-infrastructure
communications, and integrated communication and sensing experiments.

</details>


### [70] [Indoor-Office Large-Scale Wireless Channel Characterization in cmWave/FR3 Spectrum](https://arxiv.org/abs/2510.00269)
*O. Kanhere,K. F. Nieman,S. S. Ghassemzadeh*

Main category: cs.IT

TL;DR: 本文通过6.9、8.3和14.5 GHz频段的广泛测量，对商业办公楼室内热点信道参数进行了全面表征，包括路径损耗、阴影衰落、时延扩展和角度扩展建模。


<details>
  <summary>Details</summary>
Motivation: 研究厘米波频段在多样化室内办公环境中的无线信号衰减和色散特性，为改进商业建筑室内网络设计和性能提供基础数据。

Method: 在商业办公楼四个楼层的不同室内环境（隔间、会议室、走廊、实验室）进行了广泛的实验测量，覆盖6.9、8.3和14.5 GHz三个频段。

Result: 获得了室内热点信道的路径损耗、阴影衰落、时延扩展和角度扩展等关键参数模型，揭示了厘米波频段在多样化室内环境中的传播特性。

Conclusion: 研究结果为厘米波频段室内无线网络设计和性能优化提供了重要参考，有助于改善商业建筑中的室内网络性能。

Abstract: This paper presents comprehensive findings on the characterization of Indoor
Hotspot channel parameters, derived from an extensive experimental campaign
conducted at 6.9, 8.3, and 14.5 GHz in a commercial office building. Extensive
measurements were carried out in diverse indoor office settings, including
cubicles, conference rooms, hallways, and laboratory spaces across four floors.
The path loss, shadow fading, delay spread, and angular spread was modeled. Our
results offer significant insights into the attenuation and dispersion
characteristics of wireless signals in diverse indoor settings in the
centimeter-wave frequency band, and can be used for improving indoor network
design and performance in commercial buildings.

</details>


### [71] [cmWave/FR3 Large-Scale Channel Characterization for Urban Macro/Micro and Suburban Environments](https://arxiv.org/abs/2510.00275)
*K. F. Nieman,O. Kanhere,S. S. Ghassemzadeh*

Main category: cs.IT

TL;DR: 本研究在7-15 GHz厘米波频段对城市宏/微小区和郊区环境的大规模信道进行了全面表征，分析了路径损耗、大规模衰落和角度信道统计特性。


<details>
  <summary>Details</summary>
Motivation: 为下一代无线网络开发更高效和自适应的通信策略，为网络规划者和工程师提供有价值的见解。

Method: 在7-15 GHz频段对城市宏/微小区和郊区环境进行大规模信道测量，分析路径损耗、大规模衰落和角度信道统计特性。

Result: 城市环境由于密集障碍物表现出更高的路径损耗和延迟扩展，而郊区区域路径损耗相对较低但由于较少但较大的障碍物而具有显著变异性。

Conclusion: 提出了增强的信道预测和系统设计模型，有助于推进下一代无线网络的发展。

Abstract: This study delves into the comprehensive characterization of large-scale
channels at centimeter wave frequencies 7-15 GHz for urban macro/micro and
suburban environments. Path-loss, large-scale fading, and angular channel
statistics are presented. Urban environments exhibited higher path loss and
delay spread due to dense obstacles, whereas suburban areas showed relatively
lower path loss but significant variability due to fewer but larger
obstructions. The findings provide valuable insights for network planners and
engineers, aiding in the development of more efficient and adaptive
communication strategies. Enhanced models for channel prediction and system
design are proposed, contributing to the advancement of next-generation
wireless networks.

</details>


### [72] [On the Achievable Performance in the presence of Multiple Path Interference for Intra Data Center applications](https://arxiv.org/abs/2510.00638)
*Wing Chau Ng,Scott Yam*

Main category: cs.IT

TL;DR: 首次提出了PAM4信号在存在多径干扰情况下的可达到误码率的精确解析形式，考虑了理想的多径干扰估计和补偿。


<details>
  <summary>Details</summary>
Motivation: 在多径干扰环境中，准确分析PAM4信号的误码率性能对于系统设计和优化至关重要。

Method: 推导了考虑理想多径干扰估计和补偿的PAM4信号误码率解析表达式。

Result: 获得了PAM4信号在多径干扰条件下的精确误码率分析模型。

Conclusion: 提出的解析模型为PAM4系统在多径环境下的性能评估提供了理论依据。

Abstract: An accurate analytical form of the achievable bit error rate in the presence
of multipath interference (MPI) is proposed for PAM4 for the first time, taking
into account an ideal MPI estimate and compensation.

</details>


### [73] [OTFS for Joint Radar and Communication: Algorithms, Prototypes, and Experiments](https://arxiv.org/abs/2510.00668)
*Xiaojuan Zhang,Yonghong Zeng,Francois Chin Po Shin*

Main category: cs.IT

TL;DR: 提出了基于OTFS信号的联合雷达通信系统，包含快速雷达感知算法、自干扰消除、生命体征监测以及人/非人目标区分方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时进行雷达感知和通信的系统，利用OTFS信号实现多目标检测、生命体征监测和人/非人目标区分。

Method: 使用OTFS通信信号进行快速雷达感知，采用自干扰消除技术增强多目标分离，结合信号处理和机器学习方法区分人/非人目标，基于SDR技术开发原型系统。

Result: 实验结果表明原型系统能有效检测距离、速度和生命体征，在人类和移动机器人场景中表现良好，并能成功区分人/非人目标。

Conclusion: 提出的JRC系统成功实现了雷达感知、通信和生命体征监测的集成，验证了OTFS信号在联合雷达通信应用中的有效性。

Abstract: We propose an Joint Radar and Communication (JRC) system that utilizes the
Orthogonal Time Frequency Space (OTFS) signals. The system features a fast
radar sensing algorithm for detecting target range and speed by using the OTFS
communication signals, and a self-interference cancellation for enhanced
multi-target separation. In addition to target detection, we propose methods
for monitoring human vital signs, such as breathing rate and heartbeat.
Furthermore, we explore two approaches for distinguishing between human and
nonhuman targets: one based on signal processing and the other based on machine
learning. We have developed a prototype JRC system using the software-defined
radio (SDR) technology. Experimental results are shown to demonstrate the
effectiveness of the prototype in detecting range, speed, and vital signs in
both human and mobile robot scenarios, as well as in distinguishing between
human and non-human targets.

</details>


### [74] [Layered Normalized Min-Sum Decoding with Bit Flipping for FDPC Codes](https://arxiv.org/abs/2510.01019)
*Niloufar Hosseinzadeh,Mohsen Moradi,Hessam Mahdavifar*

Main category: cs.IT

TL;DR: 本文提出了一种用于FDPC码的分层归一化最小和(LNMS)解码算法，并结合伴随式引导比特翻转(SGBF)方法提升纠错性能。该解码器在FDPC(256,192)码上相比独立LNMS解码获得约0.5dB编码增益，相比其他先进码型获得0.75-1.5dB增益。


<details>
  <summary>Details</summary>
Motivation: FDPC码相比5G系统中标准化的LDPC码在高码率区域表现出更好的性能，但需要更高效的解码算法来充分发挥其潜力。

Method: 提出LNMS消息传递解码算法，利用冲突图着色进行分层调度；结合SGBF方法，使用结合LLR幅度和伴随式错误计数的可靠性度量识别不可靠比特，生成候选序列并重新解码。

Result: 在FDPC(256,192)码上，T=128比特翻转集大小和最多5次迭代下，提出的解码器在FER=10^-3时相比独立LNMS解码获得约0.5dB编码增益，相比极化码和5G-LDPC码获得0.75-1.5dB增益。

Conclusion: 所提出的LNMS结合SGBF的解码器显著提升了FDPC码的解码性能，在高码率场景下优于现有先进码型。

Abstract: Fair-density parity-check (FDPC) codes have been recently introduced
demonstrating improved performance compared to low-density parity-check (LDPC)
codes standardized in 5G systems particularly in high-rate regimes. In this
paper, we introduce a layered normalized min-sum (LNMS) message-passing
decoding algorithm for the FDPC codes. We also introduce a syndrome-guided bit
flipping (SGBF) method to enhance the error-correction performance of our
proposed decoder. The LNMS decoder leverages conflict graph coloring for
efficient layered scheduling, enabling faster convergence by grouping
non-conflicting check nodes and updating variable nodes immediately after each
layer. In the event of decoding failure, the SGBF method is activated,
utilizing a novel reliability metric that combines log-likelihood ratio (LLR)
magnitudes and syndrome-derived error counts to identify the least reliable
bits. A set of candidate sequences is then generated by performing single-bit
flips at these positions, with each candidate re-decoded via LNMS. The optimal
candidate is selected based on the minimum syndrome weight. Extensive
simulation results demonstrate the superiority of the proposed decoder.
Numerical simulations on FDPC$(256,192)$ code with a bit-flipping set size of
$T = 128$ and a maximum of $5$ iterations demonstrate that the proposed decoder
achieves approximately a $0.5\,\mathrm{dB}$ coding gain over standalone LNMS
decoding at a frame error rate (FER) of $10^{-3}$, while providing coding gains
of $0.75-1.5\,\mathrm{dB}$ over other state-of-the-art codes including polar
codes and 5G-LDPC codes at the same length and rate and also under belief
propagation decoding.

</details>
