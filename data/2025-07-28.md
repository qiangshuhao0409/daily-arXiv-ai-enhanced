<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Third-Party Assessment of Mobile Performance in the 5G Era](https://arxiv.org/abs/2507.18834)
*ASM Rizvi,John Heidemann,David Plonka*

Main category: cs.NI

TL;DR: 本文通过全球分布的CDN数据分析了移动设备的网络体验，包括延迟、吞吐量和稳定性。研究发现，移动用户偶尔能体验到极低延迟（如6毫秒），但仅5%的用户能持续低于20毫秒。吞吐量方面，60%用户低于50 Mb/s。最小延迟在特定位置通常稳定，可用于异常检测。


<details>
  <summary>Details</summary>
Motivation: 随着5G时代的到来，移动设备对高性能网络的需求日益增长，但现有研究缺乏对全球范围内移动体验的全面分析。本文旨在通过CDN数据提供中立、跨运营商的视角。

Method: 利用商业、全球分布的CDN数据，对移动客户端的延迟、吞吐量和稳定性进行测量和分析。

Result: 移动用户偶尔能体验极低延迟（6毫秒），但仅5%用户持续低于20毫秒。60%用户吞吐量低于50 Mb/s。最小延迟在特定位置稳定。

Conclusion: 移动网络体验存在显著差异，最小延迟的稳定性可用于异常检测，为优化移动网络性能提供依据。

Abstract: The web experience using mobile devices is important since a significant
portion of the Internet traffic is initiated from mobile devices. In the era of
5G, users expect a high-performance data network to stream media content and
for other latency-sensitive applications. In this paper, we characterize mobile
experience in terms of latency, throughput, and stability measured from a
commercial, globally-distributed CDN. Unlike prior work, CDN data provides a
relatively neutral, carrier-agnostic perspective, providing a clear view of
multiple and international providers. Our analysis of mobile client traffic
shows mobile users sometimes experience markedly low latency, even as low as 6
ms. However, only the top 5% users regularly experience less than 20 ms of
minimum latency. While 100 Mb/s throughput is not rare, we show around 60%
users observe less than 50 Mb/s throughput. We find the minimum mobile latency
is generally stable at a specific location which can be an important
characteristic for anomaly detection.

</details>


### [2] [Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks](https://arxiv.org/abs/2507.19050)
*Qiong Wu,Yu Xie,Pingyi Fan,Dong Qin,Kezhi Wang,Nan Cheng,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 提出了一种基于数字孪生边缘计算的网络，研究任务卸载策略、队列稳定性和资源分配，采用LLM方法替代传统MARL框架，性能表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多车辆生成计算任务时卸载到服务器导致的排队问题。

Method: 利用Lyapunov优化将长期约束转化为短期决策，采用基于LLM的上下文学习方法。

Result: LLM方法性能与MARL相当或更优。

Conclusion: LLM方法在任务卸载和资源分配中具有潜力。

Abstract: In this paper, we propose a general digital twin edge computing network
comprising multiple vehicles and a server. Each vehicle generates multiple
computing tasks within a time slot, leading to queuing challenges when
offloading tasks to the server. The study investigates task offloading
strategies, queue stability, and resource allocation. Lyapunov optimization is
employed to transform long-term constraints into tractable short-term
decisions. To solve the resulting problem, an in-context learning approach
based on large language model (LLM) is adopted, replacing the conventional
multi-agent reinforcement learning (MARL) framework. Experimental results
demonstrate that the LLM-based method achieves comparable or even superior
performance to MARL.

</details>


### [3] [iPLAN: Redefining Indoor Wireless Network Planning Through Large Language Models](https://arxiv.org/abs/2507.19096)
*Jinbo Hou,Stefanos Bakirtzis,Kehai Qiu,Sichong Liao,Hui Song,Haonan Hu,Kezhi Wang,Jie Zhang*

Main category: cs.NI

TL;DR: iPLAN框架利用大型语言模型（LLM）优化室内无线网络（IWN）规划，通过多模态环境表示和智能代理协作，显著提升5G室内服务质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以应对室内环境与无线网络需求的复杂交互，iPLAN旨在通过LLM优化解决这一问题。

Method: iPLAN整合多模态室内环境表示到LLM优化器中，支持基于现有环境的规划和新建无线友好建筑的联合设计。

Result: 仿真结果表明，iPLAN在IWN规划任务中表现优异，并通过联合设计优化建筑无线性能。

Conclusion: iPLAN为IWN规划带来范式转变，展示了LLM在复杂网络规划中的潜力。

Abstract: Efficient indoor wireless network (IWN) planning is crucial for providing
high-quality 5G in-building services. However, traditional meta-heuristic and
artificial intelligence-based planning methods face significant challenges due
to the intricate interplay between indoor environments (IEs) and IWN demands.
In this article, we present an indoor wireless network Planning with large
LANguage models (iPLAN) framework, which integrates multi-modal IE
representations into large language model (LLM)-powered optimizers to improve
IWN planning. First, we instate the role of LLMs as optimizers, outlining
embedding techniques for IEs, and introducing two core applications of iPLAN:
(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE
for new wireless-friendly buildings. For the former, we embed essential
information into LLM optimizers by leveraging indoor descriptions,
domain-specific knowledge, and performance-driven perception. For the latter,
we conceptualize a multi-agent strategy, where intelligent agents
collaboratively address key planning sub-tasks in a step-by-step manner while
ensuring optimal trade-offs between the agents. The simulation results
demonstrate that iPLAN achieves superior performance in IWN planning tasks and
optimizes building wireless performance through the joint design of IEs and
IWNs, exemplifying a paradigm shift in IWN planning.

</details>


### [4] [AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR](https://arxiv.org/abs/2507.19124)
*Muhammad Ahmed Mohsin,Sagnik Bhattacharya,Abhiram Gorle,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.NI

TL;DR: 论文提出了一种针对多用户无线VR场景的低秩信道优化方法，结合非线性收发器和深度强化学习（DRL），显著提升了数据率和能效。


<details>
  <summary>Details</summary>
Motivation: 解决多用户无线VR中因低秩信道导致的性能瓶颈问题。

Method: 采用非线性收发器（如广义决策反馈或连续干扰消除）和DRL优化能量分配与解码顺序。

Result: 实验显示数据率提升39%、28%、16%（相比OMA、NOMA、MC-NOMA），能效提升75%、45%、40%。

Conclusion: 该方法在多用户无线VR中实现了接近理论最优的性能，且DRL框架显著降低了实时复杂度。

Abstract: Wireless support of virtual reality (VR) has challenges when a network has
multiple users, particularly for 3D VR gaming, digital AI avatars, and remote
team collaboration. This work addresses these challenges through investigation
of the low-rank channels that inevitably occur when there are more active users
than there are degrees of spatial freedom, effectively often the number of
antennas. The presented approach uses optimal nonlinear transceivers,
equivalently generalized decision-feedback or successive cancellation for
uplink and superposition or dirty-paper precoders for downlink. Additionally, a
powerful optimization approach for the users' energy allocation and decoding
order appears to provide large improvements over existing methods, effectively
nearing theoretical optima. As the latter optimization methods pose real-time
challenges, approximations using deep reinforcement learning (DRL) are used to
approximate best performance with much lower (5x at least) complexity.
Experimental results show significantly larger sum rates and very large power
savings to attain the data rates found necessary to support VR. Experimental
results show the proposed algorithm outperforms current industry standards like
orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as
well as the highly researched methods in multi-carrier NOMA (MC-NOMA),
enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power
level. For the same data rate, it achieves power savings of 75%, 45%, and 40%,
making it ideal for VR applications. Additionally, a near-optimal deep
reinforcement learning (DRL)-based resource allocation framework for real-time
use by being 5x faster and reaching 83% of the global optimum is introduced.

</details>


### [5] [Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV](https://arxiv.org/abs/2507.19234)
*Tianfu Wang,Liwei Deng,Xi Chen,Junyang Wang,Huiguo He,Leilei Ding,Wei Wu,Qilin Fan,Hui Xiong*

Main category: cs.NI

TL;DR: Virne是一个用于NFV资源分配问题的综合基准测试框架，专注于支持深度强化学习方法，提供多样化网络场景模拟和丰富的方法实现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性基准测试框架和深入分析，阻碍了新兴网络的探索和更鲁棒算法的开发，导致评估不一致。

Method: Virne提供可定制的网络场景模拟（如云、边缘、5G环境），支持30多种方法的模块化实现，并包含实用性评估视角（如可扩展性、泛化性）。

Result: 通过大量实验深入分析，Virne为高效实现提供了性能权衡的见解，并为未来研究方向提供了实用指导。

Conclusion: Virne可作为NFV资源分配方法和深度强化学习应用的全面基准，推动相关研究发展。

Abstract: Resource allocation (RA) is critical to efficient service deployment in
Network Function Virtualization (NFV), a transformative networking paradigm.
Recently, deep Reinforcement Learning (RL)-based methods have been showing
promising potential to address this complexity. However, the lack of a
systematic benchmarking framework and thorough analysis hinders the exploration
of emerging networks and the development of more robust algorithms while
causing inconsistent evaluation. In this paper, we introduce Virne, a
comprehensive benchmarking framework for the NFV-RA problem, with a focus on
supporting deep RL-based methods. Virne provides customizable simulations for
diverse network scenarios, including cloud, edge, and 5G environments. It also
features a modular and extensible implementation pipeline that supports over 30
methods of various types, and includes practical evaluation perspectives beyond
effectiveness, such as scalability, generalization, and scalability.
Furthermore, we conduct in-depth analysis through extensive experiments to
provide valuable insights into performance trade-offs for efficient
implementation and offer actionable guidance for future research directions.
Overall, with its diverse simulations, rich implementations, and extensive
evaluation capabilities, Virne could serve as a comprehensive benchmark for
advancing NFV-RA methods and deep RL applications. The code is publicly
available at https://github.com/GeminiLight/virne.

</details>


### [6] [Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access Point Coordination](https://arxiv.org/abs/2507.19377)
*David Nunez,Francesc Wilhelmi,Maksymilian Wojnar,Katarzyna Kosek-Szott,Szymon Szott,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度强化学习（DRL）的多接入点协调（MAPC）调度方法，以最小化网络最坏情况延迟，并在密集Wi-Fi部署中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在重叠基本服务集（OBSS）中，实现高效的调度策略以应对多样化的流量和干扰条件是一项复杂任务。

Method: 通过将MAPC调度建模为顺序决策问题，并采用近端策略优化（PPO）训练DRL代理，利用空间重用（SR）组调度多AP-站点对。

Result: 仿真显示，该方法在多种网络负载和流量模式下优于现有启发式策略，99%延迟降低高达30%。

Conclusion: 该方法显著提升了密集Wi-Fi网络的延迟性能，为未来Wi-Fi网络优化提供了有效解决方案。

Abstract: Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn,
with a potential impact on future Wi-Fi networks. MAPC enables joint scheduling
decisions across multiple access points (APs) to improve throughput, latency,
and reliability in dense Wi-Fi deployments. However, implementing efficient
scheduling policies under diverse traffic and interference conditions in
overlapping basic service sets (OBSSs) remains a complex task. This paper
presents a method to minimize the network-wide worst-case latency by
formulating MAPC scheduling as a sequential decision-making problem and
proposing a deep reinforcement learning (DRL) mechanism to minimize worst-case
delays in OBSS deployments. Specifically, we train a DRL agent using proximal
policy optimization (PPO) within an 802.11bn-compatible Gymnasium environment.
This environment provides observations of queue states, delay metrics, and
channel conditions, enabling the agent to schedule multiple AP-station pairs to
transmit simultaneously by leveraging spatial reuse (SR) groups. Simulations
demonstrate that our proposed solution outperforms state-of-the-art heuristic
strategies across a wide range of network loads and traffic patterns. The
trained machine learning (ML) models consistently achieve lower 99th-percentile
delays, showing up to a 30% improvement over the best baseline.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Initial Steps in Integrating Large Reasoning and Action Models for Service Composition](https://arxiv.org/abs/2507.18775)
*Ilche Georgievski,Marco Aiello*

Main category: cs.AI

TL;DR: 论文提出了一种结合大型推理模型（LRM）和大型动作模型（LAM）的框架，以解决服务组合中的语义推理和动态执行问题。


<details>
  <summary>Details</summary>
Motivation: 服务组合在构建自适应和智能软件系统中面临挑战，现有方法在推理能力和执行机制上存在局限。

Method: 提出集成LRM和LAM的框架，LRM处理语义推理和复杂性，LAM负责动态执行和互操作性。

Result: 该框架能够自动化服务组合，通过自然语言意图驱动，实现从需求推理到动态执行的闭环。

Conclusion: LRM-LAM集成框架为服务组合的自动化和用户友好性提供了新方向。

Abstract: Service composition remains a central challenge in building adaptive and
intelligent software systems, often constrained by limited reasoning
capabilities or brittle execution mechanisms. This paper explores the
integration of two emerging paradigms enabled by large language models: Large
Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs
address the challenges of semantic reasoning and ecosystem complexity while
LAMs excel in dynamic action execution and system interoperability. However,
each paradigm has complementary limitations - LRMs lack grounded action
capabilities, and LAMs often struggle with deep reasoning. We propose an
integrated LRM-LAM architectural framework as a promising direction for
advancing automated service composition. Such a system can reason about service
requirements and constraints while dynamically executing workflows, thus
bridging the gap between intention and execution. This integration has the
potential to transform service composition into a fully automated,
user-friendly process driven by high-level natural language intent.

</details>


### [8] [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](https://arxiv.org/abs/2507.19458)
*Amir Fard,Arnold X. -X. Yuan*

Main category: cs.AI

TL;DR: 本文提出了一种分层深度强化学习方法，用于多年度基础设施规划，通过分解问题为预算规划与维护优先级两个层次，有效解决了现有方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 基础设施资产管理中的预算规划与维护优化面临组合动作空间、资产退化多样性、严格预算约束和环境不确定性等复杂性问题，限制了现有方法的可扩展性。

Method: 采用分层深度强化学习框架，将问题分解为高层预算规划（明确可行性边界）和低层维护优先级（在预算内优化资产维护），并整合线性规划投影。

Result: 案例研究表明，该方法在10、15和20个污水管网规模下均表现优异，相比传统深度Q学习和遗传算法，收敛更快、扩展性更强且解接近最优。

Conclusion: 分层深度强化学习方法在多年度基础设施规划中表现出高效性和可扩展性，为复杂资产管理提供了新思路。

Abstract: Budget planning and maintenance optimization are crucial for infrastructure
asset management, ensuring cost-effectiveness and sustainability. However, the
complexity arising from combinatorial action spaces, diverse asset
deterioration, stringent budget constraints, and environmental uncertainty
significantly limits existing methods' scalability. This paper proposes a
Hierarchical Deep Reinforcement Learning methodology specifically tailored to
multi-year infrastructure planning. Our approach decomposes the problem into
two hierarchical levels: a high-level Budget Planner allocating annual budgets
within explicit feasibility bounds, and a low-level Maintenance Planner
prioritizing assets within the allocated budget. By structurally separating
macro-budget decisions from asset-level prioritization and integrating linear
programming projection within a hierarchical Soft Actor-Critic framework, the
method efficiently addresses exponential growth in the action space and ensures
rigorous budget compliance. A case study evaluating sewer networks of varying
sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed
approach. Compared to conventional Deep Q-Learning and enhanced genetic
algorithms, our methodology converges more rapidly, scales effectively, and
consistently delivers near-optimal solutions even as network size grows.

</details>


### [9] [Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization](https://arxiv.org/abs/2507.18795)
*Fatima Al-Ani,Molly Wang,Jevon Charles,Aaron Ong,Joshua Forday,Vinayak Modi*

Main category: cs.AI

TL;DR: 开发了一种基于模拟的强化学习框架（Dyna-DDPG），用于优化复杂排队网络中的路由决策，特别适用于制造和通信领域。


<details>
  <summary>Details</summary>
Motivation: 传统排队方法在动态和不确定环境中表现不佳，需要一种更鲁棒的方法。

Method: 结合Deep Deterministic Policy Gradient (DDPG)和Dyna-style planning，开发了Dyna-DDPG框架，包含灵活的模拟环境和分离的预测模型。

Result: 实验表明，该框架能快速学习有效的路由策略，在干扰下保持鲁棒性，并能扩展到更大网络。

Conclusion: 该框架具有实际部署潜力，并强调了软件工程实践以确保可重复性和可维护性。

Abstract: This study focuses on the development of a simulation-driven reinforcement
learning (RL) framework for optimizing routing decisions in complex queueing
network systems, with a particular emphasis on manufacturing and communication
applications. Recognizing the limitations of traditional queueing methods,
which often struggle with dynamic, uncertain environments, we propose a robust
RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with
Dyna-style planning (Dyna-DDPG). The framework includes a flexible and
configurable simulation environment capable of modeling diverse queueing
scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG
implementation incorporates separate predictive models for next-state
transitions and rewards, significantly improving stability and sample
efficiency. Comprehensive experiments and rigorous evaluations demonstrate the
framework's capability to rapidly learn effective routing policies that
maintain robust performance under disruptions and scale effectively to larger
network sizes. Additionally, we highlight strong software engineering practices
employed to ensure reproducibility and maintainability of the framework,
enabling practical deployment in real-world scenarios.

</details>


### [10] [A Neuroscience-Inspired Dual-Process Model of Compositional Generalization](https://arxiv.org/abs/2507.18868)
*Alex Noviello,Claas Beger,Jacob Groner,Kevin Ellis,Weinan Sun*

Main category: cs.AI

TL;DR: MIRAGE框架通过模拟人脑的HPC-PFC交互机制，实现了系统化的组合泛化能力，在SCAN基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在组合泛化方面的核心挑战，模仿人脑中海马体与前额叶皮层的协作机制。

Method: MIRAGE包含两个模块：基于元训练的Transformer神经分解器（模拟直觉模式识别）和模式引擎（模拟HPC-PFC循环）。

Result: 在SCAN基准测试中达到>99%的准确率，仅需1.19M参数。

Conclusion: MIRAGE的系统性依赖于提取的模式质量和迭代优化过程。

Abstract: Systematic compositional generalization - constructing and understanding
novel combinations of known building blocks - remains a core challenge for AI
systems. Human cognition achieves this flexibility via the interplay of the
hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes
episodes, and the prefrontal cortex consolidates them into reusable schemas for
reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with
Rules and Abstractions from Generalized Experience), a framework that achieves
systematic generalization on compositional tasks. MIRAGE has two interacting
modules mirroring the brain's deliberative HPC-PFC loop and intuitive
neocortical pattern recognition. (1) The meta-trained Transformer Neural
Decomposer, paralleling neocortical "System 1" computation, is trained on a
task-agnostic stream of randomly sampled compositional grammars and applies one
decomposition step per pass, with successive passes iteratively refining the
sequence representation. (2) The Schema Engine, analogous to the HPC-PFC
"System 2" loop, dynamically extracts, ranks, and applies reusable schemas,
storing variable bindings in episodic memory and expanding them when needed. By
explicitly equipping the Transformer component of MIRAGE with actively managed
schematic structures, our model performs systematic compositional operations
through explicit schema application and transformation, relying solely on
frozen weights when solving entirely novel tasks. This approach demonstrates
systematic compositional generalization on the SCAN benchmark, achieving > 99%
accuracy on all task splits with only 1.19M parameters in the transformer
module. Ablation studies confirm that MIRAGE's systematicity critically depends
on the quality of extracted schemas and the model's iterative refinement
process.

</details>


### [11] [Success in Humanoid Reinforcement Learning under Partial Observation](https://arxiv.org/abs/2507.18883)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.AI

TL;DR: 首次在部分可观测的Gymnasium Humanoid-v4环境中成功训练出稳定的人形机器人策略，性能接近全状态访问的先进结果。


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测性下机器人控制策略学习的挑战，尤其是在高维任务（如人形机器人行走）中。

Method: 提出一种新颖的历史编码器，处理固定长度的过去观测序列，并将其集成到标准无模型算法中。

Result: 学习到的策略仅使用原始状态的三分之一到三分之二，性能与全状态访问基线相当，并能适应机器人属性变化。

Conclusion: 历史编码器通过重构关键上下文信息，实现了在部分可观测条件下的稳健决策。

Abstract: Reinforcement learning has been widely applied to robotic control, but
effective policy learning under partial observability remains a major
challenge, especially in high-dimensional tasks like humanoid locomotion. To
date, no prior work has demonstrated stable training of humanoid policies with
incomplete state information in the benchmark Gymnasium Humanoid-v4
environment. The objective in this environment is to walk forward as fast as
possible without falling, with rewards provided for staying upright and moving
forward, and penalties incurred for excessive actions and external contact
forces. This research presents the first successful instance of learning under
partial observability in this environment. The learned policy achieves
performance comparable to state-of-the-art results with full state access,
despite using only one-third to two-thirds of the original states. Moreover,
the policy exhibits adaptability to robot properties, such as variations in
body part masses. The key to this success is a novel history encoder that
processes a fixed-length sequence of past observations in parallel. Integrated
into a standard model-free algorithm, the encoder enables performance on par
with fully observed baselines. We hypothesize that it reconstructs essential
contextual information from recent observations, thereby enabling robust
decision-making.

</details>


### [12] [Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling](https://arxiv.org/abs/2507.18977)
*Mehrnoosh Mirtaheri,Ryan A. Rossi,Sungchul Kim,Kanak Mahadik,Tong Yu,Xiang Chen,Mohammad Rostami*

Main category: cs.AI

TL;DR: 提出了一种增量训练框架，用于解决时序知识图谱（TKG）完成中未观察或稀疏连接的实体问题，结合模型无关的增强层和加权采样策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统TKG完成模型假设训练时可访问整个图谱，忽略了图谱动态演化带来的挑战，如新知识融入和稀疏连接实体处理。

Method: 结合模型无关的增强层（基于全局实体相似性）和加权采样策略（强调稀疏实体边），适用于任何现有TKG完成方法。

Result: 在两个基准数据集上，总链接预测、归纳链接预测及长尾实体处理均优于现有方法，MRR提升15%。

Conclusion: 该框架有效缓解灾难性遗忘，增强了TKG完成方法的鲁棒性，特别适用于增量训练场景。

Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access
to the entire graph during training. This overlooks challenges stemming from
the evolving nature of TKGs, such as: (i) the model's requirement to generalize
and assimilate new knowledge, and (ii) the task of managing new or unseen
entities that often have sparse connections. In this paper, we present an
incremental training framework specifically designed for TKGs, aiming to
address entities that are either not observed during training or have sparse
connections. Our approach combines a model-agnostic enhancement layer with a
weighted sampling strategy, that can be augmented to and improve any existing
TKG completion method. The enhancement layer leverages a broader, global
definition of entity similarity, which moves beyond mere local neighborhood
proximity of GNN-based methods. The weighted sampling strategy employed in
training accentuates edges linked to infrequently occurring entities. We
evaluate our method on two benchmark datasets, and demonstrate that our
framework outperforms existing methods in total link prediction, inductive link
prediction, and in addressing long-tail entities. Notably, our method achieves
a 10\% improvement and a 15\% boost in MRR for these datasets. The results
underscore the potential of our approach in mitigating catastrophic forgetting
and enhancing the robustness of TKG completion methods, especially in an
incremental training context

</details>


### [13] [Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation](https://arxiv.org/abs/2507.19089)
*Shuhao Li,Weidong Yang,Yue Cui,Xiaoxing Liu,Lingkai Meng,Lipeng Ma,Fan Zhang*

Main category: cs.AI

TL;DR: 论文提出了Fine-grained Road Traffic Inference (FRTI)任务，通过有限的道路数据生成更详细的车道级交通信息，设计了两阶段框架RoadDiff来解决此任务。


<details>
  <summary>Details</summary>
Motivation: 由于传感器类型和数量的限制以及跟踪算法的准确性，获取车道级交通数据成为数据驱动模型的关键瓶颈。

Method: 设计了RoadDiff框架，包括Road-Lane Correlation Autoencoder-Decoder和Lane Diffusion Module，充分利用道路数据的时空依赖性和分布关系。

Result: 在六个不同道路条件的数据集上验证了RoadDiff模型的有效性。

Conclusion: FRTI任务和RoadDiff框架为精确交通管理提供了更高效和经济的解决方案。

Abstract: Fine-grained traffic management and prediction are fundamental to key
applications such as autonomous driving, lane change guidance, and traffic
signal control. However, obtaining lane-level traffic data has become a
critical bottleneck for data-driven models due to limitations in the types and
number of sensors and issues with the accuracy of tracking algorithms. To
address this, we propose the Fine-grained Road Traffic Inference (FRTI) task,
which aims to generate more detailed lane-level traffic information using
limited road data, providing a more energy-efficient and cost-effective
solution for precise traffic management. This task is abstracted as the first
scene of the spatio-temporal graph node generation problem. We designed a
two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.
This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the
Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies
and distribution relationships of road data to accurately infer fine-grained
lane traffic states. Based on existing research, we designed several baseline
models with the potential to solve the FRTI task and conducted extensive
experiments on six datasets representing different road conditions to validate
the effectiveness of the RoadDiff model in addressing the FRTI task. The
relevant datasets and code are available at
https://github.com/ShuhaoLii/RoadDiff.

</details>


### [14] [Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization](https://arxiv.org/abs/2507.19109)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli*

Main category: cs.AI

TL;DR: Pareto-NRPA是一种新的蒙特卡洛算法，用于离散搜索空间的多目标优化问题，扩展了单目标NRPA算法，并在多目标优化中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多目标优化问题，特别是在离散搜索空间中，现有算法可能无法同时兼顾收敛性和多样性。

Method: 扩展NRPA算法，引入多目标策略，通过并行探索解空间的不同区域，并在搜索的每个层级维护非支配前沿。

Result: 在MO-TSPTW和神经架构搜索任务中，Pareto-NRPA表现优异，尤其在受限搜索空间中超越现有进化算法。

Conclusion: Pareto-NRPA是NRPA算法在多目标优化中的首次成功扩展，具有竞争力和潜力。

Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for
multi-objective optimization problems over discrete search spaces. Extending
the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for
single-objective problems, Pareto-NRPA generalizes the nested search and policy
update mechanism to multi-objective optimization. The algorithm uses a set of
policies to concurrently explore different regions of the solution space and
maintains non-dominated fronts at each level of search. Policy adaptation is
performed with respect to the diversity and isolation of sequences within the
Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel
bi-objective variant of the Traveling Salesman Problem with Time Windows
problem (MO-TSPTW), and a neural architecture search task on well-known
benchmarks. Results demonstrate that Pareto-NRPA achieves competitive
performance against state-of-the-art multi-objective algorithms, both in terms
of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly
outperforms state-of-the-art evolutionary multi-objective algorithms on
constrained search spaces. To our knowledge, this work constitutes the first
adaptation of NRPA to the multi-objective setting.

</details>


### [15] [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
*Xuetian Chen,Yinghao Chen,Xinfeng Yuan,Zhuo Peng,Lu Chen,Yuekeng Li,Zhoujia Zhang,Yingqian Huang,Leyan Huang,Jiaqing Liang,Tianbao Xie,Zhiyong Wu,Qiushi Sun,Biqing Qi,Bowen Zhou*

Main category: cs.AI

TL;DR: OS-MAP是一个用于日常计算机自动化任务的基准测试，通过五级分类和需求层次结构评估代理能力，揭示当前代理在感知、推理和协调方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能考虑任务异质性和代理能力与实际用户需求的匹配，阻碍了针对性能力开发和实际部署。

Method: 提出OS-MAP基准，包含416个任务，按五级分类和需求层次结构评估代理能力。

Result: 实验表明，即使最先进的代理在高级任务（如感知、推理和协调）中表现不佳。

Conclusion: OS-MAP为计算机代理的研究和部署提供了结构化评估框架，揭示了当前技术的局限性。

Abstract: Computer-using agents have shown strong potential to boost human productivity
and enable new application forms across platforms. While recent advances have
led to usable applications, existing benchmarks fail to account for the
internal task heterogeneity and the corresponding agent capabilities, as well
as their alignment with actual user demands-hindering both targeted capability
development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily
computer-using automation that organizes its 416 realistic tasks across 15
applications along two key dimensions: a five-level taxonomy of automation and
a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with
real-world scenarios, OS-MAP evaluates agents along two dimensions: automation
level across a five-level taxonomy, and generalization scope across a demand
hierarchy. This design captures varying levels of required agent autonomy and
generalization, forming a performance-generalization evaluation matrix for
structured and comprehensive assessment. Experiments show that even
State-of-the-Art agents with VLM backbones struggle with higher-level tasks
involving perception, reasoning, and coordination-highlighting the need for a
deeper understanding of current strengths and limitations to drive the future
progress in computer-using agents research and deployment. All code,
environments, baselines, and data are publicly available at
https://github.com/OS-Copilot/OS-Map.

</details>


### [16] [PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring](https://arxiv.org/abs/2507.19172)
*Jiyao Wang,Xiao Yang,Qingyong Hu,Jiankai Tang,Can Liu,Dengbo He,Yuntao Wang,Yingcong Chen,Kaishun Wu*

Main category: cs.AI

TL;DR: PhysDrive是一个大规模多模态数据集，专注于车内无接触生理监测，解决了现有数据集的局限性，并提供了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有远程生理监测（RPM）数据集在规模、多样性和实际驾驶条件覆盖上存在不足，限制了其在真实场景中的应用。

Method: PhysDrive收集了48名驾驶员的多模态数据，包括RGB、近红外摄像头和毫米波雷达数据，并同步了六种生理信号作为真实值。

Result: 数据集覆盖了多种自然驾驶条件，并通过信号处理和深度学习方法进行了全面评估，建立了多模态基准。

Conclusion: PhysDrive将成为多模态驾驶员监测和智能座舱系统研究的基础资源，推动相关领域的发展。

Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for
ensuring driving safety and user experience. While remote physiological
measurement (RPM) offers a promising non-invasive solution, its translation to
real-world driving scenarios is critically constrained by the scarcity of
comprehensive datasets. Existing resources are often limited in scale, modality
diversity, the breadth of biometric annotations, and the range of captured
conditions, thereby omitting inherent real-world challenges in driving. Here,
we present PhysDrive, the first large-scale multimodal dataset for contactless
in-vehicle physiological sensing with dedicated consideration on various
modality settings and driving factors. PhysDrive collects data from 48 drivers,
including synchronized RGB, near-infrared camera, and raw mmWave radar data,
accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,
and SpO2). It covers a wide spectrum of naturalistic driving conditions,
including driver motions, dynamic natural light, vehicle types, and road
conditions. We extensively evaluate both signal-processing and deep-learning
methods on PhysDrive, establishing a comprehensive benchmark across all
modalities, and release full open-source code with compatibility for mainstream
public toolboxes. We envision PhysDrive will serve as a foundational resource
and accelerate research on multimodal driver monitoring and smart-cockpit
systems.

</details>


### [17] [Faster Lifting for Ordered Domains with Predecessor Relations](https://arxiv.org/abs/2507.19182)
*Kuncheng Zou,Jiahao Mai,Yonggang Zhang,Yuyi Wang,Ondřej Kuželka,Yuanhong Wang,Yi Chang*

Main category: cs.AI

TL;DR: 本文提出了一种新算法，直接在公理中处理前驱关系，显著提升了有序域上的提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理有序域上的前驱关系时效率低下，尤其是在涉及前驱关系时。

Method: 将前驱关系作为公理的本部分，设计了一种新算法，支持直接处理这些关系。

Result: 新算法在实验中对前驱关系实现了指数级加速，并在组合数学问题上表现优异。

Conclusion: 该算法显著提升了有序域上提升推理的效率，具有实际应用价值。

Abstract: We investigate lifted inference on ordered domains with predecessor
relations, where the elements of the domain respect a total (cyclic) order, and
every element has a distinct (clockwise) predecessor. Previous work has
explored this problem through weighted first-order model counting (WFOMC),
which computes the weighted sum of models for a given first-order logic
sentence over a finite domain. In WFOMC, the order constraint is typically
encoded by the linear order axiom introducing a binary predicate in the
sentence to impose a linear ordering on the domain elements. The immediate and
second predecessor relations are then encoded by the linear order predicate.
Although WFOMC with the linear order axiom is theoretically tractable, existing
algorithms struggle with practical applications, particularly when the
predecessor relations are involved. In this paper, we treat predecessor
relations as a native part of the axiom and devise a novel algorithm that
inherently supports these relations. The proposed algorithm not only provides
an exponential speedup for the immediate and second predecessor relations,
which are known to be tractable, but also handles the general k-th predecessor
relations. The extensive experiments on lifted inference tasks and
combinatorics math problems demonstrate the efficiency of our algorithm,
achieving speedups of a full order of magnitude.

</details>


### [18] [Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments](https://arxiv.org/abs/2507.19261)
*Osama Almurshed,Ashish Kaushal,Asmail Muftah,Nitin Auluck,Omer Rana*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识嫁接的新方法，通过将大型模型的特征移植到小型模型中，显著减小模型体积并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在资源受限环境中部署的挑战，优化模型大小与性能的权衡。

Method: 引入知识嫁接机制，将大型模型的特征（scion）移植到小型模型（rootstock）中。

Result: 模型体积减少88.54%，验证准确率提升至89.97%，在测试数据上表现优异（90.45%）。

Conclusion: 该方法为资源受限设备上的AI部署提供了高效解决方案，可广泛应用于边缘计算场景。

Abstract: The increasing adoption of Artificial Intelligence (AI) has led to larger,
more complex models with numerous parameters that require substantial computing
power -- resources often unavailable in many real-world application scenarios.
Our paper addresses this challenge by introducing knowledge grafting, a novel
mechanism that optimizes AI models for resource-constrained environments by
transferring selected features (the scion) from a large donor model to a
smaller rootstock model. The approach achieves an 88.54% reduction in model
size (from 64.39 MB to 7.38 MB), while improving generalization capability of
the model. Our new rootstock model achieves 89.97% validation accuracy (vs.
donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and
performs exceptionally well on unseen test data with 90.45% accuracy. It
addresses the typical size vs performance trade-off, and enables deployment of
AI frameworks on resource-constrained devices with enhanced performance. We
have tested our approach on an agricultural weed detection scenario, however,
it can be extended across various edge computing scenarios, potentially
accelerating AI adoption in areas with limited hardware/software support -- by
mirroring in a similar manner the horticultural grafting enables productive
cultivation in challenging agri-based environments.

</details>


### [19] [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](https://arxiv.org/abs/2507.19263)
*Achille Morenville,Éric Piette*

Main category: cs.AI

TL;DR: 论文研究了在不完全信息游戏中，基于约束和概率两种信念表示方法对代理决策的影响，发现约束方法效果接近概率方法。


<details>
  <summary>Details</summary>
Motivation: 解决不完全信息游戏中代理需基于部分知识决策的挑战，减少游戏特定推理逻辑的需求。

Method: 使用约束满足问题和信念传播两种方法表示信念，并在两种游戏中评估其影响。

Result: 约束基信念与概率推理效果相当，代理性能差异微小。

Conclusion: 约束基信念状态在许多场景下足以支持有效决策。

Abstract: In imperfect-information games, agents must make decisions based on partial
knowledge of the game state. The Belief Stochastic Game model addresses this
challenge by delegating state estimation to the game model itself. This allows
agents to operate on externally provided belief states, thereby reducing the
need for game-specific inference logic. This paper investigates two approaches
to represent beliefs in games with hidden piece identities: a constraint-based
model using Constraint Satisfaction Problems and a probabilistic extension
using Belief Propagation to estimate marginal probabilities. We evaluated the
impact of both representations using general-purpose agents across two
different games. Our findings indicate that constraint-based beliefs yield
results comparable to those of probabilistic inference, with minimal
differences in agent performance. This suggests that constraint-based belief
states alone may suffice for effective decision-making in many settings.

</details>


### [20] [Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges](https://arxiv.org/abs/2507.19364)
*Patrick Taillandier,Jean Daniel Zucker,Arnaud Grignard,Benoit Gaudou,Nghi Quang Huynh,Alexis Drogoul*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在社交模拟中的应用潜力与限制，提出结合传统建模平台的混合方法。


<details>
  <summary>Details</summary>
Motivation: 从计算社会科学角度分析LLMs在模拟人类认知和社会行为中的潜力与问题。

Method: 分为三部分：1) 评估LLMs的认知能力与限制；2) 调查多代理模拟框架中的应用；3) 区分适用场景并提倡混合方法。

Result: LLMs在交互模拟中表现良好，但在解释性或预测性建模中存在不足，需结合传统方法。

Conclusion: 建议将LLMs与传统代理建模平台结合，以平衡灵活性与透明度。

Abstract: This position paper examines the use of Large Language Models (LLMs) in
social simulation, analyzing both their potential and their limitations from a
computational social science perspective. The first part reviews recent
findings on the ability of LLMs to replicate key aspects of human cognition,
including Theory of Mind reasoning and social inference, while also
highlighting significant limitations such as cognitive biases, lack of true
understanding, and inconsistencies in behavior. The second part surveys
emerging applications of LLMs in multi-agent simulation frameworks, focusing on
system architectures, scale, and validation strategies. Notable projects such
as Generative Agents (Smallville) and AgentSociety are discussed in terms of
their design choices, empirical grounding, and methodological innovations.
Particular attention is given to the challenges of behavioral fidelity,
calibration, and reproducibility in large-scale LLM-driven simulations. The
final section distinguishes between contexts where LLMs, like other black-box
systems, offer direct value-such as interactive simulations and serious
games-and those where their use is more problematic, notably in explanatory or
predictive modeling. The paper concludes by advocating for hybrid approaches
that integrate LLMs into traditional agent-based modeling platforms (GAMA,
Netlogo, etc), enabling modelers to combine the expressive flexibility of
language-based reasoning with the transparency and analytical rigor of
classical rule-based systems.

</details>


### [21] [Learning neuro-symbolic convergent term rewriting systems](https://arxiv.org/abs/2507.19372)
*Flavio Petruzzellis,Alberto Testolin,Alessandro Sperduti*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经符号架构的框架，用于学习收敛的项重写系统，并实现了两种模块化架构（NRS和FastNRS），在泛化和效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决符号算法学习的泛化和分布外性能问题。

Method: 采用神经符号架构，设计NRS和FastNRS两种实现，结合算法启发式设计和关键架构元素。

Result: 在数学公式简化任务中表现优异，FastNRS在内存效率、训练速度和推理时间上显著改进，且在多领域学习中表现灵活。

Conclusion: 提出的系统在性能上优于多个强基线模型，包括专用和通用大模型，展示了其在符号算法学习中的潜力。

Abstract: Building neural systems that can learn to execute symbolic algorithms is a
challenging open problem in artificial intelligence, especially when aiming for
strong generalization and out-of-distribution performance. In this work, we
introduce a general framework for learning convergent term rewriting systems
using a neuro-symbolic architecture inspired by the rewriting algorithm itself.
We present two modular implementations of such architecture: the Neural
Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a
result of algorithmic-inspired design and key architectural elements, both
models can generalize to out-of-distribution instances, with FastNRS offering
significant improvements in terms of memory efficiency, training speed, and
inference time. We evaluate both architectures on four tasks involving the
simplification of mathematical formulas and further demonstrate their
versatility in a multi-domain learning scenario, where a single model is
trained to solve multiple types of problems simultaneously. The proposed system
significantly outperforms two strong neural baselines: the Neural Data Router,
a recent transformer variant specifically designed to solve algorithmic
problems, and GPT-4o, one of the most powerful general-purpose large-language
models. Moreover, our system matches or outperforms the latest o1-preview model
from OpenAI that excels in reasoning benchmarks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [22] [RIS Codebook Index Assignment under Imperfect Control Links Using TSP-Inspired Optimization](https://arxiv.org/abs/2507.18727)
*Liangshun Wu,Wen Chen,Qingqing Wu,Xudong Bai,Kunlun Wang*

Main category: cs.IT

TL;DR: 论文提出了一种针对RIS码本索引分配的组合优化方法，通过三阶段启发式算法解决反馈错误导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在无线反馈信道中，即使罕见的比特错误也可能导致RIS状态与预期不匹配，从而降低系统性能。

Method: 将问题建模为旅行商问题（TSP），并提出三阶段启发式算法（预备阶段、散弹阶段和模糊连接阶段）。

Result: 仿真结果表明，该方法优于传统索引策略，对索引错误具有接近最优的鲁棒性，且可扩展性强。

Conclusion: 未来工作包括多比特纠错和时变信道的在线自适应映射。

Abstract: Reconfigurable Intelligent Surfaces (RIS) promise transformative gains in
wireless communications by enabling programmable control of the propagation
environment through discrete phase configurations. In practical deployments,
the control of RIS phase states is typically managed using finite codebooks,
with configuration indices transmitted over low latency, yet imperfect,
wireless feedback channels. Even rare feedback bit errors can lead to
significant mismatches between intended and applied RIS states, degrading
system performance. This paper addresses the challenge of robust RIS codebook
index assignment by formulating it as a combinatorial optimization problem,
equivalent to the Traveling Salesman Problem (TSP), where codewords are
"cities" and edge weights reflect SNR degradation under codeword confusion. A
novel three-phase heuristic algorithm is proposed to solve this, consisting of
a provision phase, a shotgun phase, and a fuzzy concatenation phase. Simulation
results show that the method outperforms conventional indexing strategies and
achieves near-optimal robustness to index errors, while also being scalable and
hardwareagnostic for real time deployment. Future work includes multiple bits
error correction and online adaptive mapping for time varying channels.

</details>


### [23] [EDPC: Accelerating Lossless Compression via Lightweight Probability Models and Decoupled Parallel Dataflow](https://arxiv.org/abs/2507.18969)
*Zeyi Lu,Xiaoxiao Ma,Yujun Huang,Minxiao Chen,Bin Chen,Baoyi An,Shu-Tao Xia*

Main category: cs.IT

TL;DR: 提出了一种高效的双路径并行压缩框架（EDPC），通过分层优化提升压缩效率和建模能力，实验表明其压缩速度和压缩比均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多源多媒体数据的爆炸性增长对传输和存储提出了更高要求，现有自回归压缩模型（ACMs）在压缩比和实时处理方面存在不足。

Method: 提出EDPC框架，包括信息流细化（IFR）指标、多路径字节细化块（MBRB）、潜在转换引擎（LTE）和解耦管道压缩架构（DPCA）。

Result: EDPC压缩速度提升2.7倍，压缩比提高3.2%。

Conclusion: EDPC是带宽受限场景下大规模多媒体数据实时处理的高效解决方案。

Abstract: The explosive growth of multi-source multimedia data has significantly
increased the demands for transmission and storage, placing substantial
pressure on bandwidth and storage infrastructures. While Autoregressive
Compression Models (ACMs) have markedly improved compression efficiency through
probabilistic prediction, current approaches remain constrained by two critical
limitations: suboptimal compression ratios due to insufficient fine-grained
feature extraction during probability modeling, and real-time processing
bottlenecks caused by high resource consumption and low compression speeds. To
address these challenges, we propose Efficient Dual-path Parallel Compression
(EDPC), a hierarchically optimized compression framework that synergistically
enhances modeling capability and execution efficiency via coordinated dual-path
operations. At the modeling level, we introduce the Information Flow Refinement
(IFR) metric grounded in mutual information theory, and design a Multi-path
Byte Refinement Block (MBRB) to strengthen cross-byte dependency modeling via
heterogeneous feature propagation. At the system level, we develop a Latent
Transformation Engine (LTE) for compact high-dimensional feature representation
and a Decoupled Pipeline Compression Architecture (DPCA) to eliminate
encoding-decoding latency through pipelined parallelization. Experimental
results demonstrate that EDPC achieves comprehensive improvements over
state-of-the-art methods, including a 2.7x faster compression speed, and a 3.2%
higher compression ratio. These advancements establish EDPC as an efficient
solution for real-time processing of large-scale multimedia data in
bandwidth-constrained scenarios. Our code is available at
https://github.com/Magie0/EDPC.

</details>


### [24] [Dynamic Agile Reconfigurable Intelligent Surface Antenna (DARISA) MIMO: DoF Analysis and Effective DoF Optimization](https://arxiv.org/abs/2507.19136)
*Jiale Bai,Hui-Ming Wang,Liang Jin*

Main category: cs.IT

TL;DR: 本文提出了一种动态可重构智能表面天线（DARISA）阵列，集成到MIMO收发器中，通过快速相位响应调整策略（DAAPR）提升系统自由度（DoF）和有效自由度（EDoF），并通过仿真验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统MIMO系统中相位响应调整速度慢的问题，通过动态可重构智能表面天线阵列实现快速智能调整，以提升系统性能。

Method: 提出DARISA MIMO系统，采用DAAPR策略，结合分数规划（FP）和半定松弛（SDR）算法优化相位响应，最大化EDoF。

Result: 理论分析表明，DAAPR策略在接收DARISA数量少于发射时能显著提升系统DoF；仿真显示增加调整频率、元件密度和相位量化精度可增强EDoF。

Conclusion: DARISA MIMO系统通过DAAPR策略和优化算法显著提升了系统性能，密集部署元件可弥补相位量化精度不足的损失。

Abstract: In this paper, we propose a dynamic agile reconfigurable intelligent surface
antenna (DARISA) array integrated into multi-input multi-output (MIMO)
transceivers. Each DARISA comprises a number of metasurface elements activated
simultaneously via a parallel feed network. The proposed system enables rapid
and intelligent phase response adjustments for each metasurface element within
a single symbol duration, facilitating a dynamic agile adjustment of phase
response (DAAPR) strategy. By analyzing the theoretical degrees of freedom
(DoF) of the DARISA MIMO system under the DAAPR framework, we derive an
explicit relationship between DoF and critical system parameters, including
agility frequentness (i.e., the number of phase adjustments of metasurface
elements during one symbol period), cluster angular spread of wireless
channels, DARISA array size, and the number of transmit/receive DARISAs. The
DoF result reveals a significant conclusion: when the number of receive DARISAs
is smaller than that of transmit DARISAs, the DAAPR strategy of the DARISA MIMO
enhances the overall system DoF. Furthermore, relying on DoF alone to measure
channel capacity is insufficient, so we analyze the effective DoF (EDoF) that
reflects the impacts of the DoF and channel matrix singular value distribution
on capacity. We show channel capacity monotonically increases with EDoF, and
optimize the agile phase responses of metasurface elements by using fractional
programming (FP) and semidefinite relaxation (SDR) algorithms to maximize the
EDoF. Simulations validate the theoretical DoF gains and reveal that increasing
agility frequentness, metasurface element density, and phase quantization
accuracy can enhance the EDoF. Additionally, densely deployed elements can
compensate for the loss in communication performance caused by lower phase
quantization accuracy.

</details>


### [25] [Achievable Rates for a Distributed Antenna System with No Channel State Information at the Central Processor](https://arxiv.org/abs/2507.19177)
*Yi Song,Hao Xu,Kai Wan,Kai-Kit Wong,Giuseppe Caire,Shlomo Shamai*

Main category: cs.IT

TL;DR: 论文研究了无线通信中分散式架构下的上行链路容量问题，重点关注信道状态随机性及导频开销对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 实际中信道状态是随机过程，需通过导频符号估计，而导频开销可能占用大量信道相干块，影响系统效率。

Method: 采用钻石网络模型，分析单用户与两个中继下的遍历可达速率，假设中继已知信道状态而中央处理器未知。

Result: 在信道状态随机且导频开销显著的情况下，推导了系统的遍历可达速率。

Conclusion: 论文为分散式架构下的无线通信系统设计提供了理论支持，强调了信道状态估计和导频开销管理的重要性。

Abstract: A recent trend in wireless communications considers the migration of
traditional monolithic base stations to the so-called disaggregated
architecture, where radio units (RUs) implement only the low-level physical
layer functionalities such as demodulation, and A/D conversion, while the
high-level physical layer, such as channel decoding, is implemented as
software-defined functions running on general-purpose hardware in some remote
central processing unit (CP). The corresponding information theoretic model for
the uplink (from the wireless users to the CP) is a multiaccess-relay channel
with primitive oblivious relays. The relays (RUs) are oblivious, as they are
agnostic of the users codebooks, and primitive, since the fronthaul links (from
RUs to CP) are error-free with limited capacity. This class of networks has
been intensely studied in the information theoretic literature, where several
approximated or exact (under certain conditions) capacity results have been
derived. In particular, in the Gaussian case, the model has been analyzed for
fixed and known channel state. This paper is motivated by the fact that, in
practice, the channel state is a random process, and it is estimated at the
base station side through uplink pilot symbols sent by the users. The pilot
dimension may take up a large portion of the channel coherence block, i.e., the
number of symbols over which the channel state remains approximately constant.
Hence, sending both pilot and data symbols from the relays to the CP may
require a significant overhead, especially when the fronthaul capacity is
small. As a prototypical problem, we consider the ergodic achievable rate for a
diamond network formed by a single user and two relays where the channel state
is known at the relays, but not known at the CP.

</details>


### [26] [Overview of 3GPP Release 19 Study on Channel Modeling Enhancements to TR 38.901 for 6G](https://arxiv.org/abs/2507.19266)
*Hitesh Poddar,Dimitri Gold,Daewon Lee,Nan Zhang,Gokul Sridharan,Henrik Asplund,Mansoor Shaf*

Main category: cs.IT

TL;DR: 本文介绍了3GPP Rel 19对7-24 GHz频段信道模型的改进，填补了6G技术中的关键空白。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的发展，7-24 GHz频段的准确信道模型对6G技术至关重要，但现有模型存在不足。

Method: 通过3GPP Rel 19研究，改进了信道模型，包括SMa场景建模、UT天线模型、簇和射线数量可变性等。

Result: 研究成功增强了信道模型的准确性，包括NF传播和SNS效应等关键特性。

Conclusion: 这些改进为未来6G部署提供了更可靠的信道模型基础。

Abstract: Channel models are a fundamental component of wireless communication systems,
providing critical insights into the physics of radio wave propagation. As
wireless systems evolve every decade, the development of accurate and
standardized channel models becomes increasingly important for the development,
evaluation and performance assessment of emerging technologies. An effort to
develop a standardized channel model began around 2000 through the Third
Generation Partnership Project (3GPP) and the International Telecommunication
Union (ITU) with the aim of addressing a broad range of frequencies from sub-1
GHz to 100 GHz. Prior efforts focused heavily on sub-6 GHz bands and mmWave
bands, and there exist some gaps in accurately modeling the 7-24 GHz frequency
range, a promising candidate band for 6G. To address these gaps, 3GPP approved
a Release (Rel) 19 channel modeling study. This study resulted in several
enhancements to the channel models, including the ability to accurately model a
Suburban Macrocell (SMa) scenario, realistic User Terminal (UT) antenna models,
variability in the number of clusters, variability in the number of rays per
cluster, a framework for capturing variability in power among all
polarizations, near field (NF) propagation, and spatial non-stationarity (SNS)
effects, all of which may be crucial for future 6G deployments. This paper
presents the outcomes of this study and provides an overview of the underlying
rationale, and key discussions that guided the validation, refinement, and
enhancements of the 3GPP TR 38.901 channel models.

</details>


### [27] [Sparse Recovery from Group Orbits](https://arxiv.org/abs/2507.19274)
*Timm Gilles,Hartmut Führ*

Main category: cs.IT

TL;DR: 本文提出了一种基于有限群表示的随机轨道生成的结构化测量框架，用于稀疏恢复。分析了固定采样集和随机采样集两种情况，并推导了确保受限等距性所需的测量数量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏恢复方法对测量方案的结构限制较多，而实际问题中测量通常具有显著结构。本文旨在填补这一空白。

Method: 通过有限群表示的随机轨道生成结构化测量，分析固定和随机采样集两种情况，推导所需测量数量。

Result: 结果表明，所需测量数量取决于具体表示形式，并分析了多种表示（如左正则表示）对恢复效果的影响。

Conclusion: 本文不仅为群结构测量的稀疏恢复建立了框架，还推广了现有测量方案（如部分随机循环矩阵）。

Abstract: While most existing sparse recovery results allow only minimal structure
within the measurement scheme, many practical problems possess significant
structure. To address this gap, we present a framework for structured
measurements that are generated by random orbits of a group representation
associated with a finite group. We differentiate between two scenarios: one in
which the sampling set is fixed and another in which the sampling set is
randomized. For each case, we derive an estimate for the number of measurements
required to ensure that the restricted isometry property holds with high
probability. These estimates are contingent upon the specific representation
employed. For this reason, we analyze and characterize various representations
that yield favorable recovery outcomes, including the left regular
representation. Our work not only establishes a comprehensive framework for
sparse recovery of group-structured measurements but also generalizes
established measurement schemes, such as those derived from partial random
circulant matrices.

</details>


### [28] [Low-Complexity 6DMA Rotation and Position Optimization Based on Statistical Channel Information](https://arxiv.org/abs/2507.19309)
*Qijun Jiang,Xiaodan Shao,Rui Zhang*

Main category: cs.IT

TL;DR: 论文提出了一种新的6DMA优化方法，通过顺序优化旋转和位置，显著降低了计算复杂度，同时保持了与交替优化方法相当的通信性能。


<details>
  <summary>Details</summary>
Motivation: 6DMA技术通过灵活调整天线的3D位置和旋转，可以充分利用无线信道的空间变化性，但现有方法计算复杂度高。

Method: 提出了一种顺序优化方法，先确定最优旋转，再在约束条件下找到可实现的位置。

Result: 仿真结果表明，该方法显著降低了计算复杂度，同时性能与交替优化方法相当，优于固定位置/旋转天线阵列。

Conclusion: 该方法为6DMA设计提供了一种高效且性能优越的解决方案。

Abstract: The six-dimensional movable antenna (6DMA) is a promising technology to fully
exploit spatial variation in wireless channels by allowing flexible adjustment
of three-dimensional (3D) positions and rotations of antennas at the
transceiver. In this paper, we consider a 6DMA-equipped base station (BS) and
aim to maximize the average sum logarithmic rate of all users served by the BS
by jointly designing 6DMA surface positions and rotations based on statistical
channel information (SCI). Different from prior works on 6DMA design which use
alternating optimization to iteratively update surface positions and rotations,
we propose a new sequential optimization method that first determines the
optimal rotations and then identifies feasible positions to realize these
rotations under practical antenna placement constraints. Simulation results
show that our proposed optimization scheme significantly reduces the
computational complexity of conventional alternating optimization (AO), while
achieving communication performance comparable to the AO-based approach and
superior to existing fixed-position/rotation antenna arrays.

</details>


### [29] [On Anti-collusion Codes for Averaging Attack in Multimedia Fingerprinting](https://arxiv.org/abs/2507.19384)
*Jing Jiang,Cailin Wen,Minquan Cheng*

Main category: cs.IT

TL;DR: 论文提出了一种新型的追踪算法和指纹编码（SMIPPC），用于多媒体指纹技术中抵抗平均攻击，提高了编码率。


<details>
  <summary>Details</summary>
Motivation: 现有指纹编码的编码率较低，原因是其强组合结构和简单追踪算法。

Method: 提出新型追踪算法和二进制强可识别父属性码（SMIPPC）及其级联码。

Result: SMIPPC的编码率高于现有编码，且追踪算法具有更强的追踪能力。

Conclusion: SMIPPC及其追踪算法在多媒体指纹技术中表现出更高的效率和更强的追踪能力。

Abstract: Multimedia fingerprinting is a technique to protect the copyrighted contents
against being illegally redistributed under various collusion attack models.
Averaging attack is the most fair choice for each colluder to avoid detection,
and also makes the pirate copy have better perceptional quality. This makes
such an attack one of the most feasible approaches to carrying out collusion.
In order to trace all the colluders, several types of multimedia fingerprinting
codes were introduced to construct fingerprints resistant to averaging attacks
on multimedia contents, such as AND anti-collusion codes (AND-ACCs), binary
separable codes (SCs), logical anti-collusion codes (LACCs), binary frameproof
codes (FPCs), binary strongly-separable codes (SSCs) and binary secure code
with list decoding (SCLDs). Then codes with the rate as high as possible are
desired. However, the existing fingerprinting codes have low code rate due to
the strong combinatorial structure. The reason is that the previous research
methods adopted simple tracing algorithms. In this paper, we first propose
novel tracing algorithms and then find appropriate fingerprinting codes with
weaker combinatorial structure, i.e., the binary strongly identifiable parent
property code for multimedia fingerprinting (SMIPPC) and its concatenated code.
Theoretical comparisons and numerical comparisons show that SMIPPCs have higher
code rates than those of the existing codes due to their weaker combinatorial
structures. It is worth noting that SMIPPCs can only trace a part of colluders
by using the previous tracing algorithm and the concatenated SMIPPC may be not
an SMIPPC. This implies that our tracing algorithms have strong traceability.

</details>


### [30] [Sample Abundance for Signal Processing: A Brief Introduction](https://arxiv.org/abs/2507.19415)
*Arian Eamaz,Farhang Yeganegi,Mojtaba Soltanalian*

Main category: cs.IT

TL;DR: 论文介绍了样本丰富性概念及其在低精度信号处理中的优势，展示了如何通过大量低精度测量简化传统复杂约束，并提供了算法、理论保证和计算复杂度降低的现象。


<details>
  <summary>Details</summary>
Motivation: 探讨样本丰富性在低精度信号处理中的潜力，解决传统方法中矩阵半定性和秩条件的高成本问题。

Method: 利用大量低精度测量，将复杂约束转化为简单的超定线性可行性问题，并通过关键算法和有限体积属性提供理论保证。

Result: 展示了样本丰富性现象，其中计算复杂度显著降低，证明了方法的有效性。

Conclusion: 样本丰富性为低精度信号处理提供了高效解决方案，简化了传统复杂问题。

Abstract: This paper reports, by way of introduction, on the advances made by our group
and the broader signal processing community on the concept of sample abundance;
a phenomenon that naturally arises in one-bit and few-bit signal processing
frameworks. By leveraging large volumes of low-precision measurements, we show
how traditionally costly constraints, such as matrix semi-definiteness and rank
conditions, become redundant, yielding simple overdetermined linear feasibility
problems. We illustrate key algorithms, theoretical guarantees via the Finite
Volume Property, and the sample abundance singularity phenomenon, where
computational complexity sharply drops.

</details>
