<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.IT](#cs.IT) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 论文提出LiLM-RDB-SFC方法，结合轻量级语言模型（LiLM）和关系数据库（RDB）优化SFC供应，FLAN-T5模型表现优于BART和SQLCoder。


<details>
  <summary>Details</summary>
Motivation: 现代SDN和NFV环境中，SFC管理和VNF放置是关键挑战，传统DRL方法因依赖结构化数据和固定规则而适应性不足。

Method: 结合LiLM（BART和FLAN-T5）与RDB，通过查询网络状态指导DRL模型，优化SFC供应。

Result: FLAN-T5在测试损失（0.00161）、准确率（94.79%）和处理时间（2h 2min）上优于BART和SQLCoder。

Conclusion: LiLM-RDB-SFC方法显著提升SFC管理效率和适应性，FLAN-T5是更优选择。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [2] [Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability](https://arxiv.org/abs/2507.10928)
*Matthew Yang Liu,Chuang Chen,Pengcheng Lv,Hui Guo,Yanan Zhang,Cong Wang,Yusen Li,Zhenyu Li,Yu-Chu Tian*

Main category: cs.NI

TL;DR: Arcturus是一个云原生全球加速器框架，通过利用低成本、异构云资源，解决了现有GA服务的高成本和部署不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 现有全球加速器服务绑定特定云提供商，导致高成本和部署不灵活，尤其在大规模或预算敏感场景下。

Method: Arcturus采用两平面设计：转发平面构建自适应代理网络，调度平面通过轻量级定量优化协调负载和路由。

Result: 在百万RPS测试中，Arcturus性能提升1.7倍，成本降低71%，资源效率超过80%。

Conclusion: Arcturus证明了在云资源大规模使用中的高效性，优于商业GA服务。

Abstract: Global Accelerator (GA) services play a vital role in ensuring low-latency,
high-reliability communication for real-time interactive applications. However,
existing GA offerings are tightly bound to specific cloud providers, resulting
in high costs, rigid deployment, and limited flexibility, especially for
large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA
framework that revisits the design of GA systems by leveraging low-cost,
heterogeneous cloud resources across multiple providers. Rather than relying on
fixed, high-end infrastructure, Arcturus dynamically constructs its
acceleration network and balances performance, stability, and resource
efficiency. To achieve this, Arcturus introduces a two-plane design: a
forwarding plane that builds a proxy network with adaptive control, and a
scheduling plane that coordinates load and routing through lightweight,
quantitative optimization. Evaluations under millions of RPS show that Arcturus
outperforms commercial GA services by up to 1.7X in acceleration performance,
reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating
efficient use of cloud resources at scale.

</details>


### [3] [SIMCODE: A Benchmark for Natural Language to ns-3 Network Simulation Code Generation](https://arxiv.org/abs/2507.11014)
*Tasnim Ahmed,Mirza Mohammad Azwad,Salimur Choudhury*

Main category: cs.NI

TL;DR: SIMCODE是首个评估LLMs生成ns-3仿真代码能力的基准，包含400个任务，测试了三种主流LLMs的性能，发现GPT-4.1表现最佳但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有工具主要关注交互式自动化，缺乏对LLMs在特定领域（如ns-3仿真）代码生成能力的系统评估。

Method: 引入SIMCODE基准，包含400个任务，评估三种LLMs（Gemini-2.0、GPT-4.1、Qwen-3）在六种提示技术下的表现，并研究任务特定微调的影响。

Result: GPT-4.1表现最佳，但执行准确率仍有限，主要错误为缺少头文件和API不匹配。

Conclusion: SIMCODE为评估LLMs和领域感知生成系统提供了基础，未来需进一步提升准确率。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation across various domains. However, their effectiveness in
generating simulation scripts for domain-specific environments like ns-3
remains underexplored. Despite the growing interest in automating network
simulations, existing tools primarily focus on interactive automation over
rigorous evaluation. To facilitate systematic evaluation, we introduce SIMCODE,
the first benchmark to evaluate LLMs' ability to generate ns-3 simulation code
from natural language. SIMCODE includes 400 tasks across introductory,
intermediate, and advanced levels, with solutions and test cases. Using
SIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3,
across six prompt techniques. Furthermore, investigating task-specific
fine-tuning's impact reveals that while GPT-4.1 outperforms others, execution
accuracy remains modest, with substantial room for improvement. Error analysis
identifies missing headers and API mismatches as dominant failures.
Nevertheless, SIMCODE provides a foundational step toward evaluating LLMs and
research in domain-aware generative systems.

</details>


### [4] [Graph-based Fingerprint Update Using Unlabelled WiFi Signals](https://arxiv.org/abs/2507.11038)
*Ka Ho Chiu,Handi Yin,Weipeng Zhuo,Chul-Ho Lee,S. -H. Gary Chan*

Main category: cs.NI

TL;DR: 论文提出了一种名为GUFU的图神经网络方法，用于利用未标记信号更新WiFi指纹数据库，显著提高了指纹适应性和定位精度。


<details>
  <summary>Details</summary>
Motivation: WiFi信号环境随时间变化，现有方法无法有效更新指纹数据库或处理新AP，需要一种更高效的方法。

Method: GUFU结合图神经网络和链接预测算法，利用未标记信号和新AP重新训练网络，并更新指纹信号向量。

Result: 在四个大型场景实验中，GUFU在RSS值和定位预测上的误差分别减少了21.4%和29.8%。

Conclusion: GUFU是一种高效的指纹更新方法，显著优于现有技术。

Abstract: WiFi received signal strength (RSS) environment evolves over time due to
movement of access points (APs), AP power adjustment, installation and removal
of APs, etc. We study how to effectively update an existing database of
fingerprints, defined as the RSS values of APs at designated locations, using a
batch of newly collected unlabelled (possibly crowdsourced) WiFi signals. Prior
art either estimates the locations of the new signals without updating the
existing fingerprints or filters out the new APs without sufficiently embracing
their features. To address that, we propose GUFU, a novel effective graph-based
approach to update WiFi fingerprints using unlabelled signals with possibly new
APs. Based on the observation that similar signal vectors likely imply physical
proximity, GUFU employs a graph neural network (GNN) and a link prediction
algorithm to retrain an incremental network given the new signals and APs.
After the retraining, it then updates the signal vectors at the designated
locations. Through extensive experiments in four large representative sites,
GUFU is shown to achieve remarkably higher fingerprint adaptivity as compared
with other state-of-the-art approaches, with error reduction of 21.4% and 29.8%
in RSS values and location prediction, respectively.

</details>


### [5] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 利用机器学习预测Wi-Fi网络中的信道质量，优化工业应用网络性能。


<details>
  <summary>Details</summary>
Motivation: 工业及关键任务应用对无线网络的鲁棒性、可靠性和确定性需求增加。

Method: 使用卷积神经网络和长短期记忆网络分析真实Wi-Fi数据集，比较预测精度和计算复杂度。

Result: 帧交付比可可靠预测，卷积神经网络在计算效率和内存消耗上更优。

Conclusion: 卷积神经网络适合嵌入式及工业系统，提升网络性能。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


### [6] [Resilient Time-Sensitive Networking for Industrial IoT: Configuration and Fault-Tolerance Evaluation](https://arxiv.org/abs/2507.11250)
*Mohamed Seliem,Dirk Pesch,Utz Roedig,Cormac Sreenan*

Main category: cs.NI

TL;DR: IN2C是一个基于OMNeT++/INET的模块化仿真框架，用于评估TSN在故障条件下的性能，结果表明FRER能消除丢包并实现亚毫秒级恢复，但会增加链路利用率。


<details>
  <summary>Details</summary>
Motivation: 评估TSN在工业系统中的容错能力，特别是在现实故障条件下的表现。

Method: 使用IN2C框架模拟两个同步生产单元，集成TSN核心功能（如时间同步、流量整形、FRER等），并通过XML驱动的故障注入评估四种故障场景。

Result: FRER消除了丢包并实现亚毫秒级恢复，但链路利用率提高了2-3倍。

Conclusion: 研究结果为在带宽受限的工业环境中部署TSN提供了实用指导。

Abstract: Time-Sensitive Networking (TSN) is increasingly adopted in industrial systems
to meet strict latency, jitter, and reliability requirements. However,
evaluating TSN's fault tolerance under realistic failure conditions remains
challenging. This paper presents IN2C, a modular OMNeT++/INET-based simulation
framework that models two synchronized production cells connected to
centralized infrastructure. IN2C integrates core TSN features, including time
synchronization, traffic shaping, per-stream filtering, and Frame Replication
and Elimination for Redundancy (FRER), alongside XML-driven fault injection for
link and node failures. Four fault scenarios are evaluated to compare TSN
performance with and without redundancy. Results show that FRER eliminates
packet loss and achieves submillisecond recovery, though with 2-3x higher link
utilization. These findings offer practical guidance for deploying TSN in
bandwidth-constrained industrial environments.

</details>


### [7] [JamShield: A Machine Learning Detection System for Over-the-Air Jamming Attacks](https://arxiv.org/abs/2507.11483)
*Ioannis Panitsas,Yagmur Yigit,Leandros Tassiulas,Leandros Maglaras,Berk Canberk*

Main category: cs.NI

TL;DR: JamShield是一种动态干扰检测系统，通过混合特征选择和实时分类算法调整，显著提高了无线网络中干扰攻击的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无线网络因共享通信媒介易受干扰攻击，现有检测方法依赖模拟数据或有限数据集，难以在真实场景中有效应用。

Method: JamShield采用混合特征选择和动态分类算法调整，基于自收集和公开数据集进行训练。

Result: 实验结果显示，JamShield在检测率、精确度和召回率上显著优于现有方法，同时减少了误报和漏检。

Conclusion: JamShield为无线网络中的干扰攻击检测提供了可靠且高效的解决方案。

Abstract: Wireless networks are vulnerable to jamming attacks due to the shared
communication medium, which can severely degrade performance and disrupt
services. Despite extensive research, current jamming detection methods often
rely on simulated data or proprietary over-the-air datasets with limited
cross-layer features, failing to accurately represent the real state of a
network and thus limiting their effectiveness in real-world scenarios. To
address these challenges, we introduce JamShield, a dynamic jamming detection
system trained on our own collected over-the-air and publicly available
dataset. It utilizes hybrid feature selection to prioritize relevant features
for accurate and efficient detection. Additionally, it includes an
auto-classification module that dynamically adjusts the classification
algorithm in real-time based on current network conditions. Our experimental
results demonstrate significant improvements in detection rate, precision, and
recall, along with reduced false alarms and misdetections compared to
state-of-the-art detection algorithms, making JamShield a robust and reliable
solution for detecting jamming attacks in real-world wireless networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP是一种新型框架，通过持久、安全且可语义搜索的内存共享解决AI代理的短暂内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构存在短暂内存限制，阻碍了跨会话和代理边界的有效协作与知识共享。

Method: SAMEP采用分布式内存存储库，结合向量语义搜索、加密访问控制（AES-256-GCM）和标准化API（兼容MCP、A2A）。

Result: 实验显示，SAMEP减少了73%的冗余计算，提升了89%的上下文相关性得分，并完全符合监管要求。

Conclusion: SAMEP为持久、协作的AI代理生态系统提供了新范式，同时保障安全与隐私。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [9] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于VQ-VAE的“AI母语”框架，证明无需外部归纳偏置，代理可通过内生符号系统实现有效通信，并提出了三个理论见解。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化多代理强化学习中因“联合探索困境”导致的“通信真空均衡”问题，质疑传统方法中人工归纳偏置的必要性。

Method: 采用基于VQ-VAE的“AI母语”框架，研究代理内生符号系统的自发语义压缩和纳什均衡驱动的语义收敛。

Result: 代理实现了无需外部偏置的有效符号通信，符号使用呈现幂律分布，并提出了三个理论见解。

Conclusion: 内生符号系统可自然实现通信，为符号主义与连接主义的结合提供了新思路，未来将探索HQ-VAE和RL低层预训练。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [10] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 论文提出了一种模块化的多智能体AI视觉分类框架，结合了通用多模态智能体、非视觉推理协调器和RAG模块，显著提升了零样本场景下的分类准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI在零样本场景下的可信度问题，特别是在无需微调的情况下。

Method: 采用模块化框架，结合多模态智能体、推理协调器和RAG模块，通过置信度校准和图像检索优化性能。

Result: 在零样本场景下，通过信任感知协调和RAG，准确率提升了77.94%，总体达到85.63%。

Conclusion: 该框架可扩展至诊断、生物学等信任关键领域，并开源了所有模型和代码以支持透明性和社区基准测试。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [11] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: 论文分析了大语言模型（LLMs）在符号推理、算术准确性和逻辑一致性任务中的系统性失败，揭示了其理解与能力之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在任务中表现出的表面流畅性与实际能力之间的不一致，以揭示其根本原因。

Method: 通过控制实验和架构分析，研究LLMs在原则表达与实际应用之间的差距。

Result: 发现LLMs存在计算性“分裂脑综合征”，即指令与执行路径在几何和功能上分离，导致模型行为脆弱。

Conclusion: LLMs是强大的模式完成引擎，但缺乏结构化推理能力，未来模型需引入元认知控制和结构化执行。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [12] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data系统结合知识图谱、LLM、ReAct代理和工具使用技术，提升气象领域数据获取和查询的智能性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在知识密集型领域（如气象学）中API调用能力不足的问题，提升复杂查询和术语处理的性能。

Method: 集成知识图谱作为持久记忆，结合虚拟API评估API调用准确性，包括名称识别失败、幻觉失败和调用正确性。

Result: KG2data在三个指标上表现优异（1.43%、0%、88.57%），显著优于RAG2data和chat2data。

Conclusion: KG2data为高知识需求领域提供了智能、基于知识的问答和数据分析新方案。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [13] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文综述了Web of Agents (WoA)的演变，提出了一个四轴分类法，揭示了智能从外部数据或平台转移到代理核心模型的范式转变，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究WoA的碎片化现状，整合多智能体系统（MAS）和语义Web的历史，以提供对该领域发展的全面理解。

Method: 引入四轴分类法（语义基础、通信范式、智能核心、发现机制），系统比较不同世代的代理架构。

Result: 揭示了智能核心的范式转变，并指出现代Agentic AI的可扩展性和适应性。

Conclusion: 新协议不足以保证生态系统的稳健性，未来研究需解决去中心化身份、经济模型、安全与治理等挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [14] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 提出了一种基于规则的、通过变异现有曲调生成音乐的新方法，利用语法和突变操作生成新曲调，并分析其变化。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过变异现有曲调的语法结构生成新音乐，并研究多次变异对曲调的影响。

Method: 使用Sequitur算法解析曲调为语法结构（PA），随机应用19种突变类型（如添加、删除、交换等）操作语法，再扩展为新曲调。

Result: 通过编辑距离、结构复杂度和长度分析曲调变化，评估每种突变类型的效果，并检查生成曲调的音乐性。

Conclusion: 该方法能有效生成与原曲调相关的新曲调，但仅关注音高序列生成，基于爱尔兰传统曲调数据集。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [15] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: AI的快速发展推动了数据中心的建设，短期内会增加能源消耗和碳排放，但长期来看，AI有望通过优化流程显著减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在数据中心的能源消耗及其对温室气体排放的影响，分析AI对2035年碳排放的潜在影响。

Method: 通过分析短期（至2030年）和长期（2035年及以后）的能源消耗情景，评估AI对碳排放的净影响。

Result: 短期内AI会增加碳排放，但长期来看，AI的优化能力可能显著减少碳足迹。

Conclusion: AI虽在初期可能增加环境负担，但长期有望成为气候缓解的重要工具。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [16] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 该论文通过深度学习模型检测物联网恶意攻击，评估了多种模型（如GraphSAGE、BERT、TCN等）在恶意流量检测中的表现，其中BERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 物联网系统的流量模式具有时序性和多样性，为模型提供了丰富的学习数据，因此研究如何利用深度学习模型高效检测恶意攻击具有重要意义。

Method: 采用了多种深度学习模型（GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM等），通过建模时序模式和特征显著性来检测恶意流量。

Result: BERT表现最优，准确率达99.94%，其他指标（如F1-score、AUC-ROC）接近99.99%。Multi-Head Attention表现良好但处理时间长，GraphSAGE训练时间短但性能较低。

Conclusion: BERT在捕获时序依赖性和检测性能上表现最佳，是物联网恶意流量检测的理想选择，而其他模型在特定场景下也有应用潜力。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [17] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 论文探讨了如何利用人工神经网络检测AI辅助，特别是在抽象任务中。通过预处理数据，构建了四种图像形式和一个时间序列形式，验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中的普及，检测AI辅助变得重要，但人类难以处理抽象任务数据。

Method: 将AI辅助检测视为分类任务，构建了四种图像形式和一个时间序列形式，测试了不同深度学习架构的性能。

Result: 实验表明，适当预处理后，常见模型能有效分类抽象任务数据，CNN-RNN架构结合时空信息表现最佳。

Conclusion: 编码时空信息对检测抽象任务中的AI辅助至关重要，CNN-RNN架构效果显著。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [18] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling动态调整决策点，提高移动健康干预的及时性。


<details>
  <summary>Details</summary>
Motivation: 固定间隔的决策点调度对习惯性行为干预效果不佳，需个性化方法。

Method: 基于行为时间预测的不确定性动态调度决策点。

Result: 在68名参与者试验中，70%以上的刷牙事件前成功调度决策点。

Conclusion: SigmaScheduling提升精准移动健康干预效果，适用于时间敏感的习惯行为。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [19] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型（LLM）在主题分析任务中的表现，发现GPT-4o在少量样本提示下表现最佳，为定性研究提供了可扩展的补充。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要深度解释和领域专业知识的主题分析任务中面临挑战，研究旨在探索其可行性。

Method: 使用Reddit数据集，将任务建模为一系列二元分类，采用零样本、单样本和少量样本提示策略，评估模型性能。

Result: GPT-4o在少量样本提示下表现最佳（准确率90.9%，F1分数0.71），模型结果与专家分类高度一致。

Conclusion: 少量样本提示的LLM方法可以自动化主题分析，为定性研究提供可扩展的补充。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [20] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架（AFs），帮助非专家理解论证接受和模糊性来源。


<details>
  <summary>Details</summary>
Motivation: 法律推理中的论证框架（AFs）存在模糊性和解释难度，非专家难以理解。AF-XRAY旨在解决这一问题。

Method: AF-XRAY提供分层可视化、攻击边分类、替代解决方案叠加可视化，并通过生成关键攻击集解决模糊性。

Result: 工具能将模糊场景转化为明确的解决方案，帮助用户识别模糊性原因并探索替代方案。

Conclusion: AF-XRAY通过实际法律案例验证，支持目的性法律推理，展示不同假设如何导致不同结论。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [21] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer自动生成高质量导航指令，NavInstrCritic提供无标注评估，支持大规模研究。


<details>
  <summary>Details</summary>
Motivation: 解决专家指令数量有限和合成指令质量不足的问题。

Method: NavComposer分解语义实体并重组为自然语言指令，NavInstrCritic从三个维度评估指令质量。

Result: 实验证明方法有效，支持大规模和通用化研究。

Conclusion: 方法提升了指令生成和评估的规模与质量。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [22] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨了基于大型语言模型（LLM）的多代理系统（MAS）在慢性多病患者治疗推荐中的可行性和价值，模拟多学科团队（MDT）决策，结果显示单代理GP表现与MDT相当，但建议存在不完整和不必要的药物问题。


<details>
  <summary>Details</summary>
Motivation: 慢性多病患者的治疗推荐因治疗冲突风险而具有挑战性，现有决策支持系统存在可扩展性限制。

Method: 设计了单代理和多代理框架，模拟MDT决策，通过LLM代理间的讨论解决医疗冲突，并定义了超越技术精度的评估指标。

Result: 当前LLM下，单代理GP表现与MDT相当，但建议不完整且可能包含不必要的药物。

Conclusion: 研究展示了LLM在多病治疗推荐中的潜力，但仍需改进建议的完整性和减少不必要的药物冲突。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [23] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，以减少生成有害蛋白质序列的风险。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在序列生成中具有优势，但也可能生成有害序列，带来生物安全和伦理挑战。

Method: 结合蛋白质安全知识图谱，采用图剪枝策略和强化学习优化序列生成。

Result: KPO有效降低有害序列生成概率，同时保持高功能性。

Conclusion: KPO为生物技术中生成模型的应用提供了安全保证。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [24] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 结合卷积神经网络（CNN）和表格数据，准确预测鸟类在特定栖息地的存在。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化导致栖息地范围变化，需要一种可靠的方法来预测鸟类分布。

Method: 利用卫星图像和环境特征（如温度、降水、海拔），结合CNN和表格数据建模。

Result: 模型平均准确率达85%，能有效预测鸟类分布。

Conclusion: 该方法为理解鸟类迁徙提供了可扩展且可靠的解决方案。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [25] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec是一个结合语义知识追踪的个性化习题推荐框架，通过强化学习方法优化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有习题推荐方法忽视问题的语义内容和学生学习的结构化进展，ExRec旨在解决这一问题。

Method: 采用端到端流程，包括问题知识标注、语义表示学习、知识追踪模型训练及强化学习优化。

Result: 在多个真实任务中验证有效性，能泛化到新问题并生成可解释的学习轨迹。

Conclusion: ExRec展示了知识追踪引导的强化学习在教育个性化中的潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [26] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于视觉语言模型的指挥官系统，用于无人地面车辆对抗中的智能感知到决策推理，结合视觉语言模型和轻量级大语言模型，实现了高适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统规则方法和强化学习在复杂战场环境下缺乏适应性和可解释性的问题。

Method: 整合视觉语言模型进行场景理解，结合轻量级大语言模型进行战略推理，实现感知与决策的统一。

Result: 在仿真和消融实验中，胜率超过80%，优于基线模型。

Conclusion: 该方法通过模拟人类指挥官的认知过程，实现了高效且可解释的战术决策。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [27] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans提出了一种分阶段的代码翻译方法，通过功能学习和风格学习提升LLMs的性能，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码翻译任务中取得进展，但确保翻译的正确性和可读性仍是挑战，限制了实际应用。

Method: F2STrans分为功能学习（优化正确性）和风格学习（提升可读性），并引入新基准进行综合评估。

Result: 实验表明，F2STrans显著提升性能，Qwen-1.5B在20种场景中平均优于Qwen-32B和GPT-4。

Conclusion: F2STrans通过分阶段学习和新基准，有效解决了代码翻译的正确性和可读性问题。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [28] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS是一个研究导向的架构，利用多个专用AI代理自动化和安全地将实物黄金代币化为区块链稳定币（OZ），满足合规性、流动性和风险管理要求。


<details>
  <summary>Details</summary>
Motivation: 解决实物资产托管与区块链系统之间的桥梁问题，同时满足严格的合规性、流动性和风险管理需求。

Method: 结合链上智能合约（关键风险控制）和链下AI代理（决策），设计了四个协作代理（合规、代币发行、做市和风险控制）及协调核心。

Result: 原型实现按需代币发行（<1.2秒），做市代理保持流动性（价差<0.5%），故障注入测试显示高韧性（攻击检测<10秒）。

Conclusion: AI代理驱动的去中心化交易所可满足高性能和安全需求，为传统非流动性资产提供民主化访问，并通过治理模型确保透明度和适应性。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [29] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文提出了神经符号AI的形式化定义，将其推理抽象为逻辑函数与信念函数乘积的积分。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI领域缺乏统一的定义，本文旨在填补这一空白。

Method: 通过形式化定义神经符号推理为逻辑函数与信念函数乘积的积分。

Result: 该定义能够抽象化现有的代表性神经符号AI系统。

Conclusion: 本文的形式化定义为神经符号AI领域提供了理论基础。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [30] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 提出了一种基于协作数据共享的自主系统可信决策方法，利用感知质量等属性评估系统可信度，并采用BDD模型进行高效推理。


<details>
  <summary>Details</summary>
Motivation: 自主系统在动态复杂环境中确保安全可靠行为面临挑战，需要提高决策的可信度。

Method: 利用感知质量等属性评估系统可信度，结合社会认识论定义聚合与传播规则，使用BDD模型进行高效推理。

Result: 通过协作数据共享和BDD模型，提高了自主系统的决策可信度和效率。

Conclusion: 该方法为自主系统在复杂环境中的可信决策提供了有效解决方案。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [31] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 论文提出了一种使用ASP计算集成电路中组合模块实际最大延迟的方法，以替代传统的静态时序分析。


<details>
  <summary>Details</summary>
Motivation: 传统静态时序分析只能提供延迟的上界，可能导致处理器性能未达最优。

Method: 将问题建模为ASP（答案集编程），并提出非平凡的编码方法。

Result: 实验证明ASP能有效解决硬件设计中的复杂问题。

Conclusion: ASP是计算实际最大延迟的可行方案。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [32] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph提出了一种双路径全局-局部融合的知识图谱推理机制，通过分离全局和局部信息处理路径，解决了现有方法中的分数过平滑问题，显著提升了推理质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱推理方法在处理全局和局部信息时容易导致分数过平滑，模糊正确答案与错误答案的区分，影响推理效果。

Method: DuetGraph采用双路径机制，分别处理局部信息（消息传递）和全局信息（注意力机制），避免相互干扰；并通过粗到细的优化策略，将实体分为高、低分数子集，缩小候选空间。

Result: 实验表明，DuetGraph在推理质量上提升高达8.7%，训练效率加速1.8倍，达到SOTA性能。

Conclusion: DuetGraph通过双路径融合和粗到细优化，有效解决了分数过平滑问题，显著提升了知识图谱推理的效果和效率。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [33] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: 论文提出了AgentOps框架，用于观察、分析和优化基于LLM的智能代理系统，解决其不确定性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能代理系统广泛应用，其不确定性（如概率推理、动态记忆状态）对传统软件运维方法提出了新挑战。

Method: 提出AgentOps框架，包括行为观察、指标收集、问题检测、根因分析、优化建议和运行时自动化六个阶段。

Result: 框架支持开发者、测试者、SRE和业务用户，通过自动化管理不确定性，实现系统的自适应和高效运行。

Conclusion: AgentOps通过自动化驯服不确定性，为智能代理系统的安全、自适应和高效运行提供了解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [34] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: Opus Prompt Intention Framework通过引入中间层（Intention Capture）提升LLM在复杂工作流生成中的表现，显著改善语义相似性指标。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂查询下直接生成工作流时逻辑性和可扩展性不足的问题。

Method: 提出Opus Workflow Intention Framework，包括从用户查询中提取Workflow Signals、解析为结构化Workflow Intention对象，并基于此生成工作流。

Result: 在1,000个多意图查询-工作流对的基准测试中，语义相似性指标显著提升。

Conclusion: Opus框架有效提高了工作流生成质量，尤其在混合意图场景下表现突出。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [35] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种基于梯度关系归因解释（G-RAEs）的方法，用于在边加权定量双极论证框架（EW-QBAFs）中实现可争议的AI决策。


<details>
  <summary>Details</summary>
Motivation: 研究如何在AI驱动的决策中实现与人类偏好一致的可争议性，填补了EW-QBAFs在此领域的空白。

Method: 提出G-RAEs方法，量化主题论证强度对边权重变化的敏感性，并开发迭代算法调整权重以实现目标强度。

Result: 实验证明该方法在模拟个性化推荐系统和多层感知器的合成EW-QBAFs中有效解决了可争议性问题。

Conclusion: G-RAEs和迭代算法为EW-QBAFs中的可争议性提供了一种有效且可解释的解决方案。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [36] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN是一个基于视觉语言模型（VLM）的框架，通过模拟人类认知机制提升机器人在未知环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的需求导航方法依赖预收集数据，泛化能力有限。CogDDN旨在通过模拟人类认知机制解决这一问题。

Method: CogDDN结合快速和慢速思考系统，语义对齐检测对象与指令，并采用双过程决策模块（启发式和分析过程）和链式思维推理。

Result: 在AI2Thor模拟器上，CogDDN比单视角相机方法性能提升15%，导航准确性和适应性显著提高。

Conclusion: CogDDN通过模拟人类认知机制，显著提升了机器人在未知环境中的导航能力。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [37] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 论文提出了一种神经符号框架，结合自然语言对话的可访问性和目标解释的可验证保证，用于复杂物流决策。


<details>
  <summary>Details</summary>
Motivation: 物流决策需要快速、连续的重新规划，但现有方法（如整数规划）速度慢且无法处理不确定性，而大语言模型（LLMs）虽能加速但存在误解和幻觉风险。

Method: 引入神经符号框架，将用户请求转换为结构化规划规范，量化不确定性，并在置信度低于自适应阈值时启动交互式澄清循环。

Result: 轻量级模型在100个不确定性过滤示例上微调后，性能超过GPT-4.1，推理延迟降低近50%。

Conclusion: 该框架为复杂物流提供了可认证、实时且用户对齐的决策路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [38] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 提出了一种结合代码文本和结构化形式建模的新方法，弥补了现有Transformer模型在代码结构化分析能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的代码大语言模型（LLMs）在代码生成等任务中表现优异，但在分析代码的结构化属性（如控制流和数据流）方面能力有限。

Method: 结合代码文本和结构化形式的建模方法，以弥补现有LLMs的不足。

Result: 未明确提及具体实验结果。

Conclusion: 新方法有望结合代码文本和结构化建模的优势，提升代码分析能力。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [39] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI系统通过人类语言“思考”为AI安全提供了新机会，可以监控其思维链（CoT）以防止不当行为。尽管监控不完美，但前景广阔，建议进一步研究并投资CoT监控。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统通过人类语言“思考”的特性，利用思维链监控提升AI安全性。

Method: 提出监控AI的思维链（CoT）以检测潜在不当行为。

Result: 思维链监控虽不完美，但显示出潜力，建议结合现有安全方法进一步研究。

Conclusion: 建议前沿模型开发者考虑开发决策对思维链监控的影响，以确保其有效性。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [40] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI与XR，利用Chronicles模型实现基于用户身份的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强XR系统因用户建模浅显和认知上下文有限而表现不足。

Method: 提出PAiR框架，基于Chronicles模型（多模态数字足迹学习）构建闭环系统，动态链接用户状态与沉浸环境。

Result: 通过Unity引擎实现两个概念验证场景，展示了PAiR的实用性。

Conclusion: PAiR为人类-AI交互开辟了新方向，将基于视角的身份模型嵌入沉浸式系统。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [41] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 论文提出一个基于开放式进化理论的框架，重新审视强化学习的三个核心假设，并探讨其理论和应用意义。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习中关于代理定义、学习目标和奖励假设的三个核心假设进行概念性修订，以推动理论和应用的发展。

Method: 借鉴开放式进化理论，重新审视这三个假设，并结合进化动力学和生命起源理论进行分析。

Result: 进化理论可以丰富强化学习的视角，但代理问题需要结合生命起源理论来解决。

Conclusion: 进化理论为强化学习提供了新的视角，但代理问题需进一步结合生命起源理论来完善。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [42] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench是一个用于评估LLM代理在土木工程图纸修订任务中的开源基准，包含12类任务、46个定制工具和1920个任务，旨在系统测试代理的多方面能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从工业角度（如土木工程）系统评估自动化代理的基准，DrafterBench填补了这一空白。

Method: 提出DrafterBench，包含真实图纸文件总结的12类任务，通过46个定制工具和1920个任务全面评估代理能力。

Result: DrafterBench能详细分析任务准确性和错误统计，为LLM在工程应用中的改进提供依据。

Conclusion: DrafterBench为LLM代理在工程领域的应用提供了系统评估工具，有助于提升其实际任务处理能力。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [43] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale是一个评估LLM在高密度指令下性能的基准测试，发现即使最先进的模型在500条指令时准确率仅68%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估单一或少量指令的任务，无法反映生产级LLM系统需同时处理大量指令的需求。

Method: 引入IFScale基准测试，包含500条关键词包含指令，用于商业报告写作任务，评估指令密度增加时的性能下降。

Result: 评估20个前沿模型，发现模型大小和推理能力与性能下降模式相关，最佳模型在500条指令时准确率为68%。

Conclusion: 研究结果可为实际应用中高密度指令设计提供参考，并揭示性能与延迟的权衡关系。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [44] [Asymptotically Optimal Repair of Reed-Solomon Codes with Small Sub-Packetization under Rack-Aware Model](https://arxiv.org/abs/2507.11009)
*Ke Wang,Zhongyan Liu,Rengang Li,Yaqian Zhao,Yaqiang Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种针对机架感知分布式存储系统的RS码渐近最优修复方法，通过多基展开和单项式构造线性修复方案，显著降低了子分组化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决RS码在分布式存储系统中修复带宽和子分组化的优化问题。

Method: 利用多基展开和单项式构造线性修复方案，适用于所有可接受的参数。

Result: 方案实现了渐近最优修复带宽，同时显著降低了子分组化，优于现有方法。

Conclusion: 该方法在机架感知系统中表现出色，为RS码修复提供了高效解决方案。

Abstract: This paper presents a comprehensive study on the asymptotically optimal
repair of Reed-Solomon (RS) codes with small sub-packetization, specifically
tailored for rack-aware distributed storage systems. Through the utilization of
multi-base expansion, we introduce a novel approach that leverages monomials to
construct linear repair schemes for RS codes. Our repair schemes which adapt to
all admissible parameters achieve asymptotically optimal repair bandwidth while
significantly reducing the sub-packetization compared with existing schemes.
Furthermore, our approach is capable of repairing RS codes with asymptotically
optimal repair bandwidth under the homogeneous storage model, achieving smaller
sub-packetization than existing methods.

</details>


### [45] [Extropy Rate: Properties and Application in Feature Selection](https://arxiv.org/abs/2507.11242)
*Naveen Kumar,Vivek Vijay*

Main category: cs.IT

TL;DR: 本文研究了离散随机变量的条件外熵，定义了联合与条件外熵的关键性质，并引入了外熵率的概念，展示了其在信息量化、时间序列复杂性和混沌动力学中的应用。


<details>
  <summary>Details</summary>
Motivation: 外熵作为熵的互补对偶概念，近年来受到广泛关注。本文旨在探索离散随机变量的外熵及其性质，并研究其在信息理论和实际应用中的潜力。

Method: 定义了条件外熵和联合外熵的性质，引入了外熵率的概念，并通过数值实验验证了其在信息量化、时间序列分析和混沌动力学中的有效性。

Result: 研究发现外熵率在无限平稳和遍历随机过程中具有渐近等价性，并在特征选择方法中表现出优于现有方法的性能。

Conclusion: 外熵率是一种有效的工具，可用于量化信息复杂性，并在实际应用中展现出优越性。

Abstract: Extropy, a complementary dual of entropy, (proposed by Lad et al.
\cite{lad2015extropy} in 2015) has attracted considerable interest from the
research community. In this study, we focus on discrete random variables and
define conditional extropy, establishing key properties of joint and
conditional extropy such as bounds, uncertainty reduction due to additional
information, and Lipschitz continuity. We further introduce the concept of
extropy rate for a stochastic process of discrete random variables as a measure
of the average uncertainty per random variable within the process. It is
observed that for infinite stationary and ergodic stochastic processes, as well
as for identically and independently distributed sequences, the extropy rate
exhibits asymptotic equivalence. We explore the extropy rate for finite
stochastic processes and numerically illustrate its effectiveness in capturing
the underlying information across various distributions, quantifying complexity
in time series data, and characterizing chaotic dynamics in dynamical systems.
The behaviour of estimated extropy rate is observed to be closely aligned with
Simpson's diversity index. The real-life applicability of the extropy rate is
presented through a novel feature selection method based on the fact that
features with higher extropy rates contain greater inherent information. Using
six publicly available datasets, we show the superiority of the proposed
feature selection method over some other existing popular approaches.

</details>
