{"id": "2510.16576", "categories": ["cs.IT", "cs.IR", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16576", "abs": "https://arxiv.org/abs/2510.16576", "authors": ["Zijian Zhang", "Mingyao Cui"], "title": "Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design", "comment": "5 pages, 2 figures", "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a promising\ntechnology for enhancing wireless communications through dense antenna arrays.\nAccurate channel estimation is critical to unlocking their full performance\npotential. To enhance RIS channel estimators, this paper proposes a novel\nobservation matrix design scheme. Bayesian optimization framework is adopted to\ngenerate observation matrices that maximize the mutual information between\nreceived pilot signals and RIS channels. To solve the formulated problem\nefficiently, we develop an alternating Riemannian manifold optimization (ARMO)\nalgorithm to alternately update the receiver combiners and RIS phase-shift\nmatrices. An adaptive kernel training strategy is further introduced to\niteratively refine the channel covariance matrix without requiring additional\npilot resources. Simulation results demonstrate that the proposed ARMO-enhanced\nestimator achieves substantial gains in estimation accuracy over\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u4fe1\u9053\u4f30\u8ba1\u89c2\u6d4b\u77e9\u9635\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u91c7\u7528\u4ea4\u66ff\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6", "motivation": "\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u901a\u8fc7\u5bc6\u96c6\u5929\u7ebf\u9635\u5217\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd\uff0c\u800c\u51c6\u786e\u7684\u4fe1\u9053\u4f30\u8ba1\u5bf9\u5176\u6027\u80fd\u53d1\u6325\u81f3\u5173\u91cd\u8981", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u8bbe\u8ba1\u89c2\u6d4b\u77e9\u9635\u4ee5\u6700\u5927\u5316\u63a5\u6536\u5bfc\u9891\u4fe1\u53f7\u4e0eRIS\u4fe1\u9053\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u63d0\u51fa\u4ea4\u66ff\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\u4ea4\u66ff\u66f4\u65b0\u63a5\u6536\u673a\u7ec4\u5408\u5668\u548cRIS\u76f8\u79fb\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u6838\u8bad\u7ec3\u7b56\u7565\u8fed\u4ee3\u4f18\u5316\u4fe1\u9053\u534f\u65b9\u5dee\u77e9\u9635", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684ARMO\u589e\u5f3a\u4f30\u8ba1\u5668\u5728\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347RIS\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u5bfc\u9891\u8d44\u6e90"}}
{"id": "2510.15871", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "math.PR", "94A17, 94A15, 68T05, 62F15, 68P30, 68T27, 68T50, 30B42", "H.1.1; I.1.2; I.2.6; I.2.8; I.2.4; E.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.15871", "abs": "https://arxiv.org/abs/2510.15871", "authors": ["Chenguang Lu"], "title": "A Semantic Generalization of Shannon's Information Theory and Applications", "comment": "45 pages, 18 Figures, a review paper", "summary": "Does semantic communication require a semantic information theory parallel to\nShannon's information theory, or can Shannon's work be generalized for semantic\ncommunication? This paper advocates for the latter and introduces a semantic\ngeneralization of Shannon's information theory (G theory for short). The core\nidea is to replace the distortion constraint with the semantic constraint,\nachieved by utilizing a set of truth functions as a semantic channel. These\ntruth functions enable the expressions of semantic distortion, semantic\ninformation measures, and semantic information loss. Notably, the maximum\nsemantic information criterion is equivalent to the maximum likelihood\ncriterion and similar to the Regularized Least Squares criterion. This paper\nshows G theory's applications to daily and electronic semantic communication,\nmachine learning, constraint control, Bayesian confirmation, portfolio theory,\nand information value. The improvements in machine learning methods involve\nmultilabel learning and classification, maximum mutual information\nclassification, mixture models, and solving latent variables. Furthermore,\ninsights from statistical physics are discussed: Shannon information is similar\nto free energy; semantic information to free energy in local equilibrium\nsystems; and information efficiency to the efficiency of free energy in\nperforming work. The paper also proposes refining Friston's minimum free energy\nprinciple into the maximum information efficiency principle. Lastly, it\ncompares G theory with other semantic information theories and discusses its\nlimitation in representing the semantics of complex data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Shannon\u4fe1\u606f\u7406\u8bba\u7684\u8bed\u4e49\u6cdb\u5316\uff08G\u7406\u8bba\uff09\uff0c\u7528\u8bed\u4e49\u7ea6\u675f\u66ff\u4ee3\u5931\u771f\u7ea6\u675f\uff0c\u901a\u8fc7\u771f\u503c\u51fd\u6570\u4f5c\u4e3a\u8bed\u4e49\u901a\u9053\u6765\u8868\u8fbe\u8bed\u4e49\u5931\u771f\u3001\u8bed\u4e49\u4fe1\u606f\u5ea6\u91cf\u548c\u8bed\u4e49\u4fe1\u606f\u635f\u5931\u3002\u8be5\u7406\u8bba\u5728\u673a\u5668\u5b66\u4e60\u3001\u901a\u4fe1\u7b49\u591a\u4e2a\u9886\u57df\u6709\u5e94\u7528\u3002", "motivation": "\u63a2\u8ba8\u8bed\u4e49\u901a\u4fe1\u662f\u5426\u9700\u8981\u72ec\u7acb\u4e8eShannon\u4fe1\u606f\u7406\u8bba\u7684\u8bed\u4e49\u4fe1\u606f\u7406\u8bba\uff0c\u8fd8\u662f\u53ef\u4ee5\u57fa\u4e8eShannon\u7406\u8bba\u8fdb\u884c\u6cdb\u5316\u3002\u4f5c\u8005\u652f\u6301\u540e\u4e00\u79cd\u89c2\u70b9\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u7ea6\u675f\u66ff\u4ee3\u5931\u771f\u7ea6\u675f\uff0c\u4f7f\u7528\u771f\u503c\u51fd\u6570\u4f5c\u4e3a\u8bed\u4e49\u901a\u9053\u6765\u5b9a\u4e49\u8bed\u4e49\u5931\u771f\u3001\u8bed\u4e49\u4fe1\u606f\u5ea6\u91cf\u548c\u8bed\u4e49\u4fe1\u606f\u635f\u5931\u3002\u6700\u5927\u8bed\u4e49\u4fe1\u606f\u51c6\u5219\u7b49\u4ef7\u4e8e\u6700\u5927\u4f3c\u7136\u51c6\u5219\u3002", "result": "G\u7406\u8bba\u53ef\u5e94\u7528\u4e8e\u65e5\u5e38\u548c\u7535\u5b50\u8bed\u4e49\u901a\u4fe1\u3001\u673a\u5668\u5b66\u4e60\u3001\u7ea6\u675f\u63a7\u5236\u3001\u8d1d\u53f6\u65af\u786e\u8ba4\u3001\u6295\u8d44\u7ec4\u5408\u7406\u8bba\u548c\u4fe1\u606f\u4ef7\u503c\u8bc4\u4f30\u3002\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u6539\u8fdb\u4e86\u591a\u6807\u7b7e\u5b66\u4e60\u3001\u5206\u7c7b\u3001\u6df7\u5408\u6a21\u578b\u548c\u6f5c\u53d8\u91cf\u6c42\u89e3\u65b9\u6cd5\u3002", "conclusion": "G\u7406\u8bba\u662fShannon\u4fe1\u606f\u7406\u8bba\u7684\u6709\u6548\u8bed\u4e49\u6cdb\u5316\uff0c\u5728\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u590d\u6742\u6570\u636e\u8bed\u4e49\u8868\u793a\u7684\u5c40\u9650\u6027\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u5c06Friston\u6700\u5c0f\u81ea\u7531\u80fd\u539f\u7406\u6539\u8fdb\u4e3a\u6700\u5927\u4fe1\u606f\u6548\u7387\u539f\u7406\u3002"}}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "VisuoAlign\u662f\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u5b9e\u73b0\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u5a01\u80c1\u4e0b\u7684\u5b89\u5168\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u8106\u5f31\uff0c\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u6a21\u6001\u878d\u5408\u65f6\u5bf9\u9f50\u6548\u679c\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u5b9e\u73b0\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVisuoAlign\u80fd\u4e3b\u52a8\u66b4\u9732\u98ce\u9669\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u6570\u636e\u96c6\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u5347LVLMs\u5bf9\u6297\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VisuoAlign\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aLVLMs\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u4fdd\u969c\u3002"}}
{"id": "2510.16144", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16144", "abs": "https://arxiv.org/abs/2510.16144", "authors": ["Sukhdeep Singh", "Avinash Bhat", "Shweta M", "Subhash K Singh", "Moonki Hong", "Madhan Raj K", "Kandeepan Sithamparanathan", "Sunder A. Khowaja", "Kapal Dev"], "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance", "comment": null, "summary": "The increasing complexity of Beyond 5G and 6G networks necessitates new\nparadigms for autonomy and assur- ance. Traditional O-RAN control loops rely\nheavily on RIC- based orchestration, which centralizes intelligence and exposes\nthe system to risks such as policy conflicts, data drift, and unsafe actions\nunder unforeseen conditions. In this work, we argue that the future of\nautonomous networks lies in a multi-agentic architecture, where specialized\nagents collaborate to perform data collection, model training, prediction,\npolicy generation, verification, deployment, and assurance. By replacing\ntightly- coupled centralized RIC-based workflows with distributed agents, the\nframework achieves autonomy, resilience, explainability, and system-wide\nsafety. To substantiate this vision, we design and evaluate a traffic steering\nuse case under surge and drift conditions. Results across four KPIs: RRC\nconnected users, IP throughput, PRB utilization, and SINR, demonstrate that a\nnaive predictor-driven deployment improves local KPIs but destabilizes\nneighbors, whereas the agentic system blocks unsafe policies, preserving global\nnetwork health. This study highlights multi- agent architectures as a credible\nfoundation for trustworthy AI- driven autonomy in next-generation RANs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u67b6\u6784\u66ff\u4ee3\u4f20\u7edfO-RAN\u96c6\u4e2d\u5f0fRIC\u63a7\u5236\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u7f51\u7edc\u81ea\u4e3b\u6027\u3001\u5f39\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edfO-RAN\u63a7\u5236\u73af\u8def\u8fc7\u5ea6\u4f9d\u8d56RIC\u7f16\u6392\uff0c\u5b58\u5728\u7b56\u7565\u51b2\u7a81\u3001\u6570\u636e\u6f02\u79fb\u548c\u4e0d\u53ef\u9884\u89c1\u6761\u4ef6\u4e0b\u7684\u4e0d\u5b89\u5168\u64cd\u4f5c\u7b49\u98ce\u9669\uff0c\u9700\u8981\u65b0\u7684\u81ea\u4e3b\u7f51\u7edc\u8303\u5f0f\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u4f5c\u6267\u884c\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u9884\u6d4b\u3001\u7b56\u7565\u751f\u6210\u3001\u9a8c\u8bc1\u3001\u90e8\u7f72\u548c\u4fdd\u969c\u7b49\u4efb\u52a1\uff0c\u66ff\u4ee3\u7d27\u5bc6\u8026\u5408\u7684\u96c6\u4e2d\u5f0f\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u6d41\u91cf\u8f6c\u5411\u7528\u4f8b\u6d4b\u8bd5\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u963b\u6b62\u4e0d\u5b89\u5168\u7b56\u7565\uff0c\u4fdd\u6301\u5168\u5c40\u7f51\u7edc\u5065\u5eb7\uff0c\u800c\u7b80\u5355\u9884\u6d4b\u5668\u9a71\u52a8\u90e8\u7f72\u867d\u7136\u6539\u5584\u5c40\u90e8KPI\u4f46\u4f1a\u7834\u574f\u90bb\u5c45\u7a33\u5b9a\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3RAN\u4e2d\u53ef\u4fe1AI\u9a71\u52a8\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u5177\u5907\u81ea\u4e3b\u6027\u3001\u5f39\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7cfb\u7edf\u7ea7\u5b89\u5168\u6027\u3002"}}
{"id": "2510.16432", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16432", "abs": "https://arxiv.org/abs/2510.16432", "authors": ["Zahra Mobini", "Ahmet Hasim Gokceoglu", "Li Wang", "Gunnar Peters", "Hyundong Shin", "Hien Quoc Ngo"], "title": "Cluster-wise processing in fronthaul-aware cell-free massive MIMO systems", "comment": null, "summary": "We exploit a general cluster-based network architecture for a\nfronthaul-limited user-centric cell-free massive multiple-input multiple-output\n(CF-mMIMO) system under different degrees of cooperation among the access\npoints (APs) to achieve scalable implementation. In particular, we consider a\nCF-mMIMO system wherein the available APs are grouped into multiple processing\nclusters (PCs) to share channel state information (CSI), ensuring that they\nhave knowledge of the CSI for all users assigned to the given cluster for the\npurposes of designing resource allocation and precoding. We utilize the sum\npseudo-SE metric, which accounts for intra-cluster interference and\nintercluster-leakage, providing a close approximation to the true sum\nachievable SE. For a given PC, we formulate two optimization problems to\nmaximize the cluster-wise weighted sum pseudo-SE under fronthaul constraints,\nrelying solely on local CSI. These optimization problems are associated with\ndifferent computational complexity requirements. The first optimization problem\njointly designs precoding, user association, and power allocation, and is\nperformed at the small-scale fading time scale. The second optimization problem\noptimizes user association and power allocation at the large-scale fading time\nscale. Accordingly, we develop a novel application of modified weighted minimum\nmean square error (WMMSE)-based approach to solve the challenging formulated\nnon-convex mixed-integer problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u7fa4\u67b6\u6784\u7684\u7528\u6237\u4e2d\u5fc3\u5316\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0d\u540c\u63a5\u5165\u70b9\u534f\u4f5c\u7a0b\u5ea6\u5b9e\u73b0\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u524d\u4f20\u5bb9\u91cf\u9650\u5236\u4e0b\u7684\u8d44\u6e90\u5206\u914d\u548c\u9884\u7f16\u7801\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u524d\u4f20\u5bb9\u91cf\u9650\u5236\u4e0b\u7684\u7528\u6237\u4e2d\u5fc3\u5316\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u5b9e\u73b0\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u7fa4\u5316\u5904\u7406\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u3002", "method": "\u5c06\u63a5\u5165\u70b9\u5206\u7ec4\u4e3a\u591a\u4e2a\u5904\u7406\u96c6\u7fa4\uff0c\u5229\u7528\u5c40\u90e8\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u65b9\u6cd5\u89e3\u51b3\u975e\u51f8\u6df7\u5408\u6574\u6570\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u8054\u5408\u9884\u7f16\u7801\u3001\u7528\u6237\u5173\u8054\u548c\u529f\u7387\u5206\u914d\u3002", "result": "\u5f00\u53d1\u4e86\u4e24\u79cd\u4f18\u5316\u65b9\u6848\uff1a\u4e00\u79cd\u5728\u5c0f\u5c3a\u5ea6\u8870\u843d\u65f6\u95f4\u5c3a\u5ea6\u8054\u5408\u4f18\u5316\uff0c\u53e6\u4e00\u79cd\u5728\u5927\u5c3a\u5ea6\u8870\u843d\u65f6\u95f4\u5c3a\u5ea6\u4f18\u5316\u7528\u6237\u5173\u8054\u548c\u529f\u7387\u5206\u914d\uff0c\u6709\u6548\u903c\u8fd1\u771f\u5b9e\u53ef\u8fbe\u548c\u901f\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u7fa4\u67b6\u6784\u548c\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u5728\u524d\u4f20\u5bb9\u91cf\u9650\u5236\u4e0b\u5b9e\u73b0\u7528\u6237\u4e2d\u5fc3\u5316\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u54f2\u5b66\u6d1e\u89c1\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u7684\u8ba4\u77e5\u7406\u89e3\uff0c\u66b4\u9732\u4e86\u8ba4\u77e5\u67b6\u6784\u7684\u7f3a\u5931\u3002\u4f20\u7edfAI\u7814\u7a76\u5173\u6ce8\"\u4ec0\u4e48\u662f\u667a\u80fd\"\u7684\u672c\u4f53\u8bba\u95ee\u9898\uff0c\u800cSCL\u5173\u6ce8\"\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8ba4\u77e5\u6d8c\u73b0\"\u7684\u8ba4\u77e5\u8bba\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5fc3\u667a\u54f2\u5b66\u548c\u8ba4\u77e5\u73b0\u8c61\u5b66\uff0c\u7ed3\u5408\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u5c06\u667a\u80fd\u5b9a\u4e49\u4e3a\u6267\u884c\u8fc7\u7a0b\u2014\u2014\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u8fde\u7eed\u5faa\u73af\u3002", "result": "SCL\u5b9e\u73b0\u4e86\u4e09\u4e2a\u8d21\u732e\uff1a\u5c06\u54f2\u5b66\u6d1e\u89c1\u64cd\u4f5c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff1b\u8bc1\u660e\u8ba4\u77e5\u67b6\u6784\u4e2d\u7684\u529f\u80fd\u5206\u79bb\u6bd4\u5355\u4e00\u63d0\u793a\u7cfb\u7edf\u4ea7\u751f\u66f4\u4e00\u81f4\u548c\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff1b\u91cd\u65b0\u5b9a\u4e49\u667a\u80fd\u4e3a\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "\u771f\u6b63\u7684\u8fdb\u6b65\u4e0d\u5728\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u800c\u5728\u4e8e\u7ed3\u6784\u4e0a\u5b9e\u73b0\u8ba4\u77e5\u539f\u5219\u7684\u67b6\u6784\u3002\u8be5\u6846\u67b6\u5bf9\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548cAI\u90fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5c06\u77e5\u8bc6\u89c6\u4e3a\u5728\u73b0\u8c61\u5b66\u4e00\u81f4\u5faa\u73af\u5185\u7684\u6301\u7eed\u91cd\u5efa\u3002"}}
{"id": "2510.17009", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17009", "abs": "https://arxiv.org/abs/2510.17009", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Indrakshi Dey"], "title": "Traffic Prioritization Mechanisms for Mission and Time Critical Applications in Industrial Internet of Things", "comment": null, "summary": "Industrial Internet of Things (IIoT) promises to revolutionize industrial\noperations and productions through utilizing Machine-to-Machine (M2M)\ncommunications. Since each node in such environments generates various types of\ndata with diverse service requirements, MAC protocol holds crucial importance\nto ensure efficient delivery. In this context, simple to complex MAC schemes\nare found in literature. This paper focuses on evaluating the performance of\ntwo major techniques \"slot stealing\" and \"packet fragmentation\" for the IIoT;\nrepresentative protocols SS-MAC and FROG-MAC have been chosen from each\ncategory respectively. We conducted realistic simulations for the two protocols\nusing Contiki. Delay and packet loss comparison for SS-MAC and FROG-MAC\nindicates the superiority of FROG-MAC due to reduction in the waiting time for\nurgent traffic. Thus, a simple fragmentation scheme could be deployed for\nefficient scheduling of heterogenous traffic in the industrial environments.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86IIoT\u73af\u5883\u4e2d\u4e24\u79cdMAC\u6280\u672f\uff08\u65f6\u9699\u7a83\u53d6\u548c\u5206\u7ec4\u5206\u7247\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7SS-MAC\u548cFROG-MAC\u534f\u8bae\u5bf9\u6bd4\uff0c\u53d1\u73b0FROG-MAC\u56e0\u51cf\u5c11\u7d27\u6025\u6d41\u91cf\u7b49\u5f85\u65f6\u95f4\u800c\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u901a\u8fc7\u673a\u5668\u5bf9\u673a\u5668\u901a\u4fe1\u6539\u53d8\u5de5\u4e1a\u8fd0\u8425\uff0c\u4f46\u8282\u70b9\u751f\u6210\u7684\u6570\u636e\u7c7b\u578b\u591a\u6837\u4e14\u670d\u52a1\u9700\u6c42\u4e0d\u540c\uff0c\u56e0\u6b64MAC\u534f\u8bae\u5bf9\u786e\u4fdd\u9ad8\u6548\u4f20\u8f93\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u8bc4\u4f30\u4e0d\u540cMAC\u6280\u672f\u5728IIoT\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u9009\u62e9SS-MAC\uff08\u4ee3\u8868\u65f6\u9699\u7a83\u53d6\u6280\u672f\uff09\u548cFROG-MAC\uff08\u4ee3\u8868\u5206\u7ec4\u5206\u7247\u6280\u672f\uff09\u4e24\u79cd\u4ee3\u8868\u6027\u534f\u8bae\uff0c\u4f7f\u7528Contiki\u8fdb\u884c\u771f\u5b9e\u4eff\u771f\uff0c\u6bd4\u8f83\u5ef6\u8fdf\u548c\u4e22\u5305\u7387\u3002", "result": "FROG-MAC\u5728\u5ef6\u8fdf\u548c\u4e22\u5305\u65b9\u9762\u8868\u73b0\u4f18\u4e8eSS-MAC\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u51cf\u5c11\u4e86\u7d27\u6025\u6d41\u91cf\u7684\u7b49\u5f85\u65f6\u95f4\u3002", "conclusion": "\u7b80\u5355\u7684\u5206\u7ec4\u5206\u7247\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u5730\u8c03\u5ea6\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5f02\u6784\u6d41\u91cf\uff0cFROG-MAC\u6280\u672f\u66f4\u9002\u5408IIoT\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.16539", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16539", "abs": "https://arxiv.org/abs/2510.16539", "authors": ["Zhaowei Guan", "Wenkun Wen", "Peiran Wu", "Chen Wang", "Minghua Xia"], "title": "Hybrid CNN-Transformer Based Sparse Channel Prediction for High-Mobility OTFS Systems", "comment": "5 pages, 9 figures. To appear in IEEE Wireless Communications Letters", "summary": "High-mobility scenarios in next-generation wireless networks, such as those\ninvolving vehicular communications, require ultra-reliable and low-latency\ncommunications (URLLC). However, rapidly time-varying channels pose significant\nchallenges to traditional OFDM-based systems due to the Doppler effect and\nchannel aging. Orthogonal time frequency space (OTFS) modulation offers\nresilience by representing channels in the quasi-static delay-Doppler (DD)\ndomain. This letter proposes a novel channel prediction framework for OTFS\nsystems using a hybrid convolutional neural network and transformer\n(CNN-Transformer) architecture. The CNN extracts compact features that exploit\nthe DD-domain sparsity of the channel matrices, while the transformer models\ntemporal dependencies with causal masking for consistency. Simulation\nexperiments under extreme $500$ \\si{km/h} mobility conditions demonstrate that\nthe proposed method outperforms state-of-the-art baselines, reducing the root\nmean square error and mean absolute error by $12.2\\%$ and $9.4\\%$,\nrespectively. These results demonstrate the effectiveness of DD-domain\nrepresentations and the proposed model in accurately predicting channels in\nhigh-mobility scenarios, thereby supporting the stringent URLLC requirements in\nfuture wireless systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-Transformer\u6df7\u5408\u67b6\u6784\u7684OTFS\u7cfb\u7edf\u4fe1\u9053\u9884\u6d4b\u6846\u67b6\uff0c\u5728500km/h\u9ad8\u901f\u79fb\u52a8\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684URLLC\u9700\u6c42\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u9ad8\u901f\u79fb\u52a8\u573a\u666f\uff08\u5982\u8f66\u8054\u7f51\uff09\u9700\u8981\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff0c\u4f46\u5feb\u901f\u65f6\u53d8\u4fe1\u9053\u7ed9\u4f20\u7edfOFDM\u7cfb\u7edf\u5e26\u6765\u6311\u6218\u3002OTFS\u8c03\u5236\u901a\u8fc7\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u8868\u793a\u4fe1\u9053\u6765\u63d0\u4f9b\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528CNN-Transformer\u6df7\u5408\u67b6\u6784\uff1aCNN\u63d0\u53d6\u5229\u7528DD\u57df\u4fe1\u9053\u77e9\u9635\u7a00\u758f\u6027\u7684\u7d27\u51d1\u7279\u5f81\uff0cTransformer\u901a\u8fc7\u56e0\u679c\u63a9\u7801\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "result": "\u5728500km/h\u6781\u7aef\u79fb\u52a8\u6761\u4ef6\u4e0b\u7684\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8612.2%\u548c9.4%\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86DD\u57df\u8868\u793a\u548c\u6240\u63d0\u6a21\u578b\u5728\u9ad8\u901f\u79fb\u52a8\u573a\u666f\u4e0b\u51c6\u786e\u9884\u6d4b\u4fe1\u9053\u7684\u6709\u6548\u6027\uff0c\u4ece\u800c\u652f\u6301\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u4e25\u683c\u7684URLLC\u8981\u6c42\u3002"}}
{"id": "2510.15959", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.15959", "abs": "https://arxiv.org/abs/2510.15959", "authors": ["Isabelle Hupont", "Marisa Ponti", "Sven Schade"], "title": "Exploring the Potential of Citiverses for Regulatory Learning", "comment": "26 pages", "summary": "Citiverses hold the potential to support regulatory learning by offering\nimmersive, virtual environments for experimenting with policy scenarios and\ntechnologies. This paper proposes a science-for-policy agenda to explore the\npotential of citiverses as experimentation spaces for regulatory learning,\ngrounded in a consultation with a high-level panel of experts, including\npolicymakers from the European Commission, national government science advisers\nand leading researchers in digital regulation and virtual worlds. It identifies\nkey research areas, including scalability, real-time feedback, complexity\nmodelling, cross-border collaboration, risk reduction, citizen participation,\nethical considerations and the integration of emerging technologies. In\naddition, the paper analyses a set of experimental topics, spanning\ntransportation, urban planning and the environment/climate crisis, that could\nbe tested in citiverse platforms to advance regulatory learning in these areas.\nThe proposed work is designed to inform future research for policy and\nemphasizes a responsible approach to developing and using citiverses. It\nprioritizes careful consideration of the ethical, economic, ecological and\nsocial dimensions of different regulations. The paper also explores essential\npreliminary steps necessary for integrating citiverses into the broader\necosystems of experimentation spaces, including test beds, living labs and\nregulatory sandboxes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06citiverses\uff08\u57ce\u5e02\u865a\u62df\u4e16\u754c\uff09\u4f5c\u4e3a\u76d1\u7ba1\u5b66\u4e60\u5b9e\u9a8c\u7a7a\u95f4\u7684\u79d1\u5b66\u653f\u7b56\u8bae\u7a0b\uff0c\u901a\u8fc7\u4e13\u5bb6\u54a8\u8be2\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\uff0c\u5f3a\u8c03\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22citiverses\u4f5c\u4e3a\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u5728\u76d1\u7ba1\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u652f\u6301\u590d\u6742\u653f\u7b56\u573a\u666f\u548c\u6280\u672f\u7684\u6d4b\u8bd5\u3002", "method": "\u57fa\u4e8e\u4e0e\u6b27\u76df\u59d4\u5458\u4f1a\u653f\u7b56\u5236\u5b9a\u8005\u3001\u56fd\u5bb6\u653f\u5e9c\u79d1\u5b66\u987e\u95ee\u548c\u6570\u5b57\u76d1\u7ba1\u9886\u57df\u9886\u5148\u7814\u7a76\u8005\u7684\u9ad8\u5c42\u4e13\u5bb6\u54a8\u8be2\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u9886\u57df\u548c\u5b9e\u9a8c\u4e3b\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u53cd\u9988\u3001\u590d\u6742\u6027\u5efa\u6a21\u3001\u8de8\u5883\u5408\u4f5c\u3001\u98ce\u9669\u964d\u4f4e\u3001\u516c\u6c11\u53c2\u4e0e\u3001\u4f26\u7406\u8003\u91cf\u548c\u65b0\u5174\u6280\u672f\u6574\u5408\u7b49\u5173\u952e\u7814\u7a76\u9886\u57df\uff0c\u4ee5\u53ca\u4ea4\u901a\u3001\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883/\u6c14\u5019\u5371\u673a\u7b49\u5b9e\u9a8c\u4e3b\u9898\u3002", "conclusion": "citiverses\u6709\u6f5c\u529b\u6210\u4e3a\u76d1\u7ba1\u5b66\u4e60\u7684\u91cd\u8981\u5b9e\u9a8c\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u8d1f\u8d23\u4efb\u7684\u53d1\u5c55\u65b9\u6cd5\uff0c\u5145\u5206\u8003\u8651\u4f26\u7406\u3001\u7ecf\u6d4e\u3001\u751f\u6001\u548c\u793e\u4f1a\u7ef4\u5ea6\uff0c\u5e76\u6574\u5408\u5230\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.17147", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17147", "abs": "https://arxiv.org/abs/2510.17147", "authors": ["Linhan Xia", "Mingzhan Yang", "Jingjing Wang", "Ziwei Yan", "Yakun Ren", "Guo Yu", "Kai Lei"], "title": "Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking", "comment": null, "summary": "Transformer-based large language models (LLMs) are increasingly being adopted\nin networking research to address domain-specific challenges. However, their\nquadratic time complexity and substantial model sizes often result in\nsignificant computational overhead and memory constraints, particularly in\nresource-constrained environments. Drawing inspiration from the efficiency and\nperformance of the Deepseek-R1 model within the knowledge distillation\nparadigm, this paper introduces Mamba4Net, a novel cross-architecture\ndistillation framework. Mamba4Net transfers networking-specific knowledge from\ntransformer-based LLMs to student models built on the Mamba architecture, which\nfeatures linear time complexity. This design substantially enhances\ncomputational efficiency compared to the quadratic complexity of\ntransformer-based models, while the reduced model size further minimizes\ncomputational demands, improving overall performance and resource utilization.\nTo evaluate its effectiveness, Mamba4Net was tested across three diverse\nnetworking tasks: viewport prediction, adaptive bitrate streaming, and cluster\njob scheduling. Compared to existing methods that do not leverage LLMs,\nMamba4Net demonstrates superior task performance. Furthermore, relative to\ndirect applications of transformer-based LLMs, it achieves significant\nefficiency gains, including a throughput 3.96 times higher and a storage\nfootprint of only 5.48% of that required by previous LLM-based approaches.\nThese results highlight Mamba4Net's potential to enable the cost-effective\napplication of LLM-derived knowledge in networking contexts. The source code is\nopenly available to support further research and development.", "AI": {"tldr": "Mamba4Net\u662f\u4e00\u4e2a\u8de8\u67b6\u6784\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u8f6c\u79fb\u5230\u57fa\u4e8eMamba\u67b6\u6784\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4e09\u4e2a\u7f51\u7edc\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Transformer\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b58\u5728\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5927\u6a21\u578b\u5c3a\u5bf8\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMamba4Net\u8de8\u67b6\u6784\u84b8\u998f\u6846\u67b6\uff0c\u4eceTransformer LLMs\u5411\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684Mamba\u67b6\u6784\u5b66\u751f\u6a21\u578b\u8f6c\u79fb\u7f51\u7edc\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u89c6\u53e3\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u6d41\u548c\u96c6\u7fa4\u4f5c\u4e1a\u8c03\u5ea6\u4e09\u4e2a\u4efb\u52a1\u4e2d\uff0cMamba4Net\u6027\u80fd\u4f18\u4e8e\u4e0d\u5229\u7528LLM\u7684\u73b0\u6709\u65b9\u6cd5\uff1b\u76f8\u6bd4\u76f4\u63a5\u4f7f\u7528Transformer LLMs\uff0c\u541e\u5410\u91cf\u63d0\u9ad83.96\u500d\uff0c\u5b58\u50a8\u5360\u7528\u4ec5\u4e3a\u4e4b\u524d\u76845.48%\u3002", "conclusion": "Mamba4Net\u80fd\u591f\u5b9e\u73b0LLM\u884d\u751f\u77e5\u8bc6\u5728\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6210\u672c\u6548\u76ca\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u9ad8\u6548\u90e8\u7f72LLM\u77e5\u8bc6\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "PISA\u662f\u4e00\u4e2a\u53d7\u76ae\u4e9a\u6770\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\u542f\u53d1\u7684\u7edf\u4e00\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u6a21\u5f0f\u9002\u5e94\u673a\u5236\u548c\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4e14\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u5efa\u6784\u6027\u548c\u4efb\u52a1\u5bfc\u5411\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e09\u6a21\u5f0f\u9002\u5e94\u673a\u5236\uff08\u56fe\u5f0f\u66f4\u65b0\u3001\u56fe\u5f0f\u6f14\u5316\u548c\u56fe\u5f0f\u521b\u5efa\uff09\u548c\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u68c0\u7d22\u3002", "result": "\u5728LOCOMO\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684AggQA\u57fa\u51c6\u4e0a\uff0cPISA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u3002", "conclusion": "PISA\u901a\u8fc7\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u5efa\u6784\u6027\u548c\u9002\u5e94\u6027\u8fc7\u7a0b\uff0c\u4e3aAI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17342", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17342", "abs": "https://arxiv.org/abs/2510.17342", "authors": ["Alberto Ceresoli", "Viola Bernazzoli", "Roberto Pegurri", "Ilario Filippini"], "title": "AoA Services in 5G Networks: A Framework for Real-World Implementation and Systematic Testing", "comment": null, "summary": "Accurate positioning is a key enabler for emerging 5G applications. While the\nstandardized Location Management Function (LMF) operates centrally within the\ncore network, its scalability and latency limitations hinder low-latency and\nfine-grained localization. A practical alternative is to shift positioning\nintelligence toward the radio access network (RAN), where uplink sounding\nreference signal (SRS)-based angle-of-arrival (AoA) estimation offers a\nlightweight, network-native solution. In this work, we present the first fully\nopen-source 5G testbed for AoA estimation, enabling systematic and repeatable\nexperimentation under realistic yet controllable channel conditions. The\nframework integrates the NVIDIA Sionna RT with a Keysight PROPSIM channel\nemulator and includes a novel phase calibration procedure for USRP N310\ndevices. Experimental results show sub-degree to few-degree accuracy,\nvalidating the feasibility of lightweight, single-anchor, network-native\nlocalization within next-generation 5G systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u76845G\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u5230\u8fbe\u89d2\u4f30\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u57285G\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3001\u5355\u951a\u70b9\u3001\u7f51\u7edc\u539f\u751f\u5b9a\u4f4d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u6807\u51c6\u5316\u7684\u4f4d\u7f6e\u7ba1\u7406\u529f\u80fd\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u5ef6\u8fdf\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u9700\u6c42\uff0c\u9700\u8981\u5c06\u5b9a\u4f4d\u667a\u80fd\u8f6c\u79fb\u5230\u65e0\u7ebf\u63a5\u5165\u7f51\u3002", "method": "\u96c6\u6210NVIDIA Sionna RT\u4e0eKeysight PROPSIM\u4fe1\u9053\u4eff\u771f\u5668\uff0c\u5f00\u53d1\u4e86USRP N310\u8bbe\u5907\u7684\u65b0\u76f8\u4f4d\u6821\u51c6\u7a0b\u5e8f\uff0c\u6784\u5efa\u4e86\u5b8c\u5168\u5f00\u6e90\u76845G\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8fbe\u5230\u4e9a\u5ea6\u5230\u51e0\u5ea6\u7684\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u5355\u951a\u70b9\u7f51\u7edc\u539f\u751f\u5b9a\u4f4d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5f00\u6e90\u6d4b\u8bd5\u5e73\u53f0\u4e3a5G\u7cfb\u7edf\u4e2d\u7684\u5230\u8fbe\u89d2\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u548c\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u73af\u5883\uff0c\u8bc1\u660e\u4e86\u7f51\u7edc\u539f\u751f\u5b9a\u4f4d\u65b9\u6848\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.16620", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16620", "abs": "https://arxiv.org/abs/2510.16620", "authors": ["Yingyao Zhou", "Natasha Devroye", "Onur G\u00fcnl\u00fc"], "title": "Feedback Lunch: Deep Feedback Codes for Wiretap Channels", "comment": "submitted to IEEE COMMUNICATIONS LETTERS", "summary": "We consider reversely-degraded wiretap channels, for which the secrecy\ncapacity is zero if there is no channel feedback. This work focuses on a seeded\nmodular code design for the Gaussian wiretap channel with channel output\nfeedback, combining universal hash functions for security and learned\nfeedback-based codes for reliability to achieve positive secrecy rates. We\nstudy the trade-off between communication reliability and information leakage,\nillustrating that feedback enables agreeing on a secret key shared between\nlegitimate parties, overcoming the security advantage of the wiretapper. Our\nfindings also motivate code designs for sensing-assisted secure communication,\nto be used in next-generation integrated sensing and communication methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u65af\u7a83\u542c\u4fe1\u9053\u7684\u6a21\u5757\u5316\u7f16\u7801\u8bbe\u8ba1\uff0c\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u548c\u5b66\u4e60\u578b\u53cd\u9988\u7f16\u7801\uff0c\u5728\u4fe1\u9053\u8f93\u51fa\u53cd\u9988\u4e0b\u5b9e\u73b0\u6b63\u4fdd\u5bc6\u901f\u7387", "motivation": "\u89e3\u51b3\u53cd\u5411\u9000\u5316\u7a83\u542c\u4fe1\u9053\u5728\u65e0\u53cd\u9988\u65f6\u4fdd\u5bc6\u5bb9\u91cf\u4e3a\u96f6\u7684\u95ee\u9898\uff0c\u5229\u7528\u53cd\u9988\u4f7f\u5408\u6cd5\u65b9\u5171\u4eab\u5bc6\u94a5\u4ee5\u514b\u670d\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf", "method": "\u91c7\u7528\u57fa\u4e8e\u79cd\u5b50\u7684\u6a21\u5757\u5316\u7f16\u7801\u8bbe\u8ba1\uff0c\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u5b66\u4e60\u578b\u53cd\u9988\u7f16\u7801\u4fdd\u8bc1\u53ef\u9760\u6027", "result": "\u5b9e\u73b0\u4e86\u6b63\u4fdd\u5bc6\u901f\u7387\uff0c\u5c55\u793a\u4e86\u901a\u4fe1\u53ef\u9760\u6027\u4e0e\u4fe1\u606f\u6cc4\u9732\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb", "conclusion": "\u53cd\u9988\u673a\u5236\u4f7f\u5408\u6cd5\u65b9\u80fd\u591f\u534f\u5546\u5171\u4eab\u5bc6\u94a5\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u65b9\u6cd5\u4e2d\u7684\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u8bbe\u8ba1\u601d\u8def"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u6765\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\uff0c\u4e5f\u65e0\u6cd5\u907f\u514d\u5176\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\u3002\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u4f1a\u504f\u79bb\u6700\u4f18\u7b56\u7565\u548c\u968f\u673a\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\u7684\u73b0\u8c61\uff0c\u7814\u7a76\u8005\u60f3\u63a2\u7a76\u8fd9\u662f\u5426\u4e0e\u6a21\u578b\u9700\u8981\u81ea\u884c\u8ddf\u8e2a\u72b6\u6001\u7a7a\u95f4\u6709\u5173\uff0c\u56e0\u6b64\u901a\u8fc7\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u6765\u6d4b\u8bd5\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u4e3aLLM\u63d0\u4f9b\u6cb3\u5185\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u79fb\u52a8\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u7ed3\u679c\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u81ea\u6211\u63d0\u793a\u8fdb\u884c\u4e0b\u4e00\u6b65\u79fb\u52a8\u3002", "result": "\u73af\u5883\u63a5\u53e3\u7684\u63a5\u5165\u5e76\u4e0d\u80fd\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\u3002\u7b56\u7565\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u504f\u79bb\u6700\u4f18\u7b56\u7565\u548c\u968f\u673a\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\u3002", "conclusion": "\u6027\u80fd\u5d29\u6e83\u73b0\u8c61\u4e0e\u73af\u5883\u63a5\u53e3\u65e0\u5173\uff0c\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u4f1a\u9677\u5165\u7279\u5b9a\u7684\u9519\u8bef\u6a21\u5f0f\u3002\u8fd9\u4e00\u73b0\u8c61\u53ef\u80fd\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u3002"}}
{"id": "2510.17395", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17395", "abs": "https://arxiv.org/abs/2510.17395", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Aleksei Shashin", "Evgeny Khorov"], "title": "Enhancing 5G V2X Mode 2 for Sporadic Traffic", "comment": null, "summary": "The emerging road safety and autonomous vehicle applications require timely\nand reliable data delivery between vehicles and between vehicles and\ninfrastructure. To satisfy this demand, 3GPP develops a 5G\nVehicle-to-Everything (V2X) technology. Depending on the served traffic type,\n5G V2X specifications propose two channel access methods: (i) Mode 1, according\nto which a base station allocates resources to users, and (ii) Mode 2,\naccording to which users autonomously select resources for their transmissions.\nIn the paper, we consider a scenario with sporadic traffic, e.g., a vehicle\ngenerates a packet at a random time moment when it detects a dangerous\nsituation, which imposes strict requirements on delay and reliability. To\nsatisfy strict delay requirements, vehicles use Mode 2. We analyze the\nperformance of Mode 2 for sporadic traffic and propose several approaches to\nimprove it. Simulation results show that the proposed approaches can increase\nthe system capacity by up to 40% with a low impact on complexity.", "AI": {"tldr": "\u5206\u67905G V2X\u4e2dMode 2\u5728\u5076\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u7cfb\u7edf\u5bb9\u91cf\u8fbe40%\u4e14\u590d\u6742\u5ea6\u5f71\u54cd\u5c0f", "motivation": "\u9053\u8def\u5b89\u5168\u548c\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u9700\u8981\u8f66\u8f86\u95f4\u53ca\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u95f4\u53ca\u65f6\u53ef\u9760\u7684\u6570\u636e\u4f20\u8f93\uff0c5G V2X\u6280\u672f\u4e3a\u6b64\u800c\u5f00\u53d1\u3002\u9488\u5bf9\u5076\u53d1\u6d41\u91cf\u573a\u666f\uff08\u5982\u8f66\u8f86\u68c0\u6d4b\u5230\u5371\u9669\u60c5\u51b5\u65f6\u968f\u673a\u751f\u6210\u6570\u636e\u5305\uff09\uff0c\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u8981\u6c42", "method": "\u8003\u8651\u8f66\u8f86\u4f7f\u7528Mode 2\uff08\u7528\u6237\u81ea\u4e3b\u9009\u62e9\u4f20\u8f93\u8d44\u6e90\uff09\u6765\u6ee1\u8db3\u4e25\u683c\u5ef6\u8fdf\u8981\u6c42\u3002\u5206\u6790Mode 2\u5728\u5076\u53d1\u6d41\u91cf\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u6539\u8fdb\u65b9\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u7cfb\u7edf\u5bb9\u91cf\u63d0\u5347\u9ad8\u8fbe40%\uff0c\u540c\u65f6\u5bf9\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u5f88\u5c0f", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdb\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53475G V2X Mode 2\u5728\u5076\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u9053\u8def\u5b89\u5168\u548c\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u901a\u4fe1\u4fdd\u969c"}}
{"id": "2510.16792", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16792", "abs": "https://arxiv.org/abs/2510.16792", "authors": ["Zhi Gu", "Wai Ho Mow"], "title": "Non-Orthogonal Pilot Sequence Design for Multi-Cells Interference Networks", "comment": null, "summary": "In wireless communications, the performance of non-orthogonal sequence sets\nsignificantly affects the level of multi-user interference when the number of\nusers surpasses the sequence length. The design of non-orthogonal sequences\nplays a crucial role in both the non-orthogonality of the pilots in multi-cell\nsystems and the signature sequences in overloaded code-division multiple-access\n(CDMA) systems. In multi-cell systems, considering the strength disparity\nbetween channels originating from the home cell and the neighboring cells, the\nextended total squared correlation (ETSC) is proposed as a new sequence design\ncriterion, which is defined as the sum of squares of the weighted correlations\namong sequences. In this paper, we derive a closed-form expression for the\nlower bound of ETSC for multi-cell systems with a given sequence length $\\tau$,\nwhere $\\tau \\leq K$ and $K$ is the number of users per cell. This can be\nregarded as a generalization of the well-known Welch bound (Welch, 1974, IEEE\nTIT) and the extended Welch bound (Wang et al., 2021, IEEE TWC). Additionally,\nfrom the necessary conditions of the bound, the optimal sequence set can be\neasily obtained when the interference power factor matrix is positive definite.\nOn the other hand, to address the lack of sequence generation methods under\ncertain parameter conditions, we propose the ETSC-MM algorithm, which generates\nsequence sets with low ETSC based on a Majorization-Minimization (MM)\noptimization framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u5c0f\u533a\u7cfb\u7edf\u4e2d\u6269\u5c55\u603b\u5e73\u65b9\u76f8\u5173(ETSC)\u7684\u65b0\u5e8f\u5217\u8bbe\u8ba1\u51c6\u5219\uff0c\u63a8\u5bfc\u4e86ETSC\u7684\u4e0b\u754c\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86ETSC-MM\u7b97\u6cd5\u6765\u751f\u6210\u4f4eETSC\u5e8f\u5217\u96c6\u3002", "motivation": "\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\uff0c\u5f53\u7528\u6237\u6570\u8d85\u8fc7\u5e8f\u5217\u957f\u5ea6\u65f6\uff0c\u975e\u6b63\u4ea4\u5e8f\u5217\u96c6\u7684\u6027\u80fd\u663e\u8457\u5f71\u54cd\u591a\u7528\u6237\u5e72\u6270\u6c34\u5e73\u3002\u591a\u5c0f\u533a\u7cfb\u7edf\u4e2d\u8003\u8651\u672c\u5c0f\u533a\u548c\u90bb\u5c0f\u533a\u4fe1\u9053\u5f3a\u5ea6\u5dee\u5f02\uff0c\u9700\u8981\u65b0\u7684\u5e8f\u5217\u8bbe\u8ba1\u51c6\u5219\u3002", "method": "\u63d0\u51faETSC\u4f5c\u4e3a\u65b0\u5e8f\u5217\u8bbe\u8ba1\u51c6\u5219\uff0c\u63a8\u5bfcETSC\u4e0b\u754c\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8eMajorization-Minimization\u4f18\u5316\u6846\u67b6\u5f00\u53d1ETSC-MM\u7b97\u6cd5\u751f\u6210\u4f4eETSC\u5e8f\u5217\u96c6\u3002", "result": "\u63a8\u5bfc\u4e86ETSC\u4e0b\u754c\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u8fd9\u662f\u5bf9Welch\u754c\u548c\u6269\u5c55Welch\u754c\u7684\u63a8\u5e7f\u3002\u5f53\u5e72\u6270\u529f\u7387\u56e0\u5b50\u77e9\u9635\u6b63\u5b9a\u65f6\uff0c\u53ef\u4ee5\u4ece\u8fb9\u754c\u5fc5\u8981\u6761\u4ef6\u8f7b\u677e\u83b7\u5f97\u6700\u4f18\u5e8f\u5217\u96c6\u3002", "conclusion": "ETSC\u51c6\u5219\u548cETSC-MM\u7b97\u6cd5\u4e3a\u591a\u5c0f\u533a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u975e\u6b63\u4ea4\u5e8f\u5217\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u964d\u4f4e\u591a\u7528\u6237\u5e72\u6270\u5e76\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.15980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15980", "abs": "https://arxiv.org/abs/2510.15980", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition", "comment": null, "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8ba4\u77e5\u8d1f\u8377\u75d5\u8ff9\uff08CLTs\uff09\u4f5c\u4e3a\u6df1\u5ea6\u6a21\u578b\u7684\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u3001\u65f6\u53d8\u51fd\u6570\u91cf\u5316\u6a21\u578b\u5185\u90e8\u8d44\u6e90\u5206\u914d\uff0c\u5305\u542b\u5185\u5728\u3001\u5916\u5728\u548c\u5173\u8054\u8d1f\u8377\u4e09\u4e2a\u5206\u91cf\uff0c\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u63d0\u9ad8\u63a8\u7406\u6548\u738715-30%\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u6df1\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e2d\u5c42\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u52a8\u6001\u3002", "method": "\u5c06CLTs\u5b9a\u4e49\u4e3a\u4e09\u5206\u91cf\u968f\u673a\u8fc7\u7a0b\uff08\u5185\u5728\u8d1f\u8377\u3001\u5916\u5728\u8d1f\u8377\u3001\u5173\u8054\u8d1f\u8377\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u3001KV\u7f13\u5b58\u672a\u547d\u4e2d\u7387\u3001\u8868\u793a\u5206\u6563\u5ea6\u548c\u89e3\u7801\u7a33\u5b9a\u6027\u7b49\u53ef\u6d4b\u91cf\u4ee3\u7406\u8fdb\u884c\u5b9e\u4f8b\u5316\uff0c\u5e76\u63d0\u51fa\u7b26\u53f7\u5316\u516c\u5f0f\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\uff08\u8d1f\u8377\u66f2\u7ebf\u3001\u5355\u7eaf\u5f62\u56fe\uff09\u3002", "result": "\u5728\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLTs\u80fd\u591f\u9884\u6d4b\u9519\u8bef\u53d1\u751f\u3001\u63ed\u793a\u8ba4\u77e5\u7b56\u7565\uff0c\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u5e72\u9884\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad815-30%\u3002", "conclusion": "CLTs\u4e3a\u6df1\u5ea6\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u80fd\u591f\u91cf\u5316\u5206\u6790\u63a8\u7406\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u8d1f\u8377\u5f15\u5bfc\u4f18\u5316\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002"}}
{"id": "2510.17410", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17410", "abs": "https://arxiv.org/abs/2510.17410", "authors": ["Dmitry Bankov", "Artem Krasilov", "Artem Otmakhov", "Pavel Savlukovich", "Evgeny Khorov"], "title": "Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?", "comment": null, "summary": "5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to\nsupport inter-vehicle communication. In contrast to 4G V2X which allows only\nbroadcast communication, 5G V2X enables groupcast and unicast communication.\nSuch types of communication are needed for new V2X scenarios: platooning,\nextended sensors, remote driving, etc. To improve the data transmission\nreliability and assist in the selection of the transmission parameters in these\nscenarios, 5G V2X introduces a feedback channel that allows receivers to send\nacknowledgments in response to data packets. However, some part of the overall\nresource shall be allocated for the feedback channel, which reduces the amount\nof channel resources available for data transmission. In this paper, we\nconsider a scenario with a platoon, which generates groupcast traffic, and\nsurrounding vehicles, which generate legacy broadcast traffic. Using extensive\nsimulations in NS-3, we analyze how the usage of the feedback channel\ninfluences the overall system capacity. Our results show that depending on the\nplatoon size, groupcast, and broadcast traffic intensities, and their quality\nof service requirements, the usage of the feedback channel can in some cases\nsignificantly increase the system capacity (up to 2x), while in other cases it\nalmost halves the system capacity. We explain the reasons for such effects and\ndiscuss how to adaptively select the feedback channel parameters.", "AI": {"tldr": "5G V2X\u5f15\u5165\u53cd\u9988\u4fe1\u9053\u63d0\u5347\u4f20\u8f93\u53ef\u9760\u6027\uff0c\u4f46\u4f1a\u5360\u7528\u6570\u636e\u4fe1\u9053\u8d44\u6e90\u3002\u7814\u7a76\u53d1\u73b0\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u8f66\u961f\u89c4\u6a21\u3001\u901a\u4fe1\u5f3a\u5ea6\u548cQoS\u8981\u6c42\uff0c\u53ef\u80fd\u4f7f\u5bb9\u91cf\u7ffb\u500d\u6216\u51cf\u534a\u3002", "motivation": "5G V2X\u652f\u6301\u7ec4\u64ad\u548c\u5355\u64ad\u901a\u4fe1\uff0c\u9700\u8981\u53cd\u9988\u4fe1\u9053\u6765\u63d0\u5347\u4f20\u8f93\u53ef\u9760\u6027\u548c\u53c2\u6570\u9009\u62e9\uff0c\u4f46\u53cd\u9988\u4fe1\u9053\u4f1a\u5360\u7528\u6570\u636e\u4fe1\u9053\u8d44\u6e90\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528NS-3\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0c\u5206\u6790\u8f66\u961f\u573a\u666f\u4e0b\u53cd\u9988\u4fe1\u9053\u4f7f\u7528\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u8003\u8651\u8f66\u961f\u89c4\u6a21\u3001\u7ec4\u64ad\u548c\u5e7f\u64ad\u6d41\u91cf\u5f3a\u5ea6\u53ca\u5176QoS\u8981\u6c42\u3002", "result": "\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u5177\u6709\u53cc\u91cd\u6027\uff1a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u5bb9\u91cf\uff08\u6700\u9ad8\u53ef\u8fbe2\u500d\uff09\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u51e0\u4e4e\u4f7f\u7cfb\u7edf\u5bb9\u91cf\u51cf\u534a\u3002", "conclusion": "\u53cd\u9988\u4fe1\u9053\u5bf9\u7cfb\u7edf\u5bb9\u91cf\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5177\u4f53\u573a\u666f\u53c2\u6570\uff0c\u9700\u8981\u81ea\u9002\u5e94\u9009\u62e9\u53cd\u9988\u4fe1\u9053\u53c2\u6570\u4ee5\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.16948", "categories": ["cs.IT", "cs.CV", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16948", "abs": "https://arxiv.org/abs/2510.16948", "authors": ["Ruiming Guo", "Ayush Bhandari"], "title": "Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude", "comment": "28 Pages, 10 figures. To appear in IEEE Journal of Selected Topics in\n  Signal Processing", "summary": "The recovery of Dirac impulses, or spikes, from filtered measurements is a\nclassical problem in signal processing. As the spikes lie in the continuous\ndomain while measurements are discrete, this task is known as super-resolution\nor off-the-grid sparse recovery. Despite significant theoretical and\nalgorithmic advances over the past decade, these developments often overlook\ncritical challenges at the analog-digital interface. In particular, when spikes\nexhibit strong-weak amplitude disparity, conventional digital acquisition may\nresult in clipping of strong components or loss of weak ones beneath the\nquantization noise floor. This motivates a broader perspective:\nsuper-resolution must simultaneously resolve both amplitude and temporal\nstructure. Under a fixed bit budget, such information loss is unavoidable. In\ncontrast, the emerging theory and practice of the Unlimited Sensing Framework\n(USF) demonstrate that these fundamental limitations can be overcome. Building\non this foundation, we demonstrate that modulo encoding within USF enables\ndigital super-resolution by enhancing measurement precision, thereby unlocking\ntemporal super-resolution beyond conventional limits. We develop new\ntheoretical results that extend to non-bandlimited kernels commonly encountered\nin practice and introduce a robust algorithm for off-the-grid sparse recovery.\nTo demonstrate practical impact, we instantiate our framework in the context of\ntime-of-flight imaging. Both numerical simulations and hardware experiments\nvalidate the effectiveness of our approach under low-bit quantization, enabling\nsuper-resolution in amplitude and time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u9650\u5236\u611f\u77e5\u6846\u67b6(USF)\u7684\u6570\u5b57\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u6570\u7f16\u7801\u589e\u5f3a\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u91cf\u5316\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5e45\u5ea6\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u8d85\u5206\u8fa8\u7387\u6062\u590d\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u91c7\u96c6\u5728\u5904\u7406\u5f3a-\u5f31\u5e45\u5ea6\u5dee\u5f02\u7684\u8109\u51b2\u4fe1\u53f7\u65f6\uff0c\u4f1a\u51fa\u73b0\u5f3a\u5206\u91cf\u524a\u6ce2\u6216\u5f31\u5206\u91cf\u88ab\u91cf\u5316\u566a\u58f0\u6df9\u6ca1\u7684\u95ee\u9898\u3002\u5728\u56fa\u5b9a\u6bd4\u7279\u9884\u7b97\u4e0b\uff0c\u8fd9\u79cd\u4fe1\u606f\u635f\u5931\u4e0d\u53ef\u907f\u514d\uff0c\u56e0\u6b64\u9700\u8981\u540c\u65f6\u89e3\u51b3\u5e45\u5ea6\u548c\u65f6\u95f4\u7ed3\u6784\u7684\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65e0\u9650\u5236\u611f\u77e5\u6846\u67b6(USF)\u4e2d\u7684\u6a21\u6570\u7f16\u7801\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u975e\u5e26\u9650\u6838\u7684\u65b0\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u9c81\u68d2\u7684\u79bb\u7f51\u683c\u7a00\u758f\u6062\u590d\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4f4e\u4f4d\u91cf\u5316\u4e0b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5e45\u5ea6\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "\u65e0\u9650\u5236\u611f\u77e5\u6846\u67b6\u901a\u8fc7\u6a21\u6570\u7f16\u7801\u514b\u670d\u4e86\u4f20\u7edf\u6570\u5b57\u91c7\u96c6\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65f6\u95f4\u98de\u884c\u6210\u50cf\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.15981", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.15981", "abs": "https://arxiv.org/abs/2510.15981", "authors": ["Rafael Cabral", "Tuan Manh Do", "Xuejun Yu", "Wai Ming Tai", "Zijin Feng", "Xin Shen"], "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization", "comment": null, "summary": "Proof autoformalization, the task of translating natural language theorems\nand proofs into machine-verifiable code, is a critical step for integrating\nlarge language models into rigorous mathematical workflows. Current approaches\nfocus on producing executable code, but they frequently fail to preserve the\nsemantic meaning and logical structure of the original human-written argument.\nTo address this, we introduce ProofFlow, a novel pipeline that treats\nstructural fidelity as a primary objective. ProofFlow first constructs a\ndirected acyclic graph (DAG) to map the logical dependencies between proof\nsteps. Then, it employs a novel lemma-based approach to systematically\nformalize each step as an intermediate lemma, preserving the logical structure\nof the original argument. To facilitate evaluation, we present a new benchmark\nof 184 undergraduate-level problems, manually annotated with step-by-step\nsolutions and logical dependency graphs, and introduce ProofScore, a new\ncomposite metric to evaluate syntactic correctness, semantic faithfulness, and\nstructural fidelity. Experimental results show our pipeline sets a new\nstate-of-the-art for autoformalization, achieving a ProofScore of 0.545,\nsubstantially exceeding baselines like full-proof formalization (0.123), which\nprocesses the entire proof at once, and step-proof formalization (0.072), which\nhandles each step independently. Our pipeline, benchmark, and score metric are\nopen-sourced to encourage further progress at\nhttps://github.com/Huawei-AI4Math/ProofFlow.", "AI": {"tldr": "ProofFlow\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u8bc1\u660e\u7ba1\u9053\uff0c\u901a\u8fc7\u6784\u5efa\u903b\u8f91\u4f9d\u8d56\u56fe\u548c\u4f7f\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u6765\u4fdd\u6301\u8bc1\u660e\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\uff0c\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f46\u7ecf\u5e38\u65e0\u6cd5\u4fdd\u6301\u539f\u59cb\u8bc1\u660e\u7684\u8bed\u4e49\u542b\u4e49\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e25\u8c28\u6570\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u96c6\u6210\u3002", "method": "ProofFlow\u9996\u5148\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u6765\u6620\u5c04\u8bc1\u660e\u6b65\u9aa4\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5f15\u7406\u7684\u65b9\u6cd5\u7cfb\u7edf\u5730\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5f62\u5f0f\u5316\u4e3a\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u4fdd\u6301\u539f\u59cb\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aProofFlow\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0cProofScore\u5f97\u5206\u4e3a0.545\uff0c\u663e\u8457\u4f18\u4e8e\u5b8c\u6574\u8bc1\u660e\u5f62\u5f0f\u5316(0.123)\u548c\u6b65\u9aa4\u8bc1\u660e\u5f62\u5f0f\u5316(0.072)\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProofFlow\u901a\u8fc7\u5173\u6ce8\u7ed3\u6784\u4fdd\u771f\u5ea6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u65b0\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.17445", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17445", "abs": "https://arxiv.org/abs/2510.17445", "authors": ["Mohd Saif Ali Khan", "Karthik RM", "Samar Agnihotri"], "title": "Adaptive Local Combining with Decentralized Decoding for Distributed Massive MIMO", "comment": null, "summary": "A major bottleneck in uplink distributed massive multiple-input\nmultiple-output networks is the sub-optimal performance of local combining\nschemes, coupled with high fronthaul load and computational cost inherent in\ncentralized large scale fading decoding (LSFD) architectures. This paper\nintroduces a decentralized decoding architecture that fundamentally breaks from\nthe conventional LSFD, by allowing each AP calculates interference-suppressing\nlocal weights independently and applies them to its data estimates before\ntransmission. Furthermore, two generalized local zero-forcing (ZF) framework,\ngeneralized partial full-pilot ZF (G-PFZF) and generalized protected weak PFZF\n(G-PWPFZF), are introduced, where each access point (AP) adaptively and\nindependently determines its combining strategy through a local sum spectral\nefficiency optimization that classifies user equipments (UEs) as strong or weak\nusing only local information, eliminating the fixed thresholds used in PFZF and\nPWPFZF. To further enhance scalability, pilot-dependent combining vectors\ninstead of user-dependent ones are introduced and are shared among users with\nthe same pilot. The corresponding closed-form spectral efficiency expressions\nare derived. Numerical results show that the proposed generalized schemes\nconsistently outperform fixed-threshold counterparts, while the introduction of\nlocal weights yields lower overhead and computation costs with minimal\nperformance penalty compared to them.", "AI": {"tldr": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u89e3\u7801\u67b6\u6784\uff0c\u901a\u8fc7AP\u72ec\u7acb\u8ba1\u7b97\u5e72\u6270\u6291\u5236\u6743\u91cd\uff0c\u5f15\u5165\u5e7f\u4e49\u5c40\u90e8\u8feb\u96f6\u6846\u67b6(G-PFZF\u548cG-PWPFZF)\uff0c\u81ea\u9002\u5e94\u5206\u7c7b\u7528\u6237\u5e76\u4f18\u5316\u9891\u8c31\u6548\u7387\uff0c\u4f7f\u7528\u5bfc\u9891\u76f8\u5173\u7ec4\u5408\u5411\u91cf\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0a\u884c\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7f51\u7edc\u4e2d\u672c\u5730\u7ec4\u5408\u65b9\u6848\u6027\u80fd\u4e0d\u4f73\u3001\u524d\u4f20\u8d1f\u8f7d\u9ad8\u548c\u96c6\u4e2d\u5f0fLSFD\u67b6\u6784\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u53bb\u4e2d\u5fc3\u5316\u89e3\u7801\u67b6\u6784\uff0cAP\u72ec\u7acb\u8ba1\u7b97\u5e72\u6270\u6291\u5236\u6743\u91cd\uff1b\u5e7f\u4e49\u5c40\u90e8\u8feb\u96f6\u6846\u67b6(G-PFZF\u548cG-PWPFZF)\uff0c\u57fa\u4e8e\u5c40\u90e8\u9891\u8c31\u6548\u7387\u4f18\u5316\u81ea\u9002\u5e94\u5206\u7c7b\u7528\u6237\uff1b\u5bfc\u9891\u76f8\u5173\u7ec4\u5408\u5411\u91cf\u66ff\u4ee3\u7528\u6237\u76f8\u5173\u5411\u91cf\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u5e7f\u4e49\u65b9\u6848\u6301\u7eed\u4f18\u4e8e\u56fa\u5b9a\u9608\u503c\u5bf9\u5e94\u65b9\u6848\uff0c\u672c\u5730\u6743\u91cd\u5f15\u5165\u964d\u4f4e\u4e86\u5f00\u9500\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\u548c\u5e7f\u4e49\u5c40\u90e8\u8feb\u96f6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7f51\u7edc\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u964d\u4f4e\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2510.17093", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17093", "abs": "https://arxiv.org/abs/2510.17093", "authors": ["Yunfeng Wen", "Fang Yang", "Jian Song", "Zhu Han"], "title": "Channel Capacity for FMCW-based Optical Wireless Integrated Sensing and Communication: Asymptotic Analysis and Envelope Design", "comment": "This work has been submitted to the IEEE for possible publication. 13\n  pages, 7 figures", "summary": "Optical wireless integrated sensing and communication (OW-ISAC) is rapidly\nburgeoning as a complement and augmentation to its radio-frequency counterpart.\nIn this paper, the channel capacity is analyzed to guide the design of a\ncoherent OW-ISAC system based on frequency-modulated continuous wave (FMCW).\nFirstly, the system model of FMCW-based OW-ISAC is recast into an\ninformation-theoretic formulation, where an additional harmonic-mean constraint\nis imposed to ensure the sensing performance. Subsequently, both lower and\nupper bounds for channel capacity are derived under the imposed sensing\nconstraint, based on which asymptotic expressions for channel capacity are\npresented for both low and high signal-to-noise-ratio regions. Moreover, the\nanalysis of channel capacity provides guidance for the envelope design based on\npulse amplitude modulation, whose capacity-achieving capabilities are\ndemonstrated by numerical results. Furthermore, simulations reveal the\ntrade-off between communication and sensing functionalities. In summary, the\nanalysis of channel capacity under the sensing constraint provides insights\ninto both the optimality and the practicality of OW-ISAC design.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u4e8eFMCW\u7684\u76f8\u5e72\u5149\u65e0\u7ebf\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u4fe1\u9053\u5bb9\u91cf\uff0c\u63a8\u5bfc\u4e86\u611f\u77e5\u7ea6\u675f\u4e0b\u7684\u5bb9\u91cf\u4e0a\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u8109\u51b2\u5e45\u5ea6\u8c03\u5236\u7684\u5305\u7edc\u8bbe\u8ba1\u6307\u5bfc\u3002", "motivation": "\u5149\u65e0\u7ebf\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(OW-ISAC)\u4f5c\u4e3a\u5c04\u9891\u5bf9\u5e94\u6280\u672f\u7684\u8865\u5145\u548c\u589e\u5f3a\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u5206\u6790\u4fe1\u9053\u5bb9\u91cf\u6765\u6307\u5bfc\u7cfb\u7edf\u8bbe\u8ba1\u3002", "method": "\u5c06FMCW\u57faOW-ISAC\u7cfb\u7edf\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\u4e3a\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u65bd\u52a0\u8c10\u6ce2\u5747\u503c\u7ea6\u675f\u786e\u4fdd\u611f\u77e5\u6027\u80fd\uff0c\u63a8\u5bfc\u4fe1\u9053\u5bb9\u91cf\u4e0a\u4e0b\u754c\uff0c\u63d0\u51fa\u8109\u51b2\u5e45\u5ea6\u8c03\u5236\u7684\u5305\u7edc\u8bbe\u8ba1\u3002", "result": "\u83b7\u5f97\u4e86\u4f4e\u4fe1\u566a\u6bd4\u548c\u9ad8\u4fe1\u566a\u6bd4\u533a\u57df\u7684\u6e10\u8fd1\u5bb9\u91cf\u8868\u8fbe\u5f0f\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5bb9\u91cf\u5b9e\u73b0\u80fd\u529b\uff0c\u4eff\u771f\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u611f\u77e5\u529f\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u5728\u611f\u77e5\u7ea6\u675f\u4e0b\u7684\u4fe1\u9053\u5bb9\u91cf\u5206\u6790\u4e3aOW-ISAC\u8bbe\u8ba1\u7684\u6700\u4f18\u6027\u548c\u5b9e\u7528\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.15983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15983", "abs": "https://arxiv.org/abs/2510.15983", "authors": ["Sarah Rebecca Ondraszek", "J\u00f6rg Waitelonis", "Katja Keller", "Claudia Niessner", "Anna M. Jacyszyn", "Harald Sack"], "title": "Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science", "comment": "10 pages, 2 figures. Camera-ready version. Accepted to the 5th\n  International Workshop on Scientific Knowledge: Representation, Discovery,\n  and Assessment; 2 November 2025 - Nara, Japan; co-located with The 24th\n  International Semantic Web Conference, ISWC 2025. To be published in CEUR\n  proceedings", "summary": "An essential component for evaluating and comparing physical and cognitive\ncapabilities between populations is the testing of various factors related to\nhuman performance. As a core part of sports science research, testing motor\nperformance enables the analysis of the physical health of different\ndemographic groups and makes them comparable.\n  The Motor Research (MO|RE) data repository, developed at the Karlsruhe\nInstitute of Technology, is an infrastructure for publishing and archiving\nresearch data in sports science, particularly in the field of motor performance\nresearch. In this paper, we present our vision for creating a knowledge graph\nfrom MO|RE data. With an ontology rooted in the Basic Formal Ontology, our\napproach centers on formally representing the interrelation of plan\nspecifications, specific processes, and related measurements. Our goal is to\ntransform how motor performance data are modeled and shared across studies,\nmaking it standardized and machine-understandable. The idea presented here is\ndeveloped within the Leibniz Science Campus ``Digital Transformation of\nResearch'' (DiTraRe).", "AI": {"tldr": "\u63d0\u51fa\u5c06MO|RE\u8fd0\u52a8\u7814\u7a76\u6570\u636e\u4ed3\u5e93\u8f6c\u6362\u4e3a\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u7684\u65b9\u5f0f\u5efa\u6a21\u548c\u5171\u4eab\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u3002", "motivation": "\u8fd0\u52a8\u8868\u73b0\u6d4b\u8bd5\u662f\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u4eba\u7fa4\u8eab\u4f53\u548c\u8ba4\u77e5\u80fd\u529b\u7684\u5173\u952e\uff0c\u9700\u8981\u6807\u51c6\u5316\u7684\u6570\u636e\u5efa\u6a21\u548c\u5171\u4eab\u65b9\u6cd5\u6765\u652f\u6301\u8de8\u7814\u7a76\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53(BFO)\u7684\u672c\u4f53\uff0c\u6b63\u5f0f\u8868\u793a\u8ba1\u5212\u89c4\u8303\u3001\u5177\u4f53\u8fc7\u7a0b\u548c\u76f8\u5173\u6d4b\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5c06MO|RE\u6570\u636e\u4ed3\u5e93\u8f6c\u6362\u4e3a\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u8fd0\u52a8\u8868\u73b0\u7814\u7a76\u6570\u636e\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u4f7f\u6570\u636e\u80fd\u591f\u8de8\u7814\u7a76\u6807\u51c6\u5316\u548c\u673a\u5668\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u53ef\u4ee5\u6539\u53d8\u8fd0\u52a8\u8868\u73b0\u6570\u636e\u7684\u5efa\u6a21\u548c\u5171\u4eab\u65b9\u5f0f\uff0c\u5b9e\u73b0\u6570\u636e\u7684\u6807\u51c6\u5316\u548c\u673a\u5668\u53ef\u7406\u89e3\u6027\uff0c\u4fc3\u8fdb\u8de8\u7814\u7a76\u6bd4\u8f83\u3002"}}
{"id": "2510.17647", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.17647", "abs": "https://arxiv.org/abs/2510.17647", "authors": ["P. Brach del Prever", "P. Testolina", "A. Masihi", "S. Petrushkevich", "M. Polese", "T. Melodia", "J. M. Jornet"], "title": "Pointing-Error-Induced Fading in an Open-Loop THz Uplink with Hardware Impairments", "comment": "To be published in: Proceedings of 2nd IEEE International Workshop on\n  Terahertz Communications, Sensing, and Security at IEEE Military\n  Communications Conference (MILCOM) 2025. 6 pages, 6 figures", "summary": "We analyze the open-loop mechanical tracking performance of a sub-Terahertz\n(sub-THz) and Terahertz (THz) uplink communication system. These high-frequency\nbands enable multi-gigabit links through large bandwidths and narrow beams, but\nrequire precise pointing to overcome spreading loss. A tracking system can be\nused to orient horn antennas toward mobile targets. We develop a mathematical\nmodel that captures the mechanical dynamics of a real tracking system, which\nincludes motion latency and acceleration and velocity limits, to quantify\npointing errors during satellite passes and integrate these effects into the\nlink budget. We evaluate the trade-offs between beam directionality and\npointing tolerance across different Low Earth Orbit (LEO) satellite\ntrajectories and control strategies. The results link the hardware limitations\nto the communications performance, providing design guidelines for\nhigh-frequency Non-Terrestrial Network (NTN) uplink under practical mechanical\nconstraints.", "AI": {"tldr": "\u5206\u6790\u4e9a\u592a\u8d6b\u5179\u548c\u592a\u8d6b\u5179\u4e0a\u884c\u94fe\u8def\u901a\u4fe1\u7cfb\u7edf\u7684\u5f00\u73af\u673a\u68b0\u8ddf\u8e2a\u6027\u80fd\uff0c\u91cf\u5316\u536b\u661f\u8fc7\u5883\u65f6\u7684\u6307\u5411\u8bef\u5dee\uff0c\u5e76\u5c06\u673a\u68b0\u7ea6\u675f\u4e0e\u901a\u4fe1\u6027\u80fd\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u9ad8\u9891\u6bb5\u901a\u4fe1\u9700\u8981\u7cbe\u786e\u6307\u5411\u6765\u514b\u670d\u4f20\u64ad\u635f\u8017\uff0c\u4f46\u5b9e\u9645\u8ddf\u8e2a\u7cfb\u7edf\u5b58\u5728\u673a\u68b0\u52a8\u6001\u9650\u5236\uff0c\u9700\u8981\u91cf\u5316\u8fd9\u4e9b\u9650\u5236\u5bf9\u901a\u4fe1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u5305\u542b\u8fd0\u52a8\u5ef6\u8fdf\u3001\u52a0\u901f\u5ea6\u548c\u901f\u5ea6\u9650\u5236\u7684\u673a\u68b0\u52a8\u6001\u6570\u5b66\u6a21\u578b\uff0c\u5c06\u6307\u5411\u8bef\u5dee\u96c6\u6210\u5230\u94fe\u8def\u9884\u7b97\u4e2d\uff0c\u8bc4\u4f30\u4e0d\u540cLEO\u536b\u661f\u8f68\u8ff9\u548c\u63a7\u5236\u7b56\u7565\u4e0b\u7684\u6743\u8861\u3002", "result": "\u5efa\u7acb\u4e86\u786c\u4ef6\u9650\u5236\u4e0e\u901a\u4fe1\u6027\u80fd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u91cf\u5316\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6307\u5411\u8bef\u5dee\u5bf9\u94fe\u8def\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u5b9e\u9645\u673a\u68b0\u7ea6\u675f\u4e0b\u7684\u9ad8\u9891\u975e\u5730\u9762\u7f51\u7edc\u4e0a\u884c\u94fe\u8def\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2510.17466", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17466", "abs": "https://arxiv.org/abs/2510.17466", "authors": ["Fathima Jesbin", "Ananthanarayanan Chockalingam"], "title": "Delay-Doppler Pulse Shaping in Zak-OTFS Using Hermite Basis Functions", "comment": "Submitted to IEEE journal for possible publication", "summary": "The performance of Zak-OTFS modulation is critically dependent on the choice\nof the delay-Doppler (DD) domain pulse shaping filter. The design of pulses for\n$L^2(\\mathbb{R})$ is constrained by the Balian-Low Theorem, which imposes an\ninescapable trade-off between time-frequency localization and orthogonality for\nspectrally efficient systems. In Zak-OTFS, this trade-off requires balancing\nthe need for localization for input/output (I/O) relation estimation with the\nneed for orthogonality for reliable data detection when operating without time\nor bandwidth expansion. The well-known sinc and Gaussian pulse shapes represent\nthe canonical extremes of this trade-off, while composite constructions such as\nthe Gaussian-sinc (GS) pulse shape offer a good compromise. In this work, we\npropose a systematic DD pulse design framework for Zak-OTFS that expresses the\npulse as a linear combination of Hermite basis functions. We obtain the optimal\ncoefficients for the Hermite basis functions that minimize the inter-symbol\ninterference (ISI) energy at the DD sampling points by solving a constrained\noptimization problem via singular value decomposition. For the proposed class\nof Hermite pulses, we derive closed-form expressions for the I/O relation and\nnoise covariance in Zak-OTFS. Simulation results of Zak-OTFS with embedded\npilot and model-free I/O relation estimation in Vehicular-A channels with\nfractional DDs demonstrate that the optimized pulse shape achieves a bit error\nrate performance that is significantly superior compared to those of the\ncanonical sinc and Gaussian pulses and is on par with that of the\nstate-of-the-art GS pulse, validating the proposed framework which provides\ngreater design flexibility in terms of control of ISI and sidelobe energies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHermite\u57fa\u51fd\u6570\u7684\u7cfb\u7edf\u5316DD\u8109\u51b2\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u6700\u5c0f\u5316\u7b26\u53f7\u95f4\u5e72\u6270\u80fd\u91cf\uff0c\u5728Zak-OTFS\u8c03\u5236\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edfsinc\u548c\u9ad8\u65af\u8109\u51b2\u7684\u6027\u80fd\u3002", "motivation": "Zak-OTFS\u8c03\u5236\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8eDD\u57df\u8109\u51b2\u6210\u5f62\u6ee4\u6ce2\u5668\u7684\u9009\u62e9\uff0c\u9700\u8981\u5728\u65f6\u95f4-\u9891\u7387\u5b9a\u4f4d\u548c\u6b63\u4ea4\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u4f20\u7edfsinc\u548c\u9ad8\u65af\u8109\u51b2\u4ee3\u8868\u4e86\u8fd9\u79cd\u6743\u8861\u7684\u4e24\u4e2a\u6781\u7aef\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u8109\u51b2\u8868\u793a\u4e3aHermite\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u6c42\u89e3\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u83b7\u5f97\u6700\u5c0f\u5316\u7b26\u53f7\u95f4\u5e72\u6270\u80fd\u91cf\u7684\u6700\u4f18\u7cfb\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u8109\u51b2\u5728Vehicular-A\u4fe1\u9053\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfsinc\u548c\u9ad8\u65af\u8109\u51b2\uff0c\u4e0e\u6700\u5148\u8fdb\u7684GS\u8109\u51b2\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684ISI\u548c\u65c1\u74e3\u80fd\u91cf\u63a7\u5236\u7075\u6d3b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684Hermite\u8109\u51b2\u8bbe\u8ba1\u6846\u67b6\u4e3aZak-OTFS\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8109\u51b2\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u8bbe\u8ba1\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.16001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16001", "abs": "https://arxiv.org/abs/2510.16001", "authors": ["Ruolan Cheng", "Yong Deng", "Enrique Herrera-Viedma"], "title": "A Non-overlap-based Conflict Measure for Random Permutation Sets", "comment": null, "summary": "Random permutation set (RPS) is a new formalism for reasoning with\nuncertainty involving order information. Measuring the conflict between two\npieces of evidence represented by permutation mass functions remains an urgent\nresearch topic in order-structured uncertain information fusion. In this paper,\na detailed analysis of conflicts in RPS is carried out from two different\nperspectives: random finite set (RFS) and Dempster-Shafer theory (DST).\nStarting from the observation of permutations, we first define an inconsistency\nmeasure between permutations inspired by the rank-biased overlap(RBO) measure\nand further propose a non-overlap-based conflict measure method for RPSs. This\npaper regards RPS theory (RPST) as an extension of DST. The order information\nnewly added in focal sets indicates qualitative propensity, characterized by\ntop-ranked elements occupying a more critical position. Some numerical examples\nare used to demonstrate the behavior and properties of the proposed conflict\nmeasure. The proposed method not only has the natural top-weightedness property\nand can effectively measure the conflict between RPSs from the DST view but\nalso provides decision-makers with a flexible selection of weights, parameters,\nand truncated depths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u7f6e\u6362\u96c6(RPS)\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ece\u968f\u673a\u6709\u9650\u96c6(RFS)\u548cDempster-Shafer\u7406\u8bba(DST)\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790RPS\u4e2d\u7684\u51b2\u7a81\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0(RBO)\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\u3002", "motivation": "\u5728\u6d89\u53ca\u987a\u5e8f\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u4e2d\uff0c\u5982\u4f55\u5ea6\u91cf\u7531\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u8868\u793a\u7684\u4e24\u4e2a\u8bc1\u636e\u4e4b\u95f4\u7684\u51b2\u7a81\u662f\u4e00\u4e2a\u7d27\u8feb\u7684\u7814\u7a76\u8bfe\u9898\uff0c\u9700\u8981\u4e3a\u6709\u5e8f\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u6709\u6548\u7684\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u4ece\u7f6e\u6362\u89c2\u5bdf\u51fa\u53d1\uff0c\u9996\u5148\u57fa\u4e8e\u79e9\u504f\u91cd\u53e0(RBO)\u5ea6\u91cf\u5b9a\u4e49\u7f6e\u6362\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u975e\u91cd\u53e0\u7684RPS\u51b2\u7a81\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5c06RPS\u7406\u8bba\u89c6\u4e3aDST\u7684\u6269\u5c55\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u6240\u63d0\u51b2\u7a81\u5ea6\u91cf\u7684\u884c\u4e3a\u548c\u7279\u6027\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u81ea\u7136\u7684\u9876\u90e8\u52a0\u6743\u7279\u6027\uff0c\u8fd8\u80fd\u4eceDST\u89d2\u5ea6\u6709\u6548\u5ea6\u91cfRPS\u95f4\u7684\u51b2\u7a81\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u6743\u91cd\u3001\u53c2\u6570\u548c\u622a\u65ad\u6df1\u5ea6\u7684\u7075\u6d3b\u9009\u62e9\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6709\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u4e2d\u7684\u51b2\u7a81\u5ea6\u91cf\u95ee\u9898\u3002"}}
{"id": "2510.17544", "categories": ["cs.IT", "cs.FL", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17544", "abs": "https://arxiv.org/abs/2510.17544", "authors": ["Neil Lutz"], "title": "Multihead Finite-State Compression", "comment": null, "summary": "This paper develops multihead finite-state compression, a generalization of\nfinite-state compression, complementary to the multihead finite-state\ndimensions of Huang, Li, Lutz, and Lutz (2025). In this model, an infinite\nsequence of symbols is compressed by a compressor that produces outputs\naccording to finite-state rules, based on the symbols read by a constant number\nof finite-state read heads moving forward obliviously through the sequence. The\nmain theorem of this work establishes that for every sequence and every\npositive integer $h$, the infimum of the compression ratios achieved by\n$h$-head finite-state information-lossless compressors equals the $h$-head\nfinite-state predimension of the sequence. As an immediate corollary, the\ninfimum of these ratios over all $h$ is the multihead finite-state dimension of\nthe sequence.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u591a\u5934\u6709\u9650\u72b6\u6001\u538b\u7f29\u6a21\u578b\uff0c\u4f5c\u4e3a\u6709\u9650\u72b6\u6001\u538b\u7f29\u7684\u63a8\u5e7f\uff0c\u4e0eHuang\u7b49\u4eba(2025)\u7684\u591a\u5934\u6709\u9650\u72b6\u6001\u7ef4\u5ea6\u4e92\u8865\u3002\u4e3b\u8981\u5b9a\u7406\u8bc1\u660e\u5bf9\u4e8e\u4efb\u4f55\u5e8f\u5217\u548c\u6b63\u6574\u6570h\uff0ch\u5934\u6709\u9650\u72b6\u6001\u65e0\u635f\u538b\u7f29\u5668\u8fbe\u5230\u7684\u538b\u7f29\u6bd4\u7387\u4e0b\u786e\u754c\u7b49\u4e8e\u8be5\u5e8f\u5217\u7684h\u5934\u6709\u9650\u72b6\u6001\u9884\u7ef4\u5ea6\u3002", "motivation": "\u63a8\u5e7f\u4f20\u7edf\u7684\u6709\u9650\u72b6\u6001\u538b\u7f29\u6a21\u578b\uff0c\u5f15\u5165\u591a\u4e2a\u8bfb\u5934\u7684\u6982\u5ff5\uff0c\u7814\u7a76\u5728\u591a\u4e2a\u6709\u9650\u72b6\u6001\u8bfb\u5934\u540c\u65f6\u626b\u63cf\u5e8f\u5217\u7684\u60c5\u51b5\u4e0b\u80fd\u8fbe\u5230\u7684\u6700\u4f18\u538b\u7f29\u6bd4\u7387\u3002", "method": "\u4f7f\u7528\u591a\u5934\u6709\u9650\u72b6\u6001\u538b\u7f29\u5668\u6a21\u578b\uff0c\u5176\u4e2d\u538b\u7f29\u5668\u6839\u636e\u56fa\u5b9a\u6570\u91cf\u7684\u6709\u9650\u72b6\u6001\u8bfb\u5934\u5411\u524d\u626b\u63cf\u5e8f\u5217\u65f6\u8bfb\u53d6\u7684\u7b26\u53f7\uff0c\u6309\u7167\u6709\u9650\u72b6\u6001\u89c4\u5219\u4ea7\u751f\u8f93\u51fa\u3002", "result": "\u4e3b\u8981\u5b9a\u7406\u8bc1\u660e\uff1a\u5bf9\u4e8e\u4efb\u4f55\u5e8f\u5217\u548c\u6b63\u6574\u6570h\uff0ch\u5934\u6709\u9650\u72b6\u6001\u65e0\u635f\u538b\u7f29\u5668\u8fbe\u5230\u7684\u538b\u7f29\u6bd4\u7387\u4e0b\u786e\u754c\u7b49\u4e8e\u8be5\u5e8f\u5217\u7684h\u5934\u6709\u9650\u72b6\u6001\u9884\u7ef4\u5ea6\u3002", "conclusion": "\u591a\u5934\u6709\u9650\u72b6\u6001\u538b\u7f29\u6bd4\u7387\u7684\u4e0b\u786e\u754c\u4e0e\u591a\u5934\u6709\u9650\u72b6\u6001\u7ef4\u5ea6\u7406\u8bba\u4e2d\u7684\u9884\u7ef4\u5ea6\u6982\u5ff5\u5b8c\u5168\u5bf9\u5e94\uff0c\u4e3a\u538b\u7f29\u7406\u8bba\u4e0e\u7ef4\u5ea6\u7406\u8bba\u5efa\u7acb\u4e86\u91cd\u8981\u8054\u7cfb\u3002"}}
{"id": "2510.16004", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "AI": {"tldr": "PAINT\u662f\u4e00\u79cd\u7528\u4e8e\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\u7684\u5e76\u884c\u65f6\u95f4\u795e\u7ecf\u5b6a\u751f\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u5e76\u884c\u5efa\u6a21\u72b6\u6001\u5206\u5e03\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u51c6\u786e\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u5e76\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u4ee3\u7406\u5728\u6a21\u62df\u52a8\u6001\u7cfb\u7edf\u65f6\u5177\u6709\u5b9e\u65f6\u80fd\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u795e\u7ecf\u5b6a\u751f\u6765\u521b\u5efa\u771f\u5b9e\u7cfb\u7edf\u7684\u6570\u5b57\u526f\u672c\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "method": "PAINT\u8bad\u7ec3\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u5e76\u884c\u5efa\u6a21\u65f6\u95f4\u4e0a\u7684\u72b6\u6001\u5206\u5e03\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u4ece\u6d4b\u91cf\u4e2d\u9884\u6d4b\u72b6\u6001\u3002", "result": "\u5728\u4e8c\u7ef4\u6e4d\u6d41\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\uff0cPAINT\u80fd\u591f\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u9ad8\u4fdd\u771f\u5730\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u3002", "conclusion": "PAINT\u5177\u6709\u5f00\u53d1\u4fdd\u6301\u8f68\u8ff9\u8ddf\u8e2a\u7684\u795e\u7ecf\u5b6a\u751f\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2510.17613", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17613", "abs": "https://arxiv.org/abs/2510.17613", "authors": ["MohammadHossein Alishahi", "Ming Zeng", "Paul Fortier", "Ji Wang", "Nian Xia", "Gongpu Wang"], "title": "Mode Switching-based STAR-RIS with Discrete Phase Shifters", "comment": "accepted by IEEE WCL", "summary": "The increasing demand for cost-effective, high-speed Internet of Things (IoT)\napplications in the coming sixth-generation (6G) networks has driven research\ntoward maximizing spectral efficiency and simplifying hardware designs. In this\ncontext, we investigate the sum rate maximization problem for a mode-switching\ndiscrete-phase shifters simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS)-aided multi-antenna access point\nnetwork, emphasizing hardware efficiency and reduced cost. A mixed-integer\nnonlinear optimization framework is formulated for joint optimization of the\nactive beamforming matrix, user power allocation, and STAR-RIS phase shift\nvectors, including binary transmission/reflection amplitudes and discrete phase\nshifters. To solve the formulated problem, we employ a block coordinate descent\nmethod, dividing it into three subproblems tackled using difference-of-concave\nprogramming and combinatorial optimization techniques. Numerical results\nvalidate the effectiveness of the proposed joint optimization approach,\nconsistently achieving superior sum rate performance compared to partial\noptimization methods, thereby underscoring its potential for efficient and\nscalable 6G IoT systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e6G\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u6a21\u5f0f\u5207\u6362\u79bb\u6563\u76f8\u79fbSTAR-RIS\u8f85\u52a9\u591a\u5929\u7ebf\u63a5\u5165\u70b9\u7f51\u7edc\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u4f18\u5316\u6846\u67b6\u6700\u5927\u5316\u7cfb\u7edf\u548c\u901f\u7387\u3002", "motivation": "6G\u7f51\u7edc\u5bf9\u6210\u672c\u6548\u76ca\u9ad8\u3001\u9ad8\u901f\u7269\u8054\u7f51\u5e94\u7528\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u5e76\u7b80\u5316\u786c\u4ef6\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5757\u5750\u6807\u4e0b\u964d\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u4f7f\u7528\u5dee\u5206\u51f9\u89c4\u5212\u548c\u7ec4\u5408\u4f18\u5316\u6280\u672f\u6c42\u89e3\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u3001\u7528\u6237\u529f\u7387\u5206\u914d\u548cSTAR-RIS\u76f8\u79fb\u5411\u91cf\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u90e8\u5206\u4f18\u5316\u65b9\u6cd5\u59cb\u7ec8\u83b7\u5f97\u66f4\u4f18\u7684\u548c\u901f\u7387\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u9ad8\u6548\u548c\u53ef\u6269\u5c556G\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16033", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16033", "abs": "https://arxiv.org/abs/2510.16033", "authors": ["Junyu Ren", "Wensheng Gan", "Guangyu Zhang", "Wei Zhong", "Philip S. Yu"], "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis", "comment": "Preprint. 16 figures, 12 tables", "summary": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN", "AI": {"tldr": "\u63d0\u51faISGFAN\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u5206\u79bb\u548c\u5168\u5c40-\u5c40\u90e8\u5bf9\u6297\u5b66\u4e60\u89e3\u51b3\u566a\u58f0\u5e72\u6270\u548c\u57df\u504f\u79fb\u5171\u5b58\u4e0b\u7684\u8de8\u57df\u6545\u969c\u8bca\u65ad\u95ee\u9898", "motivation": "\u73b0\u6709\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5e72\u51c0\u6216\u57df\u76f8\u4f3c\u6027\u8db3\u591f\uff0c\u4f46\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4e25\u91cd\u566a\u58f0\u5e72\u6270\u548c\u57df\u504f\u79fb\u540c\u65f6\u5b58\u5728\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "method": "\u57fa\u4e8e\u4fe1\u606f\u5206\u79bb\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6297\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6b63\u4ea4\u635f\u5931\u6765\u89e3\u8026\u57df\u4e0d\u53d8\u6545\u969c\u8868\u793a\uff1b\u91c7\u7528\u5168\u5c40-\u5c40\u90e8\u57df\u5bf9\u6297\u65b9\u6848\u7ea6\u675f\u6a21\u578b\u7684\u8fb9\u7f18\u5206\u5e03\u548c\u6761\u4ef6\u5206\u5e03", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5", "conclusion": "ISGFAN\u6846\u67b6\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u8de8\u57df\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027"}}
{"id": "2510.17625", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17625", "abs": "https://arxiv.org/abs/2510.17625", "authors": ["Jaehyup Seong", "Byungju Lee", "Aryan Kaushik", "Wonjae Shin"], "title": "Space-Time Rate-Splitting Multiple Access for Multibeam LEO Satellite Networks", "comment": "17 pages, 3 figures, accepted for publication in IEEE Transactions on\n  Vehicular Technology", "summary": "This paper proposes a novel space-time rate-splitting multiple access\n(ST-RSMA) framework for multibeam low Earth orbit (LEO) satellite\ncommunications (SATCOM) systems, where space-time coding is integrated into the\ncommon stream transmission. This design enables full diversity gain in the\ncommon stream transmission for all users, regardless of the uncertainty of the\nchannel state information (CSI) and network load conditions, thereby overcoming\nthe performance limitations of conventional RSMA that employs a single\nbeamforming vector for all users. To further enhance performance, we develop a\nweighted minimum mean square error (WMMSE)-based algorithm tailored to ST-RSMA\nthat jointly optimizes the power allocation for the common stream and the\npower/beamforming vectors for private streams, aiming to maximize the minimum\nuser rate. Numerical results show that ST-RSMA significantly outperforms\nconventional RSMA and other multiple access techniques, offering a robust and\nscalable solution for LEO SATCOM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u901f\u7387\u5206\u5272\u591a\u5740(ST-RSMA)\u6846\u67b6\uff0c\u5c06\u7a7a\u65f6\u7f16\u7801\u96c6\u6210\u5230\u516c\u5171\u6d41\u4f20\u8f93\u4e2d\uff0c\u4e3a\u591a\u6ce2\u675f\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u5168\u5206\u96c6\u589e\u76ca\u3002", "motivation": "\u514b\u670d\u4f20\u7edfRSMA\u4f7f\u7528\u5355\u4e00\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u5e26\u6765\u7684\u6027\u80fd\u9650\u5236\uff0c\u89e3\u51b3\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\u548c\u7f51\u7edc\u8d1f\u8f7d\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee(WMMSE)\u7684\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u516c\u5171\u6d41\u7684\u529f\u7387\u5206\u914d\u548c\u79c1\u6709\u6d41\u7684\u529f\u7387/\u6ce2\u675f\u6210\u5f62\u5411\u91cf\uff0c\u4ee5\u6700\u5927\u5316\u6700\u5c0f\u7528\u6237\u901f\u7387\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660eST-RSMA\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRSMA\u548c\u5176\u4ed6\u591a\u5740\u6280\u672f\uff0c\u4e3aLEO\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ST-RSMA\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u7a7a\u65f6\u7f16\u7801\u5b9e\u73b0\u4e86\u5168\u5206\u96c6\u589e\u76ca\uff0c\u5728\u4e0d\u786e\u5b9a\u4fe1\u9053\u6761\u4ef6\u548c\u7f51\u7edc\u8d1f\u8f7d\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u662fLEO\u536b\u661f\u901a\u4fe1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16047", "abs": "https://arxiv.org/abs/2510.16047", "authors": ["Ioan Hedea"], "title": "Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks", "comment": "8 pages 2 column, 11 figures. Bachelor's thesis", "summary": "Modern manufacturing systems must meet hard delivery deadlines while coping\nwith stochastic task durations caused by process noise, equipment variability,\nand human intervention. Traditional deterministic schedules break down when\nreality deviates from nominal plans, triggering costly last-minute repairs.\nThis thesis combines offline constraint-programming (CP) optimisation with\nonline temporal-network execution to create schedules that remain feasible\nunder worst-case uncertainty. First, we build a CP model of the flexible\njob-shop with per-job deadline tasks and insert an optimal buffer $\\Delta^*$ to\nobtain a fully pro-active baseline. We then translate the resulting plan into a\nSimple Temporal Network with Uncertainty (STNU) and verify dynamic\ncontrollability, which guarantees that a real-time dispatcher can retime\nactivities for every bounded duration realisation without violating resource or\ndeadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4\nbenchmark suite show that our hybrid approach eliminates 100\\% of deadline\nviolations observed in state-of-the-art meta-heuristic schedules, while adding\nonly 3--5\\% makespan overhead. Scalability experiments confirm that CP\nsolve-times and STNU checks remain sub-second on medium-size instances. The\nwork demonstrates how temporal-network reasoning can bridge the gap between\nproactive buffering and dynamic robustness, moving industry a step closer to\ntruly digital, self-correcting factories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u7ea6\u675f\u7f16\u7a0b\u4f18\u5316\u548c\u5728\u7ebf\u65f6\u95f4\u7f51\u7edc\u6267\u884c\u7684\u6df7\u5408\u8c03\u5ea6\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6700\u574f\u60c5\u51b5\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u8c03\u5ea6\u53ef\u884c\u6027\uff0c\u5b8c\u5168\u6d88\u9664\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u540c\u65f6\u4ec5\u589e\u52a03-5%\u7684\u5236\u9020\u5468\u671f\u5f00\u9500\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u4ea4\u4ed8\u622a\u6b62\u65f6\u95f4\uff0c\u540c\u65f6\u5e94\u5bf9\u7531\u8fc7\u7a0b\u566a\u58f0\u3001\u8bbe\u5907\u53d8\u5f02\u6027\u548c\u4eba\u4e3a\u5e72\u9884\u5f15\u8d77\u7684\u968f\u673a\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3002\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u8c03\u5ea6\u5728\u73b0\u5b9e\u504f\u79bb\u540d\u4e49\u8ba1\u5212\u65f6\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u6700\u540e\u4e00\u5206\u949f\u4fee\u590d\u3002", "method": "\u9996\u5148\u6784\u5efa\u5177\u6709\u6bcf\u4efb\u52a1\u622a\u6b62\u65f6\u95f4\u7684\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u7ea6\u675f\u7f16\u7a0b\u6a21\u578b\uff0c\u5e76\u63d2\u5165\u6700\u4f18\u7f13\u51b2\u533a\u0394*\u83b7\u5f97\u5b8c\u5168\u4e3b\u52a8\u57fa\u7ebf\u3002\u7136\u540e\u5c06\u7ed3\u679c\u8ba1\u5212\u8f6c\u6362\u4e3a\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7b80\u5355\u65f6\u95f4\u7f51\u7edc\uff0c\u9a8c\u8bc1\u52a8\u6001\u53ef\u63a7\u6027\uff0c\u786e\u4fdd\u5b9e\u65f6\u8c03\u5ea6\u5668\u80fd\u591f\u4e3a\u6bcf\u4e2a\u6709\u754c\u6301\u7eed\u65f6\u95f4\u5b9e\u73b0\u91cd\u65b0\u5b89\u6392\u6d3b\u52a8\u65f6\u95f4\uff0c\u800c\u4e0d\u8fdd\u53cd\u8d44\u6e90\u6216\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u3002", "result": "\u5728Kacem 1-4\u57fa\u51c6\u5957\u4ef6\u4e0a\u7684\u5e7f\u6cdb\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u6700\u5148\u8fdb\u5143\u542f\u53d1\u5f0f\u8c03\u5ea6\u4e2d\u89c2\u5bdf\u5230\u7684100%\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\uff0c\u540c\u65f6\u4ec5\u589e\u52a03-5%\u7684\u5236\u9020\u5468\u671f\u5f00\u9500\u3002\u53ef\u6269\u5c55\u6027\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u4e2d\u7b49\u89c4\u6a21\u5b9e\u4f8b\u4e0a\uff0cCP\u6c42\u89e3\u65f6\u95f4\u548cSTNU\u68c0\u67e5\u4fdd\u6301\u5728\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u65f6\u95f4\u7f51\u7edc\u63a8\u7406\u5982\u4f55\u5f25\u5408\u4e3b\u52a8\u7f13\u51b2\u548c\u52a8\u6001\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u5de5\u4e1a\u5411\u771f\u6b63\u7684\u6570\u5b57\u5316\u3001\u81ea\u6821\u6b63\u5de5\u5382\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.17781", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.17781", "abs": "https://arxiv.org/abs/2510.17781", "authors": ["Hua Sun", "Syed A. Jafar"], "title": "On the Capacity of Erasure-prone Quantum Storage with Erasure-prone Entanglement Assistance", "comment": null, "summary": "A quantum message is encoded into $N$ storage nodes (quantum systems\n$Q_1\\dots Q_N$) with assistance from $N_B$ maximally entangled bi-partite\nquantum systems $A_1B_1, \\dots, A_{N_B}B_{N_B}$, that are prepared in advance\nsuch that $B_1\\dots B_{N_B}$ are stored separately as entanglement assistance\n(EA) nodes, while $A_1\\dots A_{N_B}$ are made available to the encoder. Both\nthe storage nodes and EA nodes are erasure-prone. The quantum message must be\nrecoverable given any $K$ of the $N$ storage nodes along with any $K_B$ of the\n$N_B$ EA nodes. The capacity for this setting is the maximum size of the\nquantum message, given that the size of each EA node is $\\lambda_B$. All node\nsizes are relative to the size of a storage node, which is normalized to unity.\nThe exact capacity is characterized as a function of $N,K,N_B,K_B, \\lambda_B$\nin all cases, with one exception. The capacity remains open for an intermediate\nrange of $\\lambda_B$ values when a strict majority of the $N$ storage nodes,\nand a strict non-zero minority of the $N_B$ EA nodes, are erased. As a key\nstepping stone, an analogous classical storage (with shared-randomness\nassistance) problem is introduced. A set of constraints is identified for the\nclassical problem, such that classical linear code constructions translate to\nquantum storage codes, and the converse bounds for the two settings utilize\nsimilar insights. In particular, the capacity characterizations for the\nclassical and quantum settings are shown to be identical in all cases where the\ncapacity is settled.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u5b58\u50a8\u7f16\u7801\u7684\u5bb9\u91cf\u95ee\u9898\uff0c\u5176\u4e2d\u91cf\u5b50\u6d88\u606f\u88ab\u7f16\u7801\u5230N\u4e2a\u5b58\u50a8\u8282\u70b9\u548cNB\u4e2a\u7ea0\u7f20\u8f85\u52a9\u8282\u70b9\u4e2d\uff0c\u8981\u6c42\u4ece\u4efb\u610fK\u4e2a\u5b58\u50a8\u8282\u70b9\u548c\u4efb\u610fKB\u4e2a\u7ea0\u7f20\u8f85\u52a9\u8282\u70b9\u4e2d\u6062\u590d\u91cf\u5b50\u6d88\u606f\u3002", "motivation": "\u7814\u7a76\u5728\u5b58\u50a8\u8282\u70b9\u548c\u7ea0\u7f20\u8f85\u52a9\u8282\u70b9\u90fd\u53ef\u80fd\u53d1\u751f\u64e6\u9664\u7684\u60c5\u51b5\u4e0b\uff0c\u91cf\u5b50\u6d88\u606f\u7684\u5b58\u50a8\u5bb9\u91cf\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u5b58\u50a8\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f15\u5165\u4e86\u7c7b\u4f3c\u7684\u7ecf\u5178\u5b58\u50a8\u95ee\u9898\uff0c\u8bc6\u522b\u51fa\u4e00\u7ec4\u7ea6\u675f\u6761\u4ef6\uff0c\u4f7f\u5f97\u7ecf\u5178\u7ebf\u6027\u7801\u6784\u9020\u53ef\u4ee5\u8f6c\u5316\u4e3a\u91cf\u5b50\u5b58\u50a8\u7801\uff0c\u5e76\u5bf9\u4e24\u79cd\u8bbe\u7f6e\u4f7f\u7528\u76f8\u4f3c\u7684\u63a8\u7406\u8fdb\u884c\u9006\u5411\u754c\u9650\u5206\u6790\u3002", "result": "\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u7cbe\u786e\u523b\u753b\u4e86\u5bb9\u91cf\u4f5c\u4e3aN,K,NB,KB,\u03bbB\u7684\u51fd\u6570\uff0c\u4f46\u5728\u4e00\u4e2a\u4e2d\u95f4\u8303\u56f4\u7684\u03bbB\u503c\u4e0b\u5bb9\u91cf\u4ecd\u672a\u89e3\u51b3\u3002", "conclusion": "\u7ecf\u5178\u548c\u91cf\u5b50\u8bbe\u7f6e\u7684\u5bb9\u91cf\u8868\u5f81\u5728\u6240\u6709\u5bb9\u91cf\u5df2\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u88ab\u8bc1\u660e\u662f\u76f8\u540c\u7684\uff0c\u4e3a\u91cf\u5b50\u5b58\u50a8\u7f16\u7801\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\"\u9ec4\u91d1\u6807\u51c6\u6df1\u5ea6\"\u548c\"\u4ee3\u8868\u6027\u591a\u6837\u6027\"\u7684\u53cc\u539f\u5219\u6846\u67b6\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\uff0c\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe\u5bf9\u4e8e\u53ef\u89e3\u91ca\u533b\u7597AI\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLM\u751f\u6210\u7684\u4e34\u5e8a\u6570\u636e\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u5728\u8f85\u52a9\u751f\u6b96\u6280\u672f\u9886\u57df\u8fdb\u884c\u76f2\u6cd5\u6bd4\u8f83\u7814\u7a76\uff0c\u8d44\u6df1\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e09\u79cd\u7b56\u7565\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff1a\u96f6\u6837\u672c\u3001\u968f\u673a\u5c11\u6837\u672c\u548c\u9009\u62e9\u6027\u5c11\u6837\u672c\uff0c\u5e76\u4e0eGPT-4o\u7684\u8bc4\u4f30\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff0c\u800c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u65e0\u663e\u8457\u6539\u8fdb\u3002AI\u8bc4\u4f30\u5668\u672a\u80fd\u8bc6\u522b\u8fd9\u4e9b\u5173\u952e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5408\u6210\u601d\u7ef4\u94fe\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7b56\u7565\u6027\u63d0\u793a\u8bcd\u8bbe\u8ba1\u800c\u975e\u793a\u4f8b\u6570\u91cf\uff0c\u63d0\u51fa\u4e86\"\u53cc\u539f\u5219\"\u6846\u67b6\u4f5c\u4e3a\u751f\u6210\u53ef\u4fe1\u6570\u636e\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u786e\u8ba4\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u9ad8\u98ce\u9669\u4e34\u5e8aAI\u8bc4\u4f30\u4e2d\u7684\u4e0d\u53ef\u6216\u7f3a\u4f5c\u7528\u3002"}}
{"id": "2510.16193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16193", "abs": "https://arxiv.org/abs/2510.16193", "authors": ["Elija Perrier"], "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability", "comment": "Under review", "summary": "Corporate responsibility turns on notions of corporate \\textit{mens rea},\ntraditionally imputed from human agents. Yet these assumptions are under\nchallenge as generative AI increasingly mediates enterprise decision-making.\nBuilding on the theory of extended cognition, we argue that in response\ncorporate knowledge may be redefined as a dynamic capability, measurable by the\nefficiency of its information-access procedures and the validated reliability\nof their outputs. We develop a formal model that captures epistemic states of\ncorporations deploying sophisticated AI or information systems, introducing a\ncontinuous organisational knowledge metric $S_S(\\varphi)$ which integrates a\npipeline's computational cost and its statistically validated error rate. We\nderive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and\na firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall\ncapability. We then operationally map these quantitative metrics onto the legal\nstandards of actual knowledge, constructive knowledge, wilful blindness, and\nrecklessness. Our work provides a pathway towards creating measurable and\njusticiable audit artefacts, that render the corporate mind tractable and\naccountable in the algorithmic age.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u8ba4\u77e5\u7406\u8bba\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c06\u4f01\u4e1a\u77e5\u8bc6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u7684\u52a8\u6001\u80fd\u529b\uff0c\u901a\u8fc7\u4fe1\u606f\u8bbf\u95ee\u7a0b\u5e8f\u7684\u6548\u7387\u548c\u8f93\u51fa\u53ef\u9760\u6027\u6765\u91cf\u5316\u4f01\u4e1a\u77e5\u8bc6\u72b6\u6001\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u4e2d\u4ecb\u4f5c\u7528\u65e5\u76ca\u589e\u5f3a\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u4eba\u7c7b\u4ee3\u7406\u7684\u4f01\u4e1a\u72af\u7f6a\u610f\u56fe\u5f52\u8d23\u5047\u8bbe\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u4f01\u4e1a\u77e5\u8bc6\u7684\u6982\u5ff5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5f15\u5165\u8fde\u7eed\u7684\u7ec4\u7ec7\u77e5\u8bc6\u5ea6\u91cfS_S(\u03c6)\uff0c\u6574\u5408\u7ba1\u9053\u7684\u8ba1\u7b97\u6210\u672c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u7684\u9519\u8bef\u7387\uff0c\u63a8\u5bfc\u51fa\u9608\u503c\u77e5\u8bc6\u8c13\u8bcdK_S\u548c\u4f01\u4e1a\u8303\u56f4\u8ba4\u77e5\u80fd\u529b\u6307\u6570K_{S,t}\u3002", "result": "\u5c06\u5b9a\u91cf\u6307\u6807\u6620\u5c04\u5230\u5b9e\u9645\u77e5\u8bc6\u3001\u63a8\u5b9a\u77e5\u8bc6\u3001\u6545\u610f\u65e0\u89c6\u548c\u9c81\u83bd\u7b49\u6cd5\u5f8b\u6807\u51c6\uff0c\u4e3a\u521b\u5efa\u53ef\u6d4b\u91cf\u548c\u53ef\u53f8\u6cd5\u5ba1\u8ba1\u7684\u4eba\u5de5\u5236\u54c1\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u7b97\u6cd5\u65f6\u4ee3\u4f7f\u4f01\u4e1a\u601d\u7ef4\u53d8\u5f97\u53ef\u5904\u7406\u548c\u53ef\u95ee\u8d23\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u4f7f\u4f01\u4e1a\u5fc3\u667a\u5728\u6cd5\u5f8b\u4e0a\u53ef\u8ffd\u8e2a\u3002"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "TEAM-PHI\u662f\u4e00\u4e2a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30PHI\u53bb\u6807\u8bc6\u5316\u8d28\u91cf\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u9ec4\u91d1\u6807\u51c6\u6807\u6ce8\u3002", "motivation": "PHI\u53bb\u6807\u8bc6\u5316\u5bf9\u4e8e\u4e34\u5e8a\u7b14\u8bb0\u7684\u5b89\u5168\u91cd\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u4f9d\u8d56\u6602\u8d35\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u90e8\u7f72\u591a\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u72ec\u7acb\u5224\u65adPHI\u63d0\u53d6\u6b63\u786e\u6027\uff0c\u901a\u8fc7LLM\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u4e0d\u540c\u8bc4\u4f30\u8005\u89c2\u70b9\uff0c\u751f\u6210\u7a33\u5b9a\u53ef\u590d\u73b0\u7684\u6392\u540d\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u4ea7\u751f\u4e00\u81f4\u51c6\u786e\u7684\u6392\u540d\uff0cLLM\u6295\u7968\u53ef\u9760\u5730\u6536\u655b\u4e8e\u76f8\u540c\u7684\u6700\u4f73\u7cfb\u7edf\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "TEAM-PHI\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u8bc4\u4f30\u667a\u80fd\u4f53\u548cLLM\u591a\u6570\u6295\u7968\uff0c\u4e3aPHI\u53bb\u6807\u8bc6\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u8bc4\u4f30\u548c\u6700\u4f73\u6a21\u578b\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2510.16206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16206", "abs": "https://arxiv.org/abs/2510.16206", "authors": ["Alex Zhavoronkov", "Dominika Wilczok", "Roman Yampolskiy"], "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI", "comment": null, "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory.To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u88ab\u8bb0\u4f4f\u6743\"\u6982\u5ff5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u504f\u89c1\u3001\u4fe1\u606f\u9057\u6f0f\u548c\u96c6\u4f53\u8bb0\u5fc6\u91cd\u5851\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u7684\u591a\u5143\u5316\u7ed3\u679c\u88ab\u5355\u4e00\u6743\u5a01\u6027\u56de\u7b54\u53d6\u4ee3\uff0c\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u7fa4\u4f53\u548c\u89c2\u70b9\u88ab\u7cfb\u7edf\u6027\u5730\u5ffd\u89c6\u6216\u653e\u5927\uff0c\u5a01\u80c1\u4fe1\u606f\u591a\u6837\u6027\u548c\u96c6\u4f53\u8bb0\u5fc6\u3002", "method": "\u63d0\u51fa\"\u88ab\u8bb0\u4f4f\u6743\"\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u6700\u5c0f\u5316AI\u9a71\u52a8\u4fe1\u606f\u9057\u6f0f\u98ce\u9669\u3001\u4fdd\u969c\u516c\u5e73\u5bf9\u5f85\u6743\u5229\u3001\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u6700\u5927\u7a0b\u5ea6\u771f\u5b9e\u3002", "result": "\u5efa\u7acb\u4e86\u5e94\u5bf9AI\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u6f5c\u5728\u504f\u89c1\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u4fdd\u62a4\u6570\u5b57\u5f31\u52bf\u7fa4\u4f53\u548c\u7ef4\u6301\u4fe1\u606f\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\"\u88ab\u8bb0\u4f4f\u6743\"\u662f\u5e94\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u6f5c\u5728\u98ce\u9669\u7684\u91cd\u8981\u6982\u5ff5\uff0c\u9700\u8981\u5728AI\u53d1\u5c55\u4e2d\u5e73\u8861\u4fe1\u606f\u6743\u5a01\u6027\u4e0e\u591a\u6837\u6027\uff0c\u4fdd\u62a4\u96c6\u4f53\u8bb0\u5fc6\u514d\u53d7\u6280\u672f\u504f\u89c1\u5f71\u54cd\u3002"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "ScholarEval\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u7684\u7814\u7a76\u60f3\u6cd5\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6807\u51c6\uff1asoundness\uff08\u57fa\u4e8e\u73b0\u6709\u6587\u732e\u7684\u65b9\u6cd5\u6709\u6548\u6027\uff09\u548ccontribution\uff08\u76f8\u5bf9\u4e8e\u5148\u524d\u7814\u7a76\u7684\u521b\u65b0\u7a0b\u5ea6\uff09\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u7814\u7a76\u6784\u601d\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5f3a\u5927\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u751f\u6210\u60f3\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165ScholarEval\u6846\u67b6\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u8bc4\u4f30\u7814\u7a76\u60f3\u6cd5\uff1b\u521b\u5efaScholarIdeas\u6570\u636e\u96c6\uff0c\u5305\u542b117\u4e2a\u8de8\u5b66\u79d1\u4e13\u5bb6\u6807\u6ce8\u7684\u7814\u7a76\u60f3\u6cd5\u548c\u8bc4\u8bba\u3002", "result": "ScholarEval\u5728\u4e13\u5bb6\u6807\u6ce8\u6807\u51c6\u8986\u76d6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8bc4\u4f30\u53ef\u64cd\u4f5c\u6027\u3001\u6df1\u5ea6\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u6301\u7eed\u4f18\u4e8eOpenAI\u7684o4-mini-deep-research\u7cfb\u7edf\uff1b\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u663e\u793a\u5728\u6587\u732e\u53c2\u4e0e\u3001\u60f3\u6cd5\u7cbe\u70bc\u548c\u5b9e\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u7814\u7a76\u3002", "conclusion": "ScholarEval\u4e3aAI\u751f\u6210\u7814\u7a76\u60f3\u6cd5\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.", "AI": {"tldr": "\u8bba\u6587\u8bc6\u522b\u5e76\u5206\u6790\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u4e2d\u7684\"\u63a8\u7406\u5206\u5fc3\"\u6f0f\u6d1e\uff0c\u5373\u6a21\u578b\u88ab\u6076\u610f\u5d4c\u5165\u7684\u590d\u6742\u65e0\u5173\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5bfc\u81f4\u4e3b\u8981\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\uff0c\u53ef\u5c06\u9c81\u68d2\u6027\u63d0\u9ad850\u591a\u4e2a\u70b9\u3002", "motivation": "\u968f\u7740\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u6f0f\u6d1e\uff1a\u5bb9\u6613\u88ab\u6076\u610f\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u504f\u79bb\u4e3b\u8981\u76ee\u6807\u3002\u8fd9\u79cd\"\u63a8\u7406\u5206\u5fc3\"\u5a01\u80c1\u5230LRM\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u591a\u79cd\u6a21\u578b\u548c\u57fa\u51c6\u7684\u7efc\u5408\u7814\u7a76\uff0c\u5206\u6790\u63a8\u7406\u5206\u5fc3\u6f0f\u6d1e\u3002\u63d0\u51fa\u57fa\u4e8e\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u5728\u5408\u6210\u7684\u5bf9\u6297\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684LRM\u4e5f\u9ad8\u5ea6\u6613\u53d7\u653b\u51fb\uff0c\u6ce8\u5165\u7684\u5206\u5fc3\u4efb\u52a1\u53ef\u4f7f\u4efb\u52a1\u51c6\u786e\u7387\u964d\u4f4e\u9ad8\u8fbe60%\u3002\u67d0\u4e9b\u5bf9\u9f50\u6280\u672f\u4f1a\u653e\u5927\u8fd9\u79cd\u5f31\u70b9\uff0c\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u9690\u853d\u670d\u4ece\u884c\u4e3a\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u53ef\u5c06\u9c81\u68d2\u6027\u63d0\u9ad850\u591a\u4e2a\u70b9\u3002", "conclusion": "\u63a8\u7406\u5206\u5fc3\u662fLRM\u53ef\u9760\u6027\u9762\u4e34\u7684\u72ec\u7279\u4e14\u7d27\u8feb\u7684\u5a01\u80c1\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u4fe1\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u589e\u5f3a\u6a21\u578b\u5bf9\u6076\u610f\u5206\u5fc3\u4efb\u52a1\u7684\u62b5\u6297\u529b\u3002"}}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u964d\u4f4e\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u7cfb\u7edf\u6548\u7387\u95ee\u9898\u3002\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u89e3\u4e3aLLM API\u5ef6\u8fdf\u548c\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u4e24\u90e8\u5206\uff0c\u901a\u8fc715\u4e2a\u6a21\u578b\u548c5\u4e2a\u63d0\u4f9b\u5546\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u4f18\u5316\u5ef6\u8fdf\u3002", "result": "\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u53ef\u5360\u603b\u4f53\u5ef6\u8fdf\u768453.7%\uff0cSpecCache\u76f8\u6bd4\u968f\u673a\u7f13\u5b58\u7b56\u7565\u5c06\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u534758\u500d\uff0c\u7f51\u7edc\u73af\u5883\u5f00\u9500\u964d\u4f4e3.2\u500d\uff0c\u4e14\u4e0d\u635f\u5bb3\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0cSpecCache\u6846\u67b6\u80fd\u6709\u6548\u964d\u4f4e\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u7f51\u7edc\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16302", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.16302", "abs": "https://arxiv.org/abs/2510.16302", "authors": ["Changhao Wang", "Yanfang Liu", "Xinxin Fan", "Anzhi Zhou", "Lao Tian", "Yunfeng Lu"], "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA", "comment": "13 pages, 5 figures", "summary": "Multi-hop reasoning for question answering (QA) plays a critical role in\nretrieval-augmented generation (RAG) for modern large language models (LLMs).\nThe accurate answer can be obtained through retrieving relational structure of\nentities from knowledge graph (KG). Regarding the inherent relation-dependency\nand reasoning pattern, multi-hop reasoning can be in general classified into\ntwo categories: i) parallel fact-verification multi-hop reasoning question,\ni.e., requiring simultaneous verifications of multiple independent\nsub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding\nsequential multi-step inference with intermediate conclusions serving as\nessential premises for subsequent reasoning. Currently, the multi-hop reasoning\napproaches singly employ one of two techniques: LLM response-based fact\nverification and KG path-based chain construction. Nevertheless, the former\nexcels at parallel fact-verification but underperforms on chained reasoning\ntasks, while the latter demonstrates proficiency in chained multi-hop reasoning\nbut suffers from redundant path retrieval when handling parallel\nfact-verification reasoning. These limitations deteriorate the efficiency and\naccuracy for multi-hop QA tasks. To address this challenge, we propose a novel\ndual-track KG verification and reasoning framework DTKG, which is inspired by\nthe Dual Process Theory in cognitive science. Specifically, DTKG comprises two\nmain stages: the Classification Stage and the Branch Processing Stage.", "AI": {"tldr": "\u63d0\u51faDTKG\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u6784\u5efa\u548cLLM\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u95ee\u7b54\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5e76\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u548c\u94fe\u5f0f\u591a\u8df3\u63a8\u7406\u65f6\u5404\u6709\u5c40\u9650\uff1aLLM\u9a8c\u8bc1\u64c5\u957f\u5e76\u884c\u9a8c\u8bc1\u4f46\u4e0d\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\uff0cKG\u8def\u5f84\u6784\u5efa\u64c5\u957f\u94fe\u5f0f\u63a8\u7406\u4f46\u5728\u5e76\u884c\u9a8c\u8bc1\u65f6\u5b58\u5728\u5197\u4f59\u8def\u5f84\u68c0\u7d22\u95ee\u9898", "method": "\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u53cc\u8fc7\u7a0b\u7406\u8bba\u7684\u53cc\u8f68KG\u9a8c\u8bc1\u63a8\u7406\u6846\u67b6DTKG\uff0c\u5305\u542b\u5206\u7c7b\u9636\u6bb5\u548c\u5206\u652f\u5904\u7406\u9636\u6bb5", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e", "conclusion": "DTKG\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u95ee\u7b54\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898"}}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.", "AI": {"tldr": "MedRule-KG\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u5e26\u7c7b\u578b\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5f3a\u5236\u6267\u884c\u6570\u5b66\u53ef\u89e3\u91ca\u89c4\u5219\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u5e76\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u4f46\u8fdd\u53cd\u7b80\u5355\u7684\u6570\u5b66\u6216\u903b\u8f91\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u786e\u4fdd\u63a8\u7406\u7684\u6570\u5b66\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165MedRule-KG\u77e5\u8bc6\u56fe\u8c31\u7f16\u7801\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u4e09\u4e2a\u9886\u57df\u542f\u53d1\u89c4\u5219\uff0c\u914d\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\u68c0\u67e5\u9884\u6d4b\u5e76\u5e94\u7528\u6700\u5c0f\u4fee\u6b63\u4ee5\u4fdd\u8bc1\u4e00\u81f4\u6027\u3002", "result": "\u572890\u4e2aFDA\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528MedRule-KG\u5c06\u7cbe\u786e\u5339\u914d\u4ece0.767\u63d0\u9ad8\u52300.900\uff0c\u52a0\u4e0a\u9a8c\u8bc1\u5668\u540e\u8fbe\u52301.000\u7cbe\u786e\u5339\u914d\u5e76\u5b8c\u5168\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "conclusion": "MedRule-KG\u4e3a\u5b89\u5168\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u786e\u4fdd\u63a8\u7406\u7684\u6570\u5b66\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "\u63d0\u51fa\u4e86SELECT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u951a\u70b9\u654f\u611f\u6027\u95ee\u9898\uff0c\u514b\u670d\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\u5bfc\u81f4\u7684\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\uff0c\u5b58\u5728\u6982\u5ff5\u91cd\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u951a\u70b9\u9009\u62e9\u5bf9\u64e6\u9664\u6548\u679c\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u8ffd\u8e2a\u63ed\u793a\u64e6\u9664\u5bf9\u951a\u70b9\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5b9a\u4e49\u5144\u5f1f\u6392\u4ed6\u6982\u5ff5\u4f5c\u4e3a\u66f4\u4f18\u951a\u70b9\u7c7b\u522b\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u951a\u70b9\u8fdb\u884c\u7cbe\u786e\u64e6\u9664\uff0c\u540c\u65f6\u8bc6\u522b\u5173\u952e\u8fb9\u754c\u951a\u70b9\u4ee5\u4fdd\u7559\u76f8\u5173\u6982\u5ff5\u3002", "result": "SELECT\u4f5c\u4e3a\u901a\u7528\u951a\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u9ad8\u6548\u9002\u914d\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5355\u4e2a\u6982\u5ff5\u951a\u70b9\u6316\u6398\u5e73\u5747\u4ec5\u97004\u79d2\u3002", "conclusion": "SELECT\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6709\u6548\u89e3\u51b3\u4e86\u6982\u5ff5\u64e6\u9664\u4e2d\u7684\u951a\u70b9\u654f\u611f\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u6982\u5ff5\u64e6\u9664\u6548\u679c\u3002"}}
{"id": "2510.16368", "categories": ["cs.AI", "cs.HC", "cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.16368", "abs": "https://arxiv.org/abs/2510.16368", "authors": ["Ali Shirali"], "title": "The Burden of Interactive Alignment with Inconsistent Preferences", "comment": "Published as a conference paper at NeurIPS 2025", "summary": "From media platforms to chatbots, algorithms shape how people interact,\nlearn, and discover information. Such interactions between users and an\nalgorithm often unfold over multiple steps, during which strategic users can\nguide the algorithm to better align with their true interests by selectively\nengaging with content. However, users frequently exhibit inconsistent\npreferences: they may spend considerable time on content that offers little\nlong-term value, inadvertently signaling that such content is desirable.\nFocusing on the user side, this raises a key question: what does it take for\nsuch users to align the algorithm with their true interests?\n  To investigate these dynamics, we model the user's decision process as split\nbetween a rational system 2 that decides whether to engage and an impulsive\nsystem 1 that determines how long engagement lasts. We then study a\nmulti-leader, single-follower extensive Stackelberg game, where users,\nspecifically system 2, lead by committing to engagement strategies and the\nalgorithm best-responds based on observed interactions. We define the burden of\nalignment as the minimum horizon over which users must optimize to effectively\nsteer the algorithm. We show that a critical horizon exists: users who are\nsufficiently foresighted can achieve alignment, while those who are not are\ninstead aligned to the algorithm's objective. This critical horizon can be\nlong, imposing a substantial burden. However, even a small, costly signal\n(e.g., an extra click) can significantly reduce it. Overall, our framework\nexplains how users with inconsistent preferences can align an engagement-driven\nalgorithm with their interests in a Stackelberg equilibrium, highlighting both\nthe challenges and potential remedies for achieving alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7528\u6237\u5982\u4f55\u901a\u8fc7\u7b56\u7565\u6027\u4e92\u52a8\u6765\u5f15\u5bfc\u7b97\u6cd5\u4e0e\u5176\u771f\u5b9e\u5174\u8da3\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7cfb\u7edf\u51b3\u7b56\u6a21\u578b\u548cStackelberg\u535a\u5f08\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7528\u6237\u9700\u8981\u8db3\u591f\u7684\u524d\u77bb\u6027\u624d\u80fd\u6709\u6548\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4f46\u5c0f\u7684\u4ee3\u4ef7\u4fe1\u53f7\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u8d1f\u62c5\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u5728\u7b97\u6cd5\u4ea4\u4e92\u4e2d\u7684\u4e0d\u4e00\u81f4\u504f\u597d\u95ee\u9898\uff0c\u63a2\u7d22\u7528\u6237\u5982\u4f55\u901a\u8fc7\u9009\u62e9\u6027\u4e92\u52a8\u6765\u5f15\u5bfc\u7b97\u6cd5\u66f4\u597d\u5730\u5339\u914d\u5176\u771f\u5b9e\u5174\u8da3\uff0c\u89e3\u51b3\u7528\u6237\u5728\u77ed\u671f\u51b2\u52a8\u548c\u957f\u671f\u4ef7\u503c\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u5c06\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u7406\u6027\u7cfb\u7edf2\uff08\u51b3\u5b9a\u662f\u5426\u4e92\u52a8\uff09\u548c\u51b2\u52a8\u7cfb\u7edf1\uff08\u51b3\u5b9a\u4e92\u52a8\u65f6\u957f\uff09\u7684\u53cc\u7cfb\u7edf\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9886\u5bfc\u8005-\u5355\u8ddf\u968f\u8005\u7684Stackelberg\u6269\u5c55\u535a\u5f08\u6846\u67b6\uff0c\u7528\u6237\u4f5c\u4e3a\u9886\u5bfc\u8005\u901a\u8fc7\u627f\u8bfa\u4e92\u52a8\u7b56\u7565\u6765\u5f15\u5bfc\u7b97\u6cd5\u54cd\u5e94\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u5bf9\u9f50\u65f6\u95f4\u8303\u56f4\uff1a\u8db3\u591f\u6709\u8fdc\u89c1\u7684\u7528\u6237\u53ef\u4ee5\u5b9e\u73b0\u7b97\u6cd5\u5bf9\u9f50\uff0c\u800c\u7f3a\u4e4f\u8fdc\u89c1\u7684\u7528\u6237\u5219\u4f1a\u88ab\u7b97\u6cd5\u76ee\u6807\u6240\u5bf9\u9f50\u3002\u8fd9\u4e2a\u5173\u952e\u65f6\u95f4\u8303\u56f4\u53ef\u80fd\u5f88\u957f\uff0c\u4f46\u5373\u4f7f\u4e00\u4e2a\u5c0f\u7684\u4ee3\u4ef7\u4fe1\u53f7\uff08\u5982\u989d\u5916\u70b9\u51fb\uff09\u4e5f\u80fd\u663e\u8457\u51cf\u5c11\u5bf9\u9f50\u8d1f\u62c5\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u91ca\u4e86\u5177\u6709\u4e0d\u4e00\u81f4\u504f\u597d\u7684\u7528\u6237\u5982\u4f55\u5728Stackelberg\u5747\u8861\u4e2d\u5b9e\u73b0\u4e0e\u4e92\u52a8\u9a71\u52a8\u7b97\u6cd5\u7684\u5bf9\u9f50\uff0c\u65e2\u63ed\u793a\u4e86\u5b9e\u73b0\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4e5f\u6307\u51fa\u4e86\u901a\u8fc7\u4ee3\u4ef7\u4fe1\u53f7\u51cf\u8f7b\u5bf9\u9f50\u8d1f\u62c5\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5728GSM8K\u4e0a\u53d6\u5f97\u4e8675.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9700\u8981\u66f4\u5c11\u7684\u5c1d\u8bd5\u6b21\u6570\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u5206\u79bb\u95ee\u9898\uff1a\u76d1\u63a7-\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\uff0c\u751f\u6210-\u9a8c\u8bc1\u65b9\u6cd5\u7f3a\u4e4f\u4efb\u52a1\u8bc4\u4f30\u3002\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u7b56\u7565\u5931\u8d25\u65e0\u53cd\u9988\uff0c\u4f18\u5316\u7f3a\u4e4f\u6218\u7565\u57fa\u7840\u3002", "method": "\u5b9e\u73b0\u4e86Flavell\u7684\u8ba4\u77e5\u76d1\u63a7\u6a21\u578b\uff0c\u4f5c\u4e3a\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u6846\u67b6\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\u3002", "result": "\u5728GSM8K\u4e0a\u8fbe\u523075.42%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSELF-REFINE(68.44%)\u548cSelf-Verification(67.07%)\uff0c\u5c1d\u8bd5\u6b21\u6570\u66f4\u5c11(1.3 vs 2.0)\uff0c\u63a8\u7406\u6210\u672c\u589e\u52a027-37%\u3002", "conclusion": "\u524d\u671f\u76d1\u63a7\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4f18\u5316\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u7b97\u672f\u63a8\u7406\u4e4b\u5916\u8fdb\u884c\u66f4\u591a\u8bc4\u4f30\u4ee5\u786e\u7acb\u901a\u7528\u6027\u3002"}}
{"id": "2510.16382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16382", "abs": "https://arxiv.org/abs/2510.16382", "authors": ["Ze Tao", "Jian Zhang", "Haowei Li", "Xianshuai Li", "Yifei Peng", "Xiyao Liu", "Senzhang Wang", "Chao Liu", "Sheng Ren", "Shichao Zhang"], "title": "Humanoid-inspired Causal Representation Learning for Domain Generalization", "comment": null, "summary": "This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a\nnovel causal framework inspired by human intelligence, designed to overcome the\nlimitations of conventional domain generalization models. Unlike approaches\nthat rely on statistics to capture data-label dependencies and learn\ndistortion-invariant representations, HSCM replicates the hierarchical\nprocessing and multi-level learning of human vision systems, focusing on\nmodeling fine-grained causal mechanisms. By disentangling and reweighting key\nimage attributes such as color, texture, and shape, HSCM enhances\ngeneralization across diverse domains, ensuring robust performance and\ninterpretability. Leveraging the flexibility and adaptability of human\nintelligence, our approach enables more effective transfer and learning in\ndynamic, complex environments. Through both theoretical and empirical\nevaluations, we demonstrate that HSCM outperforms existing domain\ngeneralization models, providing a more principled method for capturing causal\nrelationships and improving model robustness. The code is available at\nhttps://github.com/lambett/HSCM.", "AI": {"tldr": "\u63d0\u51faHSCM\u56e0\u679c\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u91cd\u52a0\u6743\u56fe\u50cf\u5c5e\u6027\u6765\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b", "motivation": "\u514b\u670d\u4f20\u7edf\u57df\u6cdb\u5316\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u6a21\u62df\u4eba\u7c7b\u667a\u80fd\u7684\u5c42\u6b21\u5904\u7406\u548c\u591a\u7ea7\u5b66\u4e60\u673a\u5236", "method": "\u57fa\u4e8e\u4eba\u5f62\u542f\u53d1\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u89e3\u8026\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u7b49\u5173\u952e\u56fe\u50cf\u5c5e\u6027\u5e76\u91cd\u65b0\u52a0\u6743", "result": "\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u57df\u6cdb\u5316\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "HSCM\u4e3a\u6355\u6349\u56e0\u679c\u5173\u7cfb\u548c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u66f4\u539f\u5219\u6027\u7684\u65b9\u6cd5"}}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.", "AI": {"tldr": "\u63d0\u51faRGMem\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8bb0\u5fc6\u7ec4\u7ec7\u5b9e\u73b0LLM\u5bf9\u8bdd\u7cfb\u7edf\u7684\u957f\u671f\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709RAG\u548c\u663e\u5f0f\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u7ea7\u5b58\u50a8\uff0c\u7f3a\u4e4f\u4ece\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u6f5c\u5728\u504f\u597d\u548c\u6df1\u5c42\u7279\u8d28\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u957f\u671f\u6709\u6548\u7684\u7528\u6237\u5efa\u6a21", "method": "\u53d7\u7269\u7406\u5b66\u91cd\u6574\u5316\u7fa4\u601d\u60f3\u542f\u53d1\uff0c\u901a\u8fc7\u5206\u5c42\u7c97\u7c92\u5316\u548c\u91cd\u6807\u5ea6\u64cd\u4f5c\uff0c\u4ece\u7247\u6bb5\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7528\u6237\u6d1e\u5bdf\uff0c\u9010\u6b65\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf", "result": "\u5b9e\u73b0\u4e86\u4ece\u566a\u58f0\u5fae\u89c2\u4ea4\u4e92\u5230\u9ad8\u5c42\u6b21\u51c6\u786e\u7528\u6237\u753b\u50cf\u7684\u4fe1\u606f\u538b\u7f29\u548c\u6d8c\u73b0\u8fc7\u7a0b", "conclusion": "RGMem\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u6311\u6218"}}
{"id": "2510.16466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16466", "abs": "https://arxiv.org/abs/2510.16466", "authors": ["Siddhartha Krothapalli", "Tridib Kumar Das", "Praveen Kumar", "Naveen Suravarpu", "Pratik Narang"], "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights", "comment": "11 pages, 1 figure, 4 tables", "summary": "As customer feedback becomes increasingly central to strategic growth, the\nability to derive actionable insights from unstructured reviews is essential.\nWhile traditional AI-driven systems excel at predicting user preferences, far\nless work has focused on transforming customer reviews into prescriptive,\nbusiness-facing recommendations. This paper introduces ReviewSense, a novel\nprescriptive decision support framework that leverages advanced large language\nmodels (LLMs) to transform customer reviews into targeted, actionable business\nrecommendations. By identifying key trends, recurring issues, and specific\nconcerns within customer sentiments, ReviewSense extends beyond\npreference-based systems to provide businesses with deeper insights for\nsustaining growth and enhancing customer loyalty. The novelty of this work lies\nin integrating clustering, LLM adaptation, and expert-driven evaluation into a\nunified, business-facing pipeline. Preliminary manual evaluations indicate\nstrong alignment between the model's recommendations and business objectives,\nhighlighting its potential for driving data-informed decision-making. This\nframework offers a new perspective on AI-driven sentiment analysis,\ndemonstrating its value in refining business strategies and maximizing the\nimpact of customer feedback.", "AI": {"tldr": "ReviewSense\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u53ef\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5546\u4e1a\u5efa\u8bae\uff0c\u8d85\u8d8a\u4f20\u7edf\u504f\u597d\u9884\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u64c5\u957f\u9884\u6d4b\u7528\u6237\u504f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u9762\u5411\u4e1a\u52a1\u7684\u89c4\u8303\u6027\u5efa\u8bae\u7684\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u5546\u4e1a\u6d1e\u5bdf\u7684\u6846\u67b6\u3002", "method": "\u96c6\u6210\u805a\u7c7b\u3001LLM\u9002\u914d\u548c\u4e13\u5bb6\u9a71\u52a8\u8bc4\u4f30\u7684\u7edf\u4e00\u4e1a\u52a1\u5bfc\u5411\u6d41\u7a0b\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u8d8b\u52bf\u3001\u91cd\u590d\u95ee\u9898\u548c\u5177\u4f53\u5173\u6ce8\u70b9\u6765\u751f\u6210\u5efa\u8bae\u3002", "result": "\u521d\u6b65\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u5efa\u8bae\u4e0e\u5546\u4e1a\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5177\u6709\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5728\u4f18\u5316\u5546\u4e1a\u7b56\u7565\u548c\u6700\u5927\u5316\u5ba2\u6237\u53cd\u9988\u4ef7\u503c\u65b9\u9762\u5c55\u73b0\u51fa\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86NP-ENGINE\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLMs\u5728NP\u96be\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u5305\u62ec\u4efb\u52a1\u751f\u6210\u5668\u3001\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u5e76\u5c55\u793a\u4e86\u5728NP-BENCH\u57fa\u51c6\u4e0a\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u51b3NP\u96be\u4f18\u5316\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f00\u53d1NP-ENGINE\u6846\u67b6\uff0c\u5305\u542b\u53ef\u63a7\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u89c4\u5219\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u91c7\u7528\u5206\u5c42\u96be\u5ea6\u7684\u53ef\u6269\u5c55RLVR\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u8bad\u7ec3\u7684QWEN2.5-7B-NP\u6a21\u578b\u5728NP-BENCH\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4efb\u52a1\u4e30\u5bcc\u7684RLVR\u8bad\u7ec3\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u63ed\u793a\u4e86RLVR\u7684\u6269\u5c55\u89c4\u5f8b\u3002"}}
{"id": "2510.16533", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16533", "abs": "https://arxiv.org/abs/2510.16533", "authors": ["Eilene Tomkins-Flanagan", "Connor Hanley", "Mary A. Kelly"], "title": "Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination", "comment": null, "summary": "We present a typed computer language, Doug, in which all typed programs may\nbe proved to halt in polynomial time, encoded in a vector-symbolic architecture\n(VSA). Doug is just an encoding of the light linear functional programming\nlanguage (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are\nencoded using a slot-value encoding scheme based on holographic declarative\nmemory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the\nLisp VSA defined by (Flanagan, 2024). Doug allows for some points on the\nembedding space of a neural network to be interpreted as types, where the types\nof nearby points are similar both in structure and content. Types in Doug are\ntherefore learnable by a neural network. Following (Chollet, 2019), (Card,\n1983), and (Newell, 1981), we view skill as the application of a procedure, or\nprogram of action, that causes a goal to be satisfied. Skill acquisition may\ntherefore be expressed as program synthesis. Using Doug, we hope to describe a\nform of learning of skilled behaviour that follows a human-like pace of skill\nacquisition (i.e., substantially faster than brute force; Heathcote, 2000),\nexceeding the efficiency of all currently existing approaches (Kaplan, 2020;\nJones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling\nhuman mental representations, as they must actually exist in the brain, and\nthose representations' acquisition, as they are actually learned.", "AI": {"tldr": "Doug\u662f\u4e00\u79cd\u7c7b\u578b\u5316\u8ba1\u7b97\u673a\u8bed\u8a00\uff0c\u6240\u6709\u7c7b\u578b\u5316\u7a0b\u5e8f\u90fd\u80fd\u88ab\u8bc1\u660e\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u505c\u6b62\uff0c\u7f16\u7801\u5728\u5411\u91cf\u7b26\u53f7\u67b6\u6784(VSA)\u4e2d\u3002\u5b83\u57fa\u4e8e\u8f7b\u91cf\u7ebf\u6027\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00(LLFPL)\uff0c\u7c7b\u578b\u4f7f\u7528\u5168\u606f\u58f0\u660e\u6027\u5185\u5b58(HDM)\u7f16\u7801\uff0c\u672f\u8bed\u4f7f\u7528Lisp VSA\u53d8\u4f53\u7f16\u7801\u3002\u8be5\u8bed\u8a00\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u70b9\u88ab\u89e3\u91ca\u4e3a\u7c7b\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u7c7b\u578b\u5b66\u4e60\u3002", "motivation": "\u5c06\u6280\u80fd\u83b7\u53d6\u8868\u8fbe\u4e3a\u7a0b\u5e8f\u5408\u6210\uff0c\u5e0c\u671b\u63cf\u8ff0\u4e00\u79cd\u9075\u5faa\u4eba\u7c7b\u6280\u80fd\u83b7\u53d6\u901f\u5ea6\u7684\u5b66\u4e60\u5f62\u5f0f\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u66f4\u63a5\u8fd1\u6a21\u62df\u4eba\u8111\u4e2d\u7684\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u5b9e\u9645\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u57fa\u4e8eSchimanski2009\u7684\u8f7b\u91cf\u7ebf\u6027\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00(LLFPL)\uff0c\u4f7f\u7528Kelly2020\u7684\u5168\u606f\u58f0\u660e\u6027\u5185\u5b58(HDM)\u8fdb\u884c\u7c7b\u578b\u7f16\u7801\uff0c\u91c7\u7528Flanagan2024\u7684Lisp VSA\u53d8\u4f53\u8fdb\u884c\u672f\u8bed\u7f16\u7801\uff0c\u5b9e\u73b0\u7c7b\u578b\u5316\u7a0b\u5e8f\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u505c\u6b62\u7684\u8bc1\u660e\u3002", "result": "\u5f00\u53d1\u4e86Doug\u8bed\u8a00\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u70b9\u80fd\u591f\u88ab\u89e3\u91ca\u4e3a\u7c7b\u578b\uff0c\u4e14\u90bb\u8fd1\u70b9\u7684\u7c7b\u578b\u5728\u7ed3\u6784\u548c\u5185\u5bb9\u4e0a\u76f8\u4f3c\uff0c\u4ece\u800c\u5b9e\u73b0\u7c7b\u578b\u7684\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "Doug\u8bed\u8a00\u4e3a\u6a21\u62df\u4eba\u8111\u5fc3\u7406\u8868\u5f81\u53ca\u5176\u5b66\u4e60\u8fc7\u7a0b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u671b\u5b9e\u73b0\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u7684\u4eba\u7c7b\u5f0f\u6280\u80fd\u83b7\u53d6\u3002"}}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.", "AI": {"tldr": "Urban-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7Group Relative Policy Optimization (GRPO)\u548c\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4efb\u52a1\u6765\u51cf\u8f7b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u504f\u89c1\uff0c\u63d0\u5347\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u52a0\u5267\u4e86\u5bf9\u57ce\u5e02\u901a\u7528\u667a\u80fd(UGI)\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6301\u7eed\u7684\u5730\u7406\u504f\u89c1\uff0c\u5bfc\u81f4\u533a\u57df\u9884\u6d4b\u504f\u5dee\u548c\u6709\u9650\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUrban-R1\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528GRPO\u4f18\u5316\u8de8\u5730\u7406\u7fa4\u4f53\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\u4ece\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u4e2d\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u5956\u52b1\u3002", "result": "\u8de8\u591a\u4e2a\u533a\u57df\u548c\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUrban-R1\u6709\u6548\u51cf\u8f7b\u5730\u7406\u504f\u89c1\u5e76\u6539\u5584\u8de8\u533a\u57df\u6cdb\u5316\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u662f\u5b9e\u73b0\u516c\u5e73\u53ef\u4fe1\u57ce\u5e02\u667a\u80fd\u7684\u6709\u524d\u666f\u9014\u5f84\u3002"}}
{"id": "2510.16559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16559", "abs": "https://arxiv.org/abs/2510.16559", "authors": ["Tian Xia", "Tianrun Gao", "Wenhao Deng", "Long Wei", "Xiaowei Qian", "Yixian Jiang", "Chenglei Yu", "Tailin Wu"], "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "comment": "33 pages, 10 figures", "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.", "AI": {"tldr": "BuildArena\u662f\u9996\u4e2a\u9762\u5411\u8bed\u8a00\u9a71\u52a8\u5de5\u7a0b\u5efa\u8bbe\u7684\u7269\u7406\u5bf9\u9f50\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5de5\u7a0b\u5efa\u7b51\u81ea\u52a8\u5316\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5de5\u7a0b\u5efa\u7b51\u81ea\u52a8\u5316\u9700\u8981\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u884c\u7684\u7ed3\u6784\uff0c\u4f46\u73b0\u4ee3LLM\u5728\u6b64\u9886\u57df\u7684\u5efa\u8bbe\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u8bbe\u8ba1\u7b56\u7565\u30013D\u7a7a\u95f4\u51e0\u4f55\u8ba1\u7b97\u5e93\u548c\u57fa\u7ebfLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u516b\u4e2a\u524d\u6cbfLLM\u4e0a\u5168\u9762\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u9a71\u52a8\u548c\u7269\u7406\u57fa\u7840\u5efa\u8bbe\u81ea\u52a8\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "BuildArena\u586b\u8865\u4e86LLM\u5728\u5de5\u7a0b\u5efa\u7b51\u81ea\u52a8\u5316\u9886\u57df\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u9996\u4e2a\u7269\u7406\u5bf9\u9f50\u7684\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents", "AI": {"tldr": "\u63d0\u51fa\u4e86Ripple Effect Protocol (REP)\uff0c\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u8ba9\u667a\u80fd\u4f53\u4e0d\u4ec5\u5206\u4eab\u51b3\u7b56\uff0c\u8fd8\u5206\u4eab\u8f7b\u91cf\u7ea7\u654f\u611f\u5ea6\u4fe1\u53f7\uff0c\u4ece\u800c\u5728\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u534f\u8c03\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\uff08\u5982A2A\u548cACP\uff09\u5f3a\u8c03\u901a\u4fe1\u800c\u975e\u534f\u8c03\uff0c\u968f\u7740\u4ee3\u7406\u7fa4\u4f53\u89c4\u6a21\u589e\u957f\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8106\u5f31\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5373\u4f7f\u4e2a\u4f53\u667a\u80fd\u4f53\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u826f\u7fa4\u4f53\u7ed3\u679c\u3002", "method": "\u5f15\u5165REP\u534f\u8bae\uff0c\u4ee3\u7406\u5206\u4eab\u51b3\u7b56\u548c\u8f7b\u91cf\u7ea7\u654f\u611f\u5ea6\u4fe1\u53f7\uff08\u8868\u8fbe\u5173\u952e\u73af\u5883\u53d8\u91cf\u53d8\u5316\u65f6\u9009\u62e9\u5982\u4f55\u6539\u53d8\uff09\uff0c\u8fd9\u4e9b\u654f\u611f\u5ea6\u5728\u5c40\u90e8\u7f51\u7edc\u4e2d\u4f20\u64ad\u3002\u534f\u8bae\u89c4\u8303\u5206\u79bb\u4e86\u5fc5\u9700\u7684\u6d88\u606f\u6a21\u5f0f\u548c\u53ef\u9009\u7684\u805a\u5408\u89c4\u5219\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\uff08i\uff09\u4f9b\u5e94\u94fe\u7ea7\u8054\uff08\u5564\u9152\u6e38\u620f\uff09\u3001\uff08ii\uff09\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u504f\u597d\u805a\u5408\uff08\u7535\u5f71\u8c03\u5ea6\uff09\u3001\uff08iii\uff09\u53ef\u6301\u7eed\u8d44\u6e90\u5206\u914d\uff08\u6e14\u4e1a\u94f6\u884c\uff09\uff0cREP\u76f8\u6bd4A2A\u5c06\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u9ad8\u4e8641%\u5230100%\uff0c\u5e76\u80fd\u7075\u6d3b\u5904\u7406\u6765\u81eaLLM\u7684\u591a\u6a21\u6001\u654f\u611f\u5ea6\u4fe1\u53f7\u3002", "conclusion": "\u901a\u8fc7\u5c06\u534f\u8c03\u4f5c\u4e3a\u534f\u8bae\u7ea7\u80fd\u529b\uff0cREP\u4e3a\u65b0\u5174\u7684\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.16582", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16582", "abs": "https://arxiv.org/abs/2510.16582", "authors": ["Junchi Yu", "Yujie Liu", "Jindong Gu", "Philip Torr", "Dongzhan Zhou"], "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances\nlarge language models (LLMs) by providing structured and interpretable external\nknowledge. However, existing KG-based RAG methods struggle to retrieve accurate\nand diverse information from text-rich KGs for complex real-world queries.\nProcess Reward Models (PRMs) offer a way to align the retrieval process of\nKG-based RAG with query-specific knowledge requirements, but they heavily rely\non process-level supervision signals that are expensive and hard to obtain on\nKGs. To address this challenge, we propose GraphFlow, a framework that\nefficiently retrieves accurate and diverse knowledge required for real-world\nqueries from text-rich KGs. GraphFlow employs a transition-based flow matching\nobjective to jointly optimize a retrieval policy and a flow estimator. The flow\nestimator factorizes the reward of the retrieval outcome into the intermediate\nretrieval states. Such reward factorization guides the retrieval policy to\nretrieve candidates from KGs in proportion to their reward. This allows\nGraphFlow to explore high-quality regions of KGs that yield diverse and\nrelevant results. We evaluate GraphFlow on the STaRK benchmark, which includes\nreal-world queries from multiple domains over text-rich KGs. GraphFlow\noutperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit\nrate and recall. It also shows strong generalization to unseen KGs,\ndemonstrating its effectiveness and robustness.", "AI": {"tldr": "GraphFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76ee\u6807\u4f18\u5316\u68c0\u7d22\u7b56\u7565\uff0c\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u9ad8\u6548\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u67e5\u8be2\u65f6\u96be\u4ee5\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u4fe1\u606f\uff0c\u800c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u8fc7\u7a0b\u7ea7\u76d1\u7763\u4fe1\u53f7\u3002", "method": "GraphFlow\u91c7\u7528\u57fa\u4e8e\u8f6c\u79fb\u7684\u6d41\u5339\u914d\u76ee\u6807\u8054\u5408\u4f18\u5316\u68c0\u7d22\u7b56\u7565\u548c\u6d41\u4f30\u8ba1\u5668\uff0c\u5c06\u68c0\u7d22\u7ed3\u679c\u7684\u5956\u52b1\u5206\u89e3\u5230\u4e2d\u95f4\u68c0\u7d22\u72b6\u6001\uff0c\u6307\u5bfc\u68c0\u7d22\u7b56\u7565\u6309\u5956\u52b1\u6bd4\u4f8b\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u5019\u9009\u7ed3\u679c\u3002", "result": "\u5728STaRK\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraphFlow\u5728\u547d\u4e2d\u7387\u548c\u53ec\u56de\u7387\u4e0a\u5e73\u5747\u4f18\u4e8e\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5f3a\u57fa\u7ebf10%\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GraphFlow\u80fd\u591f\u6709\u6548\u4ece\u6587\u672c\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u51c6\u786e\u591a\u6837\u7684\u77e5\u8bc6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6548\u679c\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.16601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16601", "abs": "https://arxiv.org/abs/2510.16601", "authors": ["Tianxing Wu", "Shutong Zhu", "Jingting Wang", "Ning Xu", "Guilin Qi", "Haofen Wang"], "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning", "comment": "13 pages, accepted by NeurIPS 2025 (spotlight)", "summary": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence\nscore to provide more precise knowledge representations. Recently, since\nreal-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)\ncompletion attracts more attention, aiming to complete missing triples and\nconfidences. Current studies attempt to learn UKG embeddings to solve this\nproblem, but they neglect the extremely imbalanced distributions of triple\nconfidences. This causes that the learnt embeddings are insufficient to\nhigh-quality UKG completion. Thus, in this paper, to address the above issue,\nwe propose a new semi-supervised Confidence Distribution Learning (ssCDL)\nmethod for UKG completion, where each triple confidence is transformed into a\nconfidence distribution to introduce more supervision information of different\nconfidences to reinforce the embedding learning process. ssCDL iteratively\nlearns UKG embedding by relational learning on labeled data (i.e., existing\ntriples with confidences) and unlabeled data with pseudo labels (i.e., unseen\ntriples with the generated confidences), which are predicted by meta-learning\nto augment the training data and rebalance the distribution of triple\nconfidences. Experiments on two UKG datasets demonstrate that ssCDL\nconsistently outperforms state-of-the-art baselines in different evaluation\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u534a\u76d1\u7763\u7f6e\u4fe1\u5ea6\u5206\u5e03\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u5206\u5e03\u6765\u589e\u5f3a\u5d4c\u5165\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u5143\u5b66\u4e60\u751f\u6210\u4f2a\u6807\u7b7e\u6765\u5e73\u8861\u7f6e\u4fe1\u5ea6\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7f6e\u4fe1\u5ea6\u7684\u6781\u7aef\u4e0d\u5e73\u8861\u5206\u5e03\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u3002", "method": "\u63d0\u51fassCDL\u65b9\u6cd5\uff0c\u5c06\u7f6e\u4fe1\u5ea6\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u5ea6\u5206\u5e03\uff0c\u901a\u8fc7\u5173\u7cfb\u5b66\u4e60\u5728\u6807\u8bb0\u6570\u636e\u548c\u5e26\u4f2a\u6807\u7b7e\u7684\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u8fed\u4ee3\u5b66\u4e60\u5d4c\u5165\uff0c\u4f7f\u7528\u5143\u5b66\u4e60\u9884\u6d4b\u672a\u89c1\u4e09\u5143\u7ec4\u7684\u7f6e\u4fe1\u5ea6\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cssCDL\u5728\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ssCDL\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u5e03\u5b66\u4e60\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7f6e\u4fe1\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u6765\u6fc0\u52b1\u63a2\u7d22\uff0c\u907f\u514d\u6a21\u578b\u9677\u5165\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u63a2\u7d22\uff0c\u5bfc\u81f4LLM\u503e\u5411\u4e8e\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMERCI\u7b97\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684Coin Flipping Network\u4f30\u8ba1\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u4e0eGRPO\u7b49\u5148\u8fdbRL\u6846\u67b6\u96c6\u6210\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERCI\u9f13\u52b1\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6027\u80fd\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u53d1\u73b0\u66f4\u597d\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5185\u5728\u52a8\u673a\u53ef\u4ee5\u4f7f\u63a2\u7d22\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u53d8\u5f97\u53ef\u9760\u6709\u6548\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u89c4\u6a21AI\u6a21\u578b\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u6db5\u76d6\u795e\u7ecf\u5f71\u50cf\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u3001\u4e34\u5e8a\u8f85\u52a9\u548c\u75be\u75c5\u5e94\u7528\u4e94\u5927\u9886\u57df\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5e26\u6765\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u8f6c\u5411\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u8fd9\u4e3a\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u673a\u9047\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u56de\u987e\u548c\u5206\u6790\uff0c\u63a2\u7d22\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\uff1a\u795e\u7ecf\u5f71\u50cf\u4e0e\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u4e0e\u795e\u7ecf\u89e3\u7801\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5efa\u6a21\u3001\u4e34\u5e8a\u8f85\u52a9\u4e0e\u8f6c\u5316\u6846\u67b6\u3001\u4ee5\u53ca\u795e\u7ecf\u7cfb\u7edf\u548c\u7cbe\u795e\u75be\u75c5\u7684\u7279\u5b9a\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\u5927\u89c4\u6a21AI\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u4ee5\u53ca\u4e34\u5e8a\u90e8\u7f72\u7684\u8f6c\u5316\u6846\u67b6\u5f00\u53d1\u3002\u540c\u65f6\uff0c\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u4e92\u52a8\u53d8\u5f97\u66f4\u52a0\u53cc\u5411\uff0c\u751f\u7269\u542f\u53d1\u7684\u67b6\u6784\u7ea6\u675f\u88ab\u7528\u4e8e\u5f00\u53d1\u66f4\u5177\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u663e\u8457\u524d\u666f\u548c\u5173\u952e\u5b9e\u65bd\u8003\u8651\uff0c\u7279\u522b\u5f3a\u8c03\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3001\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4ee5\u53ca\u4e34\u5e8a\u4f7f\u7528\u7684\u5168\u9762\u4f26\u7406\u6307\u5357\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u9a8c\u8bc1\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u5173\u952e\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u96c6\u7684\u7cfb\u7edf\u5217\u8868\u3002"}}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AFL\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u95ee\u9898\u5b9e\u4f8b\u5230\u89e3\u51b3\u65b9\u6848\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u5916\u90e8\u6c42\u89e3\u5668\u3002", "motivation": "\u4f20\u7edf\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u7cbe\u529b\u8fdb\u884c\u610f\u56fe\u89e3\u91ca\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u5916\u90e8\u5e72\u9884\uff0c\u5bfc\u81f4\u81ea\u4e3b\u6027\u53d7\u9650\u3001\u6267\u884c\u9519\u8bef\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u4f4e\u3002", "method": "AFL\u6846\u67b6\u5c06\u6574\u4e2a\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u56db\u4e2a\u4e13\u95e8\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u534f\u8c03\u4ea4\u4e92\u786e\u4fdd\u8de8\u529f\u80fd\u4e00\u81f4\u6027\u548c\u903b\u8f91\u5408\u7406\u6027\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f93\u5165\u63d0\u53d6\u77e5\u8bc6\u5e76\u751f\u6210\u81ea\u5305\u542b\u4ee3\u7801\u3002", "result": "\u572860\u4e2a\u590d\u6742VRP\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u51b3\u65b9\u6848\u53ef\u884c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "AFL\u6846\u67b6\u5b9e\u73b0\u4e86\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u6c42\u89e3\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u53ef\u9760\u6027\u548c\u53ef\u884c\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u5316\u6c42\u89e3\u590d\u6742\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u667a\u80fdAI\u4ece\u57fa\u4e8e\u7ba1\u9053\u7684\u7cfb\u7edf\u5411\u6a21\u578b\u539f\u751f\u8303\u5f0f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5176\u4e2d\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u80fd\u529b\u4ece\u5916\u90e8\u903b\u8f91\u7f16\u6392\u8f6c\u53d8\u4e3a\u6a21\u578b\u53c2\u6570\u5185\u90e8\u5316\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u8fd9\u4e00\u8f6c\u53d8\u7684\u7b97\u6cd5\u5f15\u64ce\uff0c\u652f\u6301LLM + RL + Task\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8ffd\u8e2a\u667a\u80fdAI\u6784\u5efa\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u5916\u90e8\u7f16\u6392\u80fd\u529b\u5230\u6a21\u578b\u5185\u90e8\u5316\u80fd\u529b\uff0c\u63a2\u7d22\u4ece\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u5411\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\u53d1\u5c55\u7684\u8f68\u8ff9\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u80fd\u529b\u5982\u4f55\u4ece\u5916\u90e8\u811a\u672c\u6a21\u5757\u6f14\u53d8\u4e3a\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\uff0c\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u7b97\u6cd5\u5f15\u64ce\u7684\u4f5c\u7528\uff0c\u5e76\u8003\u5bdf\u8fd9\u4e00\u8303\u5f0f\u8f6c\u53d8\u5982\u4f55\u91cd\u5851\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548cGUI\u4ee3\u7406\u7b49\u4e3b\u8981\u5e94\u7528\u3002", "result": "\u63ed\u793a\u4e86\u5411\u6a21\u578b\u539f\u751f\u667a\u80fdAI\u7684\u8fde\u8d2f\u53d1\u5c55\u8f68\u8ff9\uff0c\u5c55\u793a\u4e86\u80fd\u529b\u5185\u90e8\u5316\u7684\u8d8b\u52bf\uff0c\u5305\u62ec\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u53cd\u601d\u7b49\u80fd\u529b\u7684\u6301\u7eed\u5185\u90e8\u5316\u3002", "conclusion": "\u667a\u80fdAI\u6b63\u5728\u4ece\u6784\u5efa\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u5411\u5f00\u53d1\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\u8f6c\u53d8\uff0c\u6807\u5fd7\u7740\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7684\u65b0\u9636\u6bb5\u3002"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u5173\u4e8e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u641c\u7d22\u7cfb\u7edf\u7684\u7efc\u8ff0\uff0c\u9996\u6b21\u5168\u9762\u6982\u8ff0\u4e86RL\u5728\u667a\u80fd\u641c\u7d22\u4e2d\u7684\u5e94\u7528\uff0c\u4ece\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u5e94\u7528\u8303\u56f4\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u8be5\u65b0\u5174\u9886\u57df\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5b58\u5728\u5355\u8f6e\u4ea4\u4e92\u3001\u542f\u53d1\u5f0f\u65b9\u6cd5\u7b49\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u548c\u63a8\u7406\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3002\u667a\u80fd\u641c\u7d22\u901a\u8fc7\u591a\u6b65\u4ea4\u4e92\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u4e3a\u81ea\u9002\u5e94\u548c\u81ea\u6211\u6539\u8fdb\u7684\u641c\u7d22\u884c\u4e3a\u63d0\u4f9b\u4e86\u5f3a\u5927\u673a\u5236\u3002", "method": "\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u8be5\u9886\u57df\uff1a(i) RL\u7684\u529f\u80fd\u89d2\u8272\uff0c(ii) RL\u7684\u4f18\u5316\u7b56\u7565\uff0c(iii) RL\u7684\u5e94\u7528\u8303\u56f4\u3002\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u5e76\u5efa\u7acb\u4e86\u5f00\u6e90\u8d44\u6e90\u5e93\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u6784\u5efa\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684RL\u9a71\u52a8\u667a\u80fd\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u6fc0\u53d1RL\u4e0e\u667a\u80fd\u641c\u7d22\u96c6\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2510.16742", "categories": ["cs.AI", "cs.MA", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.16742", "abs": "https://arxiv.org/abs/2510.16742", "authors": ["Paul Saves", "Pramudita Satria Palar", "Muhammad Daffa Robani", "Nicolas Verstaevel", "Moncef Garouani", "Julien Aligon", "Benoit Gaudou", "Koji Shimoyama", "Joseph Morlier"], "title": "Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration", "comment": null, "summary": "Complex systems are increasingly explored through simulation-driven\nengineering workflows that combine physics-based and empirical models with\noptimization and analytics. Despite their power, these workflows face two\ncentral obstacles: (1) high computational cost, since accurate exploration\nrequires many expensive simulator runs; and (2) limited transparency and\nreliability when decisions rely on opaque blackbox components. We propose a\nworkflow that addresses both challenges by training lightweight emulators on\ncompact designs of experiments that (i) provide fast, low-latency\napproximations of expensive simulators, (ii) enable rigorous uncertainty\nquantification, and (iii) are adapted for global and local Explainable\nArtificial Intelligence (XAI) analyses. This workflow unifies every\nsimulation-based complex-system analysis tool, ranging from engineering design\nto agent-based models for socio-environmental understanding. In this paper, we\nproposea comparative methodology and practical recommendations for using\nsurrogate-based explainability tools within the proposed workflow. The\nmethodology supports continuous and categorical inputs, combines global-effect\nand uncertainty analyses with local attribution, and evaluates the consistency\nof explanations across surrogate models, thereby diagnosing surrogate adequacy\nand guiding further data collection or model refinement. We demonstrate the\napproach on two contrasting case studies: a multidisciplinary design analysis\nof a hybrid-electric aircraft and an agent-based model of urban segregation.\nResults show that the surrogate model and XAI coupling enables large-scale\nexploration in seconds, uncovers nonlinear interactions and emergent behaviors,\nidentifies key design and policy levers, and signals regions where surrogates\nrequire more data or alternative architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u6a21\u578b\u7684\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u6765\u89e3\u51b3\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6a21\u578b\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u53ef\u89e3\u91caAI\u5206\u6790\u3002", "motivation": "\u4eff\u771f\u9a71\u52a8\u5de5\u7a0b\u5de5\u4f5c\u6d41\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a(1) \u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u56e0\u4e3a\u51c6\u786e\u63a2\u7d22\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u6a21\u62df\u5668\u8fd0\u884c\uff1b(2) \u5f53\u51b3\u7b56\u4f9d\u8d56\u4e8e\u4e0d\u900f\u660e\u7684\u9ed1\u76d2\u7ec4\u4ef6\u65f6\uff0c\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5728\u7d27\u51d1\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u6602\u8d35\u6a21\u62df\u5668\u8fd1\u4f3c\uff0c\u652f\u6301\u4e25\u683c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u9002\u7528\u4e8e\u5168\u5c40\u548c\u5c40\u90e8\u53ef\u89e3\u91caAI\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2a\u5bf9\u6bd4\u6848\u4f8b\u7814\u7a76\u4e2d\uff08\u6df7\u5408\u7535\u52a8\u98de\u673a\u7684\u591a\u5b66\u79d1\u8bbe\u8ba1\u5206\u6790\u548c\u57ce\u5e02\u9694\u79bb\u7684\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\uff09\uff0c\u7ed3\u679c\u663e\u793a\u66ff\u4ee3\u6a21\u578b\u4e0eXAI\u8026\u5408\u80fd\u591f\u5728\u51e0\u79d2\u949f\u5185\u5b9e\u73b0\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u63ed\u793a\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u6d8c\u73b0\u884c\u4e3a\uff0c\u8bc6\u522b\u5173\u952e\u8bbe\u8ba1\u548c\u653f\u7b56\u6760\u6746\uff0c\u5e76\u6307\u793a\u9700\u8981\u66f4\u591a\u6570\u636e\u6216\u66ff\u4ee3\u67b6\u6784\u7684\u533a\u57df\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7edf\u4e00\u4e86\u6240\u6709\u57fa\u4e8e\u4eff\u771f\u7684\u590d\u6742\u7cfb\u7edf\u5206\u6790\u5de5\u5177\uff0c\u4ece\u5de5\u7a0b\u8bbe\u8ba1\u5230\u793e\u4f1a\u73af\u5883\u7406\u89e3\uff0c\u901a\u8fc7\u66ff\u4ee3\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63d0\u4f9b\u4e86\u6bd4\u8f83\u65b9\u6cd5\u548c\u5b9e\u7528\u5efa\u8bae\uff0c\u652f\u6301\u8fde\u7eed\u548c\u5206\u7c7b\u8f93\u5165\uff0c\u7ed3\u5408\u5168\u5c40\u6548\u5e94\u548c\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u4e0e\u5c40\u90e8\u5f52\u56e0\uff0c\u8bc4\u4f30\u89e3\u91ca\u5728\u4e0d\u540c\u66ff\u4ee3\u6a21\u578b\u95f4\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "\u63d0\u51fa\u4e86ELMM\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u7684\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5b58\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u9762\u4e34\u56fe\u50cf\u4ee4\u724c\u8fc7\u591a\u5bfc\u81f4\u7684\u8bed\u4e49\u566a\u58f0\u3001\u6a21\u6001\u51b2\u7a81\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faELMM\u6a21\u578b\uff0c\u5305\u542b\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u7684\u591a\u89c6\u56fe\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\uff0c\u4ece\u6587\u672c\u548c\u89c6\u89c9\u89c6\u56fe\u81ea\u9002\u5e94\u538b\u7f29\u56fe\u50cf\u4ee4\u724c\uff1b\u8bbe\u8ba1\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u79fb\u9664\u5197\u4f59\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u6295\u5f71\u8865\u507f\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728FB15k-237-IMG\u548cWN18-IMG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELMM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ELMM\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "ELLSA\u662f\u9996\u4e2a\u5168\u53cc\u5de5\u3001\u7aef\u5230\u7aef\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u89c6\u89c9\u3001\u6587\u672c\u3001\u8bed\u97f3\u548c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u4eba\u7c7b\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u7684\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684SA-MoE\u67b6\u6784\uff08\u81ea\u6ce8\u610f\u529b\u4e13\u5bb6\u6df7\u5408\uff09\uff0c\u5c06\u5404\u6a21\u6001\u8def\u7531\u5230\u4e13\u7528\u4e13\u5bb6\uff0c\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u8bed\u97f3\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELLSA\u4e0e\u6a21\u6001\u7279\u5b9a\u57fa\u7ebf\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u884c\u4e3a\u3002", "conclusion": "ELLSA\u4ee3\u8868\u4e86\u5411\u66f4\u81ea\u7136\u548c\u901a\u7528\u4ea4\u4e92\u667a\u80fd\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u4eba\u5de5\u901a\u7528\u667a\u80fd\u76ee\u6807\u3002"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u7ec7\u56fe\u4fe1\u606f\u548c\u4f7f\u7528\u89c4\u5212\u4ee3\u7406\u534f\u8c03\u6587\u672c\u4e0e\u89c6\u89c9\u6a21\u6001\uff0c\u89e3\u51b3\u4e86VLMs\u5728\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4e2d\u5b58\u5728\u8f93\u5165\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u7684\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u534f\u8c03\u673a\u5236\u3002", "method": "GraphVista\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\u5c06\u56fe\u4fe1\u606f\u7ec4\u7ec7\u5230\u8f7b\u91cf\u7ea7GraphRAG\u57fa\u7840\u4e2d\uff0c\u4ec5\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u5b50\u56fe\uff1b\u5f15\u5165\u89c4\u5212\u4ee3\u7406\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u8def\u7531\u5230\u6700\u9002\u5408\u7684\u6a21\u6001\u3002", "result": "GraphVista\u80fd\u591f\u6269\u5c55\u5230\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u5927\u578b\u56fe\uff0c\u5728\u8d28\u91cf\u4e0a\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u53474.4\u500d\uff0c\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u3001\u89c6\u89c9\u548c\u878d\u5408\u7684\u65b9\u6cd5\u3002", "conclusion": "GraphVista\u901a\u8fc7\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u6311\u6218\u3002"}}
{"id": "2510.16802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16802", "abs": "https://arxiv.org/abs/2510.16802", "authors": ["Chao Li", "Yuru Wang"], "title": "Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation", "comment": "14 pages", "summary": "Traditional knowledge graphs are constrained by fixed ontologies that\norganize concepts within rigid hierarchical structures. The root cause lies in\ntreating domains as implicit context rather than as explicit, reasoning-level\ncomponents. To overcome these limitations, we propose the Domain-Contextualized\nConcept Graph (CDC), a novel knowledge modeling framework that elevates domains\nto first-class elements of conceptual representation. CDC adopts a C-D-C triple\nstructure - <Concept, Relation@Domain, Concept'> - where domain specifications\nserve as dynamic classification dimensions defined on demand. Grounded in a\ncognitive-linguistic isomorphic mapping principle, CDC operationalizes how\nhumans understand concepts through contextual frames. We formalize more than\ntwenty standardized relation predicates (structural, logical, cross-domain, and\ntemporal) and implement CDC in Prolog for full inference capability. Case\nstudies in education, enterprise knowledge systems, and technical documentation\ndemonstrate that CDC enables context-aware reasoning, cross-domain analogy, and\npersonalized knowledge modeling - capabilities unattainable under traditional\nontology-based frameworks.", "AI": {"tldr": "\u63d0\u51faDomain-Contextualized Concept Graph (CDC)\u6846\u67b6\uff0c\u5c06\u9886\u57df\u4f5c\u4e3a\u77e5\u8bc6\u8868\u793a\u7684\u4e00\u7b49\u5143\u7d20\uff0c\u91c7\u7528<\u6982\u5ff5, \u5173\u7cfb@\u9886\u57df, \u6982\u5ff5>\u7684\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u8de8\u9886\u57df\u7c7b\u6bd4\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u53d7\u9650\u4e8e\u56fa\u5b9a\u672c\u4f53\u8bba\u7684\u521a\u6027\u5c42\u6b21\u7ed3\u6784\uff0c\u6839\u6e90\u5728\u4e8e\u5c06\u9886\u57df\u89c6\u4e3a\u9690\u5f0f\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u63a8\u7406\u7ec4\u4ef6\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5-\u8bed\u8a00\u540c\u6784\u6620\u5c04\u539f\u5219\uff0c\u91c7\u7528C-D-C\u4e09\u5143\u7ec4\u7ed3\u6784\uff0c\u5b9a\u4e4920\u591a\u4e2a\u6807\u51c6\u5316\u5173\u7cfb\u8c13\u8bcd\uff0c\u5e76\u5728Prolog\u4e2d\u5b9e\u73b0\u5b8c\u6574\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6559\u80b2\u3001\u4f01\u4e1a\u77e5\u8bc6\u7cfb\u7edf\u548c\u6280\u672f\u6587\u6863\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cCDC\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u8de8\u9886\u57df\u7c7b\u6bd4\u548c\u4e2a\u6027\u5316\u77e5\u8bc6\u5efa\u6a21\u3002", "conclusion": "CDC\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u57fa\u4e8e\u672c\u4f53\u7684\u77e5\u8bc6\u8868\u793a\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4f20\u7edf\u6846\u67b6\u65e0\u6cd5\u5b9e\u73b0\u7684\u80fd\u529b\u3002"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u4ee3\u7406\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u4ece\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u4ec5\u752880\u4ebf\u53c2\u6570\u5c31\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u4ee3\u7406\u5728\u7279\u5b9a\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u3002\u968f\u7740\u5f3a\u5927LLM\u7684\u51fa\u73b0\uff0c\u4ece\u539f\u59cb\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u53d8\u5f97\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u4ee3\u7406\u8bad\u7ec3\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u4f7fLLM\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9010\u6b65\u83b7\u53d6\u548c\u6574\u5408\u591a\u79cd\u80fd\u529b\uff1b\u540c\u65f6\u5f15\u5165\u6570\u636e\u9a71\u52a8\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepAnalyze-8B\u80fd\u591f\u6267\u884c\u5e7f\u6cdb\u7684\u6570\u636e\u4efb\u52a1\uff0c\u5305\u62ec\u6570\u636e\u95ee\u7b54\u3001\u4e13\u4e1a\u5206\u6790\u4efb\u52a1\u548c\u5f00\u653e\u5f0f\u6570\u636e\u7814\u7a76\uff0c\u5728\u4ec580\u4ebf\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze-8B\u4e3a\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u5176\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3VLM\u4ee3\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u548c\u53cc\u5c42GAE\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u672a\u8bad\u7ec3\u6a21\u578b\u548c\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3VLM\u4ee3\u7406\u4ece\u6587\u672c\u72b6\u6001\u8f6c\u5411\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\u65f6\u9762\u4e34\u7684\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u548c\u4e16\u754c\u5efa\u6a21\u6311\u6218\uff0c\u63a2\u7d22\u4ee3\u7406\u662f\u5426\u80fd\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u4ee3\u7406\u63a8\u7406\u8fc7\u7a0b\u67b6\u6784\u5316\u4e3aPOMDP\uff0c\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u8bbe\u8ba1\u4e86\u4e94\u79cd\u63a8\u7406\u7b56\u7565\uff0c\u5f15\u5165\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u5e76\u63d0\u51fa\u53cc\u5c42GAE\u8fdb\u884c\u56de\u5408\u611f\u77e5\u4fe1\u7528\u5206\u914d\u3002", "result": "3B\u53c2\u6570\u6a21\u578b\u5728\u4e94\u4e2a\u591a\u6837\u5316\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.82\u5206\uff0c\u6bd4\u672a\u8bad\u7ec3\u6a21\u578b(0.21)\u63d0\u53473\u500d\uff0c\u4f18\u4e8eGPT-5(0.75)\u3001Gemini 2.5 Pro(0.67)\u548cClaude 4.5(0.62)\u3002", "conclusion": "\u901a\u8fc7\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0cVLM\u4ee3\u7406\u80fd\u591f\u6709\u6548\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u6700\u4f18\u8868\u793a\u5f62\u5f0f\u53d6\u51b3\u4e8e\u4efb\u52a1\u6027\u8d28\uff0c\u8be5\u65b9\u6cd5\u5728VAGEN\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u591a\u56de\u5408VLM\u4ee3\u7406\u8bad\u7ec3\u3002"}}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u7528\u6237\u662f\u5426\u80fd\u4ece\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u51b3\u7b56\u89e3\u91ca\u4e2d\u8bc6\u522b\u51fa\u667a\u80fd\u4f53\u7684\u76ee\u6807\u3002\u5728Ms. Pacman\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u53d1\u73b0\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u7528\u6237\u666e\u904d\u5bf9\u81ea\u5df1\u7684\u9009\u62e9\u8fc7\u5ea6\u81ea\u4fe1\u3002", "motivation": "\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6838\u5fc3\u5e94\u7528\u662f\u8c03\u8bd5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u76f8\u5bf9\u6027\u80fd\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Atari\u7684Ms. Pacman\u73af\u5883\u548c\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4ed6\u4eec\u662f\u5426\u80fd\u4ece\u51b3\u7b56\u89e3\u91ca\u4e2d\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u76ee\u6807\u3002", "result": "\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\u7684\u51c6\u786e\u7387\uff1b\u7528\u6237\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u7528\u6237\u81ea\u6211\u62a5\u544a\u7684\u8bc6\u522b\u548c\u7406\u89e3\u96be\u6613\u5ea6\u4e0e\u51c6\u786e\u7387\u65e0\u5173\u3002", "conclusion": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u7528\u6237\u7684\u4e3b\u89c2\u611f\u53d7\u4e0e\u5b9e\u9645\u8868\u73b0\u4e0d\u4e00\u81f4\u3002"}}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316GPU\u5185\u6838\u4f18\u5316\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3001\u7ed3\u5408\u6027\u80fd\u5206\u6790\u53cd\u9988\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5185\u6838\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u4f18\u5316\u5bf9AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u867d\u7136LLM\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u5355\u6b21\u751f\u6210\u5668\u6216\u7b80\u5355\u4f18\u5316\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u89c4\u5219\u7684\u5185\u6838\u4f18\u5316\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u57fa\u7840\u6307\u4ee4\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\u6765\u7cfb\u7edf\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6a21\u62df\u4e13\u5bb6\u5de5\u7a0b\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7fLLM\u80fd\u591f\u63a8\u7406\u786c\u4ef6\u6743\u8861\u3001\u6574\u5408\u6027\u80fd\u5206\u6790\u53cd\u9988\u5e76\u8fed\u4ee3\u4f18\u5316\u5185\u6838\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u76f8\u6bd4\u57fa\u7ebf\u667a\u80fd\u4f53\u6709\u663e\u8457\u63d0\u5347\uff1a\u5728\u57fa\u7ebf\u7ecf\u5e38\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8fbe16\u500d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u667a\u80fd\u4f53LLM\u6846\u67b6\u5177\u6709\u63a8\u8fdb\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55GPU\u5185\u6838\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u68c0\u6d4b8\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u4f7f\u4e3bLLM\u80fd\u591f\u4fee\u6b63\u54cd\u5e94\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u7136\u963b\u788d\u5176\u53ef\u9760\u6027\u3002", "method": "ToolCritic\u68c0\u6d4b8\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff08\u5982\u8fc7\u65e9\u8c03\u7528\u3001\u53c2\u6570\u4e0d\u5bf9\u9f50\u3001\u5de5\u5177\u8f93\u51fa\u8bef\u89e3\u7b49\uff09\uff0c\u5e76\u5411\u4e3bLLM\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\u3002\u4e3bLLM\u57fa\u4e8eToolCritic\u7684\u53cd\u9988\u4fee\u6b63\u5176\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToolCritic\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6821\u6b63\u6280\u672f\uff09\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u9ad8\u4e8613%\u3002", "conclusion": "\u8fd9\u662f\u671d\u7740\u5728\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5e94\u7528\u4e2d\u66f4\u7a33\u5065\u5730\u96c6\u6210LLM\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002"}}
{"id": "2510.17064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17064", "abs": "https://arxiv.org/abs/2510.17064", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation", "comment": "22 pages, 6 figures, 2 tables", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse\ncell types and their transcriptomic signatures. However, annotating these\nsignatures-especially those involving poorly characterized genes-remains a\nmajor challenge. Traditional methods, such as Gene Set Enrichment Analysis\n(GSEA), depend on well-curated annotations and often perform poorly in these\ncontexts. Large Language Models (LLMs) offer a promising alternative but\nstruggle to represent complex biological knowledge within structured\nontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:\nhttps://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that\nintegrates free-text descriptions with ontology labels to enable more accurate\nand robust gene set annotation. By incorporating retrieval-augmented generation\n(RAG), we developed a robust agentic workflow that refines predictions using\nrelevant PubMed literature, reducing hallucinations and enhancing\ninterpretability. Using this workflow, we achieved correct annotations for 77%\nof mouse gene sets among their top predictions. Applying this approach, we\nannotated 5,322 brain cell clusters from the comprehensive mouse brain cell\natlas generated by the BRAIN Initiative Cell Census Network, enabling novel\ninsights into brain cell function by identifying region-specific gene\nco-expression patterns and inferring functional roles of gene ensembles.\nBRAINCELL-AID also identifies Basal Ganglia-related cell types with\nneurologically meaningful descriptions. Hence, we create a valuable resource to\nsupport community-driven cell type annotation.", "AI": {"tldr": "BRAINCELL-AID\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u548c\u672c\u4f53\u6807\u7b7e\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u56e0\u96c6\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u8111\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u4e2d\u53d6\u5f97\u4e8677%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u56e0\u96c6\u5bcc\u96c6\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u6ce8\u91ca\uff0c\u5728\u5904\u7406\u6d89\u53ca\u7279\u5f81\u4e0d\u826f\u57fa\u56e0\u7684\u8f6c\u5f55\u7ec4\u7279\u5f81\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u672c\u4f53\u4e2d\u8868\u793a\u590d\u6742\u751f\u7269\u5b66\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86BRAINCELL-AID\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u6574\u5408\u81ea\u7531\u6587\u672c\u63cf\u8ff0\u4e0e\u672c\u4f53\u6807\u7b7e\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u6784\u5efa\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7PubMed\u6587\u732e\u7cbe\u70bc\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728\u5c0f\u9f20\u57fa\u56e0\u96c6\u7684\u6ce8\u91ca\u4e2d\uff0c77%\u7684\u57fa\u56e0\u96c6\u5728\u5176\u524d\u51e0\u4e2a\u9884\u6d4b\u4e2d\u83b7\u5f97\u4e86\u6b63\u786e\u6ce8\u91ca\u3002\u6210\u529f\u6ce8\u91ca\u4e86\u6765\u81eaBRAIN Initiative Cell Census Network\u751f\u6210\u76845,322\u4e2a\u8111\u7ec6\u80de\u7c07\uff0c\u8bc6\u522b\u4e86\u533a\u57df\u7279\u5f02\u6027\u57fa\u56e0\u5171\u8868\u8fbe\u6a21\u5f0f\uff0c\u5e76\u63a8\u65ad\u57fa\u56e0\u96c6\u5408\u7684\u529f\u80fd\u4f5c\u7528\u3002", "conclusion": "BRAINCELL-AID\u521b\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u793e\u533a\u9a71\u52a8\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u80fd\u591f\u8bc6\u522b\u5177\u6709\u795e\u7ecf\u5b66\u610f\u4e49\u63cf\u8ff0\u7684\u57fa\u5e95\u795e\u7ecf\u8282\u76f8\u5173\u7ec6\u80de\u7c7b\u578b\uff0c\u4e3a\u8111\u7ec6\u80de\u529f\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u7528\u4e8e\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u7684\u8bc1\u636e\u63a8\u7406\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u548c\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf(KPD-MADS)\uff0c\u540e\u8005\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u7684\u6279\u5224\u6027\u5bf9\u8bdd\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u5b9a\u6027\u975e\u8d22\u52a1\u6307\u6807\u7684\u81ea\u52a8\u5316\u63a8\u7406\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u4e13\u4e1a\u8d37\u6b3e\u8bc4\u4f30\u6240\u9700\u89e3\u91ca\u6027\u5224\u65ad\u7684\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdLLM\u7cfb\u7edf\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u901a\u8fc7\u5355\u6b21\u63a8\u7406\u7ba1\u9053\u751f\u6210\u53cc\u5411\u5206\u6790\uff1b\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf(KPD-MADS)\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u6279\u5224\u6027\u5bf9\u8bdd\u6846\u67b6\uff0c\u91c7\u7528\u5341\u6b65\u7ed3\u6784\u5316\u4ea4\u4e92\u534f\u8bae\u8fdb\u884c\u5bf9\u6297\u6027\u9a8c\u8bc1\u3002", "result": "\u4e24\u4e2a\u7cfb\u7edf\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u751f\u4ea7\u529b\u63d0\u5347(NAS: 11.55\u79d2/\u6848\u4f8b\uff1bKPD-MADS: 91.97\u79d2\uff1b\u4eba\u5de5\u57fa\u51c6: 1920\u79d2)\u3002KPD-MADS\u5728\u89e3\u91ca\u5145\u5206\u6027(4.0 vs 3.0)\u3001\u5b9e\u9645\u9002\u7528\u6027(4.0 vs 3.0)\u548c\u53ef\u7528\u6027(62.5 vs 52.5)\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u8bc4\u5206\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u4ea4\u4e92\u53ef\u4ee5\u589e\u5f3a\u91d1\u878dAI\u4e2d\u7684\u63a8\u7406\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4f01\u4e1a\u4fe1\u7528\u8bc4\u4f30\u4e2d\u53ef\u6269\u5c55\u4e14\u53ef\u8fa9\u62a4\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2510.17145", "categories": ["cs.AI", "68T05, 62H30"], "pdf": "https://arxiv.org/pdf/2510.17145", "abs": "https://arxiv.org/abs/2510.17145", "authors": ["Phi-Hung Hoang", "Nam-Thuan Trinh", "Van-Manh Tran", "Thi-Thu-Hong Phan"], "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion", "comment": "35 pages, 6 figures and 11 tables", "summary": "Accurate assessment of fish freshness remains a major challenge in the food\nindustry, with direct consequences for product quality, market value, and\nconsumer health. Conventional sensory evaluation is inherently subjective,\ninconsistent, and difficult to standardize across contexts, often limited by\nsubtle, species-dependent spoilage cues. To address these limitations, we\npropose a handcrafted feature-based approach that systematically extracts and\nincrementally fuses complementary descriptors, including color statistics,\nhistograms across multiple color spaces, and texture features such as Local\nBinary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish\neye images. Our method captures global chromatic variations from full images\nand localized degradations from ROI segments, fusing each independently to\nevaluate their effectiveness in assessing freshness. Experiments on the\nFreshness of the Fish Eyes (FFE) dataset demonstrate the approach's\neffectiveness: in a standard train-test setting, a LightGBM classifier achieved\n77.56% accuracy, a 14.35% improvement over the previous deep learning baseline\nof 63.21%. With augmented data, an Artificial Neural Network (ANN) reached\n97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results\ndemonstrate that carefully engineered, handcrafted features, when strategically\nprocessed, yield a robust, interpretable, and reliable solution for automated\nfish freshness assessment, providing valuable insights for practical\napplications in food quality monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u5de5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u878d\u5408\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u4ee5\u53ca\u7eb9\u7406\u7279\u5f81\u6765\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\uff0c\u5728FFE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u611f\u5b98\u8bc4\u4f30\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u96be\u4ee5\u6807\u51c6\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5ba2\u89c2\u3001\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ece\u9c7c\u773c\u56fe\u50cf\u4e2d\u7cfb\u7edf\u63d0\u53d6\u624b\u5de5\u7279\u5f81\uff0c\u5305\u62ec\u989c\u8272\u7edf\u8ba1\u3001\u591a\u8272\u5f69\u7a7a\u95f4\u76f4\u65b9\u56fe\u3001LBP\u548cGLCM\u7eb9\u7406\u7279\u5f81\uff0c\u878d\u5408\u5168\u5c40\u8272\u5ea6\u53d8\u5316\u548c\u5c40\u90e8ROI\u9000\u5316\u7279\u5f81\uff0c\u4f7f\u7528LightGBM\u548cANN\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6807\u51c6\u8bad\u7ec3\u6d4b\u8bd5\u8bbe\u7f6e\u4e0bLightGBM\u8fbe\u523077.56%\u51c6\u786e\u7387\uff0c\u6bd4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u534714.35%\uff1b\u6570\u636e\u589e\u5f3a\u540eANN\u8fbe\u523097.16%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534719.86%\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u624b\u5de5\u7279\u5f81\u7ecf\u8fc7\u7b56\u7565\u6027\u5904\u7406\u540e\uff0c\u80fd\u591f\u4e3a\u9c7c\u7c7b\u65b0\u9c9c\u5ea6\u81ea\u52a8\u8bc4\u4f30\u63d0\u4f9b\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u98df\u54c1\u8d28\u91cf\u76d1\u63a7\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17146", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.17146", "abs": "https://arxiv.org/abs/2510.17146", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation", "comment": "NeurIPS 2025 Workshop of UrbanAI (Oral)", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a\nsubstantial share of global building energy use, making reliable anomaly\ndetection essential for improving efficiency and reducing emissions. Classical\nrule-based approaches offer explainability but lack adaptability, while deep\nlearning methods provide predictive power at the cost of transparency,\nefficiency, and physical plausibility. Recent attempts to use Large Language\nModels (LLMs) for anomaly detection improve interpretability but largely ignore\nthe physical principles that govern HVAC operations. We present PILLM, a\nPhysics-Informed LLM framework that operates within an evolutionary loop to\nautomatically generate, evaluate, and refine anomaly detection rules. Our\napproach introduces physics-informed reflection and crossover operators that\nembed thermodynamic and control-theoretic constraints, enabling rules that are\nboth adaptive and physically grounded. Experiments on the public Building Fault\nDetection dataset show that PILLM achieves state-of-the-art performance while\nproducing diagnostic rules that are interpretable and actionable, advancing\ntrustworthy and deployable AI for smart building systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86PILLM\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316HVAC\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "HVAC\u7cfb\u7edf\u80fd\u8017\u5360\u5efa\u7b51\u80fd\u8017\u5f88\u5927\u6bd4\u4f8b\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u9002\u5e94\u6027\uff08\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff09\uff0c\u8981\u4e48\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff08\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u9ad8\u6548\u53c8\u53ef\u4fe1\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "PILLM\u6846\u67b6\u5728\u8fdb\u5316\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u5f15\u5165\u7269\u7406\u611f\u77e5\u7684\u53cd\u601d\u548c\u4ea4\u53c9\u7b97\u5b50\uff0c\u5d4c\u5165\u70ed\u529b\u5b66\u548c\u63a7\u5236\u7406\u8bba\u7ea6\u675f\uff0c\u81ea\u52a8\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u89c4\u5219\u3002", "result": "\u5728\u516c\u5171\u5efa\u7b51\u6545\u969c\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPILLM\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u751f\u6210\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u89c4\u5219\u3002", "conclusion": "PILLM\u63a8\u8fdb\u4e86\u667a\u80fd\u5efa\u7b51\u7cfb\u7edf\u4e2d\u53ef\u4fe1\u8d56\u548c\u53ef\u90e8\u7f72\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2510.17149", "categories": ["cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.17149", "abs": "https://arxiv.org/abs/2510.17149", "authors": ["Hongyi Du", "Jiaqi Su", "Jisen Li", "Lijie Ding", "Yingxuan Yang", "Peixuan Han", "Xiangru Tang", "Kunlun Zhu", "Jiaxuan You"], "title": "Which LLM Multi-Agent Protocol to Choose?", "comment": "Under review at ICLR 2026.Code and benchmark artifacts:\n  https://github.com/ulab-uiuc/AgentProtocols", "summary": "As large-scale multi-agent systems evolve, the communication protocol layer\nhas become a critical yet under-evaluated factor shaping performance and\nreliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,\netc.), selection is often intuition-driven and lacks standardized guidance. We\nintroduce ProtocolBench, a benchmark that systematically compares agent\nprotocols along four measurable axes: task success, end-to-end latency, message\nor byte overhead, and robustness under failures. On ProtocolBench, protocol\nchoice significantly influences system behavior. In the Streaming Queue\nscenario, overall completion time varies by up to 36.5% across protocols, and\nmean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,\nresilience also differs consistently across protocols. Beyond evaluation, we\npresent ProtocolRouter, a learnable protocol router that selects per-scenario\n(or per-module) protocols from requirement and runtime signals. ProtocolRouter\nreduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol\nbaseline, and achieves scenario-specific gains such as higher success in GAIA.\nWe also release ProtocolRouterBench to standardize protocol evaluation and\nimprove reliability at scale.", "AI": {"tldr": "ProtocolBench\u662f\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u4fe1\u534f\u8bae\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cProtocolRouter\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u534f\u8bae\u8def\u7531\u5668\uff0c\u80fd\u6839\u636e\u573a\u666f\u9700\u6c42\u52a8\u6001\u9009\u62e9\u6700\u4f18\u534f\u8bae\u3002", "motivation": "\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5bfc\uff0c\u5f53\u524d\u9009\u62e9\u5f80\u5f80\u57fa\u4e8e\u76f4\u89c9\u800c\u975e\u6570\u636e\u9a71\u52a8\u3002", "method": "\u5f00\u53d1ProtocolBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u4efb\u52a1\u6210\u529f\u7387\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u6d88\u606f\u5f00\u9500\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u56db\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u534f\u8bae\uff1b\u63d0\u51faProtocolRouter\u5b66\u4e60\u578b\u534f\u8bae\u8def\u7531\u5668\uff0c\u6839\u636e\u9700\u6c42\u548c\u8fd0\u884c\u65f6\u4fe1\u53f7\u52a8\u6001\u9009\u62e9\u534f\u8bae\u3002", "result": "\u534f\u8bae\u9009\u62e9\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\uff1a\u5728Streaming Queue\u573a\u666f\u4e2d\uff0c\u5b8c\u6210\u65f6\u95f4\u5dee\u5f02\u8fbe36.5%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u5dee\u5f023.48\u79d2\uff1bProtocolRouter\u76f8\u6bd4\u6700\u4f73\u5355\u534f\u8bae\u57fa\u7ebf\uff0c\u6545\u969c\u6062\u590d\u65f6\u95f4\u51cf\u5c1118.1%\uff0c\u5728GAIA\u573a\u666f\u4e2d\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cProtocolBench\u548cProtocolRouter\u4e3a\u534f\u8bae\u8bc4\u4f30\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.17172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17172", "abs": "https://arxiv.org/abs/2510.17172", "authors": ["Shun Huang", "Wenlu Xing", "Shijia Geng", "Hailong Wang", "Guangkun Nie", "Gongzheng Tang", "Chenyang He", "Shenda Hong"], "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients", "comment": null, "summary": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial\ninfarction (AMI) are a major cause of in-hospital death, yet early\nidentification remains a clinical challenge. While traditional risk scores have\nlimited performance, end-to-end deep learning models often lack the\ninterpretability needed for clinical trust. This study aimed to develop a\nhybrid predictive framework that integrates a large-scale electrocardiogram\n(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to\nimprove both accuracy and interpretability. We analyzed 6,634 ECG recordings\nfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder\nmodel was used to extract 150-dimensional diagnostic probability features ,\nwhich were then refined through feature selection to train the XGBoost\nclassifier. Model performance was evaluated using AUC and F1-score , and the\nSHAP method was used for interpretability. The ECGFounder + XGBoost hybrid\nmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC\n0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that\nmodel-identified key features, such as \"premature ventricular complexes\" (risk\npredictor) and \"normal sinus rhythm\" (protective factor), were highly\nconsistent with clinical knowledge. We conclude that this hybrid framework\nprovides a novel paradigm for VT/VF risk prediction by validating the use of\nfoundation model outputs as effective, automated feature engineering for\nbuilding trustworthy, explainable AI-based clinical decision support systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408ECG\u57fa\u7840\u6a21\u578b\u548c\u53ef\u89e3\u91caXGBoost\u5206\u7c7b\u5668\u7684\u6df7\u5408\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u98ce\u9669\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u540e\u6076\u6027\u5ba4\u6027\u5fc3\u5f8b\u5931\u5e38\u662f\u9662\u5185\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f20\u7edf\u98ce\u9669\u8bc4\u5206\u6027\u80fd\u6709\u9650\uff0c\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528ECGFounder\u57fa\u7840\u6a21\u578b\u63d0\u53d6150\u7ef4\u8bca\u65ad\u6982\u7387\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u540e\u8bad\u7ec3XGBoost\u5206\u7c7b\u5668\uff0c\u5e76\u7528SHAP\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6df7\u5408\u6a21\u578bAUC\u8fbe\u52300.801\uff0c\u4f18\u4e8eKNN(0.677)\u3001RNN(0.676)\u548c1D-CNN(0.720)\uff0cSHAP\u5206\u6790\u663e\u793a\u6a21\u578b\u8bc6\u522b\u7279\u5f81\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u6709\u6548\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u63d0\u4f9b\u4e86\u9a8c\u8bc1\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684AI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u9875\u90e8\u7f72\u3001\u5de5\u5177\u589e\u5f3a\u7684LLM\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u53d1\u73b0\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u4f1a\u635f\u5bb3\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\uff0c\u7279\u522b\u662f\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\u3002\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u4ee5\u6539\u5584\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5de5\u5177\u589e\u5f3a\u7684LLM\u5065\u5eb7\u6559\u7ec3\u5728\u771f\u5b9e\u7528\u6237\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u4e2a\u6027\u5316\u7b56\u7565\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u907f\u514d\u5bf9\u7279\u5b9a\u5b50\u7fa4\u4f53\u7684\u6f5c\u5728\u4f24\u5bb3\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u51b3\u7b56\u5934\uff08\u5de5\u5177/\u98ce\u683c\uff09\u5206\u6790\u7b56\u7565\u6548\u679c\uff1b\u6784\u5efa\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u9a8c\u8bc1\u6dfb\u52a0\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u7684\u6548\u679c\uff1b\u91c7\u7528\u51bb\u7ed3\u751f\u6210\u5668\u3001\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u51b3\u7b56\u5934\u7684\u4e2a\u6027\u5316\u8def\u5f84\u3002", "result": "\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u867d\u7136\u63d0\u9ad8\u4e86\u5e73\u5747\u4ef7\u503c\uff0c\u4f46\u635f\u5bb3\u4e86\u4f4e\u5065\u5eb7\u7d20\u517b/\u9ad8\u81ea\u6211\u6548\u80fd\u7528\u6237\u7fa4\u4f53\uff1b\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u53ef\u4ee5\u7f29\u77ed\u7279\u8d28\u8bc6\u522b\u65f6\u95f4\uff0c\u63d0\u9ad8\u76ee\u6807\u6210\u529f\u7387\u548cpass@3\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u8bc4\u4f30\u4f18\u5148\u7684\u4e2a\u6027\u5316\u8def\u5f84\uff1a\u51bb\u7ed3\u751f\u6210\u5668\uff0c\u57fa\u4e8e\u7c7b\u578b\u5316\u5956\u52b1\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u51b3\u7b56\u5934\uff0c\u5e76\u59cb\u7ec8\u62a5\u544a\u6309\u539f\u578b\u5206\u7c7b\u7684\u6307\u6807\u4ee5\u63ed\u793a\u88ab\u5e73\u5747\u503c\u63a9\u76d6\u7684\u5b50\u7fa4\u4f53\u4f24\u5bb3\u95ee\u9898\u3002"}}
{"id": "2510.17211", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17211", "abs": "https://arxiv.org/abs/2510.17211", "authors": ["Tingsong Xiao", "Yao An Lee", "Zelin Xu", "Yupu Zhang", "Zibo Liu", "Yu Huang", "Jiang Bian", "Serena Jingchuan Guo", "Zhe Jiang"], "title": "Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling", "comment": null, "summary": "Disease progression modeling aims to characterize and predict how a patient's\ndisease complications worsen over time based on longitudinal electronic health\nrecords (EHRs). Accurate modeling of disease progression, such as type 2\ndiabetes, can enhance patient sub-phenotyping and inform effective and timely\ninterventions. However, the problem is challenging due to the need to learn\ncontinuous-time dynamics of progression patterns based on irregular-time event\nsamples and patient heterogeneity (\\eg different progression rates and\npathways). Existing mechanistic and data-driven methods either lack\nadaptability to learn from real-world data or fail to capture complex\ncontinuous-time dynamics on progression trajectories. To address these\nlimitations, we propose Temporally Detailed Hypergraph Neural Ordinary\nDifferential Equation (TD-HNODE), which represents disease progression on\nclinically recognized trajectories as a temporally detailed hypergraph and\nlearns the continuous-time progression dynamics via a neural ODE framework.\nTD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the\ninterdependency of disease complication markers within both intra- and\ninter-progression trajectories. Experiments on two real-world clinical datasets\ndemonstrate that TD-HNODE outperforms multiple baselines in modeling the\nprogression of type 2 diabetes and related cardiovascular diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e86TD-HNODE\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\u548c\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u57282\u578b\u7cd6\u5c3f\u75c5\u548c\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u57fa\u4e8e\u4e0d\u89c4\u5219\u65f6\u95f4\u4e8b\u4ef6\u6837\u672c\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u4ee5\u53ca\u5904\u7406\u60a3\u8005\u5f02\u8d28\u6027\uff08\u4e0d\u540c\u8fdb\u5c55\u901f\u7387\u548c\u8def\u5f84\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u4ece\u771f\u5b9e\u6570\u636e\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u3002", "method": "TD-HNODE\u6a21\u578b\u5c06\u75be\u75c5\u8fdb\u5c55\u8868\u793a\u4e3a\u65f6\u95f4\u8be6\u7ec6\u8d85\u56fe\uff0c\u901a\u8fc7\u795e\u7ecfODE\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u8fdb\u5c55\u52a8\u6001\u3002\u5305\u542b\u53ef\u5b66\u4e60\u7684TD-Hypergraph Laplacian\uff0c\u6355\u6349\u75be\u75c5\u5e76\u53d1\u75c7\u6807\u5fd7\u7269\u5728\u8fdb\u5c55\u8f68\u8ff9\u5185\u548c\u8f68\u8ff9\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTD-HNODE\u5728\u5efa\u6a212\u578b\u7cd6\u5c3f\u75c5\u548c\u76f8\u5173\u5fc3\u8840\u7ba1\u75be\u75c5\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TD-HNODE\u80fd\u591f\u6709\u6548\u5efa\u6a21\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u590d\u6742\u52a8\u6001\u6355\u6349\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.17235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17235", "abs": "https://arxiv.org/abs/2510.17235", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis", "comment": null, "summary": "The cryptocurrency market offers significant investment opportunities but\nfaces challenges including high volatility and fragmented information. Data\nintegration and analysis are essential for informed investment decisions.\nCurrently, investors use three main approaches: (1) Manual analysis across\nvarious sources, which depends heavily on individual experience and is\ntime-consuming and prone to bias; (2) Data aggregation platforms-limited in\nfunctionality and depth of analysis; (3) Large language model agents-based on\nstatic pretrained models, lacking real-time data integration and multi-step\nreasoning capabilities. To address these limitations, we present Coinvisor, a\nreinforcement learning-based chatbot that provides comprehensive analytical\nsupport for cryptocurrency investment through a multi-agent framework.\nCoinvisor integrates diverse analytical capabilities through specialized tools.\nIts key innovation is a reinforcement learning-based tool selection mechanism\nthat enables multi-step planning and flexible integration of diverse data\nsources. This design supports real-time interaction and adaptive analysis of\ndynamic content, delivering accurate and actionable investment insights. We\nevaluated Coinvisor through automated benchmarks on tool calling accuracy and\nuser studies with 20 cryptocurrency investors using our interface. Results show\nthat Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base\nmodel in tool orchestration. User studies show high satisfaction (4.64/5), with\nparticipants preferring Coinvisor to both general LLMs and existing crypto\nplatforms (4.62/5).", "AI": {"tldr": "Coinvisor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u9009\u62e9\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6295\u8d44\u5206\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff1a\u624b\u52a8\u5206\u6790\u4f9d\u8d56\u7ecf\u9a8c\u4e14\u8017\u65f6\u3001\u6570\u636e\u805a\u5408\u5e73\u53f0\u529f\u80fd\u6709\u9650\u3001\u57fa\u4e8e\u9759\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u7684LLM\u4ee3\u7406\u7f3a\u4e4f\u5b9e\u65f6\u6570\u636e\u96c6\u6210\u548c\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u5de5\u5177\u96c6\u6210\u591a\u6837\u5316\u5206\u6790\u80fd\u529b\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u5de5\u5177\u9009\u62e9\u673a\u5236\uff0c\u652f\u6301\u591a\u6b65\u89c4\u5212\u548c\u7075\u6d3b\u6570\u636e\u6e90\u96c6\u6210\u3002", "result": "\u5728\u5de5\u5177\u7f16\u6392\u65b9\u9762\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u53ec\u56de\u7387\u63d0\u534740.7%\uff0cF1\u5206\u6570\u63d0\u534726.6%\uff1b\u7528\u6237\u7814\u7a76\u663e\u793a\u9ad8\u6ee1\u610f\u5ea6\uff084.64/5\uff09\uff0c\u53c2\u4e0e\u8005\u66f4\u504f\u597dCoinvisor\u800c\u975e\u901a\u7528LLM\u548c\u73b0\u6709\u52a0\u5bc6\u5e73\u53f0\uff084.62/5\uff09\u3002", "conclusion": "Coinvisor\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a0\u5bc6\u8d27\u5e01\u6295\u8d44\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u53ef\u64cd\u4f5c\u7684\u6295\u8d44\u6d1e\u5bdf\uff0c\u5728\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.17309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17309", "abs": "https://arxiv.org/abs/2510.17309", "authors": ["Thorsten Fr\u00f6hlich", "Tim Schlippe"], "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "comment": null, "summary": "The evaluation of academic theses is a cornerstone of higher education,\nensuring rigor and integrity. Traditional methods, though effective, are\ntime-consuming and subject to evaluator variability. This paper presents\nRubiSCoT, an AI-supported framework designed to enhance thesis evaluation from\nproposal to final submission. Using advanced natural language processing\ntechniques, including large language models, retrieval-augmented generation,\nand structured chain-of-thought prompting, RubiSCoT offers a consistent,\nscalable solution. The framework includes preliminary assessments,\nmultidimensional assessments, content extraction, rubric-based scoring, and\ndetailed reporting. We present the design and implementation of RubiSCoT,\ndiscussing its potential to optimize academic assessment processes through\nconsistent, scalable, and transparent evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86RubiSCoT\u6846\u67b6\uff0c\u4f7f\u7528AI\u6280\u672f\u589e\u5f3a\u8bba\u6587\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u4ece\u63d0\u6848\u5230\u6700\u7ec8\u63d0\u4ea4\u63d0\u4f9b\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u8bba\u6587\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u5b58\u5728\u8bc4\u4f30\u8005\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u63d0\u4f9b\u521d\u6b65\u8bc4\u4f30\u3001\u591a\u7ef4\u8bc4\u4f30\u3001\u5185\u5bb9\u63d0\u53d6\u3001\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u5206\u548c\u8be6\u7ec6\u62a5\u544a\u3002", "result": "\u8bbe\u8ba1\u4e86RubiSCoT\u6846\u67b6\u5e76\u5b9e\u73b0\u4e86\u5176\u529f\u80fd\uff0c\u5c55\u793a\u4e86\u4f18\u5316\u5b66\u672f\u8bc4\u4f30\u8fc7\u7a0b\u7684\u6f5c\u529b\u3002", "conclusion": "RubiSCoT\u6846\u67b6\u901a\u8fc7\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u8bc4\u4f30\uff0c\u6709\u671b\u4f18\u5316\u5b66\u672f\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.", "AI": {"tldr": "\u63d0\u51faLaGAT\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u795e\u7ecfMAPF\u7b56\u7565MAGAT\u96c6\u6210\u5230\u641c\u7d22\u7b97\u6cd5LaCAM\u4e2d\uff0c\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u573a\u666f\u4e2d\u4f18\u4e8e\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u7ed3\u5408\u5b66\u4e60\u548c\u641c\u7d22\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u7684MAGAT\u67b6\u6784\uff0c\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u7b56\u7565\uff0c\u5e76\u52a0\u5165\u6b7b\u9501\u68c0\u6d4b\u673a\u5236\u6765\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u795e\u7ecf\u5f15\u5bfc\u3002", "result": "LaGAT\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u7eaf\u641c\u7d22\u548c\u7eaf\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u641c\u7d22\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6df7\u5408\u641c\u7d22\u65b9\u6cd5\u4e3a\u7d27\u5bc6\u8026\u5408\u7684\u6311\u6218\u6027\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86FBI_LTL\uff0c\u4e00\u79cd\u7528\u4e8e\u4eff\u771f\u89c4\u5212\u95ee\u9898\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u53ea\u751f\u6210\u5355\u4e00\u8ba1\u5212\uff0c\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u504f\u597d\u3002\u73b0\u6709\u591a\u6837\u5316\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FBI_LTL\u5c06\u57fa\u4e8eLTL\u7684\u591a\u6837\u6027\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u641c\u7d22\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cFBI_LTL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bed\u4e49\u5f15\u5bfc\u591a\u6837\u5316\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u65b9\u6cd5\u5931\u6548\u7684\u73b0\u5b9e\u975e\u7b26\u53f7\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.17450", "categories": ["cs.AI", "H.4.2; I.2.3; I.2.6; I.2.8; I.2.9; J.7"], "pdf": "https://arxiv.org/pdf/2510.17450", "abs": "https://arxiv.org/abs/2510.17450", "authors": ["Johan Schubert", "Farzad Kamrani", "Tove Gustavi"], "title": "Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions", "comment": "Presented at the 6th International Workshop on Active Inference,\n  15-17 October 2025, Montreal, Canada", "summary": "We develop an active inference route-planning method for the autonomous\ncontrol of intelligent agents. The aim is to reconnoiter a geographical area to\nmaintain a common operational picture. To achieve this, we construct an\nevidence map that reflects our current understanding of the situation,\nincorporating both positive and \"negative\" sensor observations of possible\ntarget objects collected over time, and diffusing the evidence across the map\nas time progresses. The generative model of active inference uses\nDempster-Shafer theory and a Gaussian sensor model, which provides input to the\nagent. The generative process employs a Bayesian approach to update a posterior\nprobability distribution. We calculate the variational free energy for all\npositions within the area by assessing the divergence between a pignistic\nprobability distribution of the evidence map and a posterior probability\ndistribution of a target object based on the observations, including the level\nof surprise associated with receiving new observations. Using the free energy,\nwe direct the agents' movements in a simulation by taking an incremental step\ntoward a position that minimizes the free energy. This approach addresses the\nchallenge of exploration and exploitation, allowing agents to balance searching\nextensive areas of the geographical map while tracking identified target\nobjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u81ea\u4e3b\u63a7\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u8bc1\u636e\u5730\u56fe\u548c\u8ba1\u7b97\u53d8\u5206\u81ea\u7531\u80fd\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u4fa6\u5bdf\u5730\u7406\u533a\u57df\u4ee5\u7ef4\u6301\u5171\u540c\u4f5c\u6218\u6001\u52bf\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u5728\u63a2\u7d22\u5e7f\u9614\u533a\u57df\u4e0e\u8ddf\u8e2a\u5df2\u8bc6\u522b\u76ee\u6807\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u548c\u9ad8\u65af\u4f20\u611f\u5668\u6a21\u578b\u6784\u5efa\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u66f4\u65b0\u540e\u9a8c\u6982\u7387\u5206\u5e03\uff0c\u8ba1\u7b97\u53d8\u5206\u81ea\u7531\u80fd\u6765\u6307\u5bfc\u4ee3\u7406\u79fb\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4eff\u771f\u4e2d\u6709\u6548\u6307\u5bfc\u4ee3\u7406\u79fb\u52a8\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u7684\u4f4d\u7f6e\u9009\u62e9\u6765\u5b9e\u73b0\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u3002", "conclusion": "\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u667a\u80fd\u4ee3\u7406\u5728\u5730\u7406\u4fa6\u5bdf\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17463", "abs": "https://arxiv.org/abs/2510.17463", "authors": ["Cor Steging", "Tadeusz Zbiegie\u0144"], "title": "Label Indeterminacy in AI & Law", "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 38th International Conference on Legal Knowledge and Information\n  Systems (JURIX) in Turin, December 9 to 11 of 2025", "summary": "Machine learning is increasingly used in the legal domain, where it typically\noperates retrospectively by treating past case outcomes as ground truth.\nHowever, legal outcomes are often shaped by human interventions that are not\ncaptured in most machine learning approaches. A final decision may result from\na settlement, an appeal, or other procedural actions. This creates label\nindeterminacy: the outcome could have been different if the intervention had or\nhad not taken place. We argue that legal machine learning applications need to\naccount for label indeterminacy. Methods exist that can impute these\nindeterminate labels, but they are all grounded in unverifiable assumptions. In\nthe context of classifying cases from the European Court of Human Rights, we\nshow that the way that labels are constructed during training can significantly\naffect model behaviour. We therefore position label indeterminacy as a relevant\nconcern in AI & Law and demonstrate how it can shape model behaviour.", "AI": {"tldr": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u9700\u8981\u5904\u7406\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u4e3a\u6cd5\u5f8b\u7ed3\u679c\u5e38\u53d7\u5230\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5bfc\u81f4\u6700\u7ec8\u7ed3\u679c\u53ef\u80fd\u4e0d\u540c\u3002\u672c\u6587\u5728\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4f8b\u5206\u7c7b\u4e2d\u5c55\u793a\u4e86\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5982\u4f55\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u901a\u5e38\u5c06\u8fc7\u53bb\u6848\u4f8b\u7ed3\u679c\u89c6\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u4f46\u6cd5\u5f8b\u7ed3\u679c\u5e38\u53d7\u4eba\u4e3a\u5e72\u9884\u5f71\u54cd\uff0c\u5982\u548c\u89e3\u3001\u4e0a\u8bc9\u7b49\uff0c\u9020\u6210\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u5728\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e2d\u672a\u88ab\u5145\u5206\u8003\u8651\u3002", "method": "\u5728\u6b27\u6d32\u4eba\u6743\u6cd5\u9662\u6848\u4f8b\u5206\u7c7b\u7684\u80cc\u666f\u4e0b\uff0c\u5206\u6790\u4e0d\u540c\u6807\u7b7e\u6784\u5efa\u65b9\u5f0f\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u5904\u7406\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6807\u7b7e\u7684\u6784\u5efa\u65b9\u5f0f\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u9700\u8981\u5173\u6ce8\u7684\u91cd\u8981\u95ee\u9898\u3002", "conclusion": "\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u662fAI\u4e0e\u6cd5\u5f8b\u9886\u57df\u7684\u4e00\u4e2a\u76f8\u5173\u5173\u5207\u70b9\uff0c\u5b83\u80fd\u591f\u5851\u9020\u6a21\u578b\u884c\u4e3a\uff0c\u6cd5\u5f8b\u673a\u5668\u5b66\u4e60\u5e94\u7528\u9700\u8981\u8003\u8651\u548c\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9a8c\u8bc1\u8fc7\u7a0b\u4e3a\u56db\u4e2a\u6a21\u5757\uff1a\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u3001\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u4e8b\u5b9e\u6838\u67e5\u548c\u6821\u51c6\u5224\u65ad\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u4e0a\u6bcf\u5929\u6709\u6570\u5341\u4ebf\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5e16\u5b50\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u4eba\u5de5\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u76d1\u7763\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "method": "MIRAGE\u6846\u67b6\u5c06\u591a\u6a21\u6001\u9a8c\u8bc1\u5206\u89e3\u4e3a\u56db\u4e2a\u987a\u5e8f\u6a21\u5757\uff1a\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u8bc6\u522b\u4e0a\u4e0b\u6587\u4e0d\u5f53\u91cd\u7528\uff0c\u68c0\u7d22\u589e\u5f3a\u4e8b\u5b9e\u6838\u67e5\u901a\u8fc7\u8fed\u4ee3\u95ee\u9898\u751f\u6210\u5c06\u58f0\u660e\u57fa\u4e8e\u7f51\u7edc\u8bc1\u636e\uff0c\u6821\u51c6\u5224\u65ad\u6a21\u5757\u6574\u5408\u6240\u6709\u4fe1\u53f7\u3002", "result": "\u5728MMFakeBench\u9a8c\u8bc1\u96c6\u4e0a\uff0cMIRAGE\u4e0eGPT-4o-mini\u7ec4\u5408\u8fbe\u523081.65% F1\u548c75.1%\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u96f6\u6837\u672c\u57fa\u7ebf\uff08GPT-4V\u4e0eMMD-Agent\u768474.0% F1\uff09\u63d0\u53477.65\u70b9\uff0c\u540c\u65f6\u4fdd\u630134.3%\u7684\u5047\u9633\u6027\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4ec5\u5224\u65ad\u57fa\u7ebf\u768497.3%\u3002\u6d4b\u8bd5\u96c6\u7ed3\u679c\u786e\u8ba4\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5206\u89e3\u7684\u4ee3\u7406\u63a8\u7406\u4e0e\u7f51\u7edc\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u76d1\u7763\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u591a\u6a21\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3002"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u6765\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u7684\u6807\u8bb0\u9884\u6d4b\uff0c\u66f4\u9700\u8981\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u8fd9\u4e9b\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5c0f\u6a21\u578b\u6a21\u62df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5b66\u4e60\u8bc6\u522b\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u8def\u5f84\uff0c\u5efa\u7acb\u95ee\u9898\u5b9a\u4e49\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u5728MBPP\u3001MBPP Plus\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728pass@1\u3001\u5e73\u5747\u6570\u636e\u6d41\u548c\u5e73\u5747\u8bed\u6cd5\u5339\u914d\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u5ec9\u4ef7\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\uff0c\u4f7f\u5176\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u3002"}}
{"id": "2510.17614", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17614", "abs": "https://arxiv.org/abs/2510.17614", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration", "comment": null, "summary": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.", "AI": {"tldr": "OG-Rank\u662f\u4e00\u4e2a\u4f4e\u5ef6\u8fdf\u7684\u89e3\u7801\u5668\u91cd\u6392\u5e8f\u7cfb\u7edf\uff0c\u901a\u8fc7\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u5b9e\u73b0\u5feb\u901f\u6392\u5e8f\u5e76\u5728\u5fc5\u8981\u65f6\u751f\u6210\u89e3\u91ca\uff0c\u5728\u4e34\u5e8a\u533b\u5631\u9009\u62e9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u5b9e\u65f6\u5de5\u4f5c\u5e76\u80fd\u89e3\u91ca\u9009\u62e9\u7684\u6392\u5e8f\u7cfb\u7edf\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u89e3\u7801\u5668\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5355\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408\u6c60\u5316\u9996\u8bcd\u8bc4\u5206\u4fe1\u53f7\u548c\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u89e3\u91ca\u6b65\u9aa4\uff0c\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u4e13\u6ce8\u4e8e\u56f0\u96be\u6848\u4f8b\u3002", "result": "\u5728\u4e34\u5e8a\u533b\u5631\u9009\u62e9\u4efb\u52a1\u4e2d\uff0c\u5feb\u901f\u8def\u5f84Recall@1\u7ea60.45\uff0cnDCG@20\u7ea60.625\uff1b\u5f53\u95e8\u63a7\u6fc0\u6d3b\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff08Recall@1\u7ea60.56\uff0cnDCG@20\u7ea60.699\uff0c\u95e8\u63a7\u738745%\uff09\u3002", "conclusion": "OG-Rank\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u65b9\u6848\uff1a\u9ed8\u8ba4\u5feb\u901f\u6392\u5e8f\uff0c\u5728\u9700\u8981\u65f6\u751f\u6210\u89e3\u91ca\uff0c\u8fd9\u79cd\u9009\u62e9\u6027\u751f\u6210\u6a21\u5f0f\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u51b3\u7b56\u4efb\u52a1\uff0c\u5355\u7b56\u7565\u8bbe\u8ba1\u7b80\u5316\u90e8\u7f72\u548c\u9884\u7b97\u89c4\u5212\u3002"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u4e8b\u4ef6\u56de\u5fc6\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u7b49\u5173\u952e\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5229\u7528LLMs\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u672a\u6765\u4e8b\u4ef6\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u8fd9\u79cd\u65b0\u5174\u8303\u5f0f\u88ab\u79f0\u4e3a\"LLM-as-a-Prophet\"\u3002", "method": "\u6784\u5efa\u4e86Prophet Arena\u8bc4\u4f30\u57fa\u51c6\uff0c\u6301\u7eed\u6536\u96c6\u5b9e\u65f6\u9884\u6d4b\u4efb\u52a1\u5e76\u5c06\u6bcf\u4e2a\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u6d41\u7a0b\u9636\u6bb5\uff0c\u4ee5\u652f\u6301\u53d7\u63a7\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8bb8\u591aLLMs\u5df2\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8868\u73b0\u4e3a\u8f83\u5c0f\u7684\u6821\u51c6\u8bef\u5dee\u3001\u4e00\u81f4\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6709\u524d\u666f\u7684\u5e02\u573a\u56de\u62a5\u3002", "conclusion": "LLMs\u5728\u9884\u6d4b\u667a\u80fd\u65b9\u9762\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u73b0\u5353\u8d8a\u9884\u6d4b\u80fd\u529b\u65b9\u9762\u4ecd\u9762\u4e34\u5173\u952e\u74f6\u9888\uff0c\u5982\u4e8b\u4ef6\u56de\u5fc6\u4e0d\u51c6\u786e\u3001\u6570\u636e\u6e90\u8bef\u89e3\u4ee5\u53ca\u5728\u63a5\u8fd1\u51b3\u7b56\u65f6\u4fe1\u606f\u805a\u5408\u901f\u5ea6\u8f83\u6162\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u56fe\u5f62\u5316\u6846\u67b6\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8eMAIDs\u7684\u9488\u5bf9\u6027\u5e72\u9884\u8303\u5f0f\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6280\u672f\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\uff0c\u907f\u514d\u5168\u5c40\u6307\u5bfc\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5bf9\u6574\u4e2a\u7cfb\u7edf\u7684\u5168\u5c40\u4eba\u5de5\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u73b0\u6709\u7684\u534f\u8c03\u673a\u5236\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u7814\u7a76\uff0c\u7f3a\u4e4f\u6613\u7528\u7684\u7814\u7a76\u5de5\u5177\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u5206\u6790\u6846\u67b6\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u5e72\u9884\u8303\u5f0f\uff0c\u4f7f\u7528\u9884\u7b56\u7565\u5e72\u9884(PSI)\u56e0\u679c\u63a8\u7406\u6280\u672f\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\uff0c\u901a\u8fc7\u6700\u5927\u5316\u56e0\u679c\u6548\u5e94\u6765\u8fbe\u6210\u590d\u5408\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u9488\u5bf9\u6027\u5e72\u9884\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u76f8\u5173\u6027\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "MAIDs\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u9488\u5bf9\u6027\u5e72\u9884\u8303\u5f0f\u80fd\u591f\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u56e0\u679c\u63a8\u7406\u6280\u672f\u6709\u52a9\u4e8e\u5b9e\u73b0\u590d\u5408\u76ee\u6807\u3002"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "\u63d0\u51faContextual Attention Modulation (CAM)\u673a\u5236\u548cHyCAM\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u8868\u793a\u6765\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u5e76\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\uff0c\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u4e0e\u4efb\u52a1\u7279\u5b9a\u4e13\u4e1a\u5316\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u5927\u7684\u7f3a\u9677\uff0c\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCAM\u673a\u5236\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8868\u793a\uff0c\u5e76\u6784\u5efaHyCAM\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u7684\u5168\u53c2\u6570CAM\u6a21\u5757\u548c\u591a\u4e2a\u8f7b\u91cf\u7ea7\u4e13\u7528CAM\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u7b56\u7565\u5b9e\u73b0\u81ea\u9002\u5e94\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u7b49\u5f02\u6784\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%\u3002", "conclusion": "CAM\u548cHyCAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u65f6\u4ecd\u80fd\u611f\u77e5\u5230\u6b63\u786e\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\"\u770b\u89c1\u4f46\u4e0d\u76f8\u4fe1\"\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u6765\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u591a\u4e2aVLM\u5bb6\u65cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5373\u4f7f\u5b58\u5728\u6b63\u786e\u7684\u89c6\u89c9\u8bc1\u636e\u65f6\u4ecd\u4f1a\u5931\u8d25\u3002\u4f5c\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e9b\u5931\u8d25\u662f\u7531\u4e8e\u672a\u80fd\u611f\u77e5\u8bc1\u636e\u8fd8\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u8bc1\u636e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c42\u95f4\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u800c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u63d0\u51fa\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u6765\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5728LLaVA\u3001Qwen\u3001Gemma\u548cInternVL\u7b49\u591a\u4e2aVLM\u5bb6\u65cf\u4e0a\u4e00\u81f4\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "VLMs\u5728\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\uff0c\u901a\u8fc7\u4f7f\u8fd9\u4e9b\u4fe1\u53f7\u663e\u5f0f\u5316\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u8fdb\u5bf9VLMs\u7684\u8bca\u65ad\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002"}}
