<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 14]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.IT](#cs.IT) [Total: 21]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning](https://arxiv.org/abs/2507.09094)
*Xiaoren Xu,Hao Xu,Dongyu Wei,Walid Saad,Mehdi Bennis,Mingzhe Chen*

Main category: cs.NI

TL;DR: 提出了一种基于流体天线系统（FAS）的无人机（UAV）三维定位框架，通过多智能体强化学习优化轨迹和天线端口选择，显著降低定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决移动目标UAV的实时三维定位问题，优化多无人机协作的轨迹和天线端口选择以提高定位精度。

Method: 提出基于注意力机制的循环多智能体强化学习（AR-MARL），结合RNN和注意力机制优化全局Q函数近似和目标定位。

Result: 仿真结果显示，AR-MARL方案相比VD-MARL和无FAS方法，平均定位误差分别降低17.5%和58.5%。

Conclusion: AR-MARL框架有效提升了移动目标UAV的定位精度，为无人机协作定位提供了新思路。

Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid
antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In
the proposed framework, a set of controlled UAVs cooperatively estimate the
real-time 3D position of a target UAV. Here, the active UAV transmits a
measurement signal to the passive UAVs via the reflection from the target UAV.
Each passive UAV estimates the distance of the active-target-passive UAV link
and selects an antenna port to share the distance information with the base
station (BS) that calculates the real-time position of the target UAV. As the
target UAV is moving due to its task operation, the controlled UAVs must
optimize their trajectories and select optimal antenna port, aiming to estimate
the real-time position of the target UAV. We formulate this problem as an
optimization problem to minimize the target UAV positioning error via
optimizing the trajectories of all controlled UAVs and antenna port selection
of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement
learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use
the local Q function to determine its trajectory and antenna port while
optimizing the target UAV positioning performance without knowing the
trajectories and antenna port selections of other controlled UAVs. Different
from current MARL methods, the proposed method uses a recurrent neural network
(RNN) that incorporates historical state-action pairs of each controlled UAV,
and an attention mechanism to analyze the importance of these historical
state-action pairs, thus improving the global Q function approximation accuracy
and the target UAV positioning accuracy. Simulation results show that the
proposed AR-MARL scheme can reduce the average positioning error by up to 17.5%
and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.

</details>


### [2] [Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks](https://arxiv.org/abs/2507.09124)
*Syed Danial Ali Shah,Maryam Hafeez,Abdelaziz Salama,Syed Ali Raza Zaidi*

Main category: cs.NI

TL;DR: 论文提出了一种基于O-RAN规范的CAORA框架，支持实时RAN和AI工作负载的动态共存，通过预测性编排显著提升系统适应性。


<details>
  <summary>Details</summary>
Motivation: AI-RAN融合的愿景需要探索架构框架和智能资源编排策略，以实现6G平台对AI和RAN工作负载的无缝支持。

Method: 设计了基于O-RAN的CAORA框架，包括自定义xApps和E2E编排器，采用SAC强化学习代理管理资源分配。

Result: 在巴塞罗那5G流量数据上的仿真显示，CAORA实现了99%的RAN需求满足率，并支持动态AI工作负载。

Conclusion: CAORA为未来AI-RAN融合的6G系统提供了可行的蓝图，显著提升了资源效率和系统适应性。

Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims
to unlock a unified 6G platform capable of seamlessly supporting AI and RAN
workloads over shared infrastructure. However, the architectural framework and
intelligent resource orchestration strategies necessary to realize this vision
remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN
Architectural (CAORA) framework based on O-RAN specifications, enabling the
dynamic coexistence of real-time RAN and computationally intensive AI
workloads. We design custom xApps within the Near-Real-Time RAN Intelligent
Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an
End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The
orchestrator incorporates workload forecasting and anomaly detection modules,
augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that
proactively manages resource allocation, including Multi-Instance GPU (MIG)
partitioning. Using real-world 5G traffic traces from Barcelona, our
trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment
of RAN demands, supports dynamic AI workloads, and maximizes infrastructure
utilization even under highly dynamic conditions. Our results reveal that
predictive orchestration significantly improves system adaptability, resource
efficiency, and service continuity, offering a viable blueprint for future
AI-and-RAN converged 6G systems.

</details>


### [3] [On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response](https://arxiv.org/abs/2507.09153)
*Bilal Karaman,Ilhan Baştürk,Ferdi Kara,Engin Zeydan,Esra Aycan Beyazıt,Sezai Taşkın,Emil Björnson,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 提出了一种基于高空平台站（HAPS）的需求驱动通信系统，用于在自然灾害中恢复通信，提升应急响应能力。


<details>
  <summary>Details</summary>
Motivation: 自然灾害常导致通信网络中断，现有解决方案在无线接入网和回传基础设施同时失效时表现不足。

Method: 利用HAPS支持的混合光/太赫兹链路提升回传容量和韧性，并在S和Ka波段实现可靠通信。

Result: 模拟显示HAPS在恶劣条件下仍能提升通信能力，支持实时信息交换和资源分配。

Conclusion: HAPS可显著增强网络韧性，支持高效灾害管理，有望成为应急通信框架的重要组成部分。

Abstract: Natural disasters often disrupt communication networks and severely hamper
emergency response and disaster management. Existing solutions, such as
portable communication units and cloud-based network architectures, have
improved disaster resilience but fall short if both the Radio Access Network
(RAN) and backhaul infrastructure become inoperable. To address these
challenges, we propose a demand-driven communication system supported by High
Altitude Platform Stations (HAPS) to restore communication in an affected area
and enable effective disaster relief. The proposed emergency response network
is a promising solution as it provides a rapidly deployable, resilient
communications infrastructure. The proposed HAPS-based communication can play a
crucial role not only in ensuring connectivity for mobile users but also in
restoring backhaul connections when terrestrial networks fail. As a bridge
between the disaster management center and the affected areas, it can
facilitate the exchange of information in real time, collect data from the
affected regions, and relay crucial updates to emergency responders. Enhancing
situational awareness, coordination between relief agencies, and ensuring
efficient resource allocation can significantly strengthen disaster response
capabilities. In this paper, simulations show that HAPS with hybrid optical/THz
links boosts backhaul capacity and resilience, even in harsh conditions.
HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first
responders and disaster-affected populations. This paper also explores the
integration of HAPS into emergency communication frameworks and standards, as
it has the potential to improve network resilience and support effective
disaster management.

</details>


### [4] [UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks](https://arxiv.org/abs/2507.09852)
*Zihao Zhou,Zipeng Dai,Linyi Huang,Cui Yang,Youjun Xiang,Jie Tang,Kai-kit Wong*

Main category: cs.NI

TL;DR: 介绍了一个名为UavNetSim-v1的开源Python仿真平台，用于无人机网络中的协议和算法开发与测试。


<details>
  <summary>Details</summary>
Motivation: 无人机网络的通信协议和算法需要高效且经济的仿真工具，避免昂贵的实地实验。

Method: 开发了一个基于Python的开源仿真平台，支持路由/MAC协议、拓扑控制算法及移动/能量模型，并提供可视化界面。

Result: UavNetSim-v1功能全面，易于使用，适合快速原型开发和教学。

Conclusion: 该平台是无人机通信研究中轻量且强大的仿真工具替代方案。

Abstract: In unmanned aerial vehicle (UAV) networks, communication protocols and
algorithms are essential for cooperation and collaboration between UAVs.
Simulation provides a cost-effective solution for prototyping, debugging, and
analyzing protocols and algorithms, avoiding the prohibitive expenses of field
experiments. In this paper, we present ``UavNetSim-v1'', an open-source
Python-based simulation platform designed for rapid development, testing, and
evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''
provides most of the functionalities developers may need, including
routing/medium access control (MAC) protocols, topology control algorithms and
mobility/energy models, while maintaining ease of use. Furthermore, the
platform supports comprehensive performance evaluation and features an
interactive visualization interface for in-depth algorithm analysis. In short,
``UavNetSim-v1'' lends itself to both rapid prototyping and educational
purposes, and can serve as a lightweight yet powerful alternative to mature
network simulators for UAV communication research.

</details>


### [5] [Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications](https://arxiv.org/abs/2507.09270)
*Songhan Zhao,Yusi Long,Lanhua Li,Bo Gu,Shimin Gong,Zehui Xiong*

Main category: cs.NI

TL;DR: 论文提出了一种语义感知的可重构智能表面（RIS）辅助无线网络，通过联合优化语义控制、解码顺序和RIS波束成形策略，显著降低了NOMA传输的能耗。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多用户同时传输语义信息时，通过语义提取和RIS波束成形优化系统能耗。

Method: 将优化问题分解为两个子问题，采用近似方法求解，联合优化语义控制、解码顺序和RIS波束成形。

Result: 数值结果表明，联合流量重塑和信道重构方案显著优于基准方法，提升了能效。

Conclusion: 通过语义感知和RIS辅助的联合优化，能够有效降低无线网络的能耗，满足用户需求。

Abstract: In this paper, we consider a semantic-aware reconfigurable intelligent
surface (RIS)-assisted wireless network, where multiple semantic users (SUs)
simultaneously transmit semantic information to an access point (AP) by using
the non-orthogonal multiple access (NOMA) method. The SUs can reshape their
traffic demands by modifying the semantic extraction factor, while the RIS can
reconfigure the channel conditions via the passive beamforming. This provides
the AP with greater flexibility to decode the superimposed signals from the
SUs. We aim to minimize the system's overall energy consumption, while ensuring
that each SU's traffic demand is satisfied. Hence, we formulate a joint
optimization problem of the SUs' decoding order and semantic control, as well
as the RIS's passive beamforming strategy. This problem is intractable due to
the complicated coupling in constraints. To solve this, we decompose the
original problem into two subproblems and solve them by using a series of
approximate methods. Numerical results show that the joint traffic reshaping
and channel reconfiguration scheme significantly improves the energy saving
performance of the NOMA transmissions compared to the benchmark methods.

</details>


### [6] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: 研究如何在异构边缘数据中心（DC）中优化分配大型语言模型（LLM）推理工作负载，以最小化能耗、碳排放和水资源使用，同时提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 边缘数据中心的电力价格动态变化，可再生能源可用性存在时空差异，需优化工作负载分配以减少成本和环境影响。

Method: 提出了一种新颖的优化模型，用于LLM服务提供商降低运营成本和环境影响。

Result: 数值结果验证了所提方法的有效性。

Conclusion: 该优化模型能有效减少能耗、碳排放和水资源使用，同时提升用户体验。

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [7] [Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks](https://arxiv.org/abs/2507.09341)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 论文研究了车载移动边缘计算（VEC）中的任务卸载问题，通过比较PSO、DQN和PPO模型，发现DQN在动态环境中表现最佳，显著降低了任务丢弃率和端到端延迟。


<details>
  <summary>Details</summary>
Motivation: VEC在低延迟数据处理方面潜力巨大，但车辆在RSU覆盖区域内停留时间短，导致任务卸载面临严格的时间限制。

Method: 首先在静态环境中使用PSO建立理论极限，随后在动态环境中比较PSO、DQN和PPO模型，目标是减少任务丢弃和E2E延迟。

Result: DQN模型在动态环境中表现最优，执行时间减少99.2%，任务丢弃率降低2.5%，E2E延迟降低18.6%。

Conclusion: 深度强化学习（DRL）在VEC系统中表现出高效和可扩展的任务管理能力。

Abstract: Vehicular Mobile Edge Computing (VEC) drives the future by enabling
low-latency, high-efficiency data processing at the very edge of vehicular
networks. This drives innovation in key areas such as autonomous driving,
intelligent transportation systems, and real-time analytics. Despite its
potential, VEC faces significant challenges, particularly in adhering to strict
task offloading deadlines, as vehicles remain within the coverage area of
Roadside Units (RSUs) for only brief periods. To tackle this challenge, this
paper evaluates the performance boundaries of task processing by initially
establishing a theoretical limit using Particle Swarm Optimization (PSO) in a
static environment. To address more dynamic and practical scenarios, PSO, Deep
Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented
in an online setting. The objective is to minimize dropped tasks and reduce
end-to-end (E2E) latency, covering both communication and computation delays.
Experimental results demonstrate that the DQN model considerably surpasses the
dynamic PSO approach, achieving a 99.2% reduction in execution time.
Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to
dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the
effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and
efficient task management for VEC systems.

</details>


### [8] [Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks](https://arxiv.org/abs/2507.09346)
*Arild Yonkeu,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出了一种基于指针网络的任务调度架构，用于动态边缘计算场景，显著降低了任务丢弃率和等待时间，同时保持了快速推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统任务调度方法在动态、时间敏感环境中的高计算开销问题，以及现有深度学习方法的可扩展性和适应性不足。

Method: 使用遗传算法生成合成数据集，训练指针网络模型以确定最优任务顺序。

Result: 模型在任务丢弃率和等待时间上优于基线方法，推理时间低于2秒，且具有较高的软序列准确率（89.2%）。

Conclusion: 该模型为边缘计算任务管理提供了可扩展且高效的解决方案，适用于实时动态场景。

Abstract: Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for
meeting the low-latency demands of modern IoT and dynamic task scheduling
scenarios. MEC reduces the processing burden on resource-constrained devices by
enabling task execution at nearby edge servers. However, efficient task
scheduling remains a challenge in dynamic, time-sensitive environments.
Conventional methods -- such as heuristic algorithms and mixed-integer
programming -- suffer from high computational overhead, limiting their
real-time applicability. Existing deep learning (DL) approaches offer faster
inference but often lack scalability and adaptability to dynamic workloads. To
address these issues, we propose a Pointer Network-based architecture for task
scheduling in dynamic edge computing scenarios. Our model is trained on a
generated synthetic dataset using genetic algorithms to determine the optimal
task ordering. Experimental results show that our model achieves lower drop
ratios and waiting times than baseline methods, and a soft sequence accuracy of
up to 89.2%. Our model consistently achieves inference times under 2 seconds
across all evaluated task counts, whereas the integer and binary programming
approaches require approximately up to 18 seconds and 90 seconds, respectively.
It also shows strong generalization across varying scenarios, and adaptability
to real-time changes, offering a scalable and efficient solution for edge-based
task management.

</details>


### [9] [Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling](https://arxiv.org/abs/2507.09352)
*Ghazal Asemian,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 论文提出了一种动态MEC框架，结合传输多样性解决任务调度和资源分配问题，显著降低了任务丢弃率并提升了资源利用率。


<details>
  <summary>Details</summary>
Motivation: 动态任务到达和通信威胁（如干扰）使得可靠的任务卸载和资源分配变得复杂，需要一种能够应对这些挑战的解决方案。

Method: 提出了一种基于传输多样性的干扰感知卸载和资源块分配框架，通过优化调度和分布式gNBs实现。

Result: 在SJNR为4 dB时，任务丢弃率降至0.26，优于无传输多样性（0.50）和基线策略（STF 0.52，FCFS 0.63）。

Conclusion: 该算法有效减轻了干扰影响，提升了资源利用率和任务完成率，适用于关键MEC应用。

Abstract: Mobile Edge Computing (MEC) enables low-latency applications by bringing
computation closer to the user, but dynamic task arrivals and communication
threats like jamming complicate reliable task offloading and resource
allocation. In this paper, we formulate a dynamic MEC framework considering the
transmission diversity that jointly addresses task scheduling and resource
block (RB) assignment in the presence of jamming. First, we define and evaluate
key network metrics-including dropped task ratio and bandwidth
utilization-while maintaining service continuity by accounting for the existing
commitments of the edge server to previously offloaded tasks. Then, we propose
a jamming-aware offloading and RB allocation framework that leverages
transmission diversity and optimal scheduling across distributed gNBs. The
proposed solution is compared to a similar scenario without transmission
diversity and two baseline strategies of first-come-first-served (FCFS) and
shortest task first (STF). The proposed algorithm effectively mitigates the
impact of jamming while enhancing resource utilization and minimizing task drop
rates, making it highly suitable for mission-critical MEC applications. At
signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves
a $0.26$ task drop rate, outperforming the scenario without transmission
diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52
and 0.63 task drop rates, respectively.

</details>


### [10] [MobiWorld: World Models for Mobile Wireless Network](https://arxiv.org/abs/2507.09462)
*Haoye Chai,Yuan Yuan,Yong Li*

Main category: cs.NI

TL;DR: MobiWorld是一种生成式世界模型，用于高保真、灵活模拟移动网络环境，支持网络规划和优化。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型泛化能力有限，无法满足移动网络规划和优化的需求。

Method: MobiWorld整合异构数据源和多模态数据类型，基于扩散模型实现可控生成能力。

Result: 在节能场景中，MobiWorld表现出强大的可控生成性能，优于传统方法。

Conclusion: MobiWorld为移动网络优化提供了高效、精确的模拟环境，减少了对真实网络交互的依赖。

Abstract: Accurate modeling and simulation of mobile networks are essential for
enabling intelligent and cost-effective network optimization. In this paper, we
propose MobiWorld, a generative world model designed to support high-fidelity
and flexible environment simulation for mobile network planning and
optimization. Unlike traditional predictive models constrained by limited
generalization capabilities, MobiWorld exhibits strong universality by
integrating heterogeneous data sources, including sensors, mobile devices, and
base stations, as well as multimodal data types such as sequences and images.
It is capable of generating both network element-level observations (e.g.,
traffic load, user distribution) and system-level performance indicators (e.g.,
throughput, energy consumption) to support a wide range of planning and
optimization tasks. Built upon advanced diffusion models, MobiWorld offers
powerful controllable generation capabilities by modeling the joint
distribution between mobile network data and diverse conditional factors
including spatio temporal contexts, user behaviors, and optimization policies.
This enables accurate simulation of dynamic network states under varying policy
configurations, providing optimization agents with precise environmental
feedback and facilitating effective decision-making without relying on costly
real-network interactions. We demonstrate the effectiveness of MobiWorld in a
collaborative energy-saving scenario, where an agent uses observations and
rewards generated by MobiWorld to optimize base station sleep and user
offloading policies. Experimental results show that MobiWorld exhibits strong
controllable generation performance and outperforms traditional methods in
energy optimization.

</details>


### [11] [Wi-Fi: Twenty-Five Years and Counting](https://arxiv.org/abs/2507.09613)
*Giovanni Geraci,Francesca Meneghello,Francesc Wilhelmi,David Lopez-Perez,Iñaki Val,Lorenzo Galati Giordano,Carlos Cordeiro,Monisha Ghosh,Edward Knightly,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文全面回顾了Wi-Fi从IEEE 802.11b（Wi-Fi 1）到IEEE 802.11bn（Wi-Fi 8）的技术演进，重点介绍了推动Wi-Fi发展的关键机制。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi技术在过去25年中经历了巨大变革，但缺乏一个跨越八代技术的全面教程。本文旨在填补这一空白。

Method: 文章从频谱分配、物理层、MAC层、多用户接入、节能机制、频谱聚合和AP协调等方面，系统分析了Wi-Fi的关键技术进步。

Result: 通过分析，展示了Wi-Fi数据速率提升1000倍以上、多用户接入、节能优化等重大技术突破。

Conclusion: Wi-Fi的未来发展将包括毫米波集成、传感、安全扩展和AI/ML应用等方向。

Abstract: Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding
name, today's Wi-Fi boasts entirely new capabilities that were not even on the
roadmap 25 years ago. This article aims to provide a holistic and comprehensive
technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi
1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial
article to span these eight generations. Rather than a generation-by-generation
exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin
by discussing spectrum allocation and coexistence, and detailing the IEEE
802.11 standardization cycle. Second, we provide an overview of the physical
layer and describe key elements that have enabled data rates to increase by
over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been
enhanced from the original Distributed Coordination Function to now include
capabilities spanning from frame aggregation to wideband spectrum access.
Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and
introduced multi-user access. Fifth, given the increasing use of mobile,
battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the
generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate
spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,
reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access
Points to coordinate in order to improve performance and efficiency. In the
Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including
integrated mmWave operations, sensing, security and privacy extensions, and the
adoption of AI/ML.

</details>


### [12] [Towards Robust RTC in Sparse LEO Constellations](https://arxiv.org/abs/2507.09798)
*Aashish Gottipati,Lili Qiu*

Main category: cs.NI

TL;DR: 论文研究了稀疏LEO星座中视频会议系统的性能问题，提出了一种数据驱动的队列管理机制，显著提升了视频质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: Google的拥塞控制（GCC）在低地球轨道（LEO）网络中表现脆弱，视频质量因高延迟和网络不稳定而下降，需要改进。

Method: 引入了一种基于预测切换活动的数据驱动队列管理机制，动态调整发送端队列容量以适应网络变化。

Result: 相比默认WebRTC，视频比特率提升了3倍，冻结率降低了62%。

Conclusion: 该方法显著提升了稀疏LEO网络中的实时通信性能，为未来网络优化提供了新思路。

Abstract: Google's congestion control (GCC) has become a cornerstone for real-time
video and audio communication, yet its performance remains fragile in emerging
Low Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer
longer duration links and reduced handover frequency compared to dense
deployments, presenting a unique opportunity for high-quality real-time
communication (RTC) in environments with limited terrestrial network
infrastructure. In this paper, we study the behavior of videoconferencing
systems in sparse LEO constellations. We observe that video quality degrades
due to inherent delays and network instability introduced by the high altitude
and rapid movement of LEO satellites, with these effects exacerbated by
WebRTC's conventional ``one-size-fits-all'' sender-side pacing queue
management. To boost RTC performance, we introduce a data-driven queue
management mechanism that adapts the maximum pacing queue capacity based on
predicted handover activity. Specifically, our approach employs shorter queue
limits during stable, no-handover phases to prioritize low latency
communication, and preemptively increases pacing queue capacity when entering
periods of increased handover activity to absorb disruptions. Our method yields
up to $3$x improvements in video bitrate and reduces freeze rate by $62\%$
compared to default WebRTC.

</details>


### [13] [Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit](https://arxiv.org/abs/2507.10210)
*Thijs Havinga,Xianjun Jiao,Wei Liu,Baiheng Chen,Robbe Gaeremynck,Ingrid Moerman*

Main category: cs.NI

TL;DR: 论文提出了一种基于有线回传的细粒度Co-OFDMA实现方法，解决了Wi-Fi 8中资源分配的复杂性和开销问题，并通过实验验证了其性能优越性。


<details>
  <summary>Details</summary>
Motivation: 密集部署的无线网络中，竞争性系统在延迟方面表现不佳，需要更高效的资源协调方法。Co-OFDMA虽能提升效率，但细粒度资源分配因空中调度复杂而难以实现。

Method: 利用光纤回传实现Wi-Fi 6兼容的细粒度Co-OFDMA，通过开源平台openwifi和White Rabbit进行实验验证。

Result: 实验显示，两个AP之间的载波频率偏移预补偿和时间同步性能优于无线标准要求，且Co-OFDMA帧的接收质量优于单独发送的帧。

Conclusion: 基于有线回传的Co-OFDMA方案可行且性能优越，为未来多AP协调技术奠定了基础。

Abstract: Proper coordination is needed to guarantee the performance of wireless
networks in dense deployments. Contention-based systems suffer badly in terms
of latency when multiple devices compete for the same resources. Coordinated
Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi
8 to remedy this, as it enables multiple Access Points (APs) to share spectrum
more efficiently. However, fine-grained resource allocation, namely within
20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling
overhead and complexity in terms of physical layer signaling. A wired backhaul
mitigates the need for over-the-air scheduling and synchronization, and it
allows for coordination even if APs are not in each others' range. Furthermore,
it forms the basis for more advanced multi-AP coordination schemes like
coordinated beamforming and joint transmission. In this work we demonstrate the
realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,
enabled by the open-source platforms openwifi and White Rabbit. We show that
the performance in terms of carrier frequency offset pre-compensation and time
synchronization between two APs exceeds related wireless standard requirements.
Furthermore, the quality of the received constellation of the Co-OFDMA frame as
reported by a wireless connectivity tester is better than individual frames
sent by the APs.

</details>


### [14] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: AI Video Chat introduces a new RTC paradigm with MLLMs, addressing latency challenges through Artic, a framework optimizing video streaming for AI understanding.


<details>
  <summary>Details</summary>
Motivation: To make human-AI interactions as intuitive as face-to-face chats, despite high latency from MLLM inference and network instability.

Method: Proposes Artic with Context-Aware Video Streaming and Loss-Resilient Adaptive Frame Rate to optimize bitrate and reduce latency.

Result: Develops DeViBench to evaluate video streaming quality's impact on MLLM accuracy.

Conclusion: AI Video Chat requires novel solutions for latency and quality, with ongoing research to address open questions.

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出了一种通过去除冗余推理步骤来提升大语言模型性能的方法，通过注意力分数识别冗余并修剪低贡献部分。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长链推理中存在冗余，注意力分散导致性能下降，尤其是错误答案中冗余更明显。

Method: 通过测量特殊结束标记的注意力分数识别冗余，采用结构感知修剪去除低贡献部分，随后恢复推理生成。

Result: 方法显著提升了推理密集型任务的准确性，尤其在数学竞赛基准（如AIME和AMC）上表现突出。

Conclusion: 去除冗余推理步骤能有效提升模型性能，无需额外训练即可实现显著改进。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [16] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从反应式多智能体强化学习转向生成式AI驱动的主动式多智能体智能，以解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法面临联合动作空间指数增长、非平稳环境和部分可观测性等挑战，难以应对新场景。

Method: 采用生成式AI的强化学习，将智能体视为能够预测未来交互并生成协调动作序列的生成模型。

Result: 生成式AI驱动的智能体能够进行主动决策、增强协调和动态适应，实现真正的协作智能。

Conclusion: 这一范式转变有望解决传统反应式框架无法处理的协调问题，推动分布式智能的发展。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [17] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新多标准评估（MCA）方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多标准评估方法中假设和主观判断带来的问题，尤其是在处理定量和定性标准时。

Method: 结合两种VGA模型，基于线性编程框架，提出新的MCA方法。

Result: 通过两个数值示例验证了方法的准确性和透明度。

Conclusion: 该方法为自动决策系统和决策支持系统提供了可靠且灵活的解决方案，推动了相关领域的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理系统中AI系统的互连及其重复使用特性，提供公平性和鲁棒性的闭环保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统需要满足公平性和鲁棒性的先验保证，但现有方法对此类闭环系统的建模和分析较为复杂。

Method: 开发了一个基于PyTorch的工具包，利用随机控制技术建模AI系统与代理的互连，并提供公平性和鲁棒性的闭环保证。

Result: 工具包简化了多代理系统中公平性保证的复杂性，并提供了先验的公平性和鲁棒性保证。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性建模提供了实用且高效的解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、叙事和评估等多样化应用中的需求，需要一个灵活的场景定义框架。

Method: 采用实体-组件架构模式，将游戏管理员（GM）设计为可配置的实体，由组件构成，实现工程师和设计师的关注点分离。

Result: Concordia库的持续演进证明了该方法的有效性，用户能够根据特定目标灵活配置场景。

Conclusion: 通过借鉴TTRPGs和实体-组件架构，实现了快速迭代、模块化和可扩展性，适用于生成式AI的多样化应用。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [20] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是首个专为生物多样性分析和保护规划设计的AI基础模型，通过多模态数据预训练和下游任务微调，显著提升了生态预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失加速，威胁生态平衡和可持续性，需综合监测和预测能力。AI基础模型在多领域表现优异，有望解决生物多样性保护问题。

Method: BioAnalyst采用基于Transformer的架构，预训练于物种记录、遥感数据等多模态数据集，支持物种分布建模等下游任务微调。

Result: 模型在数据稀缺场景下表现优于现有方法，为生态预测设立了新的准确性基准。

Conclusion: BioAnalyst的开放共享旨在推动生物多样性建模合作，促进AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [21] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能缩短任务完成时间，但实际上AI工具反而增加了19%的完成时间，与经济学和机器学习专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具对经验丰富的开源开发者生产力的实际影响，填补现有研究的空白。

Method: 通过随机对照试验（RCT），16名有中等AI经验的开发者在成熟项目中完成246个任务，随机分配是否使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 开发者预期AI工具能缩短24%的时间，但实际增加了19%的完成时间，与专家预测的缩短39%（经济学）和38%（ML）相反。

Conclusion: AI工具在此实验中反而降低了开发效率，且这种效果不太可能是实验设计的结果。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [22] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测器的交互。


<details>
  <summary>Details</summary>
Motivation: DeFi的无许可特性带来了市场操纵问题，缺乏中心化监管导致恶意行为频发。

Method: 采用MARL框架，结合GRPO优化、理论驱动的奖励函数和多模态智能体管道，整合语义、社交图和链上数据。

Result: 在真实数据和对抗模拟中验证，Hide-and-Shill系统在检测精度和因果归因方面表现优异。

Conclusion: 该研究为去中心化市场情报提供了新范式，推动了多智能体系统与金融监管的结合。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [23] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 论文首次系统评估了基于LLM的编码代理的安全性，发现21%的操作存在安全隐患，并提出了检测系统和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM编码代理在软件开发中的广泛应用，其安全性问题尚未被充分研究，可能引入不安全实践。

Method: 分析了五个先进模型（如GPT-4o、GPT-4.1等）在93个实际任务中的12,000多个操作，开发了高精度检测系统。

Result: 21%的操作存在安全隐患，信息暴露（CWE-200）最常见；GPT-4.1的缓解成功率高达96.8%。

Conclusion: 研究为编码代理安全性评估提供了首个框架，并强调下一代LLM编码代理需具备安全设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [24] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本文提出了由AI引发的潜在全人类灭绝事件的分类和示例，旨在通过公开讨论推动预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能导致的灾难性风险，以促进公众支持和预防行动。

Method: 提出分类法并列举可能的全人类灭绝场景。

Result: 明确了AI可能带来的极端风险，强调预防的重要性。

Conclusion: 通过公开讨论这些可能性，可以推动社会采取预防措施，避免AI引发的灾难。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [25] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，旨在提升多模态大语言模型（MLLMs）在科学任务中的推理能力，通过EduPRM和EduMCTS等技术优化推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在科学任务中表现不佳，缺乏多步推理和自校正能力，EduFlow旨在解决这些问题。

Method: 提出EduFlow框架，包括EduPRM（过程感知奖励模型）和EduMCTS（领域适应搜索框架），通过课程学习和自反射机制优化推理。

Result: 实验表明EduFlow显著提升了推理的一致性和连贯性，并构建了大规模数据集EduMCTS-160K。

Conclusion: EduFlow为科学推理任务提供了一种有效的解决方案，未来将公开代码、数据和模型。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [26] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文探讨了如何结合可解释性和适应性设计可转移且可解释的神经符号AI系统，重点关注Agentic Retrieval-Augmented Generation系统，并评估知识表示对LLM查询三元组库的影响。


<details>
  <summary>Details</summary>
Motivation: 可解释性和适应性是下一代AI系统的关键，尤其是在大型语言模型（LLMs）和生成式AI中。本文旨在结合这两点，设计可转移且可解释的神经符号AI系统。

Method: 研究聚焦于Agentic Retrieval-Augmented Generation系统，评估不同知识表示（结构和复杂性）对LLM查询三元组库的影响。

Result: 结果表明，知识表示的方式对LLM查询三元组库的效果有显著影响。

Conclusion: 研究强调了知识表示在AI系统中的重要性，为设计可解释且适应性强的AI系统提供了方向。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [27] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg游戏框架将大型语言模型（LLMs）融入领导者与追随者的战略互动中，通过结构化提示和概率行为生成，定义了两类均衡概念，展示了在网络安全等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg游戏假设完全信息和理性行为者，而现实中的决策往往涉及有限理性和信息不对称。本文旨在通过LLMs模拟更真实的战略互动。

Method: 提出LLM-Stackelberg游戏框架，定义推理均衡和行为均衡，并通过参数化模型处理对手响应的认知不确定性。

Result: 通过钓鱼案例展示了LLM介导的互动在认知丰富性和对抗性方面的潜力，验证了框架的有效性。

Conclusion: LLM-Stackelberg游戏为网络安全、错误信息和推荐系统等领域的决策建模提供了新范式。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [28] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在规划中计算成本高的问题，提升离线规划的效率和实用性。

Method: 利用一致性轨迹模型（CTM）进行单步轨迹生成，避免迭代采样。

Result: 在D4RL基准测试中表现优于现有扩散方法，计算速度提升120倍。

Conclusion: CTP在高效性和性能上均优于现有方法，适用于低延迟离线规划。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [29] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 本文提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNN）在强化学习任务中，避免了梯度方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNN）在实时控制系统中具有生物启发和高效能的优势，但其训练因脉冲通信的不可微分性而面临挑战。

Method: 采用Metropolis-Hastings采样技术，通过迭代提出并概率接受网络参数更新，基于累积奖励信号进行训练。

Result: 在AcroBot和CartPole基准测试中，该方法在累积奖励最大化、网络资源和训练次数最小化方面优于传统深度Q学习和现有SNN方法。

Conclusion: 该框架为SNN在强化学习中的应用提供了一种有效的无梯度训练方法，展示了其在动态控制任务中的潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [30] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个AIaaS平台，专注于企业数据、工作流程和LLM的整合，提供数据安全和自动化支持。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全和知识保留问题，同时提升工作效率。

Method: 结合结构化文档处理、混合向量检索和无代码编排，支持多种主流LLM，并通过THOR Agent处理结构化查询。

Result: 在检索实验中，512 token的块大小表现最佳（Top-3准确率91.3%）；生成实验中，eSapiens在事实一致性上提升23%。

Conclusion: eSapiens为高风险领域（如法律和金融）提供了可信赖的AI工作流程。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [31] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 该论文探讨了人工智能（AI）快速发展带来的环境与伦理挑战，包括能源消耗、电子垃圾、计算资源不平等和网络安全系统的隐性能源负担。


<details>
  <summary>Details</summary>
Motivation: AI的快速扩张带来了被忽视的环境和伦理问题，需要系统性研究和解决。

Method: 通过综述近期研究和机构报告，分析了AI在能源、硬件、计算资源分配和网络安全方面的负面影响。

Result: 揭示了AI发展中的系统性挑战，如高排放、硬件更新快、全球基础设施不平等和网络安全能源需求。

Conclusion: 呼吁AI发展需与伦理责任和环境保护相结合，推动可持续、透明和公平的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [32] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 本文提出了一种结合多模态语言模型与知识图谱的神经符号框架，旨在提升服务机器人在动态环境中的互操作性和任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前服务机器人依赖专有解决方案，难以适应和扩展；知识图谱和本体论虽能支持互操作性，但难以处理原始感官数据。多模态语言模型擅长处理感官输入，但缺乏透明性和知识基础。

Method: 提出神经符号框架，结合多模态语言模型和知识图谱，生成符合本体论的知识图谱以指导机器人行为。

Result: 评估了五种多模态模型，发现GPT-o1和LLaMA 4 Maverick表现最佳，但新模型不一定更优，集成策略是关键。

Conclusion: 神经符号框架有效支持机器人互操作性，但模型选择和集成策略对性能至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [33] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务上表现优异，但存在推理链冗长的问题，导致资源浪费。本文综述了简洁和自适应推理的最新进展。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs在简单问题上生成冗余推理链的问题，以提高效率和实用性。

Method: 综述了简洁和自适应推理的方法论、基准和未来挑战。

Result: 提供了该领域的全面概述，帮助研究者快速了解现状。

Conclusion: 希望激发新的自适应推理思路，优化LRMs的使用。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [34] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局优化，以快速检测异常。


<details>
  <summary>Details</summary>
Motivation: AI驱动的制造业中，数据流实时监控需求增长，但资源有限，无法在所有位置部署传感器。现有方法多忽略因果关系，或依赖不切实际的干预手段。

Method: 通过在每个Q网络训练阶段集成因果信息，开发了Causal DQ方法，实现了更快的收敛和更紧的理论误差界。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅适用于传感器布局，还可推广至其他强化学习问题，为工程应用中的因果机器学习开辟新可能。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [35] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型（LLM）整合到超一致逻辑的形式语义解释函数中的方法，以解决LLM输出逻辑不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的广泛参数知识进行形式推理，同时解决其输出逻辑不一致的问题。

Method: 将LLM直接整合到超一致逻辑的形式语义解释函数中。

Result: 实验证明该方法可行，并在多个短形式事实性基准数据集上进行了评估。

Conclusion: 该方法为神经符号推理提供了一个理论框架，既能利用LLM的知识，又能保持底层逻辑的健全性和完备性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [36] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了如何通过技术干预实现危险AI活动的协调暂停，以应对AI快速发展带来的风险。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来了失控、滥用、地缘政治不稳定和权力集中等前所未有的风险，需要政府采取行动避免最坏结果。

Method: 提出了关键的技术干预措施，用于协调暂停危险的AI开发和部署。

Result: 这些干预措施可以限制多种危险AI活动，并为AI治理计划提供技术基础。

Conclusion: 技术干预是实现AI治理和风险控制的重要手段。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [37] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量的长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 探索是否仅通过提示或最小微调就能在基础模型中诱导长链思维推理能力。

Method: 使用20个来自推理模型的长链思维示例微调基础模型，并尝试其他来源的CoT数据（如非推理模型和人工标注）结合提示工程和多轮编辑。

Result: 微调后的基础模型表现优于更大规模的模型，但其他来源的CoT数据未能达到相同效果。

Conclusion: 高质量的小规模推理监督数据可以激活基础模型的推理能力，但专家CoT的某些潜在特性难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [38] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 论文提出将指令调优的大语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，并通过模型内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的泛化学习能力和符号AI的可验证推理能力，探索新的学习和推理方法。

Method: 将大语言模型重新解释为符号AI系统，研究并开发与传统学习和推理范式结构相似的新方法。

Result: 初步评估表明，该方法在提高学习效率和推理可靠性方面有效。

Conclusion: 该框架为神经符号AI提供了一种新的视角，并展示了其潜力。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [39] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出VerifyBench，一个跨领域基准，用于系统评估验证器性能，发现专用验证器与通用LLM在精度和召回率上的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器在复杂、多样化的模型生成响应中表现不一致，缺乏跨领域系统评估，限制了RLVR的可靠发展。

Method: 构建包含4,000个专家级问题的VerifyBench，覆盖数学、物理、化学和生物领域，通过多学科专家团队标注，设计四维实验框架比较验证器性能。

Result: 专用验证器精度高但召回率低，通用LLM包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 验证器技术存在瓶颈，需进一步优化以提升跨领域性能和稳定性。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [40] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布V3和R1系列模型，以其低成本、高性能和开源优势引发关注。论文回顾了大模型的演进，介绍了DeepSeek的创新算法和工程突破，并分析了其对AI竞争格局的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型的技术创新及其对AI领域的潜在影响，为未来大模型发展提供参考。

Method: 回顾大模型演进，介绍DeepSeek的创新算法（如MLA、MoE、MTP、GRPO）及工程优化，并与主流LLM对比分析。

Result: DeepSeek模型在性能、成本和开源方面具有竞争力，推动了AI领域的技术和工程进步。

Conclusion: DeepSeek的创新为未来大模型发展提供了新方向，特别是在数据、训练和推理方面。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [41] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，通过引入最优边际Q函数和广义Q批评器，实现了高性能合作。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体强化学习中，单调改进与部分参数共享（ParPS）存在冲突，直接结合会导致策略更新基线漂移问题。

Method: 提出OMDPG算法，使用最优边际Q函数替代顺序计算的Q函数，引入广义Q批评器，并采用集中式批评器分组执行器架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于多种先进的多智能体强化学习基线。

Conclusion: OMDPG成功解决了单调改进与ParPS的冲突，实现了高性能合作。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [42] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性，低成本地评估数据中的潜在意图性，适用于基础生物体。


<details>
  <summary>Details</summary>
Motivation: 探讨意图性和语境在科学和技术中的实际意义，弥补Searle之后对意图研究的不足。

Method: 利用Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性分离意图内容和环境语境。

Result: 提供了一种低成本、无需大规模训练或推理的潜在意图性评估方法，适用于基础生物体。

Conclusion: 该方法为意图性研究提供了实用且低成本的解决方案，但概念形成水平受限于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [43] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 论文提出了一种校准CoT推理准确性的新方法，通过利用模型内在的真实性编码动态选择最优推理路径，显著提升了推理任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: CoT推理在LLMs和MLLMs中表现出强大的深度推理能力，但中间步骤的错误积累会影响其可靠性。

Method: 通过特定注意力头激活反映推理步骤的真实性，训练置信度预测器动态选择最优推理路径。

Result: 实验表明，该方法在数学、符号和常识推理任务中显著优于现有基线，适用于单模态和多模态场景。

Conclusion: 该研究为CoT推理提供了一种新颖的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [44] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型（LLM）在SPARQL查询翻译中的表现，发现不同模型和提示策略的效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱（KG）互操作性研究中SPARQL查询自动翻译的空白。

Method: 使用三种LLM模型（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体进行测试。

Result: Wikidata到DBpedia的翻译效果优于反向翻译，模型和提示策略对性能影响显著。

Conclusion: LLM在SPARQL翻译中表现不一，需进一步优化模型和策略。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [45] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文填补了假设基础论证（ABA）中渐进语义学的空白，提出了一种新的渐进语义学家族，用于为ABA框架中的核心组件（假设）赋予辩证强度。


<details>
  <summary>Details</summary>
Motivation: 渐进语义学在计算论证中是一种细粒度的方法，但尚未应用于假设基础论证（ABA），尽管ABA是一种流行的结构化论证形式，且渐进语义学可能对其应用有益。

Method: 通过将双极集基础论证框架作为ABA框架的抽象，并推广最先进的模块化渐进语义学（QBAFs），提出了一种新的渐进ABA语义学。

Result: 提出的渐进ABA语义学满足平衡性和单调性等理想性质，并通过实验与基于论证的方法进行了比较。

Conclusion: 该研究为ABA框架提供了有效的渐进语义学方法，填补了现有研究的空白。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [46] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 论文介绍了BlueGlass框架，旨在通过统一基础设施整合多样化AI安全工具，提升AI系统的安全性，并以视觉语言模型为例展示了其效用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强和普及，现有安全工具无法单独提供全面保障，需要集成化的方法。

Method: 提出BlueGlass框架，支持整合和组合多样化安全工具，覆盖模型内部和输出。

Result: 通过三种安全分析展示了框架的实用性，包括分布评估、层级动态分析和稀疏自编码器识别可解释概念。

Conclusion: 该工作为构建更健壮可靠的AI系统提供了基础架构和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [47] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动化编排问题，比较了AI规划和强化学习方法，并提出了一种基于状态空间定义的新分类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高边缘-云系统中应用迁移的服务质量（QoS）和成本效益，探索现有技术以支持新兴计算连续体环境中的迁移编排。

Method: 从马尔可夫决策过程（MDP）出发，分析和比较了AI规划和强化学习方法，特别针对可建模为汉诺塔问题的迁移问题。

Result: 提出了一种基于状态空间定义的新分类，并分析了不同模型在此框架下的表现。

Conclusion: 研究为边缘-云系统中应用迁移的自动化编排提供了技术参考，支持未来计算连续体环境的发展。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [48] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少LLMs的偏见，展示了这种方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs仍在发展中，当前的偏见可能随时间变化，因此需要通用的去偏见策略。人类决策中的去偏见方法为LLMs提供了借鉴。

Method: 采用元认知提示（如“你可能是错的吗？”）引导LLMs反思其回答，揭示潜在的偏见、错误和矛盾信息。

Result: 元认知提示能有效引导LLMs识别自身偏见，并提供更全面的反思信息，改善初始回答的局限性。

Conclusion: 人类心理学为LLMs的提示工程提供了新思路，利用元认知提示可显著提升模型的自省能力。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [49] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，以实时优化飞行控制和数据收集调度，最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火监测中至关重要，但现有深度强化学习方法（DRL）存在采样效率低、仿真与现实的差距以及复杂训练等问题，不适合时间敏感的应用。

Method: FRSICL利用自然语言任务描述和环境反馈，实时生成数据收集计划和飞行速度控制，无需大量重新训练。

Result: 仿真结果表明，FRSICL在平均AoI优化上优于PPO和最近邻基线方法。

Conclusion: FRSICL是一种高效、动态的解决方案，适用于无人机辅助的野火监测系统。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [50] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 论文提出适应性概念，用于评估多智能体强化学习（MARL）在动态环境中的可靠性，并提出了一个包含学习适应性、策略适应性和场景驱动适应性的框架。


<details>
  <summary>Details</summary>
Motivation: MARL在真实多智能体系统（MAS）中部署受限，主要因环境的复杂性和动态性，需算法在持续变化的配置和需求下保持有效。

Method: 引入适应性概念，提出包含三个维度的结构化框架：学习适应性、策略适应性和场景驱动适应性。

Result: 通过适应性视角支持更原则性的MARL性能评估，超越狭窄定义的基准测试。

Conclusion: 该研究有助于开发更适合动态真实多智能体系统的算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [51] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合食谱、食材、替代品、营养数据及饮食限制，利用LLM增强图谱信息，并展示了其在营养查询中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统忽视非视觉因素（如食材替代对营养的影响）和个体需求（如过敏、文化习惯）。瑞士缺乏整合相关营养信息的集中资源。

Method: 构建SwissFKG，结合LLM增强图谱信息，开发Graph-RAG应用评估LLM在营养查询中的表现。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供食材级信息和营养指南支持。Graph-RAG应用展示了图谱在回答用户查询中的潜力。

Conclusion: SwissFKG为融合视觉、上下文和文化维度的下一代饮食评估工具奠定了基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [52] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验比较了决策变换器（DT）与基于MLP的过滤行为克隆（FBC）在稀疏奖励环境中的表现，发现FBC表现更优，且更简单高效。


<details>
  <summary>Details</summary>
Motivation: 探讨决策变换器（DT）在稀疏奖励环境中的适用性，并验证FBC是否更具优势。

Method: 在Robomimic和D4RL任务上实验，比较DT与FBC的性能。FBC通过过滤低质量轨迹后进行行为克隆。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效。

Conclusion: DT在稀疏奖励环境中并非优选，甚至可能在其他环境中也不占优，引发对其适用性的质疑。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [53] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种基于“what, why, who”三个维度分类和比较可解释人工智能（XAI）研究的方法，旨在解决任务描述不足、脱离上下文研究及用户测试不足等问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾且缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 结合视觉分析、认知科学和仪表板设计等多个领域，提出分类和比较XAI研究的三维框架。

Result: 研究发现主要问题包括任务描述不足、脱离上下文研究及目标用户测试不足，并提出研究应明确报告用户的领域、AI及数据分析专长。

Conclusion: 论文提出的研究指南有助于XAI社区更好地解析快速发展的领域，帮助研究人员和设计师识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [54] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理在现实场景中的应用，提出了五个核心能力，并分析了当前方法的性能差距和改进方向。


<details>
  <summary>Details</summary>
Motivation: 现实中的表格任务常涉及噪声和复杂性，而现有研究多针对干净数据集，因此需要探索更鲁棒的自动化解决方案。

Method: 定义了五个核心能力（C1-C5）来分析和比较现有方法，并以Text-to-SQL代理为例详细研究了性能差距。

Result: 发现开源模型在真实场景中表现不佳，提出了改进鲁棒性、泛化性和效率的实际建议。

Conclusion: 论文为提升基于LLM的表格代理在实践中的表现提供了具体指导。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [55] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，识别了23个相关实例特征，并利用降维和机器学习方法，揭示了实例结构对元启发式算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决实例特征与元启发式算法性能之间复杂关系的研究挑战，推动CVRP领域的发展。

Method: 采用实例空间分析（ISA）方法，结合PRELIM、SIFTED和PILOT阶段，利用降维和机器学习技术，生成二维实例空间投影。

Result: 识别了23个相关实例特征，并提供了投影矩阵，便于新实例的纳入分析。

Conclusion: ISA方法为CVRP领域提供了一种新的实例分析工具，有助于理解实例结构对算法性能的影响。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [56] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 论文提出了一种结合BERT情感分析和XGBoost的模型，用于预测远程学习中的学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习中辍学问题严重，早期检测对干预至关重要。现有研究强调整合多元数据（如社会人口、行为和情感数据）以提高预测准确性。

Method: 结合BERT对学生的评论进行情感分析，并通过XGBoost分析社会人口和行为数据。使用特征重要性技术选择关键特征，并在未见数据上测试模型。

Result: 模型在未见数据上达到84%的准确率，优于基线模型的82%，且在精确率和F1分数等指标上表现更优。

Conclusion: 该模型可作为个性化策略开发的工具，有效降低辍学率并促进学生坚持学习。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [57] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种在数据稀缺领域（如计算化学、计算免疫学和医学成像）中高效获取先验知识的方法，包括使用神经记忆和超网络设计，并展示了在3D场景生成和分子属性预测中的应用。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的领域中，传统的大规模预训练模型无法适用，因此需要设计新的架构来高效获取先验知识。

Method: 采用神经记忆和超网络设计，结合模型无关元学习（MAML），并在3D场景生成和分子属性预测中应用。

Result: 超网络设计能够高效获取先验知识，仅需少量样本即可适应非平稳分布，并在3D场景生成和分子属性预测中表现出色。

Conclusion: 提出的方法在数据稀缺领域具有潜力，能够高效获取和转移先验知识，为相关应用提供了新思路。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [58] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于LLM的自动化科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细致度。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索增强生成管道在科学文献合成中的局限性，提供用户可控、透明且可配置的合成方法。

Method: 采用递归、深度和广度可控的探索策略，结合参数驱动的配置和透明推理，实现高效文献整合。

Result: 在49个生态研究问题中，源整合量提升21倍，每1000字整合源提升14.9倍，高参数设置下达到专家级分析深度。

Conclusion: DeepResearch$^{\text{Eco}}$显著提升了科学文献合成的效率和质量，适用于高要求的领域研究。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [59] [A joint channel estimation and beamforming separation principle for massive MIMO systems](https://arxiv.org/abs/2507.08947)
*Lorenzo Miretti,Slawomir Stańczak,Giuseppe Caire*

Main category: cs.IT

TL;DR: 分离波束成形和信道估计在多用户MIMO系统中无最优性损失，适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 探讨在多用户MIMO系统中分离波束成形和信道估计是否会影响最优性。

Method: 提供条件，证明在集中式和分布式架构下，最优处理可分解为MMSE信道估计和MMSE波束成形。

Result: 在多种模型下，分离处理不影响最优性，并通过数值模拟验证。

Conclusion: 分离波束成形和信道估计在广泛条件下仍能保持最优性。

Abstract: We demonstrate that separating beamforming (i.e., downlink precoding and
uplink combining) and channel estimation in multi-user MIMO wireless systems
incurs no loss of optimality under general conditions that apply to a wide
variety of models in the literature, including canonical reciprocity-based
cellular and cell-free massive MIMO system models. Specifically, we provide
conditions under which optimal processing in terms of ergodic achievable rates
can be decomposed into minimum mean-square error (MMSE) channel estimation
followed by MMSE beamforming, for both centralized and distributed
architectures. Applications of our results are illustrated in terms of concrete
examples and numerical simulations.

</details>


### [60] [Fundamental Limits of Bistatic Integrated Sensing and Communications over Memoryless Relay Channels](https://arxiv.org/abs/2507.09193)
*Yao Liu,Min Li,Lawrence Ong,Aylin Yener*

Main category: cs.IT

TL;DR: 论文研究了双基地集成感知与通信问题，通过中继信道模型分析了通信与感知性能的权衡，提出了容量-失真函数的上界和下界，并在特定信道中验证了其一致性。


<details>
  <summary>Details</summary>
Motivation: 探索在双基地中继信道中同时实现高效通信和感知的可行性，为集成设计提供理论基础。

Method: 采用离散无记忆中继信道模型，提出混合部分解码-压缩转发编码方案，并推导容量-失真函数的上界和下界。

Result: 混合方案在忽略通信任务时达到最优感知性能，且在特定中继信道中上下界一致。

Conclusion: 集成设计在通信与感知性能权衡中具有优势，特定条件下可实现最优性能。

Abstract: The problem of bistatic integrated sensing and communications over memoryless
relay channels is considered, where destination concurrently decodes the
message sent by the source and estimates unknown parameters from received
signals with the help of a relay. A state-dependent discrete memoryless relay
channel is considered to model this setup, and the fundamental limits of the
communication-sensing performance tradeoff are characterized by the
capacity-distortion function. An upper bound on the capacity-distortion
function is derived, extending the cut-set bound results to address the sensing
operation at the destination. A hybrid-partial-decode-and-compress-forward
coding scheme is also proposed to facilitate source-relay cooperation for both
message transmission and sensing, establishing a lower bound on the
capacity-distortion function. It is found that the
hybrid-partial-decode-and-compress-forward scheme achieves optimal sensing
performance when the communication task is ignored. Furthermore, the upper and
lower bounds are shown to coincide for three specific classes of relay
channels. Numerical examples are provided to illustrate the
communication-sensing tradeoff and demonstrate the benefits of integrated
design.

</details>


### [61] [Data Fusion and Aggregation Methods to Develop Composite Indexes for a Sustainable Future](https://arxiv.org/abs/2507.09204)
*Abdullah Konak*

Main category: cs.IT

TL;DR: 本文探讨了环境风险建模中多指标数据融合的替代方法，分析了其应用场景、假设、数据需求及优缺点，并通过模拟和实际案例验证其效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在简化极端气候事件风险的量化与比较过程，通过数据融合技术生成关键绩效指标，以支持政策制定。

Method: 采用多种数据融合方法，结合专家意见，通过模拟和实际案例（如干旱韧性）验证方法的适用性。

Result: 模拟揭示了不同数据融合方法在各种场景下的优缺点，实际案例展示了其在政策建议中的应用潜力。

Conclusion: 数据融合方法在环境风险建模中具有实用价值，但需根据具体场景选择合适方法，以支持可持续发展政策。

Abstract: Research on environmental risk modeling relies on numerous indicators to
quantify the magnitude and frequency of extreme climate events, their
ecological, economic, and social impacts, and the coping mechanisms that can
reduce or mitigate their adverse effects. Index-based approaches significantly
simplify the process of quantifying, comparing, and monitoring risks associated
with other natural hazards, as a large set of indicators can be condensed into
a few key performance indicators. Data fusion techniques are often used in
conjunction with expert opinions to develop key performance indicators. This
paper discusses alternative methods to combine data from multiple indicators,
with an emphasis on their use-case scenarios, underlying assumptions, data
requirements, advantages, and limitations. The paper demonstrates the
application of these data fusion methods through examples from current risk and
resilience models and simplified datasets. Simulations are conducted to
identify their strengths and weaknesses under various scenarios. Finally, a
real-life example illustrates how these data fusion techniques can be applied
to inform policy recommendations in the context of drought resilience and
sustainability.

</details>


### [62] [On Lattice Isomorphism Problems for Lattices from LCD Codes over Finite Rings](https://arxiv.org/abs/2507.09257)
*Yusaku Nishimura,Katsuyuki Takashima,Tsuyoshi Miezaki*

Main category: cs.IT

TL;DR: 本文扩展了Ducas-Gibbons的工作，研究了基于有限环构造的格同构问题，证明了在某些条件下问题可简化为平凡格和图的同构问题。


<details>
  <summary>Details</summary>
Motivation: 研究更一般的基于有限环构造的格同构问题，扩展现有基于有限域的结果。

Method: 分析由有限环构造的格，探讨其同构问题在特定条件下的简化方法。

Result: 当k为奇数、奇素数幂或不被4整除时，格同构问题可简化为平凡格和图的同构问题。

Conclusion: 有限环构造的格同构问题在特定条件下具有与有限域类似的可简化性。

Abstract: These days, post-quantum cryptography based on the lattice isomorphism
problem has been proposed. Ducas-Gibbons introduced the hull attack, which
solves the lattice isomorphism problem for lattices obtained by Construction A
from an LCD code over a finite field. Using this attack, they showed that the
lattice isomorphism problem for such lattices can be reduced to the lattice
isomorphism problem with the trivial lattice $\mathbb{Z}^n$ and the graph
isomorphism problem. While the previous work by Ducas-Gibbons only considered
lattices constructed by a code over a \textit{finite field}, this paper
considers lattices constructed by a code over a \textit{finite ring}
$\mathbb{Z}/k\mathbb{Z}$, which is a more general case. In particular, when $k$
is odd, an odd prime power, or not divisible by $4$, we show that the lattice
isomorphism problem can be reduced to the lattice isomorphism problem for
$\mathbb{Z}^n$ and the graph isomorphism problem.

</details>


### [63] [Asymptotically optimal cyclic subspace codes](https://arxiv.org/abs/2507.09290)
*Chiara Castello,Paolo Santonastaso*

Main category: cs.IT

TL;DR: 本文提出了一种构造具有大基数且指定最小距离的循环子空间码的新技术，改进了现有构造的规模，并证明了其渐近达到Johnson类型界II。


<details>
  <summary>Details</summary>
Motivation: 子空间码（尤其是循环子空间码）在随机网络编码中的纠错应用引起了广泛关注，但现有构造的规模有限，需要改进。

Method: 引入新技术构造循环子空间码，适用于Grassmannian空间，当k整除n且n/k为合数时，最小距离为2k-2。

Result: 新构造的码规模大于已知同类参数构造，且渐近达到Johnson类型界II。

Conclusion: 新技术显著提升了循环子空间码的构造效率和应用潜力。

Abstract: Subspace codes, and in particular cyclic subspace codes, have gained
significant attention in recent years due to their applications in error
correction for random network coding. In this paper, we introduce a new
technique for constructing cyclic subspace codes with large cardinality and
prescribed minimum distance. Using this new method, we provide new
constructions of cyclic subspace codes in the Grassmannian $\mathcal{G}_q(n,k)$
of all $k$-dimensional $\mathbb{F}_q$-subspaces of an $n$-dimensional vector
space over $\mathbb{F}_q$, when $k\mid n$ and $n/k$ is a composite number, with
minimum distance $2k-2$ and large size. We prove that the resulting codes have
sizes larger than those obtained from previously known constructions with the
same parameters. Furthermore, we show that our constructions of cyclic subspace
codes asymptotically reach the Johnson type bound II for infinite values of
$n/k$.

</details>


### [64] [Joint Access Point Activation and Power Allocation for Cell-Free Massive MIMO Aided ISAC Systems](https://arxiv.org/abs/2507.09425)
*Nguyen Xuan Tung,Le Tung Giang,Trinh Van Chien,Hoang Trong Minh,Lajos Hanzo*

Main category: cs.IT

TL;DR: 论文研究了基于无蜂窝大规模MIMO的集成感知与通信系统，提出了一种联合激活AP选择和功率控制的方法，以提高能效。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信系统中，仅需激活部分AP即可完成任务，而关闭冗余AP可节省功耗，因此需要优化能效。

Method: 采用基于模型的Branch-and-Bound方法作为基准，指导半监督异构图神经网络（HetGNN）选择最佳激活AP和功率分配。

Result: HetGNN将功耗降低20-25%，且运行速度比模型基准快近10,000倍。

Conclusion: HetGNN在能效和计算效率方面显著优于传统方法。

Abstract: Cell-free massive multiple-input multiple-output (MIMO)-aided integrated
sensing and communication (ISAC) systems are investigated where distributed
access points jointly serve users and sensing targets. We demonstrate that only
a subset of access points (APs) has to be activated for both tasks, while
deactivating redundant APs is essential for power savings. This motivates joint
active AP selection and power control for optimizing energy efficiency. The
resultant problem is a mixed-integer nonlinear program (MINLP). To address
this, we propose a model-based Branch-and-Bound approach as a strong baseline
to guide a semi-supervised heterogeneous graph neural network (HetGNN) for
selecting the best active APs and the power allocation. Comprehensive numerical
results demonstrate that the proposed HetGNN reduces power consumption by
20-25\% and runs nearly 10,000 times faster than model-based benchmarks.

</details>


### [65] [Introducing Meta-Fiber into Stacked Intelligent Metasurfaces for MIMO Communications: A Low-Complexity Design with only Two Layers](https://arxiv.org/abs/2507.09575)
*Hong Niu,Jiancheng An,Tuo Wu,Jiangong Chen,Yufei Zhao,Yong Liang Guan,Marco Di Renzo,Merouane Debbah,George K. Karagiannidis,H. Vincent Poor,Chau Yuen*

Main category: cs.IT

TL;DR: 论文提出了一种基于元纤维的堆叠智能超表面（SIM）设计，通过减少层数和优化相位偏移，显著提升了信道容量和能效。


<details>
  <summary>Details</summary>
Motivation: 当前多层的SIM设计存在相位偏移优化复杂和能量衰减问题，需要一种更高效的解决方案。

Method: 引入元纤维连接的两层SIM，通过交替优化算法设计相位偏移，建立无干扰并行子信道系统。

Result: 相比传统七层SIM，提出的两层SIM在信道容量上提升25%，同时减少59%的元原子数量。

Conclusion: 元纤维SIM设计在性能和效率上优于传统多层结构，为未来无线通信提供了新思路。

Abstract: Stacked intelligent metasurfaces (SIMs), which integrate multiple
programmable metasurface layers, have recently emerged as a promising
technology for advanced wave-domain signal processing. SIMs benefit from
flexible spatial degree-of-freedom (DoF) while reducing the requirement for
costly radio-frequency (RF) chains. However, current state-of-the-art SIM
designs face challenges such as complex phase shift optimization and energy
attenuation from multiple layers. To address these aspects, we propose
incorporating meta-fibers into SIMs, with the aim of reducing the number of
layers and enhancing the energy efficiency. First, we introduce a
meta-fiber-connected 2-layer SIM that exhibits the same flexible signal
processing capabilities as conventional multi-layer structures, and explains
the operating principle. Subsequently, we formulate and solve the optimization
problem of minimizing the mean square error (MSE) between the SIM channel and
the desired channel matrices. Specifically, by designing the phase shifts of
the meta-atoms associated with the transmitting-SIM and receiving-SIM, a
non-interference system with parallel subchannels is established. In order to
reduce the computational complexity, a closed-form expression for each phase
shift at each iteration of an alternating optimization (AO) algorithm is
proposed. We show that the proposed algorithm is applicable to conventional
multi-layer SIMs. The channel capacity bound and computational complexity are
analyzed to provide design insights. Finally, numerical results are
illustrated, demonstrating that the proposed two-layer SIM with meta-fiber
achieves over a 25% improvement in channel capacity while reducing the total
number of meta-atoms by 59% as compared with a conventional seven-layer SIM.

</details>


### [66] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 论文提出了一种轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计，旨在降低计算复杂度并适应资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 6G等下一代无线技术对超高数据速率、低延迟和增强连接性提出了高要求，而XL-MIMO和RIS是实现这些目标的关键技术。然而，其性能依赖于准确的CSI，现有估计模型在XL-MIMO系统中的可扩展性和实际部署仍受限。

Method: 提出了一种基于空间相关性的轻量级深度学习框架，通过分块训练机制降低输入维度，同时保留关键信息，实现大规模系统的可扩展训练。

Result: 仿真结果表明，该框架显著提高了估计精度并降低了计算复杂度，且不受XL-MIMO系统中天线和RIS元件数量增加的影响。

Conclusion: 该框架为XL-MIMO系统中的高效信道估计提供了可行的解决方案，适用于资源受限的边缘设备。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


### [67] [RDD Function: A Tradeoff Between Rate and Distortion-in-Distortion](https://arxiv.org/abs/2507.09712)
*Lingyi Chen,Haoran Tang,Shitong Wu,Jiakun Liu,Huihui Wu,Wenyi Zhang,Hao Wu*

Main category: cs.IT

TL;DR: 提出了一种名为RDD的新函数，扩展了经典RD函数，用Gromov型失真替代了期望失真约束，并通过编码定理验证了其操作性。


<details>
  <summary>Details</summary>
Motivation: 解决不同维度空间相似性度量问题，扩展经典RD函数的应用范围。

Method: 开发了一种交替镜像下降算法，通过分解、线性化和松弛技术降低计算复杂度。

Result: 仿真实验验证了算法的有效性，RDD函数在源编码中具有潜在应用价值。

Conclusion: RDD函数与RD函数的区别与联系预示其将在未来场景中发挥新作用。

Abstract: In this paper, we propose a novel function named Rate
Distortion-in-Distortion (RDD) function as an extension of the classical
rate-distortion (RD) function, where the expected distortion constraint is
replaced by the Gromov-type distortion. This distortion, integral to the
Gromov-Wasserstein (GW) distance, effectively defines the similarity in spaces
of different dimensions without a direct metric between them. While our RDD
function qualifies as an informational RD function, encoding theorems
substantiate its status as an operational RD function, thereby underscoring its
potential applicability in real-world source coding. Due to the high
computational complexity associated with Gromov-type distortion, the RDD
function cannot be solved analytically. Consequently, we develop an alternating
mirror descent algorithm that significantly reduces computational complexity by
employing decomposition, linearization, and relaxation techniques. Simulations
on classical sources and different grids demonstrate the effectiveness of our
algorithm. By examining the distinctions and connections between the RDD
function and the RD function, we anticipate that RDD function will play a novel
role in foreseeable future scenarios.

</details>


### [68] [Majority Logic Decoding of Affine Grassmann Codes Over Nonbinary Fields](https://arxiv.org/abs/2507.09741)
*Fernando Piñero González,Prasant Singh,Rohit Yadav*

Main category: cs.IT

TL;DR: 本文研究了非二进制域上仿射格拉斯曼码的解码问题，通过不同秩矩阵构造正交校验集，利用多数逻辑解码纠正大量错误。


<details>
  <summary>Details</summary>
Motivation: 解决仿射格拉斯曼码在非二进制域上的解码问题，提升错误纠正能力。

Method: 使用不同秩矩阵构造正交校验集，并利用码的自同构群为每个坐标生成正交校验，进行多数逻辑解码。

Result: 解码器的错误纠正能力和复杂度与[BS21]中的格拉斯曼码多数逻辑解码器相同。

Conclusion: 提出的方法能有效纠正仿射格拉斯曼码中的大量错误，性能与现有方法相当。

Abstract: In this article, we consider the decoding problem of affine Grassmann codes
over nonbinary fields. We use matrices of different ranks to construct a large
set consisting of parity checks of affine Grassmann codes, which are orthogonal
with respect to a fixed coordinate. By leveraging the automorphism groups of
these codes, we generate a set of orthogonal parity checks for each coordinate.
Using these parity checks, we perform majority logic decoding to correct a
large number of errors in affine Grassmann codes. The order of error correction
capability and the complexity of this decoder for affine Grassmann codes are
the same as those of the majority logic decoder for Grassmann codes proposed in
[BS21].

</details>


### [69] [Remote Safety Monitoring: Significance-Aware Status Updating for Situational Awareness](https://arxiv.org/abs/2507.09833)
*Tasmeen Zaman Ornee,Md Kamran Chowdhury Shisher,Clement Kam,Yin Sun*

Main category: cs.IT

TL;DR: 研究多传感器、多通道远程安全监控中的联合传输调度与估计设计，提出低复杂度的MGF策略，证明其渐近最优性，并展示优于其他策略的性能。


<details>
  <summary>Details</summary>
Motivation: 由于传输错误和资源限制，状态更新可能不新鲜，导致对安全状况的误判，尤其是将危险误判为安全时风险极高。

Method: 将联合设计问题转化为顺序优化的估计与调度问题，提出MGF策略，无需索引性条件即可实现渐近最优。

Result: 数值结果表明，MGF策略在性能上优于周期性更新、随机策略和MAF策略。

Conclusion: MGF策略在多传感器、多通道环境下高效且易于实现，为远程安全监控提供了实用解决方案。

Abstract: In this study, we consider a problem of remote safety monitoring, where a
monitor pulls status updates from multiple sensors monitoring several
safety-critical situations. Based on the received updates, multiple estimators
determine the current safety-critical situations. Due to transmission errors
and limited channel resources, the received status updates may not be fresh,
resulting in the possibility of misunderstanding the current safety situation.
In particular, if a dangerous situation is misinterpreted as safe, the safety
risk is high. We study the joint design of transmission scheduling and
estimation for multi-sensor, multi-channel remote safety monitoring, aiming to
minimize the loss due to the unawareness of potential danger. We show that the
joint design of transmission scheduling and estimation can be reduced to a
sequential optimization of estimation and scheduling. The scheduling problem
can be formulated as a Restless Multi-armed Bandit (RMAB) , for which it is
difficult to establish indexability. We propose a low-complexity Maximum Gain
First (MGF) policy and prove it is asymptotically optimal as the numbers of
sources and channels scale up proportionally, without requiring the
indexability condition. We also provide an information-theoretic interpretation
of the transmission scheduling problem. Numerical results show that our
estimation and scheduling policies achieves higher performance gain over
periodic updating, randomized policy, and Maximum Age First (MAF) policy.

</details>


### [70] [Incomplete Multiview Learning via Wyner Common Information](https://arxiv.org/abs/2507.09843)
*AbdAlRahman Odeh,Teng-Hui Huang,Hesham El Gamal*

Main category: cs.IT

TL;DR: 论文提出了一种名为WyIMVC的高效求解器，用于不完整多视图聚类问题，通过共享信息实现聚类和缺失值推断，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不完整多视图数据在实际应用中的聚类问题，如生成学习、跨模态检索和无线设备识别。

Method: 扩展了信息论中的共同信息框架，提出WyIMVC求解器，利用共同随机性实现联合聚类和缺失值推断，并基于凸差结构设计高效求解器。

Result: 在多种不完整多视图数据集上，WyIMVC优于现有方法，且求解器具有与初始化无关的收敛保证。

Conclusion: WyIMVC为不完整多视图聚类提供了一种高效且性能优越的解决方案。

Abstract: Incomplete multiview clustering is of high recent interest, fueled by the
advancement of common information-based deep multiview learning. The practical
scenarios where unpaired multiview data with missing values have wide
applications in generative learning, cross-modal retrieval, and wireless device
identification problems. Following the perspective that the shared information
between the incomplete multiview data aligns with the cluster targets, recent
works have generalized the well-known common information frameworks in
information theory multiview learning problems, with improved performance
reported. Different from previous works, we extend the frameworks to incomplete
multiview clustering problems and propose an efficient solver: Wyner Incomplete
MultiView Clustering (WyIMVC). Interestingly, the common randomness in WyIMVC
allows for joint clustering and missing value inference in contrast to the
compared methods in the literature. Moreover, leveraging the
difference-of-convex structure of the formulated problems, we propose an
efficient solver with a convergence guarantee independent of initialization.
Empirically, our solver outperforms the state-of-the-art solvers in a range of
incomplete multiview datasets with varying numbers of views and dimensions.

</details>


### [71] [Several new classes of self-orthogonal minimal linear codes violating the Ashikhmin-Barg condition](https://arxiv.org/abs/2507.09856)
*Wengang Jin,Kangquan Li,Longjiang Qu*

Main category: cs.IT

TL;DR: 本文研究了违反AB条件的自正交极小线性码的构造，并提供了其权重分布。


<details>
  <summary>Details</summary>
Motivation: 线性码在编码理论和密码学中有重要应用，构造同时满足极小性和自正交性的线性码具有研究价值。

Method: 通过p元函数构造了两类自正交线性码，并基于plateaued函数构造了违反AB条件的自正交线性码。

Result: 提出了几类违反AB条件的自正交极小线性码，并确定了其权重分布。

Conclusion: 本文首次研究了违反AB条件且满足自正交性的线性码构造，为相关领域提供了新思路。

Abstract: Linear codes have attracted considerable attention in coding theory and
cryptography due to their significant applications in secret sharing schemes,
secure two-party computation, Galois geometries, among others. As two special
subclasses of linear codes, minimal linear codes and self-orthogonal linear
codes are of particular interest. Constructing linear codes that possess both
minimality and self-orthogonality is very interesting. The main purpose of this
paper is to construct self-orthogonal minimal linear codes that violate the
Ashikhmin-Barg (AB for short) condition over the finite field $\mathbb{F}_p$.
First, we present several classes of self-orthogonal minimal linear codes
violating the AB condition over the finite field $\mathbb{F}_2$ and determine
their weight distributions. Next, for any odd prime $p$, we construct two
classes of self-orthogonal linear codes from $p$-ary functions, which contain
some optimal or almost optimal codes. Finally, based on plateaued functions, we
construct two classes of self-orthogonal linear codes that violate the AB
condition. Their weight distributions are also provided. To the best of our
knowledge, this paper is the first to investigate the constructions of linear
codes that violate the AB condition and satisfy self-orthogonality.

</details>


### [72] [BiD Codes: Algebraic Codes from $3 \times 3$ Kernel](https://arxiv.org/abs/2507.10068)
*Anirudh Dash,K. R. Nandakishore,Lakshmi Prasad Natarajan,Prasad Krishnan*

Main category: cs.IT

TL;DR: BiD码是一种基于3×3核矩阵的Kronecker积构造的Abelian码，其最小距离接近Reed-Muller码，且在块长度渐近时优于RM码。仿真显示其性能优于或接近RM、RM-Polar和CRC-Polar码。


<details>
  <summary>Details</summary>
Motivation: 提出BiD码以提供一种在实用块长度下性能接近RM码、在渐近块长度下性能更优的编码方案。

Method: 通过Kronecker积构造3×3核矩阵，生成长度为3^m的Abelian码。

Result: BiD码在长度为243时的仿真表现优于或接近RM、RM-Polar和CRC-Polar码。

Conclusion: BiD码是一种有潜力的编码方案，尤其在渐近块长度下表现更优。

Abstract: We introduce Berman-intersection-dual Berman (BiD) codes. These are abelian
codes of length $3^m$ that can be constructed using Kronecker products of a $3
\times 3$ kernel matrix. BiD codes offer minimum distance close to that of
Reed-Muller (RM) codes at practical blocklengths, and larger distance than RM
codes asymptotically in the blocklength. Simulations of BiD codes of length
$3^5=243$ in the erasure and Gaussian channels show that their block error
rates under maximum-likelihood decoding are similar to, and sometimes better,
than RM, RM-Polar, and CRC-aided Polar codes.

</details>


### [73] [Learning-Aided Iterative Receiver for Superimposed Pilots: Design and Experimental Evaluation](https://arxiv.org/abs/2507.10074)
*Xinjie Li,Xingyu Zhou,Yixiao Cao,Jing Zhang,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 论文提出了一种基于迭代反馈的接收机设计，结合联合信道估计、检测和解码，以解决MIMO-OFDM系统中叠加导频传输的挑战。


<details>
  <summary>Details</summary>
Motivation: 叠加导频传输在MIMO-OFDM系统中能提高频谱效率，但存在导频污染和数据干扰问题，需要改进接收机设计。

Method: 提出两种自适应信道估计策略：基于变分消息传递（VMP）及其低复杂度变体（VMP-L），以及一种深度学习（DL）估计器，结合卷积神经网络和注意力机制。

Result: 仿真和实验表明，该接收机在吞吐量和误块率上优于传统正交导频基线，DL估计器在性能和复杂度间取得良好平衡。

Conclusion: 该设计在动态无线环境中具有实际应用潜力，DL估计器尤其适合实际部署。

Abstract: The superimposed pilot transmission scheme offers substantial potential for
improving spectral efficiency in MIMO-OFDM systems, but it presents significant
challenges for receiver design due to pilot contamination and data
interference. To address these issues, we propose an advanced iterative
receiver based on joint channel estimation, detection, and decoding, which
refines the receiver outputs through iterative feedback. The proposed receiver
incorporates two adaptive channel estimation strategies to enhance robustness
under time-varying and mismatched channel conditions. First, a variational
message passing (VMP) method and its low-complexity variant (VMP-L) are
introduced to perform inference without relying on time-domain correlation.
Second, a deep learning (DL) based estimator is developed, featuring a
convolutional neural network with a despreading module and an attention
mechanism to extract and fuse relevant channel features. Extensive simulations
under multi-stream and high-mobility scenarios demonstrate that the proposed
receiver consistently outperforms conventional orthogonal pilot baselines in
both throughput and block error rate. Moreover, over-the-air experiments
validate the practical effectiveness of the proposed design. Among the methods,
the DL based estimator achieves a favorable trade-off between performance and
complexity, highlighting its suitability for real-world deployment in dynamic
wireless environments.

</details>


### [74] [Improved Differential Evolution for Enhancing the Aggregated Channel Estimation of RIS-Aided Cell-Free Massive MIMO](https://arxiv.org/abs/2507.10113)
*Trinh Van Chien,Nguyen Hoang Viet,Symeon Chatzinotas,Lajos Hanzo*

Main category: cs.IT

TL;DR: 论文研究了支持可重构智能表面（RIS）的无蜂窝大规模MIMO系统，通过优化RIS相位设计提升信道估计性能，并提出一种改进的差分进化算法以避免局部最优解。


<details>
  <summary>Details</summary>
Motivation: 在空间相关性存在的情况下，提升信道估计的准确性，优化RIS相位设计以改善系统性能。

Method: 采用线性最小均方误差（LMMSE）估计方法，提出一种改进的差分进化算法优化RIS相位设计，最小化归一化均方误差（NMSE）。

Result: 数值结果表明，所提算法显著优于现有基准方法，提升了信道估计质量。

Conclusion: 通过优化RIS相位设计和改进差分进化算法，论文实现了信道估计性能的显著提升。

Abstract: Cell-Free Massive multiple-input multiple-output (MIMO) systems are
investigated with the support of a reconfigurable intelligent surface (RIS).
The RIS phase shifts are designed for improved channel estimation in the
presence of spatial correlation. Specifically, we formulate the channel
estimate and estimation error expressions using linear minimum mean square
error (LMMSE) estimation for the aggregated channels. An optimization problem
is then formulated to minimize the average normalized mean square error (NMSE)
subject to practical phase shift constraints. To circumvent the problem of
inherent nonconvexity, we then conceive an enhanced version of the differential
evolution algorithm that is capable of avoiding local minima by introducing an
augmentation operator applied to some high-performing Diffential Evolution (DE)
individuals. Numerical results indicate that our proposed algorithm can
significantly improve the channel estimation quality of the state-of-the-art
benchmarks.

</details>


### [75] [High Girth Spatially-Coupled LDPC Codes with Hierarchical Structure](https://arxiv.org/abs/2507.10185)
*Haizheng Li,Sisi Miao,Laurent Schmalen*

Main category: cs.IT

TL;DR: 提出了一种构造大围长、小约束长度的QC-SC LDPC码的算法，通过“原型图到基图”方法实现低复杂度构造。


<details>
  <summary>Details</summary>
Motivation: QC-LDPC码因其硬件实现简单且性能优异而受到关注，但如何在保持小约束长度的同时构造大围长的SC-LDPC码仍具挑战性。

Method: 基于周期结构的循环相关矩阵（CRM），扩展了HQC构造方法至SC-LDPC码，通过小提升因子避免短循环。

Result: 数值结果表明，该算法能有效实现目标围长，且提升因子小，支持低复杂度构造。

Conclusion: 该算法为构造高性能QC-SC LDPC码提供了一种高效方法，适用于硬件实现。

Abstract: Quasi-cyclic (QC) low-density parity-check (LDPC) codes are a class of LDPC
codes with a simple construction facilitating hardware implementation while
achieving excellent performance. In this paper, we introduce an algorithm that
constructs QC spatially-coupled (SC) LDPC codes with large girth while keeping
the constraint length small. The algorithm offers a "protograph to basegraph"
construction, focusing on finding small lifting sizes of QC codes while
avoiding short cycles. This work extends the hierarchical quasi-cyclic (HQC)
construction for block LDPC codes proposed by Wang et al. to the spatially
coupled case. The construction is based on the cycle relevant matrix (CRM)
derived from the periodic structure of time-invariant SC-LDPC codes. Numerical
results show that the proposed algorithm effectively achieves the target girth
with a small lifting factor, enabling low-complexity SC code construction.

</details>


### [76] [Low-Power Wake-Up Signal Design in 3GPP 5G-Advanced Release 19](https://arxiv.org/abs/2507.10207)
*Sebastian Wagner*

Main category: cs.IT

TL;DR: 5G-Advanced Release 19引入的LP-WUS和LP-SS显著提升了物联网通信的能效，通过低功耗检测器实现主无线电关闭，节省高达80%的功耗。


<details>
  <summary>Details</summary>
Motivation: 提升物联网设备的能效，减少传统5G寻呼机制的高功耗问题。

Method: 设计LP-WUS和LP-SS程序，利用低功耗能量检测器（ED）检测信号，主无线电（MR）保持关闭。

Result: 相比传统5G寻呼机制，功耗节省高达80%。

Conclusion: LP-WUS和LP-SS是5G-Advanced中实现高效物联网通信的重要技术。

Abstract: The Low-Power Wake-Up Signal (LP-WUS) and Low-Power Synchronization Signal
(LP-SS), introduced in 3GPP 5G-Advanced Release 19, represent a major step
forward in enabling power-efficient IoT communications. This paper presents a
comprehensive overview of the LP-WUS and LP-SS procedures in the RRC_IDLE and
RRC_INACTIVE states, and outlines key physical layer design choices. The LP-WUS
is designed to be detected by a low-power energy detector (ED), allowing the
main radio (MR) to remain switched off. This architecture enables power savings
of up to 80% compared to conventional 5G paging mechanisms.

</details>


### [77] [Dimensionality increase for error correction in the interaction between information space and the physical world](https://arxiv.org/abs/2507.10234)
*Tatyana Barron*

Main category: cs.IT

TL;DR: 论文提出将有限维数学模型嵌入更高维模型以找到解决方案。


<details>
  <summary>Details</summary>
Motivation: 人类智能的进化导致信息空间中数据量巨大，处理和访问这些数据有助于解决基于有限维模型的应用问题。

Method: 将有限维数学模型嵌入更高维模型。

Result: 在高维模型中存在所需的解决方案。

Conclusion: 通过嵌入更高维模型，可以找到有限维模型中的解决方案。

Abstract: The evolution of human intelligence led to the huge amount of data in the
information space. Accessing and processing this data helps in finding
solutions to applied problems based on finite-dimensional models. We argue,
that formally, such a mathematical model can be embedded into a
higher-dimensional model inside of which a desired solution will exist.

</details>


### [78] [A Matrix Completion Approach for the Construction of MDP Convolutional Codes](https://arxiv.org/abs/2507.10417)
*Sakshi Dang,Julia Lieb,Okko Makkonen,Pedro Soto,Alex Sprintson*

Main category: cs.IT

TL;DR: 本文提出了一种降低MDP卷积码编码复杂度的方法，通过构造具有结构化稀疏生成矩阵的部分单位记忆MDP码。


<details>
  <summary>Details</summary>
Motivation: MDP码因其最大延迟约束纠错能力而重要，但编码和解码的复杂度问题未得到充分关注。

Method: 采用矩阵完成框架，将小域上的结构化超正则矩阵扩展为MDP码的稀疏滑动生成矩阵。

Result: 所提出的构造方法能够降低编码复杂度，优于现有MDP码设计。

Conclusion: 通过结构化稀疏生成矩阵，成功降低了MDP码的编码复杂度。

Abstract: Maximum Distance Profile (MDP) convolutional codes are an important class of
channel codes due to their maximal delay-constrained error correction
capabilities. The design of MDP codes has attracted significant attention from
the research community. However, only limited attention was given to addressing
the complexity of encoding and decoding operations. This paper aims to reduce
encoding complexity by constructing partial unit-memory MDP codes with
structured and sparse generator matrices. In particular, we present a matrix
completion framework that extends a structured superregular matrix (e.g.,
Cauchy) over a small field to a sparse sliding generator matrix of an MDP code.
We show that the proposed construction can reduce the encoding complexity
compared to the current state-of-the-art MDP code designs.

</details>


### [79] [A mapping of the Min-Sum decoder to reduction operations, and its implementation using CUDA kernels](https://arxiv.org/abs/2507.10424)
*Omer Shimon Sella,Thomas Heinis*

Main category: cs.IT

TL;DR: 提出了一种与奇偶校验矩阵内容无关的GPU实现Min-Sum解码器，仅依赖矩阵维度。


<details>
  <summary>Details</summary>
Motivation: 传统的LDPC解码器通常针对特定应用优化，依赖于奇偶校验矩阵的具体内容和结构，限制了通用性。

Method: 将奇偶校验矩阵作为Min-Sum解码器的参数，设计了一种仅依赖矩阵维度的GPU实现。

Result: 实现了一种通用性更强的解码器，不依赖矩阵具体内容。

Conclusion: 该方法为LDPC解码提供了更灵活的解决方案，适用于不同矩阵结构。

Abstract: Decoders for Low Density Parity Check (LDPC) codes are usually tailored to an
application and optimized once the specific content and structure of the parity
matrix are known. In this work we consider the parity matrix as an argument of
the Min-Sum decoder, and provide a GPU implementation that is independent of
the content of the parity matrix, and relies only on its dimensions.

</details>
