<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 42]
- [cs.AI](#cs.AI) [Total: 113]
- [cs.IT](#cs.IT) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Simulation-Based Study of AI-Assisted Channel Adaptation in UAV-Enabled Cellular Networks](https://arxiv.org/abs/2602.13199)
*Andrii Grekhov,Volodymyr Kharchenko,Vasyl Kondratiuk*

Main category: cs.NI

TL;DR: 该论文研究了无人机蜂窝网络中基于人工智能的信道自适应技术，使用线性回归模型实时调整传输参数以应对动态干扰环境。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决无人机蜂窝网络中动态干扰环境下的通信性能问题，通过AI辅助的信道自适应技术来优化通信质量。

Method: 采用基于线性回归的轻量级监督机器学习方法，基于数据包级性能指标实时调整传输大小，响应比特错误率和有效数据率的变化。

Result: 开发了自定义仿真环境生成训练和测试数据集，评估了静态和自适应信道配置下的系统行为，验证了AI辅助自适应方法的有效性。

Conclusion: 研究表明AI辅助的信道自适应技术能够有效提升无人机蜂窝网络在动态干扰环境下的通信性能，为实际应用提供了理论支持。

Abstract: This paper presents a simulation based study of Artificial Intelligence assisted communication channel adaptation in Unmanned Aerial Vehicle enabled cellular networks. The considered system model includes communication channel Ground Base Station Aerial Repeater UAV Base Station Cluster of Cellular Network Users. The primary objective of the study is to investigate the impact of adaptive channel parameter control on communication performance under dynamically changing interference conditions. A lightweight supervised machine learning approach based on linear regression is employed to implement cognitive channel adaptation. The AI model operates on packet level performance indicators and enables real time adjustment of Transaction Size in response to variations in Bit Error Rate and effective Data Rate. A custom simulation environment is developed to generate training and testing datasets and to evaluate system behavior under both static and adaptive channel configurations.

</details>


### [2] [Traffic Simulation in Ad Hoc Network of Flying UAVs with Generative AI Adaptation](https://arxiv.org/abs/2602.13200)
*Andrii Grekhov,Volodymyr Kharchenko,Vasyl Kondratiuk*

Main category: cs.NI

TL;DR: 该论文为无人机自组织网络建模交通流量，并展示使用人工智能自适应通信信道的方法，分析了不同参数对数据包丢失的影响，并实现了自适应数据传输。


<details>
  <summary>Details</summary>
Motivation: 研究无人机自组织网络中的通信性能问题，特别是在动态环境下如何优化数据传输，通过人工智能实现通信信道的自适应调整以提高网络可靠性。

Method: 基于包含20架无人机的原始自组织网络模型进行建模，分析数据包大小、传输功率、频率、飞行区域和无人机数量对数据包丢失的影响，并实现人工智能驱动的自适应数据传输程序。

Result: 获得了不同参数对数据包丢失的依赖关系，展示了人工智能自适应过程中数据包丢失、功率和事务大小随时间的变化趋势，证明了自适应方法的有效性。

Conclusion: 通过人工智能实现的自适应数据传输能够有效优化无人机自组织网络的通信性能，减少数据包丢失，提高网络可靠性。

Abstract: The purpose of this paper is to model traffic in Ad Hoc network of Unmanned Aerial Vehicles and demonstrate a way for adapting communication channel using Artificial Intelligence. The modeling was based on the original model of Ad Hoc network including 20 Unmanned Aerial Vehicles. The dependences of packet loss on the packet size for different transmission powers, on the packet size for different frequencies, on Unmanned Aerial Vehicles flight area and on the number of Unmanned Aerial Vehicles were obtained and analyzed. The implementation of adaptive data transmission is presented in the program code. The dependences of packet loss, power and transaction size on time during Artificial Intelligence adaptation are shown.

</details>


### [3] [CLF-ULP: Cross-Layer Fusion-Based Link Prediction in Dynamic Multiplex UAV Networks](https://arxiv.org/abs/2602.13201)
*Cunlai Pu,Fangrui Wu,Zhe Wang,Xiangbo Shu*

Main category: cs.NI

TL;DR: 提出CLF-ULP模型，用于预测动态多路无人机网络中未来链路，通过跨层注意力融合和共享参数LSTM实现时空特征提取，在多种移动模式数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 无人机网络中存在动态异构的链路，用于通信覆盖、协同感知和任务协作等目的。理解这些链路的形成和演化对于无人机网络系统的控制与维护具有理论和实践重要性。

Method: 1. 建立动态多路网络模型描述无人机网络的动态异构链路特性；2. 提出CLF-ULP模型，使用图注意力网络提取各层拓扑特征并进行跨层注意力融合；3. 采用共享参数LSTM建模各层时间演化；4. 设计联合损失函数同时考虑层内和层间无人机邻接关系。

Result: 在多种移动模式下的模拟无人机数据集上进行广泛实验，CLF-ULP在预测动态多路无人机网络链路方面达到了最先进的性能。

Conclusion: CLF-ULP模型能够有效预测动态多路无人机网络中的未来链路，为无人机网络系统的控制与维护提供了有力工具。

Abstract: In complex Unmanned Aerial Vehicle (UAV) networks, UAVs can establish dynamic and heterogeneous links with one another for various purposes, such as communication coverage, collective sensing, and task collaboration. These interactions give rise to dynamic multiplex UAV networks, where each layer represents a distinct type of interaction among UAVs. Understanding how such links form and evolve is both of theoretical interest and of practical importance for the control and maintenance of networked UAV systems. In this paper, we first develop a dynamic multiplex network model for UAV networks to characterize their dynamic and heterogeneous link properties. We then propose a cross-layer fusion-based deep learning model, termed CLF-ULP, to predict future inter-UAV links based on historical topology data. CLF-ULP incorporates graph attention networks to extract topological features within each layer and perform a cross-layer attention fusion to capture inter-layer dependencies. Furthermore, a shared-parameter long short-term memory network is employed to model the temporal evolution of each layer. To improve embedding quality and link prediction performance, we develop a joint loss function that considers both intra-layer and inter-layer UAV adjacency. Extensive experiments on simulated UAV datasets under diverse mobility patterns demonstrate that CLF-ULP achieves state-of-the-art performance in predicting links within dynamic multiplex UAV networks.

</details>


### [4] [Enhancing NOMA Handover Performance Using Hybrid AI-Driven Modulated Deterministic Sequences](https://arxiv.org/abs/2602.13202)
*Sumita Majhi,G Vasantha Reddy,Pinaki Mitra*

Main category: cs.NI

TL;DR: 提出结合Gold-Walsh调制序列与深度Q网络的混合方法，用于优化NOMA切换中的干扰管理，显著提升切换成功率、吞吐量并降低干扰。


<details>
  <summary>Details</summary>
Motivation: NOMA作为5G网络提升频谱效率的关键技术，在切换过程中容易受到干扰，影响网络性能。现有方法在动态干扰管理方面存在不足，需要智能化的解决方案。

Method: 提出混合方法：结合Gold-Walsh调制序列与深度Q网络（DQN）。Gold-Walsh序列用于调制，DQN用于智能决策。方法动态优化序列选择和功率分配，通过强化学习训练网络适应不同移动场景。

Result: 实现95.2%的切换成功率（提升23.1个百分点），吞吐量提升28%，干扰降低41%。所有改进统计显著（p<0.001）。DQN训练需要4200±400个回合，复杂度为O(N log N + d·h + log B)，可实时部署。

Conclusion: 提出的Gold-Walsh-DQN混合方法能有效管理NOMA切换干扰，显著提升网络性能指标，具有实际部署可行性，为5G网络干扰管理提供了智能解决方案。

Abstract: Non-Orthogonal Multiple Access (NOMA) is an information-theoretical approach used in 5G networks to improve spectral efficiency, but it is prone to interference during handovers. In this work, we propose a hybrid method that combines Gold-Walsh modulated sequences with Deep Q-Networks (DQN) to intelligently manage interference during NOMA handovers. This method optimizes sequence selection and power allocation dynamically. As a result, it achieves a 95.2\% handover success rate, which is an improvement of up to 23.1 percentage points. It also delivers up to 28\% throughput gain and reduces interference by up to 41\% in various mobility scenarios. All improvements are statistically significant (\(p < 0.001\)). The DQN trains in \(4{,}200 \pm 400\) episodes with a complexity of \(O(N \log N + d \cdot h + \log B)\) and can be deployed in real-time.

</details>


### [5] [Adversarial Network Imagination: Causal LLMs and Digital Twins for Proactive Telecom Mitigation](https://arxiv.org/abs/2602.13203)
*Vignesh Sriram,Yuqiao Meng,Luoxi Tang,Zhaohan Xi*

Main category: cs.NI

TL;DR: 提出Adversarial Network Imagination框架，整合因果大语言模型、知识图谱和数字孪生，主动生成、模拟和评估网络故障场景，实现从被动响应到主动预防的转变。


<details>
  <summary>Details</summary>
Motivation: 电信网络面临光纤切断、流量过载和级联故障等复杂故障，现有监控和数字孪生系统主要是被动响应，只在服务降级后才检测故障，需要转向主动预防性方法。

Method: 提出闭环框架，整合三个核心组件：1）因果大语言模型生成基于网络依赖关系的结构化故障场景；2）知识图谱编码网络依赖关系；3）数字孪生执行场景模拟，测量性能降级并评估缓解策略。通过迭代优化场景实现主动故障分析。

Result: 框架能够主动生成和模拟网络故障场景，评估性能影响和缓解策略有效性，实现从被动故障排除向主动弹性分析的转变。

Conclusion: Adversarial Network Imagination框架通过整合因果LLM、知识图谱和数字孪生，为电信网络提供主动故障预测和弹性分析能力，提升网络运营的预防性和韧性。

Abstract: Telecommunication networks experience complex failures such as fiber cuts, traffic overloads, and cascading outages. Existing monitoring and digital twin systems are largely reactive, detecting failures only after service degradation occurs. We propose Adversarial Network Imagination, a closed-loop framework that integrates a Causal Large Language Model (LLM), a Knowledge Graph, and a Digital Twin to proactively generate, simulate, and evaluate adversarial network failures. The Causal LLM produces structured failure scenarios grounded in network dependencies encoded in the Knowledge Graph. These scenarios are executed within a Digital Twin to measure performance degradation and evaluate mitigation strategies. By iteratively refining scenarios based on simulation feedback, the framework shifts network operations from reactive troubleshooting toward anticipatory resilience analysis.

</details>


### [6] [Hybrid Secure Routing in Mobile Ad-hoc Networks (MANETSs)](https://arxiv.org/abs/2602.13204)
*Soundes Oumaima Boufaida,Abdemadjid Benmachiche,Majda Maatallah,Chaouki Chemam*

Main category: cs.NI

TL;DR: 提出混合安全路由协议(HSRP)，结合信任机制与密码学技术，提升移动自组织网络(MANETs)的路由安全性和性能


<details>
  <summary>Details</summary>
Motivation: 移动自组织网络(MANETs)面临多种安全威胁（如洪泛攻击、陷洞攻击、黑洞攻击），这些威胁严重影响网络性能，需要设计更安全的路由协议来应对动态无线通信环境中的固有缺陷

Method: 提出混合安全路由协议(HSRP)，融合信任机制与密码学方法，结合主动式和反应式路由策略，能够动态适应网络变化并防御恶意活动。使用NS-2网络模拟器进行广泛仿真，并在不同攻击场景下评估性能

Result: 与传统协议相比，HSRP提高了吞吐量、降低了延迟，增强了路由效率和数据传输安全性。通过文献综述和仿真验证了协议的有效性

Conclusion: HSRP为MANETs提供了一种可扩展且可行的安全路由解决方案，特别适用于军事行动和灾难响应等关键领域。研究强调了在路由协议设计中集成先进安全特性的重要性，以确保MANETs在实际应用中的可靠性和完整性

Abstract: Because wireless communication is dynamic and has inherent defects, routing algorithms are crucial in the quickly evolving field of mobile ad hoc networks, or MANETs This study looks at the many security problems that MANETs encounter. These problems, which pose major risks to network performance, include flooding, sinkholes, and black hole assaults to address these challenges. We introduce the Hybrid Secure Routing Protocol (HSRP), which enhances the security and robustness of routing operations by fusing trust-based tactics with cryptographic approaches. HSRP combines the strengths of both proactive and reactive routing strategies, enabling it to adapt dynamically to evolving network conditions while protecting against malicious activities. We use extensive simulations with Network Simulator (NS-2) and a thorough review of the literature to assess HSRP's performance under different attack scenarios. The results show that, in comparison to traditional protocols, HSRP increases throughput and decreases latency, hence improving routing efficiency while simultaneously bolstering data transfer security. With uses in vital domains including military operations and disaster response, this study provides a scalable and workable approach for safe routing in MANETs. The findings highlight how crucial it is to include cutting-edge security features in routing protocol design to guarantee the dependability and integrity of MANETs in practical situations.

</details>


### [7] [Reinforcement Learning-Enabled Dynamic Code Assignment for Ultra-Dense IoT Networks: A NOMA-Based Approach to Massive Device Connectivity](https://arxiv.org/abs/2602.13205)
*Sumita Majhi,Kishan Thakkar,Pinaki Mitra*

Main category: cs.NI

TL;DR: 提出基于强化学习的动态Gold码分配方案，优化IoT-NOMA网络的吞吐量、能效和公平性，在智慧城市场景表现良好但工业场景可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 超密集物联网网络需要有效的非正交多址接入方案，但固定码分配会导致严重干扰，需要动态码分配来优化网络性能。

Method: 提出物联网感知的马尔可夫决策过程，联合优化吞吐量、能效和公平性；开发两种强化学习算法：自然策略梯度（NPG）学习离散动作，深度确定性策略梯度（DDPG）处理连续码嵌入。

Result: 在智慧城市条件下，NPG相比静态分配可实现11.6%的吞吐量提升和15.8%的能效提升；但在工业场景中性能较差，可靠性仅0-2%，表明动态码分配不足以实现超可靠物联网。

Conclusion: 动态码分配在智慧城市场景有效，但工业场景需要结合功率控制或重传方案；为大规模物联网网络的强化学习资源分配提供了基础。

Abstract: Ultra-dense IoT networks require an effective non-orthogonal multiple access (NOMA) scheme, yet they experience intense interference because of fixed code assignment. We suggest a reinforcement learning (RL) model of dynamic Gold code assignment in IoT-NOMA networks. Our Markov Decision Process which is IoT aware is a joint optimization of throughput, energy efficiency, and fairness. Two RL algorithms are created, including Natural Policy Gradient (NPG) to learn stable discrete actions and Deep Deterministic Policy Gradient (DDPG) with continuous code embedding. Under smart city conditions, NPG can attain throughput of 11.6% and energy efficiency of 15.8 likewise superior to its performance with a static allocation. Nonetheless, the performance is worse in organized industrial settings, and the reliability is minimal (0-2%), which points to the fact that dynamic code assignment is not a sufficient measure of ultra-reliable IoT and needs to be supplemented by power control or retransmission schemes. The work offers a basis to the RL-based resource allocation in massive IoT network.

</details>


### [8] [Toward Resource-Efficient Collaboration of Large AI Models in Mobile Edge Networks](https://arxiv.org/abs/2602.13206)
*Peichun Li,Liping Qian,Dusit Niyato,Shiwen Mao,Yuan Wu*

Main category: cs.NI

TL;DR: 本文综述了移动边缘网络中协作AI的系统架构、应用场景和资源高效协作技术，提出了一个多阶段扩散框架，在异构边缘资源上实现大型生成模型的弹性分布。


<details>
  <summary>Details</summary>
Motivation: 移动边缘网络中部署大型AI模型面临挑战：模型复杂性与边缘网络有限的计算、内存和通信资源之间存在内在不匹配。需要解决如何在资源受限的边缘环境中高效部署和协作运行大型AI模型的问题。

Method: 1. 综述了协作AI在移动边缘网络的系统架构和应用场景；2. 分类介绍了空间和时间两种资源高效协作技术；3. 提出了一个多阶段扩散框架，实现大型生成模型在异构边缘资源上的弹性分布。

Result: 实验结果表明，提出的多阶段扩散框架在数据生成方面实现了效率和适应性的性能提升，能够更好地利用异构边缘资源。

Conclusion: 协作AI是移动边缘网络中满足智能服务需求的有前景范式，通过空间和时间协作技术可以解决资源限制问题，提出的多阶段扩散框架为大型生成模型在边缘部署提供了有效解决方案。

Abstract: The collaboration of large artificial intelligence (AI) models in mobile edge networks has emerged as a promising paradigm to meet the growing demand for intelligent services at the network edge. By enabling multiple devices to cooperatively execute submodels or subtasks, collaborative AI enhances inference efficiency and service quality with constrained resources. However, deploying large AI models in such environments remains challenging due to the intrinsic mismatch between model complexity and the limited computation, memory, and communication resources in edge networks. This article provides a comprehensive overview of the system architecture for collaborative AI in mobile edge networks, along with representative application scenarios in transportation and healthcare. We further present recent advances in resource-efficient collaboration techniques, categorized into spatial and temporal approaches. The major spatial approaches include federated tuning, mixture of experts, patch-based diffusion, and hierarchical diffusion. Meanwhile, the important temporal approaches encompass split learning, cascading inference, speculative decoding, and routing inference. Building upon these foundations, we propose a multi-stage diffusion framework that enables elastic distribution of large generative models across heterogeneous edge resources. Experimental results demonstrate that our framework achieves performance improvement in both efficiency and adaptability for data generation.

</details>


### [9] [A Safety-Constrained Reinforcement Learning Framework for Reliable Wireless Autonomy](https://arxiv.org/abs/2602.13207)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.NI

TL;DR: 提出一个结合证明携带控制(PCC)和赋权预算(EB)的前向安全约束强化学习框架，用于无线系统，在保证安全的同时平衡自主性


<details>
  <summary>Details</summary>
Motivation: AI和强化学习在无线系统中展现出潜力，但在关键任务应用中存在不安全行为风险。现有安全机制主要是反应式的，无法在超可靠低延迟通信(URLLC)环境中保证可靠性

Method: 提出PCC+EB框架：通过轻量级数学证书验证每个智能体动作符合干扰约束，赋权预算调节安全覆盖频率以平衡安全与自主性。在无线上行调度任务中使用近端策略优化(PPO)实现

Result: 仿真结果显示，PCC+EB控制器消除了不安全传输，同时保持了系统吞吐量和可预测的自主性。相比无约束和反应式基线，该方法以最小性能损失实现了可证明的安全保证

Conclusion: 前向安全约束强化学习框架为未来6G网络中的可信无线自主性提供了潜力，实现了安全与性能的良好平衡

Abstract: Artificial intelligence (AI) and reinforcement learning (RL) have shown significant promise in wireless systems, enabling dynamic spectrum allocation, traffic management, and large-scale Internet of Things (IoT) coordination. However, their deployment in mission-critical applications introduces the risk of unsafe emergent behaviors, such as UAV collisions, denial-of-service events, or instability in vehicular networks. Existing safety mechanisms are predominantly reactive, relying on anomaly detection or fallback controllers that intervene only after unsafe actions occur, which cannot guarantee reliability in ultra-reliable low-latency communication (URLLC) settings. In this work, we propose a proactive safety-constrained RL framework that integrates proof-carrying control (PCC) with empowerment-budgeted (EB) enforcement. Each agent action is verified through lightweight mathematical certificates to ensure compliance with interference constraints, while empowerment budgets regulate the frequency of safety overrides to balance safety and autonomy. We implement this framework on a wireless uplink scheduling task using Proximal Policy Optimization (PPO). Simulation results demonstrate that the proposed PCC+EB controller eliminates unsafe transmissions while preserving system throughput and predictable autonomy. Compared with unconstrained and reactive baselines, our method achieves provable safety guarantees with minimal performance degradation. These results highlight the potential of proactive safety constrained RL to enable trustworthy wireless autonomy in future 6G networks.

</details>


### [10] [Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization](https://arxiv.org/abs/2602.13210)
*Jie Zheng,Ruichen Zhang,Dusit Niyato,Haijun Zhang,Jiacheng Wang,Hongyang Du,Jiawen Kang,Zehui Xiong*

Main category: cs.NI

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）增强强化学习（RL）在6G无线网络优化中的应用，提出LLM辅助的状态表示和语义提取方法，并通过案例研究验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络面临用户需求多样化和环境复杂的挑战，传统强化学习方法在处理高维状态空间和复杂环境时存在计算量大、分布式智能和结果不一致等问题。大型语言模型凭借其预训练知识和推理能力，有望增强RL在无线网络优化中的表现。

Method: 提出LLM增强的RL框架，包括LLM辅助的状态表示和语义提取方法，应用于多智能体强化学习（MARL）框架。该方法在服务迁移与请求路由、无人机-卫星网络拓扑图生成等场景中进行验证。

Result: 通过案例研究表明，该框架能有效优化无线网络性能，在服务迁移、请求路由和网络拓扑生成等任务中表现出色。

Conclusion: LLM增强的RL框架为6G无线网络优化提供了有前景的解决方案，未来可在更多协议层和应用场景中探索LLM与RL的协同作用。

Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex environments, leading to substantial computational demands, distributed intelligence, and potentially inconsistent outcomes. Large language models (LLMs), with their extensive pretrained knowledge and advanced reasoning capabilities, offer promising tools to enhance RL in optimizing 6G wireless networks. We explore RL models augmented by LLMs, emphasizing their roles and the potential benefits of their synergy in wireless network optimization. We then examine LLM-enabled RL across various protocol layers: physical, data link, network, transport, and application layers. Additionally, we propose an LLM-assisted state representation and semantic extraction to enhance the multi-agent reinforcement learning (MARL) framework. This approach is applied to service migration and request routing, as well as topology graph generation in unmanned aerial vehicle (UAV)-satellite networks. Through case studies, we demonstrate that our framework effectively performs optimization of wireless network. Finally, we outline prospective research directions for LLM-enabled RL in wireless network optimization.

</details>


### [11] [An Overlay Multicast Routing Method Based on Network Situational Aware-ness and Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.13211)
*Miao Ye,Yanye Chen,Yong Wang,Cheng Zhu,Qiuxiang Jiang,Gai Huang,Feng Ding*

Main category: cs.NI

TL;DR: 提出MA-DHRL-OM方法，通过多智能体深度分层强化学习解决覆盖组播的动态适应问题，相比传统方法在延迟、带宽利用率和收敛稳定性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖组播无法适应动态流量变化，不了解物理资源状态；现有强化学习方法未能解耦覆盖组播的紧耦合多目标特性，导致复杂度高、收敛慢且不稳定。

Method: 提出多智能体深度分层强化学习方法MA-DHRL-OM，利用SDN全局视图构建流量感知模型，将组播树构建分解为两个阶段，通过分层智能体减少动作空间，多智能体协作平衡多目标优化。

Result: 实验表明MA-DHRL-OM在延迟、带宽利用率和丢包率方面优于现有方法，具有更稳定的收敛性和更灵活的路由能力。

Conclusion: MA-DHRL-OM通过分层多智能体方法有效解决了覆盖组播的动态适应和多目标优化问题，提高了性能、可扩展性和适应性。

Abstract: Compared with IP multicast, Overlay Multicast (OM) offers better compatibility and flexible deployment in heterogeneous, cross-domain networks. However, traditional OM struggles to adapt to dynamic traffic due to unawareness of physical resource states, and existing reinforcement learning methods fail to decouple OM's tightly coupled multi-objective nature, leading to high complexity, slow convergence, and instability. To address this, we propose MA-DHRL-OM, a multi-agent deep hierarchical reinforcement learning approach. Using SDN's global view, it builds a traffic-aware model for OM path planning. The method decomposes OM tree construction into two stages via hierarchical agents, reducing action space and improving convergence stability. Multi-agent collaboration balances multi-objective optimization while enhancing scalability and adaptability. Experiments show MA-DHRL-OM outperforms existing methods in delay, bandwidth utilization, and packet loss, with more stable convergence and flexible routing.

</details>


### [12] [Network-Adaptive Cloud Preprocessing for Visual Neuroprostheses](https://arxiv.org/abs/2602.13216)
*Jiayi Liu,Yilin Wang,Michael Beyeler*

Main category: cs.NI

TL;DR: 云辅助视觉预处理用于神经假体，通过自适应编码减少网络延迟对感知稳定性的影响


<details>
  <summary>Details</summary>
Motivation: 云端机器学习作为下一代视觉神经假体的预处理策略，可以克服电池供电视觉处理单元的计算和能耗限制，但网络延迟、抖动和丢包会破坏神经刺激的时间一致性

Method: 提出网络自适应的云辅助管道，利用实时往返时间反馈动态调整图像分辨率、压缩和传输速率，优先保证网络恶劣条件下的时间连续性

Result: 自适应视觉编码在网络拥塞时显著降低端到端延迟，全局场景结构仅适度退化，但边界精度下降更明显

Conclusion: 界定了云辅助预处理在未来视觉神经假体中可行的操作范围，强调了网络感知自适应对维持感知稳定性的重要性

Abstract: Cloud-based machine learning is increasingly explored as a preprocessing strategy for next-generation visual neuroprostheses, where advanced scene understanding may exceed the computational and energy constraints of battery-powered visual processing units (VPUs). Offloading computation to remote servers enables the use of state-of-the-art vision models, but also introduces sensitivity to network latency, jitter, and packet loss, which can disrupt the temporal consistency of the delivered neural stimulus. In this work, we examine the feasibility of cloud-assisted visual preprocessing for artificial vision by framing remote inference as a perceptually constrained systems problem. We present a network-adaptive cloud-assisted pipeline in which real-time round-trip-time (RTT) feedback is used to dynamically modulate image resolution, compression, and transmission rate, explicitly prioritizing temporal continuity under adverse network conditions. Using a Raspberry Pi 4 as a simulated VPU and a client-server architecture, we evaluate system performance across a range of realistic wireless network regimes. Results show that adaptive visual encoding substantially reduces end-to-end latency during network congestion, with only modest degradation of global scene structure, while boundary precision degrades more sharply. Together, these findings delineate operating regimes in which cloud-assisted preprocessing may remain viable for future visual neuroprostheses and underscore the importance of network-aware adaptation for maintaining perceptual stability.

</details>


### [13] [An Agentic AI Control Plane for 6G Network Slice Orchestration, Monitoring, and Trading](https://arxiv.org/abs/2602.13227)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Tharaka Hewa,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Peter Foytik,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: 提出基于智能体AI的6G网络切片编排控制平面架构，整合市场感知编排、自然语言接口和多模型协同，实现自适应、可扩展的6G原生切片管理


<details>
  <summary>Details</summary>
Motivation: 现有5G切片框架依赖静态策略和人工流程，无法适应6G动态、多域、服务为中心的特性，需要新的编排方法

Method: 分层架构的多智能体AI控制平面，包含市场感知编排、基于MCP的自然语言接口、多模型协同的LLM联盟，以及专用推理模型监管

Result: 在真实测试床（Open5GS+Ericsson RAN）上验证，结合智能体自主性、闭环SLA保障、市场感知编排和自然语言控制，实现可扩展自适应6G原生控制平面

Conclusion: 智能体AI可作为未来6G网络的基础机制，为网络切片管理提供可扩展、自适应的解决方案

Abstract: 6G networks are expected to be AI-native, intent-driven, and economically programmable, requiring fundamentally new approaches to network slice orchestration. Existing slicing frameworks, largely designed for 5G, rely on static policies and manual workflows and are ill-suited for the dynamic, multi-domain, and service-centric nature of emerging 6G environments. In this paper, we propose an agentic AI control plane architecture for 6G network slice orchestration, monitoring, and trading that treats orchestration as a holistic control function encompassing slice planning, deployment, continuous monitoring, and economically informed decision-making. The proposed control plane is realized as a layered architecture in which multiple cooperating AI agents. To support flexible and on-demand slice utilization, the control plane incorporates market-aware orchestration capabilities, allowing slice requirements, pricing, and availability to be jointly considered during orchestration decisions. A natural language interface, implemented using the Model Context Protocol (MCP), enables users and applications to interact with control-plane functions through intent-based queries while enforcing safety and policy constraints. To ensure responsible and explainable autonomy, the control plane integrates fine-tuned large language models organized as a multi-model consortium, governed by a dedicated reasoning model. The proposed approach is evaluated using a real-world testbed with multiple mobile core instances (e.g Open5GS) integrated with Ericsson's RAN infrastructure. The results demonstrate that combining agentic autonomy, closed-loop SLA assurance, market-aware orchestration, and natural language control enables a scalable and adaptive 6G-native control plane for network slice management, highlighting the potential of agentic AI as a foundational mechanism for future 6G networks.

</details>


### [14] [Pocket RAG: On-Device RAG for First Aid Guidance in Offline Mobile Environment](https://arxiv.org/abs/2602.13229)
*Dong Ho Kang,Hyunjoon Lee,Hyeonjeong Cha,Minkyu Choi,Sungsoo Lim*

Main category: cs.NI

TL;DR: 提出轻量级移动端检索增强生成系统，让小语言模型能在Android设备离线运行，为急救提供可靠指导


<details>
  <summary>Details</summary>
Motivation: 灾难或偏远地区急救人员常失去网络连接，传统服务器AI系统无法提供关键指导，需要离线可用的解决方案

Method: 采用移动端优化管道，包括混合RAG、选择性压缩、批量提示解码和量化缓存等技术

Result: 系统在物理急救准确率达94.5%，心理急救达97.0%，响应时间从14.2秒降至3.7秒，实现近4倍加速

Conclusion: 该系统实用可靠，能在无网络连接情况下提供有效的急救指导，解决了紧急场景中的关键需求

Abstract: In disaster scenarios or remote areas, first responders often lose network connectivity when providing first aid. In such situations, server-based AI systems fail to provide critical guidance. To address this issue, we present a lightweight, mobile-based retrieval-augmented generation system for small language models (SLMs) that can run directly on Android devices. Our system integrates a mobile-friendly optimized pipeline featuring Hybrid RAG, selective compression, batched prompt decoding, and quantization caching. Despite the model's small size, our RAG-based system achieves 94.5\% accuracy for physical first aid and 97.0\% for psychological first aid. Additionally, we reduce response time from 14.2s to 3.7s, achieving a nearly 4x speedup. These results prove that our system is practical and can deliver reliable first aid guidance even without internet connectivity.

</details>


### [15] [An Explainable Failure Prediction Framework for Neural Networks in Radio Access Networks](https://arxiv.org/abs/2602.13231)
*Khaleda Papry,Francesco Spinnato,Marco Fiore,Mirco Nanni,Israat Haque*

Main category: cs.NI

TL;DR: 提出一个结合可解释性特征剪枝与模型精炼的框架，用于5G网络中的无线链路故障预测，能减少50%参数并提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 5G毫米波频段易受环境影响导致无线链路故障，现有预测模型多为黑箱，缺乏透明度和可操作性，需要可解释且高效的预测框架。

Method: 提出一个结合可解释性特征剪枝与模型精炼的框架，可集成到GNN Transformer、LSTM等先进架构中，通过分析输入特征贡献度来精简模型。

Result: 在真实数据集上发现天气数据对预测贡献最小，据此构建的精简模型参数减少50%，F1分数相比现有最佳方案有所提升。

Conclusion: 该框架使网络运营商能够评估和优化基于神经网络的预测模型，实现更好的可解释性、可扩展性和性能。

Abstract: As 5G networks continue to evolve to deliver high speed, low latency, and reliable communications, ensuring uninterrupted service has become increasingly critical. While millimeter wave (mmWave) frequencies enable gigabit data rates, they are highly susceptible to environmental factors, often leading to radio link failures (RLF). Predictive models leveraging radio and weather data have been proposed to address this issue; however, many operate as black boxes, offering limited transparency for operational deployment. This work bridges that gap by introducing a framework that combines explainability based feature pruning with model refinement. Our framework can be integrated into state of the art predictors such as GNN Transformer and LSTM based architectures for RLF prediction, enabling the development of accurate and explainability guided models in 5G networks. It provides insights into the contribution of input features and the decision making logic of neural networks, leading to lighter and more scalable models. When applied to RLF prediction, our framework unveils that weather data contributes minimally to the forecast in extensive real world datasets, which informs the design of a leaner model with 50 percent fewer parameters and improved F1 scores with respect to the state of the art solution. Ultimately, this work empowers network providers to evaluate and refine their neural network based prediction models for better interpretability, scalability, and performance.

</details>


### [16] [Securing SIM-Assisted Wireless Networks via Quantum Reinforcement Learning](https://arxiv.org/abs/2602.13238)
*Le-Hung Hoang,Quang-Trung Luu,Dinh Thai Hoang,Diep N. Nguyen,Van-Dinh Nguyen*

Main category: cs.NI

TL;DR: 提出混合量子近端策略优化(Q-PPO)框架，用于优化堆叠智能超表面(SIM)辅助的安全通信，在信道状态信息不完美情况下实现更高的保密率和更快收敛


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面(SIM)为物理层安全提供了前所未有的自由度，但其大量元原子导致高维强耦合优化空间，传统设计方法效率低且难以扩展。现有深度强化学习在动态无线环境和被动窃听者信道信息不完美情况下收敛慢且性能下降

Method: 提出混合量子近端策略优化(Q-PPO)框架，将参数化量子电路嵌入到actor网络中，形成混合经典-量子策略架构，联合优化发射功率分配和SIM相位偏移，在功率和服务质量约束下最大化平均保密率

Result: Q-PPO方案在信道状态信息不完美情况下，比深度强化学习基线方法实现约15%更高的保密率和30%更快的收敛速度

Conclusion: Q-PPO为SIM使能的安全无线网络建立了强大的优化范式，混合量子-经典架构在高维连续动作空间中增强了策略表示能力和探索效率

Abstract: Stacked intelligent metasurfaces (SIMs) have recently emerged as a powerful wave-domain technology that enables multi-stage manipulation of electromagnetic signals through multilayer programmable architectures. While SIMs offer unprecedented degrees of freedom for enhancing physical-layer security, their extremely large number of meta-atoms leads to a high-dimensional and strongly coupled optimization space, making conventional design approaches inefficient and difficult to scale. Moreover, existing deep reinforcement learning (DRL) techniques suffer from slow convergence and performance degradation in dynamic wireless environments with imperfect knowledge of passive eavesdroppers. To overcome these challenges, we propose a hybrid quantum proximal policy optimization (Q-PPO) framework for SIM-assisted secure communications, which jointly optimizes transmit power allocation and SIM phase shifts to maximize the average secrecy rate under power and quality-of-service constraints. Specifically, a parameterized quantum circuit is embedded into the actor network, forming a hybrid classical-quantum policy architecture that enhances policy representation capability and exploration efficiency in high-dimensional continuous action spaces. Extensive simulations demonstrate that the proposed Q-PPO scheme consistently outperforms DRL baselines, achieving approximately 15% higher secrecy rates and 30% faster convergence under imperfect eavesdropper channel state information. These results establish Q-PPO as a powerful optimization paradigm for SIM-enabled secure wireless networks.

</details>


### [17] [Embodied Intelligent Spectrum Management: A New Paradigm for Dynamic Spectrum Access](https://arxiv.org/abs/2602.13245)
*Yihe Diao,Yuhang Wu,Hongtao Liang,Ming Xu,Rui Ding,Fuhui Zhou,Qihui Wu,Jun Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种新的"具身智能频谱管理"范式，以应对无线通信进入智能体时代后频谱需求激增和管理挑战


<details>
  <summary>Details</summary>
Motivation: 无线通信正进入智能体时代，大量具备感知、推理和交互能力的智能体将在高度动态的无线环境中运行。智能体通信将大幅增加频谱需求，给频谱管理带来前所未有的挑战。现有的静态频谱分配和智能管理范式缺乏灵活性，无法适应智能体通信的动态异构需求。

Method: 提出具身智能频谱管理范式，首先设计EISM架构并阐述其关键技术，然后构建原型平台来展示EISM的优势。

Result: 提出了EISM架构和原型平台，展示了该范式在应对智能体通信频谱管理挑战方面的优势。

Conclusion: 具身智能为频谱管理提供了有前景的解决方案，EISM范式能够适应智能体时代的动态异构需求，但仍面临关键挑战和开放性问题需要未来研究解决。

Abstract: Wireless communication is evolving into an agent era, where numerous intelligent agents equipped with perception, reasoning, and interaction capabilities will operate in highly dynamic wireless environments. To complete diverse complex tasks, agent communication will play a critical role, which enables autonomous information exchange with external tools, services, and other agents. This trendy movement will dramatically increase spectrum demand and result in unprecedented challenges for spectrum management. However, current spectrum management paradigms, including static spectrum allocation and intelligent management, lack the flexibility and generalization to accommodate the dynamic and heterogeneous demands of agent communication. The recent advancements in embodied intelligence (EI) bring a promising solution, and this article will provide our vision of an emerging embodied intelligent spectrum management (EISM) paradigm. We start with an architecture for EISM, elaborating its key enabling technologies. Then, a prototype platform is presented to demonstrate the advantages of EISM. Finally, key challenges and open issues are outlined to facilitate future research in this emerging field.

</details>


### [18] [Parametric-Sensitivity Aware Retransmission for Efficient AI Downloading](https://arxiv.org/abs/2602.13607)
*You Zhou,Qunsong Zeng,Kaibin Huang*

Main category: cs.NI

TL;DR: 提出PASAR框架，基于参数敏感性进行差异化重传，显著提升边缘AI模型下载的通信效率和延迟性能


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络中边缘AI应用需要高效的AI模型下载技术，但高维模型在无线信道传输面临通信资源限制的挑战

Method: 提出参数敏感性感知重传(PASAR)框架，设计基于实时误码率测量和参数敏感性的在线重传协议，采用自适应轮次停止准则

Result: 实验表明PASAR在通信效率和延迟方面显著优于传统HARQ方案，能够保持模型功能同时减少总体延迟

Conclusion: PASAR框架通过参数重要性感知的差异化重传策略，有效解决了边缘AI模型下载中的资源效率问题

Abstract: The edge artificial intelligence (AI) applications in next-generation mobile networks demand efficient AI-model downloading techniques to support real-time, on-device inference. However, transmitting high-dimensional AI models over wireless channels remains challenging due to limited communication resources. To address this issue, we propose a parametric-sensitivity-aware retransmission (PASAR) framework that manages radio-resource usage of different parameter packets according to their importance on model inference accuracy, known as parametric sensitivity. Empirical analysis reveals a highly right-skewed sensitivity distribution, indicating that only a small fraction of parameters significantly affect model performance. Leveraging this insight, we design a novel online retransmission protocol, i.e., the PASAR protocol, that adaptively terminates packet transmission based on real-time bit error rate (BER) measurements and the associated parametric sensitivity. The protocol employs an adaptive, round-wise stopping criterion, enabling heterogeneous, packet-level retransmissions that preserve overall model functionality but reduce overall latency. Extensive experiments across diverse deep neural network architectures and real-world datasets demonstrate that PASAR substantially outperforms classical hybrid automatic repeat request (HARQ) schemes in terms of communication efficiency and latency.

</details>


### [19] [Modality-Tailored Age of Information for Multimodal Data in Edge Computing Systems](https://arxiv.org/abs/2602.13269)
*Ying Liu,Yifan Zhang,Xinyu Wang,Chao Yang,Kandaraj Piamrat,Stephan Sigg,Zheng Changr,Yusheng Ji*

Main category: cs.NI

TL;DR: 提出了一种针对多模态数据的定制化信息年龄度量标准MAoI，并开发了联合采样卸载优化算法来最小化平均MAoI


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统规模扩大和设备异构性增加，多模态数据变得无处不在。现有信息年龄(AoI)等新鲜度度量标准不适合多模态数据，因为它们无法捕捉模态特定特征。需要一种统一且与决策相关的新鲜度评估方法来支持资源管理和策略优化。

Method: 提出模态定制信息年龄(MAoI)度量标准，整合模态特定的语义和时间特征。推导平均MAoI的闭式表达式，并构建MAoI最小化问题。开发基于块坐标下降的联合采样卸载优化(JSO)算法，交替优化采样间隔和卸载决策。

Result: MAoI度量标准相比传统AoI能更有效地量化多模态新鲜度。JSO算法相比最先进算法能显著最小化平均MAoI，在仿真中得到验证。

Conclusion: MAoI为多模态数据提供了一种统一且决策相关的新鲜度评估框架，JSO算法能有效优化多接入边缘计算系统中的资源管理，提升多模态数据服务质量。

Abstract: As Internet of Things (IoT) systems scale and device heterogeneity grows, multimodal data have become ubiquitous. Meanwhile, evaluating the freshness of multimodal data is essential, as stale updates would delay task execution, degrade decision accuracy, and undermine safety in latency-sensitive services. However, existing freshness metrics such as Age of Information (AoI) are not suitable for multimodal data, as they do not capture modality-specific characteristics. In this paper, we propose a metric, namely, Modality-Tailored Age of Information (MAoI), to provide a unified and decision-relevant evaluation of freshness for resource management and policy optimization for multimodal data. This metric integrates modality-specific semantic and temporal characteristics, reflecting both age evolution and content importance for multimodal data in multi-access edge computing (MEC) systems. Then, the closed-form expression of the average MAoI is derived, and an MAoI minimization problem is formulated, where sampling intervals and offloading decisions are optimized with practical energy constraints. To effectively solve this problem, a Joint Sampling Offloading Optimization (JSO) algorithm is proposed to jointly optimize the sampling intervals and offloading decisions. It is a block coordinate descent-based algorithm where an optimal sampling-interval subalgorithm is used to update the sampling intervals, and an interference-aware best-response offloading subalgorithm is proposed to update the offloading decisions alternately. Finally, a comprehensive simulation is performed, confirming that the MAoI metric effectively quantifies multimodal freshness compared to traditional AoI, and the JSO algorithm significantly minimizes the average MAoI compared to state-of-the-art algorithms.

</details>


### [20] [Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs](https://arxiv.org/abs/2602.13277)
*Uma Mahesh Boda,Mallikharjuna Rao Nuka*

Main category: cs.NI

TL;DR: ID2P2是一个基于意图驱动扩散的路径规划框架，用于物联网密集无线传感器网络中的移动数据收集，通过显式建模高级意图（如延迟最小化、能量平衡）来生成自适应数据收集轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有无线传感器网络中的移动数据收集路径规划方法通常是启发式的，缺乏灵活性，难以适应动态网络条件下的高级操作目标。需要一种能够明确建模和纳入高级意图的自适应路径规划框架。

Method: 提出ID2P2框架，采用意图驱动的扩散规划过程，学习捕获空间节点分布和网络特征的轨迹先验，联合处理汇聚点选择和移动数据收集器路径构建，生成符合指定意图的可行轨迹。

Result: ID2P2相比传统基线方法显著提升性能：路径完成时间和旅行开销减少25-30%，数据新鲜度提高10-30%，能量效率和数据包传输性能提升15-30%，在高网络密度下保持更高的吞吐量和公平性。

Conclusion: ID2P2框架通过意图驱动的扩散规划，能够有效生成适应高级操作目标的数据收集轨迹，在密集无线传感器网络中展现出优越的性能、鲁棒性和可扩展性。

Abstract: Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs.

</details>


### [21] [GraFSTNet: Graph-based Frequency SpatioTemporal Network for Cellular Traffic Prediction](https://arxiv.org/abs/2602.13282)
*Ziyi Li,Hui Ma,Fei Xing,Chunjiong Zhang,Ming Yan*

Main category: cs.NI

TL;DR: 提出一个结合时空建模与时频分析的蜂窝流量预测框架，通过注意力机制捕捉空间依赖，使用时频分析增强周期模式表示，并引入自适应尺度LogCosh损失函数提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络快速扩张和移动设备普及导致流量数据具有复杂的时空动态特性，现有方法要么过度关注时间建模，要么依赖预定义空间拓扑，难以有效联合建模时空依赖和捕捉周期模式。

Method: 1) 构建空间建模分支，通过注意力机制捕捉小区间依赖，减少对预定义拓扑结构的依赖；2) 构建时频建模分支，增强周期模式表示；3) 引入自适应尺度LogCosh损失函数，根据流量大小调整误差惩罚，防止大误差主导训练过程。

Result: 在三个开源数据集上的实验表明，该方法取得了优于最先进方法的预测性能。

Conclusion: 提出的结合时空建模与时频分析的框架能有效解决蜂窝流量预测中的时空依赖建模和周期模式捕捉问题，自适应损失函数有助于在不同流量强度下保持稳定的预测精度。

Abstract: With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction. Previous methods often focus predominantly on temporal modeling or depend on predefined spatial topologies, which limits their ability to jointly model spatio-temporal dependencies and effectively capture periodic patterns in cellular traffic. To address these issues, we propose a cellular traffic prediction framework that integrates spatio-temporal modeling with time-frequency analysis. First, we construct a spatial modeling branch to capture inter-cell dependencies through an attention mechanism, minimizing the reliance on predefined topological structures. Second, we build a time-frequency modeling branch to enhance the representation of periodic patterns. Furthermore, we introduce an adaptive-scale LogCosh loss function, which adjusts the error penalty based on traffic magnitude, preventing large errors from dominating the training process and helping the model maintain relatively stable prediction accuracy across different traffic intensities. Experiments on three open-sourced datasets demonstrate that the proposed method achieves prediction performance superior to state-of-the-art approaches.

</details>


### [22] [Benchmarking Anomaly Detection Across Heterogeneous Cloud Telemetry Datasets](https://arxiv.org/abs/2602.13288)
*Mohammad Saiful Islam,Andriy Miranskyy*

Main category: cs.NI

TL;DR: 该研究评估了四种深度学习模型和一种经典方法在四个不同云监控数据集上的异常检测性能，发现模型性能不仅取决于架构，更关键地受校准稳定性和特征空间几何的影响。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习时间序列异常检测模型通常在单一数据集上评估，无法确定它们能否处理不同类型的大规模、高维云系统遥测数据，需要跨数据集评估来验证模型的泛化能力。

Method: 使用统一的训练和评估流程，在四个不同结构的遥测数据集上测试GRU、TCN、Transformer、TSMixer四种深度学习模型以及Isolation Forest经典基线。采用NAB风格指标评估早期检测能力，支持窗口化评分。

Result: 研究表明云系统异常检测性能不仅受模型架构影响，更关键地取决于校准稳定性和特征空间几何特性。不同模型在不同数据集上表现差异显著。

Conclusion: 通过发布预处理流程、基准配置和评估工具，支持云环境异常检测系统的可重复和部署感知评估。强调跨数据集评估的重要性以及校准和特征空间因素的关键作用。

Abstract: Anomaly detection is important for keeping cloud systems reliable and stable. Deep learning has improved time-series anomaly detection, but most models are evaluated on one dataset at a time. This raises questions about whether these models can handle different types of telemetry, especially in large-scale and high-dimensional environments.
  In this study, we evaluate four deep learning models, GRU, TCN, Transformer, and TSMixer. We also include Isolation Forest as a classical baseline. The models are tested across four telemetry datasets: the Numenta Anomaly Benchmark, Microsoft Cloud Monitoring dataset, Exathlon dataset, and IBM Console dataset. These datasets differ in structure, dimensionality, and labelling strategy. They include univariate time series, synthetic multivariate workloads, and real-world production telemetry with over 100,000 features.
  We use a unified training and evaluation pipeline across all datasets. The evaluation includes NAB-style metrics to capture early detection behaviour for datasets where anomalies persist over contiguous time intervals. This enables window-based scoring in settings where anomalies occur over contiguous time intervals, even when labels are recorded at the point level. The unified setup enables consistent analysis of model behaviour under shared scoring and calibration assumptions.
  Our results demonstrate that anomaly detection performance in cloud systems is governed not only by model architecture, but critically by calibration stability and feature-space geometry. By releasing our preprocessing pipelines, benchmark configuration, and evaluation artifacts, we aim to support reproducible and deployment-aware evaluation of anomaly detection systems for cloud environments.

</details>


### [23] [AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks](https://arxiv.org/abs/2602.13290)
*Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,Maycon Peixoto,Flavio De Oliveira Silva*

Main category: cs.NI

TL;DR: AGORA架构使用本地工具增强的LLM代理，将自然语言可持续性目标转化为网络控制动作，实现能效感知的流量调度，为B5G网络提供可持续性优先的意图驱动运营。


<details>
  <summary>Details</summary>
Motivation: 现有网络管理方法（如ZTN和SON）缺乏将人类可持续性目标转化为可执行网络策略的实用机制，难以在效率、用户满意度和能效之间平衡。

Method: 提出AGORA架构，在移动网络控制环路中嵌入本地工具增强的LLM代理，将自然语言可持续性目标转化为基于遥测的动作，通过用户平面功能执行能效感知的流量调度。

Result: 发现工具驱动控制环路中存在强延迟-能耗耦合，紧凑模型能在保持低能耗的同时正确执行策略，包括在压力MEC条件下的非零迁移行为。

Conclusion: AGORA为B5G网络提供了可持续性优先、意图驱动的运营方法，将人类目标与可执行编排对齐，推动了绿色网络管理的发展。

Abstract: Effective management and operational decision-making for complex mobile network systems present significant challenges, particularly when addressing conflicting requirements such as efficiency, user satisfaction, and energy-efficient traffic steering. The literature presents various approaches aimed at enhancing network management, including the Zero-Touch Network (ZTN) and Self-Organizing Network (SON); however, these approaches often lack a practical and scalable mechanism to consider human sustainability goals as input, translate them into energy-aware operational policies, and enforce them at runtime. In this study, we address this gap by proposing the AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks. AGORA embeds a local tool-augmented Large Language Model (LLM) agent in the mobile network control loop to translate natural-language sustainability goals into telemetry-grounded actions, actuating the User Plane Function (UPF) to perform energy-aware traffic steering. The findings indicate a strong latency-energy coupling in tool-driven control loops and demonstrate that compact models can achieve a low energy footprint while still facilitating correct policy execution, including non-zero migration behavior under stressed Multi-access Edge Computing (MEC) conditions. Our approach paves the way for sustainability-first, intent-driven network operations that align human objectives with executable orchestration in Beyond-5G infrastructures.

</details>


### [24] [Cooperative Edge Caching with Large Language Model in Wireless Networks](https://arxiv.org/abs/2602.13307)
*Ning Yang,Wentao Wang,Lingtao Ouyang,Haijun Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型的多基站协同边缘缓存编排器，通过文本到动作接口与环境交互，采用监督微调和分组相对策略优化的两阶段对齐方法，在动态环境中实现接近穷举搜索的性能。


<details>
  <summary>Details</summary>
Motivation: 重叠区域中的协同边缘缓存导致基站决策之间存在复杂耦合，使得内容替换对拓扑结构和时间复用高度敏感。传统启发式方法通常短视，而深度强化学习在动态环境下缺乏鲁棒性，因此需要更智能的解决方案。

Method: 1. 使用LLM作为唯一的自主引擎，通过验证的文本到动作接口与环境交互；2. 每个时隙将环境状态（缓存库存和频率统计）渲染为提示；3. 采用两阶段对齐范式：首先在oracle轨迹上进行监督微调以学习语法和初始化，然后使用分组相对策略优化，采用"机会感知"奖励机制，优先考虑相对于无操作基线的多步协同增益。

Result: 在相同请求轨迹评估中，编排器在5-BS场景下接近穷举搜索性能（0.610 vs. 0.617），优于经典基线方法（例如比最少使用算法提高4.1%），并在变化的缓存容量、库大小和用户密度下展现出强大的零样本迁移能力。

Conclusion: 基于LLM的协同边缘缓存编排器能够有效处理基站间的复杂耦合关系，在动态环境中表现出色，接近最优性能，并具有良好的泛化能力，为边缘缓存管理提供了新的解决方案。

Abstract: Cooperative edge caching in overlapping zones creates intricate coupling among Base Station (BS) decisions, making content replacement highly sensitive to topology and temporal reuse. While heuristics are often myopic and Deep Reinforcement Learning lacks robustness under dynamics, this paper proposes a Large Language Model (LLM)-based multi-BS orchestrator. The LLM acts as the sole autonomous engine, interacting with the environment via a validated text-to-action interface. Each time slot, the system renders environmental states -- including cache inventories and frequency statistics -- into prompts, parsing LLM-generated decisions against strict feasibility constraints. We align the model through a two-stage paradigm: Supervised Fine-Tuning on oracle trajectories for syntax and initialization, followed by Group Relative Policy Optimization. The latter employs an ``opportunity-aware'' reward that prioritizes multi-step cooperative gains relative to a No-Operation baseline. Evaluated on identical request traces, the orchestrator approaches exhaustive-search performance (0.610 vs.\ 0.617 in a 5-BS scenario), outperforms classical baselines (e.g., +4.1\% over least-frequently used), and demonstrates robust zero-shot transfer across varying cache capacities, library sizes, and user densities.

</details>


### [25] [Resilient and Freshness-Aware Scheduling for Industrial Multi-Hop IAB Networks: A Packet Duplication Approach](https://arxiv.org/abs/2602.13311)
*Shuo Zhu,Siyu Lin,Zijing Wang,Qiao Ren,Xiaoheng Deng,Bo Ai*

Main category: cs.NI

TL;DR: 提出RFAS算法，在毫米波多跳IAB网络中平衡可靠性与拥塞，通过动态调度最小化AoI并保证队列稳定性


<details>
  <summary>Details</summary>
Motivation: 工业毫米波多跳IAB网络中，移动障碍物造成的动态阻塞严重威胁网络鲁棒性和连续性。包复制虽能提高可靠性，但会加倍流量负载，导致严重拥塞和信息年龄(AoI)恶化

Method: 利用Lyapunov优化将长期随机优化问题转化为可处理的确定性子问题，提出弹性且新鲜度感知的调度(RFAS)算法高效解决这些子问题

Result: 在易阻塞环境中，RFAS显著优于基线方法，保持包交付率(PDR)高于95%，严格保证硬缓冲区约束下的队列稳定性，在高频流量场景下网络负载不平衡减少19%

Conclusion: RFAS是实时工业控制环路的鲁棒且可持续解决方案，能有效平衡可靠性-拥塞权衡

Abstract: In industrial millimeter-wave (mmWave) multi-hop Integrated Access and Backhaul (IAB) networks, dynamic blockages caused by moving obstacles pose a severe threat to robust and continuous networks. While Packet Duplication (PD) enhances reliability by path diversity, it inevitably doubles the traffic load, leading to severe congestion and degraded Age of Information (AoI). To navigate this reliability-congestion trade-off, we formulated an optimization problem in a multi-hop IAB scenario that minimizes the average AOI while satisfying strict queue stability constraints. We utilize Lyapunov optimization to transform the long-term stochastic optimization problem into tractable deterministic sub-problems. To solve these sub-problems efficiently, we propose a Resilient and Freshness-Aware Scheduling (RFAS) algorithm. Simulation results show that in blockage-prone environments, RFAS significantly outperforms baselines by maintaining a Packet Delivery Ratio (PDR) above 95\%. Crucially, it strictly guarantees queue stability under hard buffer constraints, whereas baselines suffer from buffer overflows. Furthermore, RFAS reduces the network load imbalance by 19\% compared to the baseline in high-frequency traffic scenarios. This confirms RFAS as a robust and sustainable solution for real-time industrial control loops.

</details>


### [26] [Semantic Waveforms for AI-Native 6G Networks](https://arxiv.org/abs/2602.13316)
*Nour Hello,Mohamed Amine Hamoura,Francois Rivet,Emilio Calvanese Strinati*

Main category: cs.NI

TL;DR: 提出OSSDM语义感知波形设计框架，在6G网络中联合优化物理层资源使用和语义通信效率/鲁棒性，通过正交基波形设计实现语义信息的波形级编码。


<details>
  <summary>Details</summary>
Motivation: 传统波形设计未考虑语义通信需求，无法在硬件约束下同时优化资源使用和语义效率。需要为AI原生6G网络开发能直接编码语义信息的波形设计方法。

Method: 提出正交语义序列分割复用（OSSDM）方法，采用参数化正交基波形设计，通过可控的信号退化来保留语义重要内容，在波形层面直接编码有意义信息。

Result: OSSDM在频谱效率和语义保真度上优于传统OFDM波形，增强了对信道损伤的语义鲁棒性，提高了语义频谱效率。

Conclusion: 语义波形协同设计为AI原生智能通信系统开辟了新研究方向，通过波形层面的语义直接编码实现了意义感知的物理信号构建。

Abstract: In this paper, we propose a semantic-aware waveform design framework for AI-native 6G networks that jointly optimizes physical layer resource usage and semantic communication efficiency and robustness, while explicitly accounting for the hardware constraints of RF chains. Our approach, called Orthogonal Semantic Sequency Division Multiplexing (OSSDM), introduces a parametrizable, orthogonal-base waveform design that enables controlled degradation of the wireless transmitted signal to preserve semantically significant content while minimizing resource consumption. We demonstrate that OSSDM not only reinforces semantic robustness against channel impairments but also improves semantic spectral efficiency by encoding meaningful information directly at the waveform level. Extensive numerical evaluations show that OSSDM outperforms conventional OFDM waveforms in spectral efficiency and semantic fidelity. The proposed semantic waveform co-design opens new research frontiers for AI-native, intelligent communication systems by enabling meaning-aware physical signal construction through the direct encoding of semantics at the waveform level.

</details>


### [27] [3D Wi-Fi Signal Measurement in Realistic Digital Twin Testbed Environments Using Ray Tracing](https://arxiv.org/abs/2602.13340)
*Mengyuan Wang,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.NI

TL;DR: 提出基于数字孪生的室内无线信号传播测量系统，结合3D环境重建与确定性射线追踪，实现物理基础的电磁建模，在相同运行时间下比商业测量模拟器性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 下一代Wi-Fi部署需要准确高效的室内无线信号传播建模，传统方法在物理基础和计算效率方面存在局限。

Method: 通过LiDAR扫描获取建筑几何结构，进行物体分割并分配ITU-R标准材料参数，使用GPU加速的射线追踪引擎模拟传播过程，生成路径级信道属性。

Result: 在相同运行时间约束下，比商业测量模拟器路径增益提高21dB，视距条件下信干噪比持续改善；与现场RSSI测量对比显示0.98的高空间相关性；覆盖2.4GHz、5GHz、6GHz频段分析展示频率相关材料衰减建模能力。

Conclusion: 该系统通过数字孪生技术实现高保真无线信号传播建模，提供交互式3D可视化和按需数据提取，为数字孪生驱动的无线系统设计和优化提供有力工具。

Abstract: Accurate and efficient modeling of indoor wireless signal propagation is crucial for the deployment of next-generation Wi-Fi. This paper presents a digital twin-based measurement system that integrates real-world 3D environment reconstruction with deterministic ray tracing for physically grounded electromagnetic modeling. Building geometry is obtained through LiDAR scanning, followed by object segmentation and assignment of ITU-R standard material parameters. The propagation process is simulated with a GPU-accelerated ray-tracing engine that generates path-level channel attributes, including delay, power, angular dispersion, and Ricean K-factor. Under identical runtime constraints, the proposed system is evaluated against a commercial measurement simulator, demonstrating up to 21 dB higher path gain and consistently improved signal-to-interference-plus-noise ratio in line-of-sight conditions. Additionally, experiments against onsite RSSI measurements confirm a high spatial correlation of 0.98 after calibration, proving the system's fidelity in real-world settings. Furthermore, coverage analysis across 2.4 GHz, 5 GHz, and 6 GHz bands demonstrates the capability of system to model frequency-dependent material attenuation for Wi-Fi 6E/7 networks. Finally, the system offers interactive 3D visualization and on-demand data extraction, highlighting its potential for digital twin-driven wireless system design and optimization.

</details>


### [28] [Location as a service with a MEC architecture](https://arxiv.org/abs/2602.13358)
*Christopher Schahn,Jorin Kouril,Bernd Schaeufele,Ilja Radusch*

Main category: cs.NI

TL;DR: 提出一种基于移动边缘计算和GNSS数据的协同定位方法，通过多车GPS信息融合实现车道级精确定位


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和高级驾驶辅助系统需要高精度定位，但传统GNSS定位精度不足，难以满足车道级定位需求

Method: 采用协同定位方法，在移动边缘计算云中收集多个道路使用者的GPS信息，结合GNSS定位特性和高精度地图，应用概率滤波器实现定位

Result: 通过多车协同和数据处理，能够为所有参与者提供车道级精度的定位服务

Conclusion: 协同定位方法利用移动边缘计算和GNSS数据融合，有效解决了自动驾驶系统对高精度定位的需求

Abstract: In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning. In this paper, a cooperative approach to localization is presented. The GPS information from several road users is collected in a Mobile Edge Computing cloud, and the characteristics of GNSS positioning are used to provide lane-precise positioning for all participants by applying probabilistic filters and HD maps.

</details>


### [29] [Parametric Traversal for Multi-Dimensional Cost-Aware Graph Reasoning](https://arxiv.org/abs/2602.13369)
*Nicolas Tacheny*

Main category: cs.NI

TL;DR: 提出一种名为"traversal"的路径搜索新概念，将现有边与可建设的"gap transitions"结合，用于不完整基础设施网络的多维评估。


<details>
  <summary>Details</summary>
Motivation: 传统路径搜索假设完整图和标量优化指标，但实际基础设施网络不完整且需要多维评估。工程师实际考虑的是不仅包括现有连接，还包括可实现的连接。

Method: 提出参数化框架，将规划连接作为一等转换处理，通过高效候选过滤扩展到大型图，使用多维标准决定是否继续探索或放弃遍历。

Result: 在数据中心电路设计和光通信网络路由构建的代表性场景中评估框架，展示了条件可行性、非标量化权衡和策略校准能力，超越了传统方法。

Conclusion: Traversal概念抽象了工程师对基础设施的实际推理方式，框架能够处理不完整网络的多维评估，在现实基础设施规划中具有实用价值。

Abstract: Classical path search assumes complete graphs and scalar optimization metrics, yet real infrastructure networks are incomplete and require multi-dimensional evaluation. We introduce the concept of traversal: a generalization of paths that combines existing edges with gap transitions, missing but acceptable connections representing links that can be built. This abstraction captures how engineers actually reason about infrastructure: not just what exists, but what can be realized.
  We present a parametric framework that treats planned connections as first-class transitions, scales to large graphs through efficient candidate filtering, and uses multi-dimensional criteria to decide whether a traversal should continue to be explored or be abandoned. We evaluate the framework through representative scenarios in datacenter circuit design and optical route construction in telecommunication networks, demonstrating conditional feasibility, non-scalarizable trade-offs, and policy calibration capabilities beyond the reach of classical formulations.

</details>


### [30] [Spatiotemporal Feature Alignment and Weighted Fusion in Collaborative Perception Enabled by Network Synchronization and Age of Information](https://arxiv.org/abs/2602.13439)
*Qiaomei Han,Xianbin Wang,Minghui Liwang,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出一个时空特征对齐与加权融合框架，通过连续时钟同步和AoI估计解决IoV中时空异质性问题，提升协同感知精度。


<details>
  <summary>Details</summary>
Motivation: 车联网协同感知中，由于时钟不同步、通信延迟和车辆运动变化导致的时空异质性会降低融合质量。现有方法通过空间变换或固定时间偏移校正，但忽略了时变时钟漂移和延迟导致的持续特征错位问题。

Method: 提出时空特征对齐与加权融合框架：1) 网络同步连续补偿车辆间时钟状态差异，建立共同时间参考；2) 基于特征大小和链路质量估计网络延迟，确定信息年龄(AoI)；3) 时空特征对齐将特征投影到统一空间坐标，并使用AoI校正到同步融合时刻；4) 估计同步和对齐质量的不确定性，结合AoI生成特征权重进行高效融合。

Result: 仿真实验表明，在时钟漂移和链路延迟条件下，相比强基线方法，该框架能持续提升感知精度。

Conclusion: 提出的时空特征对齐与加权融合框架能有效解决车联网协同感知中的时空异质性问题，通过连续时钟同步、AoI估计和不确定性加权融合，显著提升感知性能。

Abstract: Collaborative perception in Internet of Vehicles (IoV) aggregates multi-vehicle observations for broader scene coverage and improved decision-making. However, fusion quality degrades under spatiotemporal heterogeneity from unsynchronized clocks, communication delays, and motion variations across vehicles. Prior work mitigates these through spatial transformations or fixed time-offset corrections, overlooking time-varying clock drifts and delays that cause persistent feature misalignment. To overcome these, we propose a spatiotemporal feature alignment and weighted fusion framework. Specifically, network synchronization is designed to continuously compensate for clock state differences between vehicles and establish a common time reference, onto which all feature timestamps can be mapped. After synchronization, to align the freshness of received features since their generation, their Age of Information (AoI) is determined by estimating network delay with given feature size and link quality. Our spatiotemporal feature alignment then projects vehicles' features into one spatial coordinate and corrects them to a synchronized fusion instant using AoIs, enabling all features to describe the scene coherently. Furthermore, due to varying synchronization and alignment quality, we estimate their uncertainties and integrate with AoI to generate feature weights for efficient fusion, prioritizing fresh, reliable feature regions. Simulations show consistent perception accuracy improvements over strong baselines under clock drifts and link delays.

</details>


### [31] [SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity](https://arxiv.org/abs/2602.13542)
*George M. Gichuru,Zoe Aiyanna M. Cayetano*

Main category: cs.NI

TL;DR: SIDSense是一个边缘AI框架，用于无数据库的TV White Space操作，通过合规门控控制器、审计日志和优雅降级来维持监管意图，在模拟PAWS中断期间保持连接。


<details>
  <summary>Details</summary>
Motivation: 小岛屿发展中国家(SIDS)面临气候灾害时，依赖脆弱的陆地网络容易失效。现有TV White Space部署需要PAWS数据库连接进行信道授权，这在中断期间成为单点故障。

Method: 提出SIDSense边缘AI框架，采用CNN频谱分类与混合感知优先、尽快授权工作流，将感知和视频增强与私有5G栈共同部署在海上船只上，支持态势感知视频回传。

Result: 在巴巴多斯的实地实验显示，在模拟PAWS中断期间保持连接，在470-698 MHz频段达到94.2%感知准确率，平均决策延迟23毫秒，在GPU感知调度下保持5G L1层零错过截止时间。

Conclusion: SIDSense框架在无数据库TVWS操作中有效维持连接，发布加勒比TVWS传播和占用数据集，计划开源部分组件以加速气候脆弱地区的弹性连接部署。

Abstract: Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.
  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.
  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions.

</details>


### [32] [Multi-Modal Sensing and Fusion in mmWave Beamforming for Connected Vehicles: A Transformer Based Framework](https://arxiv.org/abs/2602.13606)
*Muhammad Baqer Mollah,Honggang Wang,Mohammad Ataul Karim,Hua Fang*

Main category: cs.NI

TL;DR: 提出多模态感知融合学习框架，利用传感器数据预测毫米波通信的最佳波束，减少波束训练开销


<details>
  <summary>Details</summary>
Motivation: 毫米波通信在车联网中面临高波束训练开销问题，传统方法需要交换导频信号和穷举波束测量，减少了可用通信时间

Method: 使用多模态感知融合学习框架：通过模态特定编码器提取特征，利用多头跨模态注意力学习模态间依赖关系，融合多模态特征预测top-k最佳波束

Result: 在真实V2I/V2V场景中，top-15波束预测准确率达96.72%，平均功率损失约0.77dB，延迟和波束搜索空间开销分别降低86.81%和76.56%

Conclusion: 多模态感知融合框架能有效减少毫米波车联网中的波束训练开销，提高通信效率，具有良好泛化能力

Abstract: Millimeter wave (mmWave) communication, utilizing beamforming techniques to address the inherent path loss limitation, is considered as one of the key technologies to support ever increasing high throughput and low latency demands of connected vehicles. However, adopting standard defined beamforming approach in highly dynamic vehicular environments often incurs high beam training overheads and reduction in the available airtime for communications, which is mainly due to exchanging pilot signals and exhaustive beam measurements. To this end, we present a multi-modal sensing and fusion learning framework as a potential alternative solution to reduce such overheads. In this framework, we first extract the representative features from the sensing modalities by modality specific encoders, then, utilize multi-head cross-modal attention to learn dependencies and correlations between different modalities, and subsequently fuse the multimodal features to obtain predicted top-k beams so that the best line-of-sight links can be proactively established. To show the generalizability of the proposed framework, we perform a comprehensive experiment in four different vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) scenarios from real world multimodal and 60 GHz mmWave wireless sensing data. The experiment reveals that the proposed framework (i) achieves up to 96.72% accuracy on predicting top-15 beams correctly, (ii) incurs roughly 0.77 dB average power loss, and (iii) improves the overall latency and beam searching space overheads by 86.81% and 76.56% respectively for top-15 beams compared to standard defined approach.

</details>


### [33] [Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing](https://arxiv.org/abs/2602.13628)
*Ruichen Zhang,Xiaofeng Luo,Jiayi He,Dusit Niyato,Jiawen Kang,Zehui Xiong,Yonghui Li*

Main category: cs.NI

TL;DR: 该论文研究移动边缘计算网络中紧凑大语言模型部署和世界模型辅助的推理卸载，提出ECLD框架压缩LLM，并开发世界模型-PPO算法优化推理延迟。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算网络中部署大语言模型面临存储、能耗和延迟挑战，需要开发紧凑模型和智能卸载策略来平衡本地执行和云端推理的效率与质量。

Method: 1) 提出ECLD框架，联合应用结构化剪枝、低位量化和知识蒸馏构建边缘可部署LLM变体；2) 制定MEC卸载优化问题，最小化长期平均推理延迟；3) 开发世界模型-PPO算法，在未知时变网络动态下求解优化问题。

Result: ECLD将基础模型压缩70-80%存储（如Llama-3.1-8B从15.3GB降至3.3GB），每查询能耗降低达50%，同时保持精度并降低幻觉。世界模型-PPO加速收敛约50%，最终奖励比原始PPO提高15.8%，平均推理延迟降低12-30%。

Conclusion: 该研究证明了紧凑LLM部署和世界模型辅助卸载在移动边缘计算中的有效性，能够在满足精度和幻觉约束的同时，接近全卸载的生成质量，同时保持本地执行的高效率。

Abstract: This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution.

</details>


### [34] [LEAD-Drift: Real-time and Explainable Intent Drift Detection by Learning a Data-Driven Risk Score](https://arxiv.org/abs/2602.13672)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: LEAD-Drift是一个用于意图驱动网络(IBN)的实时意图漂移检测框架，通过监督学习预测未来风险，提供早期预警，减少告警噪音，并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 意图驱动网络简化了网络管理，但存在"意图漂移"问题，即网络状态逐渐偏离预期目标，导致静默故障。传统方法难以检测早期细微漂移，只能在性能显著恶化时告警，限制了主动保障的有效性。

Method: 将意图故障检测重新定义为监督学习问题，训练轻量级神经网络基于固定时间窗口标签预测未来风险分数。使用指数移动平均平滑模型输出，并通过统计调优阈值生成实时告警。增加多时间窗口建模进行动态故障时间估计，并使用SHAP提供每个告警的可解释性以识别根本原因KPI。

Result: 在时间序列数据集上的评估显示，LEAD-Drift相比基于距离的基线方法，平均提前预警时间增加了7.3分钟（+17.8%）。相比加权KPI启发式方法，减少了80.2%的告警噪音，仅以轻微的时间提前量作为代价。

Conclusion: LEAD-Drift是一个高效、可解释且操作高效的解决方案，能够为意图驱动网络提供主动的网络保障，显著改善早期故障检测能力。

Abstract: Intent-Based Networking (IBN) simplifies network management, but its reliability is challenged by "intent drift", where the network's state gradually deviates from its intended goal, often leading to silent failures. Conventional approaches struggle to detect the subtle, early stages of intent drift, raising alarms only when degradation is significant and failure is imminent, which limits their effectiveness for proactive assurance. To address this, we propose LEAD-Drift, a framework that detects intent drift in real time to enable proactive failure prevention. LEAD-Drift's core contribution is reformulating intent failure detection as a supervised learning problem by training a lightweight neural network on fixed-horizon labels to predict a future risk score. The model's raw output is then smoothed with an Exponential Moving Average (EMA) and passed through a statistically tuned threshold to generate robust, real-time alerts. Furthermore, we enhance the framework with two key features for operational intelligence: a multi-horizon modeling technique for dynamic time-to-failure estimation, and per-alert explainability using SHAP to identify root-cause KPIs. Our evaluation on a time-series dataset shows LEAD-Drift provides significantly earlier warnings, improving the average lead time by 7.3 minutes (+17.8\%) compared to a distance-based baseline. It also reduces alert noise by 80.2\% compared to a weighted-KPI heuristic, with only a minor trade-off in lead time. These results demonstrate that LEAD-Drift as a highly effective, interpretable, and operationally efficient solution for proactive network assurance in IBN.

</details>


### [35] [Agent-OSI: A Layered Protocol Stack Toward a Decentralized Internet of Agents](https://arxiv.org/abs/2602.13795)
*Wenxin Xu,Taotao Wang,Yihan Xia,Shengli Zhang,Soung Chang Liew*

Main category: cs.NI

TL;DR: 提出Agent-OSI六层参考架构，基于现有互联网构建去中心化智能体网络，通过HTTP 402实现应用层支付挑战，结合链下协商与链上结算降低51%成本。


<details>
  <summary>Details</summary>
Motivation: 当前智能体被限制在平台孤岛和专有接口中，缺乏互操作性、信任和按使用付费结算的通用堆栈。随着LLM推动从信息互联网向智能体互联网转变，需要解决这些限制。

Method: 提出Agent-OSI六层参考架构：安全连接与A2A消息传递、去中心化身份与授权、结算与计量、可验证执行与溯源、语义互操作性编排。特别将HTTP 402作为应用层支付挑战，触发基于托管账户的结算和可验证收据，使用区块链托管实现原型。

Result: 原型评估显示，保持协商和交付在链下同时保留可验证结算，相比标准Web3基线可降低约51%的链上会话成本。区块链确认延迟通常不是生成工作负载的主导因素。

Conclusion: Agent-OSI为去中心化智能体网络提供了一个可行的参考架构，通过将支付挑战集成到HTTP协议中，实现了互操作性、信任和按使用付费结算，同时显著降低了成本。

Abstract: Large Language Models (LLMs) are accelerating the shift from an Internet of information to an Internet of Agents (IoA), where autonomous entities discover services, negotiate, execute tasks, and exchange value. Yet today's agents are still confined to platform silos and proprietary interfaces, lacking a common stack for interoperability, trust, and pay-per-use settlement. This article proposes \textit{Agent-OSI}, a six-layer reference stack for decentralized agent networking built on top of the existing Internet. Agent-OSI combines secure connectivity and A2A messaging, decentralized identity and authorization, settlement and metering, verifiable execution and provenance, and semantic interoperability for orchestration. In particular, we treat HTTP 402 (Payment Required) as an application-level payment challenge (analogous to HTTP 401 for authentication) that triggers escrow-based settlement and verifiable receipts (instantiated via a blockchain escrow in our prototype), rather than introducing a new network-layer protocol. We implement a prototype and evaluate cost and latency. Results show that keeping negotiation and delivery off-chain while preserving verifiable settlement reduces on-chain session costs by approximately 51\% compared with a standard Web3 baseline in our prototype setting, and that blockchain confirmation latency is often not the dominant factor for generative workloads.

</details>


### [36] [Agentic Assistant for 6G: Turn-based Conversations for AI-RAN Hierarchical Co-Management](https://arxiv.org/abs/2602.13868)
*Udhaya Srinivasan,Weisi Guo*

Main category: cs.NI

TL;DR: 开发了一个基于代理的网络管理系统和对话助手，用于AI-RAN的协同管理，通过三层架构实现意图理解、规划配置和性能调优，平均响应时间13秒，准确率67-89%。


<details>
  <summary>Details</summary>
Motivation: 新一代无线接入网络（RAN）特别是原生AI服务越来越难以由人类工程师实时管理。企业网络通常本地管理，但专家稀缺。现有研究主要关注创建RAG LLM来规划和配置RAN及核心方面，但缺乏RAN与边缘AI的协同管理，这产生了需要基于回合人类交互的分层动态问题。

Method: 创建了一个代理网络管理器和基于回合的对话助手，能够理解人类基于意图的查询，匹配AI-RAN中的分层问题。框架包括三层：(a)用户界面和评估仪表板，(b)与AI-RAN接口的智能层，(c)提供评估和建议基础的知识层。

Result: 三层能力验证性能（平均响应时间13秒）：(1)设计和规划服务（78%准确率），(2)操作特定AI-RAN工具（89%准确率），(3)调优AI-RAN性能（67%准确率）。这些初步结果表明存在幻觉的普遍挑战，但快速响应性能成功，可显著降低小规模企业用户的运营成本。

Conclusion: 该框架解决了AI-RAN协同管理的空白，通过代理网络管理器和对话助手实现了分层动态问题的有效处理，虽然存在幻觉挑战，但快速响应能力有助于降低企业运营成本，为稀缺专家环境下的网络管理提供了实用解决方案。

Abstract: New generations of radio access networks (RAN), especially with native AI services are increasingly difficult for human engineers to manage in real-time. Enterprise networks are often managed locally, where expertise is scarce. Existing research has focused on creating Retrieval-Augmented Generation (RAG) LLMs that can help to plan and configure RAN and core aspects only. Co-management of RAN and edge AI is the gap, which creates hierarchical and dynamic problems that require turn-based human interactions. Here, we create an agentic network manager and turn-based conversation assistant that can understand human intent-based queries that match hierarchical problems in AI-RAN. The framework constructed consists of: (a) a user interface and evaluation dashboard, (b) an intelligence layer that interfaces with the AI-RAN, and (c) a knowledge layer for providing the basis for evaluations and recommendations. These form 3 layers of capability with the following validation performances (average response time 13s): (1) design and planning a service (78\% accuracy), (2) operating specific AI-RAN tools (89\% accuracy), and (3) tuning AI-RAN performance (67\%). These initial results indicate the universal challenges of hallucination but also fast response performance success that can really reduce OPEX costs for small scale enterprise users.

</details>


### [37] [Toward Autonomous O-RAN: A Multi-Scale Agentic AI Framework for Real-Time Network Control and Management](https://arxiv.org/abs/2602.14117)
*Hojjat Navidan,Mohammad Cheraghinia,Jaron Fontaine,Mohamed Seif,Eli De Poorter,H. Vincent Poor,Ingrid Moerman,Adnan Shahid*

Main category: cs.NI

TL;DR: 提出用于O-RAN的多尺度智能体AI框架，通过LLM、SLM和WPFM智能体在非实时、近实时和实时控制环中协调组织RAN智能，解决O-RAN可编程性带来的操作复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: O-RAN通过解耦、软件驱动的组件和开放接口提供灵活的6G网络接入，但这种可编程性增加了操作复杂性。多个控制环共存于服务管理层和RAN智能控制器中，独立开发的控制应用可能产生意外交互。同时，生成式AI的进步使得从孤立AI模型转向能够解释目标、协调多个模型和控制功能、随时间适应行为的智能体AI系统成为可能。

Method: 提出多尺度智能体AI框架，将RAN智能组织为跨非实时、近实时和实时控制环的协调层次结构：1) 非实时RIC中的LLM智能体将运营商意图转化为策略并管理模型生命周期；2) 近实时RIC中的SLM智能体执行低延迟优化，可以激活、调优或禁用现有控制应用；3) 分布式单元附近的无线物理层基础模型智能体提供靠近空口的快速推理。这些智能体通过标准化的O-RAN接口和遥测进行协作。

Result: 使用基于开源模型、软件和数据集的概念验证实现，在两个代表性场景中展示了所提出的智能体方法：非平稳条件下的稳健操作和意图驱动的切片资源控制。

Conclusion: 多尺度智能体AI框架能够有效解决O-RAN的操作复杂性挑战，通过分层智能体协调实现意图驱动的网络管理和自适应控制，为6G网络提供灵活、智能的接入解决方案。

Abstract: Open Radio Access Networks (O-RAN) promise flexible 6G network access through disaggregated, software-driven components and open interfaces, but this programmability also increases operational complexity. Multiple control loops coexist across the service management layer and RAN Intelligent Controller (RIC), while independently developed control applications can interact in unintended ways. In parallel, recent advances in generative Artificial Intelligence (AI) are enabling a shift from isolated AI models toward agentic AI systems that can interpret goals, coordinate multiple models and control functions, and adapt their behavior over time. This article proposes a multi-scale agentic AI framework for O-RAN that organizes RAN intelligence as a coordinated hierarchy across the Non-Real-Time (Non-RT), Near-Real-Time (Near-RT), and Real-Time (RT) control loops: (i) A Large Language Model (LLM) agent in the Non-RT RIC translates operator intent into policies and governs model lifecycles. (ii) Small Language Model (SLM) agents in the Near-RT RIC execute low-latency optimization and can activate, tune, or disable existing control applications; and (iii) Wireless Physical-layer Foundation Model (WPFM) agents near the distributed unit provide fast inference close to the air interface. We describe how these agents cooperate through standardized O-RAN interfaces and telemetry. Using a proof-of-concept implementation built on open-source models, software, and datasets, we demonstrate the proposed agentic approach in two representative scenarios: robust operation under non-stationary conditions and intent-driven slice resource control.

</details>


### [38] [MILD: Multi-Intent Learning and Disambiguation for Proactive Failure Prediction in Intent-based Networking](https://arxiv.org/abs/2602.14283)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: MILD是一个主动式框架，将意图保障从反应式漂移检测转变为固定时间范围的故障预测，通过意图级消歧解决多意图网络中的共漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在多意图意图网络中，单个故障可能触发共漂移，导致多个意图同时出现KPI退化，造成根因意图识别模糊，需要解决这种共漂移的消歧问题。

Method: MILD采用教师增强的专家混合模型，其中门控消歧模块识别根因意图，而每个意图的头部输出校准的风险评分，实现意图级消歧和故障预测。

Result: 在包含非线性故障和共漂移的基准测试中，MILD提供比基线方法长3.8%-92.5%的修复提前时间，并将意图级根因消歧准确率提高9.4%-45.8%。

Conclusion: MILD通过主动式故障预测和意图消歧，显著提高了多意图网络的可靠性，并提供每个警报的KPI解释，支持可操作的诊断。

Abstract: In multi-intent intent-based networks, a single fault can trigger co-drift where multiple intents exhibit symptomatic KPI degradation, creating ambiguity about the true root-cause intent. We present MILD, a proactive framework that reformulates intent assurance from reactive drift detection to fixed-horizon failure prediction with intent-level disambiguation. MILD uses a teacher-augmented Mixture-of-Experts where a gated disambiguation module identifies the root-cause intent while per-intent heads output calibrated risk scores. On a benchmark with non-linear failures and co-drifts, MILD provides 3.8\%--92.5\% longer remediation lead time and improves intent-level root-cause disambiguation accuracy by 9.4\%--45.8\% over baselines. MILD also provides per-alert KPI explanations, enabling actionable diagnosis.

</details>


### [39] [LiSFC-Search: Lifelong Search for Network SFC Optimization under Non-stationary Drifts](https://arxiv.org/abs/2602.14360)
*Zuyuan Zhang,Vaneet Aggarwal,Tian Lan*

Main category: cs.NI

TL;DR: LiSFC：基于Lipschitz终身规划的SFC优化器，利用MCTS统计迁移处理网络拓扑漂移，降低阻塞概率和尾延迟


<details>
  <summary>Details</summary>
Motivation: 边缘云融合重塑5G/6G和算力网络的服务供给。传统SFC优化器假设静态或平稳网络，在长期拓扑/资源变化（故障、升级、扩展）导致的非平稳图漂移下性能下降。需要能适应网络配置漂移的SFC规划方法。

Method: 提出LiSFC，基于Lipschitz终身规划器，通过MDP距离界在漂移的网络配置间迁移MCTS统计。将问题建模为基于底层网络图和约束的MDP序列，定义图漂移度量上界LiZero MDP距离。设计LiSFC-Search，使用可迁移的自适应UCT奖励重用先前CPN配置的搜索统计。

Result: 在合成CPN拓扑和SFC工作负载上的初步结果显示，LiSFC相比非迁移MCTS和纯学习基线，持续降低SFC阻塞概率并改善尾延迟。

Conclusion: LiSFC展示了作为云网融合AI/ML构建块的潜力，能够有效处理网络拓扑漂移，提升SFC规划性能。

Abstract: Edge-cloud convergence is reshaping service provisioning across 5G/6G and computing power networks (CPNs). Service function chaining (SFC) requires continuously placing and scheduling virtual network functions (VNFs) chains under compute/bandwidth and end-to-end QoS constraints. Most SFC optimizers assume static or stationary networks, and degrade under long-term topology/resource changes (failures, upgrades, expansions) that induce non-stationary graph drifts. We propose LiSFC, a Lipschitz lifelong planner that transfers MCTS statistics across drifting network configurations using an MDP-distance bound. More precisely, we formulate the problem as a sequence of MDPs indexed by the underlying network graph and constraints, and we define a \emph{graph drift} metric that upper-bounds the LiZero MDP distance. This allows LiSFC to import theoretical guarantees on bias and sample efficiency from the LiZero framework while being tailored to cloud-network convergence. We then design \emph{LiSFC-Search}, an SFC-aware unified MCTS (UMCTS) procedure that uses transferable adaptive UCT (aUCT) bonuses to reuse search statistics from prior CPN configurations. Preliminary results on synthetic CPN topologies and SFC workloads show that LiSFC consistently reduces SFC blocking probability and improves tail delay compared to non-transfer MCTS and purely learning-based baselines, highlighting its potential as an AI/ML building block for cloud-network convergence.

</details>


### [40] [Bitcoin Under Stress: Measuring Infrastructure Resilience 2014-2025](https://arxiv.org/abs/2602.14372)
*Wenbin Wu,Alexander Neumueller*

Main category: cs.NI

TL;DR: 比特币网络对海底电缆故障具有较高韧性，随机故障下临界阈值pc≈0.72-0.92，但定向攻击会大幅降低韧性；Tor使用增强了而非削弱了网络韧性。


<details>
  <summary>Details</summary>
Motivation: 比特币设计上承诺通过去中心化实现韧性，但物理基础设施（如海底电缆）可能造成隐藏的依赖关系。需要研究比特币网络对基础设施故障的实际韧性。

Method: 使用11年P2P网络数据（2014-2025）、658条海底电缆数据和68个已验证的电缆故障事件，构建Buldyrev级联模型，包含国家级物理层（225个国家、354条海底电缆边、325条陆地边界边）。针对64%使用Tor的节点，开发了包含Tor中继基础设施的四层多重网络模型。

Result: 比特币清洁网络的渗流阈值pc≈0.72-0.92（随机电缆故障），从2014-2017年的pc≈0.92下降到2021年最低pc=0.72（挖矿集中度峰值时期）。定向攻击下pc降至0.05-0.20。Tor中继带宽集中在连接良好的欧洲国家，反而增强了韧性（Δpc≈+0.02-+0.10）。实证验证显示87%的电缆故障造成少于5%的节点影响。

Conclusion: 比特币网络对物理基础设施故障具有较高韧性，特别是对随机故障；Tor采用增强了网络韧性而非引入脆弱性；需要关注挖矿集中度对韧性的影响。

Abstract: Bitcoin's design promises resilience through decentralization, yet physical infrastructure creates hidden dependencies. We present the first longitudinal study of Bitcoin's resilience to infrastructure failures using 11 years of P2P network data (2014-2025), 658 submarine cables, and 68 verified cable fault events. Using a Buldyrev-style cascade model with a country-level physical layer (225 countries, 354 submarine cable edges, 325 land border edges), we find that Bitcoin's clearnet percolation threshold $p_c \approx 0.72$-$0.92$ for random cable failures, declining 22% from $p_c \approx 0.92$ (2014-2017) to a minimum of $p_c = 0.72$ in 2021 during peak mining concentration. Targeted attacks reduce $p_c$ to 0.05-0.20. To address the 64% of nodes using Tor with unobservable locations, we develop a 4-layer multiplex model incorporating Tor relay infrastructure. Tor relay bandwidth concentrates in well-connected European countries, increasing resilience by $Δp_c \approx +0.02$-$+0.10$ rather than introducing fragility. Empirical validation shows 87% of cable faults caused less than 5% node impact. We contribute: (1) a multiplex percolation framework for overlay-underlay coupling with a 4-layer Tor relay model; (2) the first empirical measurement of Bitcoin's physical-layer resilience over a decade; and (3) evidence that Tor adoption amplifies resilience with distributional bounds under partial observability.

</details>


### [41] [A Q-Learning Approach for Dynamic Resource Management in Three-Tier Vehicular Fog Computing](https://arxiv.org/abs/2602.14390)
*Bahar Mojtabaei Ranani,Mahmood Ahmadi,Sajad Ahmadian*

Main category: cs.NI

TL;DR: 提出一种基于Q-Learning的三层车载计算架构资源预测方法，优化雾计算环境中的资源分配


<details>
  <summary>Details</summary>
Motivation: 智能车辆在雾计算环境中需要动态、自适应的资源管理策略，以应对不断变化的条件并满足性能需求

Method: 采用三层车载计算架构，利用Q-Learning强化学习算法，通过持续训练和更新Q-learning代理，从历史经验中学习并做出资源分配决策

Result: Q-learning代理能有效预测内存、带宽和处理器的最优值，减少资源消耗的同时满足雾系统性能要求，相比其他方法提高了平均任务处理时间

Conclusion: Q-Learning方法在雾计算环境中能有效优化资源分配，提供动态自适应的资源管理策略，提升系统整体性能

Abstract: In this paper, a method for predicting the resources required for an intelligent vehicle client using a three-layer vehicular computing architecture is proposed. This method leverages Q-Learning to optimize resource allocation and enhance overall system performance. This approach employs reinforcement learning capabilities to provide a dynamic and adaptive strategy for resource management in a fog computing environment. The key findings of this study indicate that Q-learning can effectively predict the appropriate allocation of resources by learning from past experiences and making informed decisions. Through continuous training and updating of the Q-learning agent, the system can adapt to changing conditions and make resource allocation decisions based on real-time information. The experimental results demonstrate the effectiveness of the proposed method in optimizing resource allocation. The Q-learning agent predicts the optimal values for memory, bandwidth, and processor. These predictions not only minimize resource consumption but also meet the performance requirements of the fog system. Implementations show that this method improves the average task processing time in compared to other methods evaluated in this study

</details>


### [42] [ASA: Adaptive Smart Agent Federated Learning via Device-Aware Clustering for Heterogeneous IoT](https://arxiv.org/abs/2602.14391)
*Ali Salimi,Saadat Izadi,Mahmood Ahmadi,Hadi Tabatabaee Malazi*

Main category: cs.NI

TL;DR: ASA框架通过实时资源分析将设备自适应聚类为三个性能等级，为每个等级定制模型，在异构联邦学习中显著降低通信负担并提升资源利用率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式IoT设备中实现隐私保护的协作学习，但设备异构性（计算能力、内存可用性、网络环境差异）是主要挑战，需要解决方案来确保所有设备都能有效参与。

Method: 提出ASA（自适应智能代理）框架：1）智能代理层实时分析设备CPU、内存和网络环境；2）将设备自适应聚类为高性能、中端和低能力三个等级；3）为每个等级定制适合其计算能力的模型；4）确保网络中所有设备的包容性参与。

Result: 在MNIST和CIFAR-10数据集上的实验表明：通信负担降低43%-50%，资源利用率提升43%，最终模型准确率达到MNIST 98.89%和CIFAR-10 85.30%。

Conclusion: ASA框架有效提升了异构联邦学习环境的效率、可扩展性和公平性，适合实际IoT应用部署，解决了设备异构性带来的挑战。

Abstract: Federated learning (FL) has become a promising answer to facilitating privacy-preserving collaborative learning in distributed IoT devices. However, device heterogeneity is a key challenge because IoT networks include devices with very different computational powers, memory availability, and network environments. To this end, we introduce ASA (Adaptive Smart Agent). This new framework clusters devices adaptively based on real-time resource profiles and adapts customized models suited to every cluster's capability. ASA capitalizes on an intelligent agent layer that examines CPU power, available memory, and network environment to categorize devices into three levels: high-performance, mid-tier, and low-capability. Each level is provided with a model tuned to its computational power to ensure inclusive engagement across the network. Experimental evaluation on two benchmark datasets, MNIST and CIFAR-10, proves that ASA decreases communication burden by 43% to 50%, improves resource utilization by 43%, and achieves final model accuracies of 98.89% on MNIST and 85.30% on CIFAR-10. These results highlight ASA's efficacy in enhancing efficiency, scalability, and fairness in heterogeneous FL environments, rendering it a suitable answer for real-world IoT apps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: 提出一种决策否定、人在回路（human-in-the-loop）的代理系统，通过对抗性自我批判机制作为受监管核保工作流的安全架构，减少AI幻觉并提高准确性，同时保持人类对最终决策的权威。


<details>
  <summary>Details</summary>
Motivation: 商业保险核保是劳动密集型过程，现有AI解决方案缺乏全面的推理能力和确保在受监管高风险环境中可靠性的内部机制。完全自动化在需要人类判断和问责的场景中既不实际也不可取。

Method: 设计决策否定、人在回路的代理系统，包含对抗性自我批判机制作为安全架构。批评代理在提交建议给人类审核员之前挑战主代理的结论。同时开发了决策否定代理潜在错误的正式分类法。

Result: 在500个专家验证的核保案例中，对抗性批判机制将AI幻觉率从11.3%降至3.8%，决策准确率从92%提升至96%。该框架通过设计确保人类对所有约束性决策的严格权威。

Conclusion: 对抗性自我批判支持在受监管领域更安全的AI部署，为人类监督不可或缺的场景提供了负责任整合的模型，同时保持人类对最终决策的权威。

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [44] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出BotzoneBench框架，通过将LLM与固定技能校准的游戏AI层次结构对比，实现线性时间绝对技能测量，解决现有LLM战略能力评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要关注静态推理任务，无法捕捉动态战略能力。现有的游戏评估采用LLM对LLM锦标赛，存在二次计算成本、依赖瞬态模型池、缺乏稳定性能锚点等问题。

Method: 基于Botzone平台的竞争基础设施，构建BotzoneBench框架，在8个多样化游戏中评估LLM，包括确定性完全信息棋盘游戏和随机不完全信息纸牌游戏。通过与固定技能校准的游戏AI层次结构对比，实现绝对技能测量。

Result: 通过系统评估177,047个状态-动作对，揭示了5个旗舰模型的显著性能差异和不同战略行为。表现最佳的模型在多个领域达到中高等级专业游戏AI的熟练程度。

Conclusion: 这种锚定评估范式可推广到任何具有明确定义技能层次的领域，为评估交互式AI能力建立了可扩展、可重用的框架。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [45] [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)
*Haoran Zheng*

Main category: cs.AI

TL;DR: AMOR是一个混合架构，结合了状态空间模型（SSM）和稀疏注意力机制，通过预测熵动态判断何时需要注意力，在保持高效的同时提升长距离信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对所有位置分配均匀计算，效率不高；SSM虽然高效但在长距离信息检索上表现不佳。受认知双过程理论启发，需要一种能动态适应计算需求的架构。

Method: 提出AMOR混合架构：使用SSM作为主干，通过预测熵测量不确定性，仅在SSM"不确定"时动态激活稀疏注意力。采用Ghost KV技术从SSM隐藏状态投影键值，重用O(n)计算而非每层都需要O(n²)注意力。

Result: 在小规模合成检索任务中，AMOR优于纯SSM和纯Transformer基线，实现完美检索准确率，仅需在22%的位置上激活注意力。预测熵能可靠指示检索需求，检索位置和局部位置之间的熵差达1.09纳特（接近熵范围的一半）。

Conclusion: AMOR提供了一种高效且可解释的自适应计算方案，通过信息论原理理解路由决策，在保持SSM效率的同时解决了长距离信息检索问题。

Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is "uncertain"--as measured by prediction entropy. Compared to standard transformers, AMOR gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer. On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. We validate that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats (nearly half the entropy range) between retrieval and local positions. Additionally, our approach provides interpretable adaptive computation, where routing decisions can be understood in information-theoretic terms.

</details>


### [46] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA是一个将基准测试问题转化为可执行规范的框架，通过自动生成无限验证变体来解决当前评估方案的静态性问题，防止记忆和格式利用。


<details>
  <summary>Details</summary>
Motivation: 当前大多数评估方案存在"静态"问题：相同问题被重复使用，导致记忆、格式利用和最终饱和。为了真正衡量AI进展，需要构建性鲁棒的评估，而非事后检测。

Method: VeRA框架将基准问题转化为可执行规范，包含：(1)带占位符的自然语言模板，(2)采样有效配置的连贯生成器，(3)验证参数并计算正确答案的确定性验证器。从单个种子问题自动创建无限验证变体。

Result: 评估16个前沿模型发现：(1)VeRA-E提高评估质量并揭示污染模式，(2)VeRA-H实现无需人工的困难任务生成和可靠标注，(3)VeRA建立验证基准作为通用范式。

Conclusion: VeRA将基准从静态对象重新概念化为按需生成新鲜验证实例的可执行规范，增强评估的鲁棒性和成本效益，使任何可验证领域的评估都能无限扩展而不牺牲标签完整性。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [47] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个通过迭代合成和修复可执行的生成器-验证器程序对来扩展可验证训练信号的元合成框架，能够在任务家族级别进行扩展，并通过多门验证协议确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的扩展是强化学习从可验证奖励（RLVR）的关键瓶颈。逻辑推理是一个自然的解决方案，但现有的合成方法要么依赖专家编写的代码，要么在固定模板内操作，限制了只能进行实例级别的扩展。

Method: 提出SSLogic框架，通过生成-验证-修复的闭环循环迭代合成和修复可执行的生成器-验证器程序对，实现任务家族的连续演化。引入多门验证协议，结合多策略一致性检查和对抗性盲审，确保任务质量。

Result: 从400个种子家族开始，经过两轮演化扩展到953个家族和21,389个可验证实例。在SSLogic演化数据上训练相比种子基线有显著提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架能够有效扩展可验证训练信号，在任务家族级别实现可控难度的连续演化，为RLVR提供了可扩展的解决方案。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [48] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出幻觉的三分法：不忠实（不利用上下文）、虚构（创造无关内容）、事实错误（概念框架内错误），发现前两者在嵌入空间有可检测的几何特征，而事实错误无法通过嵌入检测。


<details>
  <summary>Details</summary>
Motivation: 当前"幻觉"一词笼统地涵盖多种不同现象，缺乏清晰分类。需要区分不同类型的幻觉，理解它们在嵌入空间中的几何特征，以明确基于嵌入的检测方法的适用范围。

Method: 提出三分法分类：Type I（不忠实）、Type II（虚构）、Type III（事实错误）。通过几何分析研究不同类型幻觉在嵌入空间的特征，使用AUROC评估检测性能，分析跨域检测能力。

Result: Type I和Type II在各自领域内可检测（AUROC 0.76-0.99），但跨域检测能力差（AUROC 0.50）。人类精心设计的虚构存在全局检测方向（AUROC 0.96）。Type III无法检测（AUROC 0.478，接近随机）。

Conclusion: 基于嵌入的检测方法适用于Type I和Type II幻觉，但无法检测Type III（事实错误）。Type III需要外部验证机制，因为嵌入编码的是分布共现模式而非与外部现实的对应关系。

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [49] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: VaryBalance：一种简单有效的LLM生成文本检测方法，通过比较原始文本与LLM重写版本之间的差异来区分人类文本和AI生成文本


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成文本检测方法存在局限性：要么依赖不切实际的白盒假设，要么仅依赖文本级特征，导致检测能力不精确。需要一种更有效且实用的检测方法。

Method: VaryBalance基于一个核心观察：与LLM生成的文本相比，人类文本与其通过LLM重写的版本之间存在更大差异。该方法通过计算平均标准差来量化这种差异，从而区分人类文本和LLM生成文本。

Result: 综合实验表明，VaryBalance在AUROC指标上优于最先进的检测器Binoculars，最高提升34.3%，并且在多种生成模型和语言中保持鲁棒性。

Conclusion: VaryBalance是一种简单、有效且实用的LLM生成文本检测方法，通过利用人类文本与AI重写版本之间的差异特性，实现了优于现有方法的检测性能。

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [50] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 该论文提出轨迹主导帕累托优化框架，将智能视为轨迹层面现象，揭示帕累托陷阱如何限制长期适应性发展，并定义陷阱逃逸难度指数来量化这些约束。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能近期取得进展，但许多系统在长期适应性方面出现停滞。作者认为这种限制并非源于学习不足、数据不足或模型容量不足，而是源于智能随时间优化的更深层次结构特性。

Method: 提出轨迹主导帕累托优化框架，将帕累托最优性推广到完整轨迹层面；定义陷阱逃逸难度指数(TEDI)来量化约束刚性；建立帕累托陷阱的形式分类学；使用最小智能体-环境模型进行说明。

Result: 动态智能上限是轨迹层面主导性的必然几何结果，与学习进展或架构规模无关；帕累托陷阱作为轨迹空间中局部非主导区域出现，限制了对全局更优发展路径的访问。

Conclusion: 智能研究的焦点应从终端性能转向优化几何，为诊断和克服自适应系统中的长期发展约束提供了原则性框架。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [51] [PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading](https://arxiv.org/abs/2602.13232)
*Mayank Ravishankara*

Main category: cs.AI

TL;DR: PlotChain：用于评估多模态大语言模型从工程图表中恢复定量值的确定性基准，包含15种图表类型、450个图表，通过检查点诊断评估模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估主要关注OCR提取或自由形式描述，缺乏对工程图表（如Bode图、应力-应变曲线等）中定量值恢复能力的系统性评估。需要一种确定性、可复现的基准来评估模型从经典工程图表中读取数值的能力。

Method: 开发PlotChain基准：1）包含15种图表家族，450个渲染图表（每个家族30个）；2）每个图表由已知参数生成，包含精确的真实值；3）引入检查点诊断评估，包含中间字段（如截止频率、峰值幅度）以定位失败原因；4）使用标准化协议（温度=0，严格JSON数值输出格式）；5）设计容差策略反映人类图表读取精度。

Result: 在'plotread'容差策略下，最佳模型表现：Gemini 2.5 Pro（80.42%）、GPT-4.1（79.84%）、Claude Sonnet 4.5（78.21%），GPT-4o落后（61.59%）。频率域任务表现脆弱：带通响应≤23%，FFT频谱仍具挑战性。

Conclusion: PlotChain为MLLM在工程图表读取任务上提供了可复现的评估基准，揭示了模型在定量值恢复方面的能力差异，特别是频率域任务的挑战性。发布完整工具链支持后续研究和不同容差策略的重新评分。

Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays low (<= 23%), and FFT spectrum remains challenging. We release the generator, dataset, raw model outputs, scoring code, and manifests with checksums to support fully reproducible runs and retrospective rescoring under alternative tolerance policies.

</details>


### [52] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出训练无关的双循环对抗自进化框架，通过攻击者循环生成更强越狱提示，防御者循环构建分层知识库，在推理时检索组合知识来同时保持角色保真度和安全性。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的角色扮演在保真度上有所提升，但更强的角色约束通常会增加对越狱攻击的脆弱性，尤其是对于风险或负面角色。现有训练时解决方案成本高、难以维护、可能降低角色行为质量，且不适用于前沿闭源LLM。

Method: 提出训练无关的双循环对抗自进化框架：1) 角色目标攻击者循环合成逐步增强的越狱提示；2) 角色扮演防御者循环将观察到的失败提炼为分层知识库（全局安全规则、角色基础约束、安全角色示例）。推理时，防御者从该层次结构中检索和组合结构化知识来指导生成。

Result: 在多个专有LLM上的广泛实验显示，在角色保真度和越狱抵抗方面均优于强基线，且对未见角色和攻击提示具有鲁棒泛化能力。

Conclusion: 该训练无关框架能有效平衡角色扮演的保真度和安全性，无需模型微调，适用于闭源LLM，并能自适应应对新角色和攻击策略。

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [53] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act提出了一种通过自涌现语言工具链实现细粒度视觉感知和推理的VRAG框架，使用两阶段强化学习训练，相比现有方法提升了4%以上性能。


<details>
  <summary>Details</summary>
Motivation: 现有VRAG框架依赖预定义的外部工具，将视觉感知与推理过程解耦，这种设计在图像裁剪等操作中会导致视觉信息的不必要损失，限制了视觉语言模型的感知能力。

Method: 提出Lang2Act框架，通过自涌现语言工具链增强视觉感知能力。采用两阶段强化学习训练：第一阶段优化VLM自探索高质量动作构建可重用语言工具箱；第二阶段优化VLM有效利用这些语言工具进行下游推理。

Result: 实验结果表明Lang2Act显著提升了VLM的视觉感知能力，实现了超过4%的性能提升。所有代码和数据已开源。

Conclusion: Lang2Act通过自涌现语言工具链实现了更细粒度的视觉感知和推理，避免了现有解耦设计中的信息损失，为VRAG框架提供了更灵活有效的解决方案。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [54] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC是一个将自然语言翻译为一阶逻辑的框架，通过引入抽象语法树作为中间表示，结合递归语义解析器和AST引导的生成器，显著提升了语法准确性和语义正确性。


<details>
  <summary>Details</summary>
Motivation: 在法律和治理等领域，自动化推理需要准确性和可解释性。现有基于大语言模型的方法（如GCD和CODE4LOGIC）存在语法控制脆弱和语义忠实度低的问题，主要原因是全局语法约束执行弱和子句级语义理解不足。

Method: 提出NL2LOGIC框架，引入抽象语法树作为中间表示。结合基于大语言模型的递归语义解析器和AST引导的生成器，确定性地生成可直接用于求解器的逻辑代码。

Result: 在FOLIO、LogicNLI和ProofWriter基准测试中，NL2LOGIC达到99%的语法准确性，语义正确性比最先进基线提升高达30%。集成到Logic-LM后，实现近乎完美的可执行性，下游推理准确性比Logic-LM原始少样本无约束翻译模块提升31%。

Conclusion: NL2LOGIC通过抽象语法树中间表示有效解决了现有方法的语法控制和语义忠实度问题，显著提升了一阶逻辑翻译的准确性和下游推理性能。

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [55] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 评估代码大语言模型的成员推断攻击方法，发现现有方法在代码领域效果有限，提出基于抽象语法树的改进方法AST-PAC


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型常在受限制许可的源代码数据集上训练，引发数据治理和版权问题。成员推断攻击可作为审计机制检测未经授权的数据使用，但在代码领域的研究不足。

Method: 探索性研究评估Loss Attack和Polarized Augment Calibration方法在3B-7B参数代码模型上的效果。提出AST-PAC方法，使用抽象语法树扰动生成语法有效的校准样本。

Result: PAC通常优于Loss基准，但其效果依赖于忽略代码严格语法的增强策略，在大型复杂文件上性能下降。AST-PAC在语法规模增长时表现改善，但在小文件和字母数字丰富代码上表现不足。

Conclusion: 研究发现需要语法感知和规模自适应的校准方法，作为代码语言模型可靠来源审计的前提条件，为未来工作提供方向。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [56] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: 提出X-Blocks框架，用于分析自动驾驶自然语言解释的层次化语言构建块，包含上下文、句法和词汇三个层面，实现场景感知的解释分类与模式分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统框架来分析人类如何在不同驾驶场景中语言化构建驾驶原理，而自然语言解释对于建立自动驾驶系统的信任和接受度至关重要。

Method: 提出X-Blocks层次分析框架：1) 上下文层面：RACE多LLM集成框架，结合思维链推理和自一致性机制，将解释分类为32个场景感知类别；2) 词汇层面：使用信息性狄利克雷先验的对数几率分析；3) 句法层面：依赖解析和模板提取。

Result: RACE在Berkeley DeepDrive-X数据集上达到91.45%准确率和0.91的Cohen's kappa，接近人类可靠性；词汇分析揭示场景特定词汇模式；句法分析显示解释使用有限的语法族库，不同上下文在谓词类型和因果结构上有系统性变化。

Conclusion: X-Blocks框架是数据集无关和任务独立的，可广泛应用于其他自动驾驶数据集和安全关键领域，为生成支持透明度、用户信任和认知可访问性的场景感知解释提供基于证据的语言设计原则。

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [57] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench基准测试基于哲学家就餐问题，评估LLM在多智能体系统中的协调能力，发现LLM在顺序决策时表现良好，但在同时决策时死锁率超过95%，通信甚至可能加剧死锁。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地部署在多智能体系统中，目前缺乏测试它们在资源竞争下协调能力的基准。需要评估LLM在多智能体环境中的协调表现，特别是在需要并发资源访问的场景中。

Method: 基于哲学家就餐问题开发DPBench基准，测试LLM在8种不同条件下的协调能力，包括决策时机（顺序vs同时）、群体规模和通信设置。使用GPT-5.2、Claude Opus 4.5和Grok 4.1等模型进行实验。

Result: LLM在顺序决策时能有效协调，但在同时决策时死锁率极高（超过95%）。这种失败源于收敛推理现象，即智能体独立得出相同策略，同时执行时导致死锁。通信不仅不能解决问题，反而可能增加死锁率。

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而不是依赖涌现的协调能力。DPBench作为开源基准发布，用于评估多智能体LLM协调能力。

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [58] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE将LLM代理的个人化能力分解为三个独立组件：记忆、学习和个人化，通过专用子代理实现更有效的用户适应


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在适应个体用户方面存在根本限制，因为现有系统将记忆、学习和个人化混为一谈，而这三者需要不同的基础设施、时间尺度和优化策略

Method: 提出MAPLE框架，将个人化能力分解为三个独立组件：记忆（存储和检索基础设施）、学习（从累积交互中异步提取智能）、个人化（在有限上下文预算内实时应用学习知识），每个组件作为专用子代理运行

Result: 在MAPLE-Personas基准测试中，个人化分数相比无状态基线提升14.6%（p < 0.01, Cohen's d = 0.95），特质整合率从45%提高到75%

Conclusion: 通过将记忆、学习和个人化分解为独立组件，MAPLE框架能够实现真正学习和适应的LLM代理，显著提升个人化效果

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [59] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST通过强化学习让基础模型能够并行生成多个相同权重的克隆，在固定推理预算下优化计算资源分配，提升数学推理和长上下文问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在测试时需要额外计算，但串行推理或无协调的并行采样在固定推理预算下计算效率低下，需要更智能的计算资源分配方法。

Method: 提出SELFCEST方法，通过智能体强化学习训练基础模型，使其能够在并行上下文中生成相同权重的克隆，在全局任务奖励下进行端到端训练，学习控制器来分配生成和上下文预算。

Result: 在具有挑战性的数学推理基准和长上下文多跳问答任务中，SELFCEST在匹配推理预算下相比单体基线改进了准确率-成本帕累托前沿，并在两个领域都表现出分布外泛化能力。

Conclusion: SELFCEST通过智能并行克隆和资源分配，在固定推理预算下显著提升了语言模型的效率和性能，为测试时计算优化提供了有效解决方案。

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [60] [Human-Centered Explainable AI for Security Enhancement: A Deep Intrusion Detection Framework](https://arxiv.org/abs/2602.13271)
*Md Muntasir Jahid Ayan,Md. Shahriar Rashid,Tazzina Afroze Hassan,Hossain Md. Mubashshir Jamil,Mahbubul Islam,Lisan Al Amin,Rupak Kumar Das,Farzana Akter,Faisal Quader*

Main category: cs.AI

TL;DR: 提出结合可解释人工智能(XAI)的入侵检测系统框架，使用CNN和LSTM网络处理流量序列，并集成SHAP增强模型透明度，在NSL-KDD数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，需要既准确又可解释的入侵检测系统。传统IDS和黑盒深度学习模型缺乏透明度，安全分析师难以理解和验证模型决策。

Method: 提出集成XAI的IDS框架：1) 结合CNN和LSTM网络捕捉流量序列的时序依赖关系；2) 集成SHAP模型提供可解释性；3) 使用NSL-KDD数据集评估；4) 通过基于IPIP6和大五人格特质的信任专家调查评估系统可靠性和可用性。

Result: CNN和LSTM准确率均达0.99，LSTM在宏观平均精确率、召回率和F1分数上优于CNN。SHAP识别出srv_serror_rate、dst_host_srv_serror_rate和serror_rate等关键特征。专家调查验证了系统的可靠性和可用性。

Conclusion: 该工作展示了结合性能和透明度的网络安全解决方案潜力，建议未来通过自适应学习实现实时威胁检测的增强。

Abstract: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are not only accurate but also interpretable. This paper presented a novel IDS framework that integrated Explainable Artificial Intelligence (XAI) to enhance transparency in deep learning models. The framework was evaluated experimentally using the benchmark dataset NSL-KDD, demonstrating superior performance compared to traditional IDS and black-box deep learning models. The proposed approach combined Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) networks for capturing temporal dependencies in traffic sequences. Our deep learning results showed that both CNN and LSTM reached 0.99 for accuracy, whereas LSTM outperformed CNN at macro average precision, recall, and F-1 score. For weighted average precision, recall, and F-1 score, both models scored almost similarly. To ensure interpretability, the XAI model SHapley Additive exPlanations (SHAP) was incorporated, enabling security analysts to understand and validate model decisions. Some notable influential features were srv_serror_rate, dst_host_srv_serror_rate, and serror_rate for both models, as pointed out by SHAP. We also conducted a trust-focused expert survey based on IPIP6 and Big Five personality traits via an interactive UI to evaluate the system's reliability and usability. This work highlighted the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

</details>


### [61] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench是一个多领域基准测试，用于评估模型在渐进丰富信息设置下的时间推理能力，揭示强预测性能不一定反映真正的时间理解


<details>
  <summary>Details</summary>
Motivation: 当前不清楚强预测性能是反映真正的时间理解能力，还是在上下文和事件驱动条件下的推理能力，需要系统评估模型的时间推理行为

Method: 设计四层任务分类：历史结构解释、无上下文预测、上下文时间推理、事件条件预测，覆盖零售、医疗、能源、物理系统四个领域，通过控制未来目标和上下文信息访问来诊断模型能力

Result: 实验显示强数值预测准确性不能可靠转化为鲁棒的上下文或事件感知时间推理；现有智能体框架表现出碎片化优势和系统性失败模式，这些在仅预测基准测试中基本被隐藏

Conclusion: 需要超越单纯预测准确性的时间推理评估，TemporalBench提供了公开数据集和排行榜，促进对模型时间理解能力的更全面评估

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [62] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench是一个统一的基准测试，用于评估11种提示范式在4个LLM家族中的道德能力与安全对齐表现，提出UMSS指标平衡准确性与安全性，发现简洁的示例引导提示优于复杂多步推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究对提示设计如何影响LLM的道德能力和安全对齐缺乏系统比较，评估结果在不同数据集和模型间碎片化，需要统一的基准测试框架。

Method: 构建ProMoral-Bench基准，包含ETHICS、Scruples、WildJailbreak数据集和新设计的ETHICS-Contrast鲁棒性测试，提出UMSS统一指标，评估11种提示范式在4个LLM家族中的表现。

Result: 紧凑的示例引导提示支架优于复杂的多阶段推理，获得更高的UMSS分数和更好的鲁棒性，且token成本更低；多轮推理在扰动下表现脆弱，而少样本示例能持续提升道德稳定性和越狱抵抗能力。

Conclusion: ProMoral-Bench为原则性、成本效益高的提示工程建立了标准化框架，证明简洁的示例引导提示在道德安全对齐方面更有效。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [63] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 论文提出通过组织架构设计而非个体对齐来实现多智能体AI系统的可靠性，借鉴人类机构通过结构而非个体可靠性来确保集体行为安全的方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究主要关注单个AI系统的可靠性，但人类机构通过组织结构而非个体可靠性来确保集体行为安全。多智能体AI系统应借鉴这种制度模型，通过架构设计而非假设个体对齐来实现可靠结果。

Method: 提出Perseverance Composition Engine多智能体文档撰写系统，包含三个角色：Composer起草文本，Corroborator验证事实依据（有完整源访问权限），Critic评估论证质量（无源访问权限）。通过系统架构强制执行信息不对称，实现分层验证。

Result: 在474个撰写任务中观察到与制度假设一致的模式。当分配需要捏造内容的不可能任务时，系统从尝试捏造转向诚实拒绝并提出替代方案，这种行为既未指令也未个体激励。结果表明架构强制可能从不可靠组件产生可靠结果。

Conclusion: 组织理论为多智能体AI安全提供了富有成效的框架。通过将验证和评估作为结构属性，通过信息隔离强制执行，制度设计为从不可靠的个体组件实现可靠的集体行为提供了一条途径。

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [64] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE是一个神经符号框架，通过整合自我调节学习理论来模拟学生在开放式问题解决环境中的真实学习行为，解决了LLM在模拟学生时存在的"能力偏差"问题。


<details>
  <summary>Details</summary>
Motivation: 模拟学生在开放式问题解决环境中的学习行为对教育研究有重要意义，但收集真实数据面临隐私问题和纵向研究的高成本。虽然大型语言模型提供了模拟学生的可能途径，但它们存在"能力偏差"，倾向于追求高效的正确性，而不是模拟新手学习者典型的、反复无常的迭代挣扎过程。

Method: BEAGLE整合了三个关键技术创新：1）半马尔可夫模型控制认知行为和元认知行为的时间与转换；2）带有显式缺陷注入的贝叶斯知识追踪，强制实施真实的知识差距和"未知的未知"；3）解耦的智能体设计，将高层策略使用与代码生成动作分离，防止模型静默地纠正自己的故意错误。

Result: 在Python编程任务评估中，BEAGLE在重现真实轨迹方面显著优于最先进的基线方法。在人类图灵测试中，用户无法区分合成轨迹和真实学生数据，准确率与随机猜测无显著差异（52.8%）。

Conclusion: BEAGLE通过整合自我调节学习理论，成功解决了LLM在模拟学生行为时的能力偏差问题，能够生成难以与真实学生数据区分的学习轨迹，为教育研究和自适应教学系统提供了有效的模拟工具。

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [65] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 研究显示人们在工作和个人生活中对AI工具准确性的要求存在显著差异：工作中要求高准确性的比例（24.1%）远高于个人生活（8.8%），且当工具不可用时，个人生活受到的干扰更大。


<details>
  <summary>Details</summary>
Motivation: 研究人们在专业和个人情境下使用AI工具时对准确性的权衡取舍，探讨这些权衡的决定因素，以及当AI/应用不可用时用户如何应对。现代AI系统（特别是生成模型）可能产生可接受但不完全相同的输出，因此需要研究用户在不同情境下的准确性要求。

Method: 通过在线调查（N=300）收集数据，定义"准确性"为情境特定的可靠性：输出在用户意图容忍阈值内的程度，该阈值取决于风险水平和修正成本。分析工作与个人情境下对准确性要求的差异，以及工具不可用时的应对策略。

Result: 工作中要求高准确性（最高等级）的比例为24.1%，个人生活中仅为8.8%（+15.3个百分点，p<0.001）。使用更宽泛的前两等级定义时，差异仍然显著（67.0% vs. 32.9%）。重度应用使用和经验模式与更严格的工作标准相关。当工具不可用时，个人生活受到的干扰（34.1%）显著高于工作（15.3%，p<0.01）。

Conclusion: 人们在专业和个人情境下对AI工具准确性的要求存在系统性差异，工作中对准确性的要求更高。工具不可用时对个人生活的干扰更大，这可能反映了个人生活对AI工具的依赖程度更高或替代方案更少。这些发现对AI系统设计和用户体验优化具有重要意义。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [66] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror是一个AI辅助伦理审查框架，通过EthicsLLM模型和双模式架构（快速审查和委员会审查）提升伦理审查的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现代研究伦理审查系统面临大规模、跨学科科学研究带来的结构性伦理风险压力，现有制度审查能力有限，而大语言模型直接应用存在伦理推理能力不足、与监管结构整合弱、隐私限制等问题。

Method: 开发Mirror框架，核心是EthicsLLM模型（在EthicsQA数据集上微调），包含两种模式：Mirror-ER通过可执行规则库自动化快速审查；Mirror-CR通过专家代理、伦理秘书代理和主要研究者代理的多代理协商模拟完整委员会审议。

Result: 实证评估显示，Mirror相比强大的通用大语言模型，显著提高了伦理评估的质量、一致性和专业性。

Conclusion: Mirror框架通过整合伦理推理、结构化规则解释和多代理协商，为AI辅助伦理审查提供了有效解决方案，能够支持不同风险水平研究的伦理审查需求。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [67] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench：一个用于评估多智能体幻灯片生成与编辑的基准框架，包含数据集、评估协议和基线系统


<details>
  <summary>Details</summary>
Motivation: 现有基准和评估协议无法充分衡量学术幻灯片自动生成和迭代编辑的挑战，包括内容选择、幻灯片组织、布局感知渲染和多轮指令跟随

Method: 构建基于论文-幻灯片对的数据集，添加模拟编辑指令；设计系统评估协议；实现模块化多智能体基线系统，将任务分解为论文解析、幻灯片规划、HTML创建和迭代编辑

Result: DECKBench能够有效揭示多智能体幻灯片生成系统的优势和失败模式，为系统改进提供可行见解

Conclusion: 该工作为学术演示文稿生成和编辑的可重复、可比较评估建立了标准化基础

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [68] [Situation Graph Prediction: Structured Perspective Inference for User Modeling](https://arxiv.org/abs/2602.13319)
*Jisung Shin,Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 论文提出情境图预测任务，通过结构化合成数据解决视角建模的数据瓶颈问题，实验表明潜在状态推理比表面信息提取更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 视角感知AI需要建模内部状态（目标、情感、上下文），而不仅仅是偏好。当前进展受限于数据瓶颈：数字足迹涉及隐私且视角状态很少被标注。

Method: 提出情境图预测任务，将视角建模视为逆向推理问题：从可观察的多模态痕迹重建结构化的、本体对齐的视角表示。采用结构优先的合成生成策略，通过设计对齐潜在标签和可观察痕迹。

Result: 使用GPT-4o进行诊断研究，发现表面信息提取与潜在视角推理之间存在差距，表明在受控设置下潜在状态推理比表面提取更难。

Conclusion: 情境图预测任务具有非平凡性，为结构优先的数据合成策略提供了证据，为解决视角建模的数据瓶颈问题提供了新思路。

Abstract: Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.

</details>


### [69] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了首个分析MCP智能体错误累积的理论框架，证明累积失真呈线性增长且高概率偏差受O(√T)约束，通过混合失真度量和鞅集中界限建立了可预测的系统行为保证。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI智能体越来越多地使用外部工具进行高风险决策，一个关键可靠性问题出现：错误如何在顺序工具调用中传播？需要理论框架来分析错误累积以确保系统可靠性。

Method: 1. 引入混合失真度量，结合离散事实匹配和连续语义相似性；2. 建立顺序工具交互中错误传播的鞅集中界限；3. 在Qwen2-7B、Llama-3-8B和Mistral-7B上进行实验验证。

Result: 1. 累积失真呈现线性增长趋势；2. 高概率偏差受O(√T)约束，排除了指数级故障模式；3. 语义加权使失真减少80%；4. 大约每9步重新接地足以控制错误。

Conclusion: 该理论框架提供了可预测的系统行为保证，并将集中性保证转化为可部署的信任智能体系统原则，为高可靠性AI智能体部署提供了理论基础和实践指导。

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [70] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 该研究提出了一种可扩展的临床LLM越狱检测方法，使用专家标注的四个核心语言特征训练BERT模型进行特征提取，然后通过多种分类器预测越狱可能性。


<details>
  <summary>Details</summary>
Motivation: 临床训练大型语言模型需要检测越狱尝试，但先前工作依赖手动标注语言特征，限制了可扩展性和表达能力。需要自动化方法来准确建模表示不安全或偏离任务用户行为的语言偏差。

Method: 使用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、上下文分心），训练通用领域和医学领域的BERT模型来预测这些特征。选择每个维度最可靠的特征回归器作为特征提取器，然后在第二层使用树基、线性、概率和集成等多种分类器从提取的特征中预测越狱可能性。

Result: 系统在交叉验证和保留集评估中均表现出色，表明LLM衍生的语言特征为自动化越狱检测提供了有效基础。错误分析揭示了当前标注和特征表示的关键限制。

Conclusion: 这项工作展示了一种可扩展且可解释的方法，用于检测安全关键临床对话系统中的越狱行为。未来改进方向包括更丰富的标注方案、更细粒度的特征提取，以及捕捉对话过程中越狱行为风险演变的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [71] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: 该研究扩展了BDI智能体的解释能力，使其能回答对比性问题（"为什么做X而不是F？"），发现对比性解释更简短且在某些方面更受用户偏好，但令人意外的是，提供完整解释有时反而不如不提供任何解释。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要提供解释来支持透明度和建立适当信任。现有BDI智能体只能回答"为什么做X"这类问题，但人类实际会问对比性问题（"为什么做X而不是F？"），因此需要扩展解释能力以回答这类更自然的问题。

Method: 扩展先前BDI智能体的解释机制，使其能够回答对比性问题。通过计算评估对比解释的长度，并进行人类主体评估，测试对比性解释的用户偏好、对信任发展的支持、透明度感知等效果，同时评估提供解释本身的益处。

Result: 计算评估显示对比性问题能显著减少解释长度。人类评估发现对比性解释在某些方面更受偏好，能带来更高的信任、感知理解和系统正确性信心。但令人意外的是，提供完整解释并不总是有益，在某些情况下甚至比不提供任何解释更差。

Conclusion: 扩展BDI智能体以回答对比性问题是可行且有价值的，能产生更简短且在某些方面更有效的解释。然而，提供解释本身需要谨慎，因为完整解释在某些情况下可能适得其反，需要进一步研究解释的最佳方式和时机。

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [72] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B是一个仅30亿参数的多功能语言模型，首次在开源小模型中同时实现了强大的智能体行为、代码生成和通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前小规模语言模型通常只能在特定任务上表现良好，缺乏同时具备广泛能力和专业特长的统一模型。本文旨在证明30亿参数的小模型也能同时实现多功能性和强专业性。

Method: 1. 结合点式和配对式奖励建模提升推理和偏好对齐；2. 设计复杂度感知的强化学习奖励优化代码正确性和效率；3. 进行复杂数据合成并加入回合级监督训练，支持稳定长时程工具交互。

Result: 模型在多项任务上显著超越同规模模型（如Nanbeige4-3B-2511和Qwen3-4B），甚至在某些方面优于更大模型（如Qwen3-30B-A3B）。能够可靠执行长达600轮工具调用的复杂问题解决。

Conclusion: 小模型可以同时实现广泛能力和强专业性，重新定义了30亿参数模型的潜力，为高效AI部署提供了新方向。

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [73] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 提出Morality Chains形式化表示道德规范，创建MoralityGym基准测试环境，用于评估AI在冲突性层级道德规范下的对齐表现


<details>
  <summary>Details</summary>
Motivation: 评估AI在冲突性、层级化人类道德规范下的道德对齐是AI安全、道德哲学和认知科学交叉领域的关键挑战，需要更可靠、透明和道德的AI系统

Method: 引入Morality Chains作为有序道义约束的道德规范表示形式，创建包含98个伦理困境问题的MoralityGym基准测试环境，采用有轨电车困境风格的Gymnasium环境，将任务解决与道德评估解耦，并引入新的道德度量指标

Result: 使用安全强化学习方法进行基线测试，揭示了现有方法的关键局限性，表明需要更原则性的伦理决策方法

Conclusion: 这项工作为开发在复杂现实世界中更可靠、透明和道德行为的AI系统奠定了基础，将心理学和哲学见解整合到规范敏感推理的评估中

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [74] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 本文提出了一种简化的训练策略"on-policy SFT"，通过移除复杂的多奖励目标和KL正则化，仅使用截断式长度惩罚，将优化问题简化为对自生成数据的监督微调，在保持准确性的同时大幅减少推理长度并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型通常使用强化学习进行训练，但复杂的多奖励目标（同时优化正确性和简洁性）会导致训练不稳定和次优权衡。作者质疑这种复杂性的必要性，希望通过简化方法获得更好的准确性与效率平衡。

Method: 提出on-policy SFT方法：1) 移除KL正则化（当正确性和长度可直接验证时失去作用）；2) 移除组间归一化（在多奖励信号下变得模糊）；3) 将奖励简化为基于截断的长度惩罚；4) 将优化问题转化为对自生成数据的监督微调，仅保留既正确又简洁的数据。

Result: 在五个基准测试中，该方法将推理链长度减少高达80%，同时保持原始准确性，超越了更复杂的基于RL的方法。训练效率显著提升：GPU内存使用减少50%，收敛速度加快70%。

Conclusion: 通过简化训练目标，on-policy SFT方法在准确性和效率之间建立了帕累托前沿，证明了复杂多奖励RL方法的非必要性，为大型推理模型的训练提供了更高效、更稳定的替代方案。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [75] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver是一个用于EEG分析的自主进化代理，通过受限优化在神经科学合理空间内搜索，生成轻量级高性能解决方案，超越特定任务方法，接近基础模型性能但参数更少。


<details>
  <summary>Details</summary>
Motivation: 基础模型在EEG分析中面临数据需求大、参数多、计算成本高的问题，不适合资源受限的临床环境；而通用自动机器学习框架缺乏神经生理学先验知识，常产生不科学的解决方案。

Method: 将流水线工程重新定义为离散约束优化问题，采用领域知情子空间初始化限制搜索到神经科学合理的流形，结合多目标进化优化动态平衡性能、新颖性和效率，通过自反思进行精炼。

Result: 在五个异构基准测试中，NeuroWeaver合成的轻量级解决方案持续超越最先进的特定任务方法，性能与大规模基础模型相当，但使用的参数显著减少。

Conclusion: NeuroWeaver通过结合领域知识和进化优化，为EEG分析提供了一种高效、轻量且科学合理的自动化解决方案，解决了基础模型和通用自动机器学习框架在该领域的局限性。

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [76] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: 研究发现多智能体系统中的编排器模式存在安全漏洞，即使有数据访问控制，攻击者仍可通过间接提示注入泄露敏感数据


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体能力增强，多智能体系统将成为实用范式。先前研究主要关注单智能体安全风险，缺乏对多智能体系统的威胁建模，特别是在有基本工程防护措施（如访问控制）的情况下

Method: 通过红队测试研究流行的编排器多智能体模式，攻击者通过间接提示注入攻击多个智能体，即使存在数据访问控制也能泄露敏感数据

Result: 发现前沿模型对不同类型的攻击都易受攻击，无论是推理模型还是非推理模型，即使攻击者不了解实现细节也能成功实施攻击

Conclusion: 安全研究需要从单智能体扩展到多智能体环境，以减少现实世界隐私泄露和财务损失的风险，维护公众对AI智能体的信任

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [77] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: 开发了一个端到端框架，将饮食标准转化为完整餐食，通过识别34种可解释的餐食原型，使用生成模型和分量预测器来满足USDA营养目标，实现营养提升47%，成本降低19-32%。


<details>
  <summary>Details</summary>
Motivation: 个性化饮食系统的重要目标是在不牺牲便利性或可负担性的情况下改善营养质量。需要将饮食指南转化为现实、预算友好的餐食和简单替换，为临床决策支持、公共卫生项目和消费者应用提供基础。

Method: 使用WWEIA摄入数据中的135,491份餐食，识别34种可解释的餐食原型，然后使用这些原型来条件化生成模型和分量预测器，以符合USDA营养目标。通过允许1-3种食物替换来创建更营养的餐食。

Result: 在原型内比较中，生成的餐食在遵循推荐每日摄入量目标方面提高了47.0%，同时保持与真实餐食的组成接近。通过1-3种食物替换，创建的餐食营养提升10%，平均成本降低19-32%。

Conclusion: 该框架能够将饮食指南转化为现实、预算友好的餐食和简单替换，可为临床决策支持、公共卫生项目和消费者应用提供基础，实现可扩展、公平的日常营养改善。

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [78] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: 论文提出"自然代理过度分享"概念，指网络代理在完成任务时无意中泄露任务无关的用户信息，并通过SPILLage框架从内容和行为两个维度分析，发现行为泄露比内容泄露严重5倍，减少过度分享可提升任务成功率17.9%。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的网络代理在开放网络上自动化用户任务时，会与第三方交互并留下行动痕迹。与受控聊天机器人不同，这些代理在"野外"操作，可能无意中泄露用户资源信息。研究关注网络代理在完成任务时如何处理用户资源，特别是任务无关信息的泄露问题。

Method: 提出SPILLage框架，从两个维度（渠道：内容vs行为；直接性：显式vs隐式）形式化自然代理过度分享。在实时电商网站上对180个任务进行基准测试，使用真实标注区分任务相关和无关属性。在两个代理框架和三个骨干LLM上进行1,080次运行实验。

Result: 过度分享普遍存在，行为过度分享比内容过度分享严重5倍。提示级缓解措施效果有限甚至可能恶化问题。在任务执行前移除任务无关信息可将任务成功率提升高达17.9%，表明减少过度分享能改善任务性能。

Conclusion: 保护网络代理隐私是基本挑战，需要更广泛的"输出"视角，不仅要考虑代理输入的内容，还要考虑其在网络上的行为。行为泄露是先前研究忽视的关键盲点，减少过度分享能同时提升隐私保护和任务成功率。

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [79] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem是一个两阶段框架，通过构建混合记忆图并采用智能检索器，显著提升了语言代理的叙事记忆和推理能力，在多个基准测试中超越现有记忆系统。


<details>
  <summary>Details</summary>
Motivation: 人类擅长在时空背景下记忆具体经历并进行跨事件推理（叙事记忆能力），而当前语言代理的记忆主要是语义性的，无法有效回忆和推理交互历史。现有工作往往忽视叙事性、缺乏明确的事件建模，或过度强调简单检索而非复杂推理。

Method: REMem采用两阶段框架：1）离线索引阶段：将经验转换为混合记忆图，灵活链接时间感知的要点和事实；2）在线推理阶段：使用配备精心设计工具的智能检索器，在记忆图上进行迭代检索。

Result: 在四个叙事记忆基准测试中，REMem显著优于Mem0和HippoRAG 2等最先进的记忆系统，在叙事回忆和推理任务上分别实现了3.4%和13.4%的绝对提升。此外，REMem对不可回答问题表现出更稳健的拒绝行为。

Conclusion: REMem通过混合记忆图和智能检索器的两阶段设计，有效解决了语言代理的叙事记忆和推理挑战，为构建更接近人类记忆能力的智能代理提供了有前景的方向。

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [80] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: 提出一个在线强化学习WebAgent，通过直接与真实网站交互优化策略，结合分层多任务微调、在线强化学习和模块化操作代理框架，在WebArena上达到71.6%的SOTA成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调或离线强化学习的方法存在严重的分布偏移问题，因为离线轨迹无法捕捉真实网站环境的随机状态转换和实时反馈，需要能够直接与无约束网站交互的鲁棒解决方案。

Method: 1) 分层多任务微调：使用规划、行动和基础三类功能原语数据集微调视觉语言模型；2) 在线强化学习：开发在线交互环境，采用混合奖励机制（WebJudge整体评估+RDT进度奖励）；3) 操作代理框架：包含规划器、基础器、反射器和总结器的模块化架构。

Result: 强化学习增强模型在WebArena上达到38.1%的成功率（pass@5），超越所有现有单体基线；完整的OpAgent框架将性能提升到71.6%的SOTA成功率。

Conclusion: 通过结合分层多任务微调、在线强化学习和模块化代理框架，能够有效解决WebAgent在真实网站环境中的分布偏移和长期导航信用分配问题，显著提升任务执行成功率。

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [81] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs在决策任务中表现出对人类专家反馈的显著偏好，即使专家反馈是错误的，这种偏好也超过对其他LLM反馈的信任。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在社交信息环境中是否表现出类似人类的判断影响模式，特别是是否更倾向于接受人类专家的反馈而非其他LLMs的反馈。

Method: 通过三个二元决策任务（阅读理解、多步推理、道德判断），向四个指令调优的LLMs展示标记为来自朋友、人类专家或其他LLMs的先前回答，操纵群体正确性并改变群体规模；第二个实验引入单个人类与单个LLM之间的直接分歧。

Result: LLMs显著更倾向于遵从标记为来自人类专家的回答，即使该信号是错误的；相比其他LLMs，它们更容易向专家反馈修正自己的答案。

Conclusion: 专家框架对当代LLMs具有强烈先验影响，表明存在一种跨决策领域的可信度敏感的社会影响形式。

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [82] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 提出一种结合自监督可微分聚类与可微分ILP的方法，直接从原始数据学习规则，避免标签泄漏问题


<details>
  <summary>Details</summary>
Motivation: 传统可微分ILP方法依赖符号数据集，难以直接从原始数据学习，存在显式标签泄漏问题（无法将连续输入映射到符号变量而不需要输入特征标签的显式监督）

Method: 集成自监督可微分聚类模型与新型可微分ILP模型，使系统能够直接从原始数据学习规则，无需显式标签监督

Result: 方法能够直观且精确地从时间序列和图像数据中学习泛化规则，有效描述原始数据的特征

Conclusion: 通过结合自监督聚类与可微分ILP，成功解决了直接从原始数据学习规则时的标签泄漏问题，提高了规则学习在原始数据上的适用性

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [83] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 多智能体证明冲刺工作流，结合快速草稿生成与对抗性验证，针对10个研究级问题，通过依赖图分解定位漏洞并协调评审驱动的修订，获得异质但明确的数学与验证状态结果。


<details>
  <summary>Details</summary>
Motivation: 提高压缩证明冲刺中的可靠性和校准，通过结构感知验证和层切换策略来应对研究级数学问题的快速验证挑战。

Method: 使用多智能体工作流，结合快速草稿生成与对抗性验证、针对性修复和显式溯源。通过声明依赖的接线图分解来定位漏洞，协调评审驱动的修订，并区分数学状态与QC验证状态。

Result: 问题3在限定准则下具有验证完备的存在路径；问题5在$F_O$-局部连通谱的限定形式下解决；问题10在明确假设下条件性解决；问题4和6在一般情形下部分解决；问题7通过旋转路径定理链暂时关闭。QC层上，问题7和9有节点级验证工件但仍有未解决的验证缺口。

Conclusion: 结构感知验证和层切换策略显著提高了压缩证明冲刺中的可靠性和校准，为研究级数学问题的快速验证提供了有效方法论。

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [84] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus是一个用于智能体AI的持久内存管理系统，使用二进制签名进行语义搜索，通过动态小波矩阵压缩和索引，实现超快速检索并减少延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 智能体AI需要超越LLM有限上下文窗口的持久内存来存储用户特定历史。现有内存系统使用密集向量数据库或知识图谱遍历，存在检索延迟高和存储可扩展性差的问题。

Method: 引入Hippocampus系统，使用紧凑二进制签名进行语义搜索，无损token-ID流进行精确内容重建。核心是动态小波矩阵(DWM)，在压缩域中压缩和共同索引两种流，避免昂贵的密集向量或图计算。

Result: 评估显示Hippocampus将端到端检索延迟降低高达31倍，每查询token占用减少高达14倍，同时在LoCoMo和LongMemEval基准测试中保持准确性。

Conclusion: Hippocampus的设计随内存大小线性扩展，适用于长期智能体部署，解决了现有内存系统的高延迟和可扩展性问题。

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [85] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: 量化缩放定律在多跳推理任务中失效：从16位降至8/4位反而增加能耗并降低准确率，出现"量化陷阱"


<details>
  <summary>Details</summary>
Motivation: 当前AI行业遵循"越小越好"的启发式方法，认为降低数值精度可以线性提升计算效率和能耗表现。但本文发现这种神经缩放定律在多跳推理任务中会失效，需要揭示量化在复杂推理任务中的实际影响。

Method: 通过理论分解方法，将量化失败归因于硬件转换开销（反量化内核的隐藏延迟成本）和顺序能量摊销失败。分析这些因素在顺序推理链中如何成为主导瓶颈。

Result: 发现量化陷阱：从16位降至8/4位精度反而增加净能耗并降低推理准确率。硬件转换开销在多跳推理中成为主导瓶颈，导致缩放定律在实践中不可避免地被打破。

Conclusion: AI行业的"越小越好"启发式方法在复杂推理任务中在数学上是适得其反的。量化缩放定律在多跳推理场景中失效，需要重新考虑精度降低策略在复杂认知任务中的应用。

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [86] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: 提出DiffusionRollout，一种用于自回归扩散模型的选择性展开规划策略，旨在减轻偏微分方程系统长期预测中的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 解决物理系统偏微分方程长期预测中的误差累积问题，利用扩散模型的概率特性来量化预测不确定性，提高长期预测的可靠性。

Method: 基于扩散模型预测误差与标准差之间的强相关性，提出自适应步长选择机制，在自回归展开过程中根据预测置信度动态调整步长，减少对不准确先前输出的条件依赖。

Result: 在长期轨迹PDE预测基准测试中验证了不确定性度量的有效性，自适应规划策略显著降低了预测误差，生成了更长且与真实值高度相关的预测轨迹。

Conclusion: DiffusionRollout通过利用扩散模型的概率不确定性来指导自适应规划，有效缓解了长期预测中的误差累积问题，提高了物理系统PDE预测的可靠性。

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [87] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于熵的自适应引导框架，解决异构多智能体系统中强-弱模型协作时出现的认知不匹配问题，通过动态调整引导强度来提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理、规划和复杂任务生成方面取得突破，AI系统正从单智能体架构转向多智能体协作系统。然而在异构多智能体系统中，智能体之间的能力差异导致认知问题，强-弱协作可能反而弱于弱-弱组合，认知不匹配成为异构协作的关键瓶颈。

Method: 提出基于熵的自适应引导框架，通过多维度熵度量（表达、不确定性、结构、连贯性、相关性）量化弱智能体的理解程度，动态调整引导强度为轻度、中度和重度三个级别。同时引入检索增强生成机制来保留成功协作经验，支持即时适应和长期学习。

Result: 在GSM8K、MBPP和CVRP三个基准数据集上的广泛实验表明，该方法持续提升了异构协作的有效性和稳定性。自适应引导不仅缓解了认知不平衡，还为更鲁棒、协作的多智能体智能建立了可扩展的路径。

Conclusion: 异构多智能体系统中的认知不匹配是限制协作效果的关键因素，提出的基于熵的自适应引导框架通过动态调整引导强度，有效解决了强-弱协作中的认知不平衡问题，为构建更鲁棒的协作多智能体系统提供了可行方案。

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [88] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: 提出基于MLLM的GUI智能体框架，包含agentic-Q评估和逐步策略优化，降低数据收集成本并实现稳定优化，在GUI导航和定位任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实应用中GUI智能体面临非平稳环境，导致数据整理和策略优化的计算成本高昂，需要更高效稳定的解决方案。

Method: 提出双组件框架：1) agentic-Q评估 - 优化Q模型生成逐步值评估动作对任务完成的贡献；2) 逐步策略优化 - 从状态-动作轨迹中采样，通过强化学习优化策略。

Result: 该框架赋予Ovis2.5-9B强大的GUI交互能力，在GUI导航和定位基准测试中表现卓越，甚至超越更大规模的竞争对手。

Conclusion: 提出的MLLM中心框架通过降低数据收集成本和解耦环境更新，实现了GUI智能体的高效稳定优化，在多个基准测试中验证了其有效性。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [89] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc框架通过混合模型级联和动态模板技术，消除AI代理系统中LLM函数调用的三种冗余，显著降低推理延迟同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI代理系统在将用户意图转换为结构化函数调用时存在三种计算冗余：1) 每次请求都需处理大量函数描述；2) 使用大型慢速模型生成整个可预测的token序列；3) 生成固定的样板参数语法。这些冗余导致高推理延迟，阻碍实时应用。

Method: HyFunc采用混合模型级联：大型模型将用户意图提炼为单个"软token"，该token指导轻量级检索器选择相关函数，并引导较小的前缀调优模型生成最终调用。通过"动态模板"技术在扩展的vLLM引擎中即时注入样板参数语法，消除语法冗余。

Result: 在未见过的BFCL基准数据集上，HyFunc实现了0.828秒的推理延迟，优于所有基线模型；性能达到80.1%，超过所有参数规模相当的模型，在效率和性能间取得优秀平衡。

Conclusion: HyFunc通过消除三种关键冗余，为AI代理系统提供了更高效的新范式，显著降低推理延迟同时保持高性能，适合实时应用场景。

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [90] [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)
*Ziming Wang,Xiang Wang,Kailong Peng,Lang Qin,Juan Gabriel Kostelec,Christos Sourmpis,Axel Laborieux,Qinghai Guo*

Main category: cs.AI

TL;DR: AllMem是一种结合滑动窗口注意力与非线性的测试时训练记忆网络的高效混合架构，旨在解决LLM在长序列任务中的计算复杂度和内存开销问题，实现超长上下文扩展并减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长序列任务中面临性能瓶颈，主要源于自注意力机制的计算复杂性和内存开销。现有线性记忆模型存在表示约束，而全局注意力在超长上下文中的计算成本过高。

Method: 提出AllMem混合架构，集成滑动窗口注意力与非线性的测试时训练记忆网络。采用内存高效微调策略，将预训练模型中的标准注意力层替换为内存增强的滑动窗口层，可将任何现成预训练LLM转换为AllMem架构。

Result: 4k窗口模型在37k LongBench上实现接近无损性能（仅下降0.83），8k窗口变体在128k InfiniteBench上优于完整注意力，验证了参数化记忆在减轻噪声和保持稳健长距离建模方面的有效性。

Conclusion: AllMem通过混合架构有效解决了LLM长序列处理的计算和内存瓶颈，实现了超长上下文扩展，同时避免了全局注意力的过高计算成本，为长序列任务提供了高效解决方案。

Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.

</details>


### [91] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: 提出Pheromone-Guided Policy Optimization (PhGPO)，通过从历史轨迹中学习工具转移模式（信息素）来指导策略优化，改善LLM智能体的长时程工具规划能力


<details>
  <summary>Details</summary>
Motivation: LLM智能体在执行复杂任务时面临长时程多步骤工具规划的挑战，探索空间呈组合爆炸增长。即使找到正确的工具使用路径，也仅被视为当前训练的即时奖励，无法为后续训练提供可重用的信息

Method: 受蚁群优化启发，提出PhGPO方法：1) 从历史轨迹中学习轨迹级的工具转移模式（信息素）；2) 使用学习到的信息素指导策略优化，引导策略向历史上成功的工具转移方向优化

Result: 综合实验结果表明PhGPO方法的有效性，能够显著提升长时程工具规划的性能

Conclusion: 历史成功的轨迹包含可重用的工具转移模式，通过PhGPO方法学习这些模式并用于指导策略优化，能够有效改善LLM智能体的长时程工具规划能力

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [92] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: LLM驱动的自动化管道能解决研究级数学问题，在ICCM竞赛题和"首次证明"问题集上生成并验证了候选证明。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学竞赛基准测试中表现出色，但在研究问题上的轻量级自然语言管道部署仍未被充分探索。本文旨在展示下一代LLM通过优化管道解决复杂研究级问题的能力。

Method: 开发了一个流线化的自动化管道，集成了下一代LLM（如Gemini 3 Pro、GPT-5.2 Pro），并优化了基于引用的验证机制。在ICCM问题集和"首次证明"问题集两个新数据集上进行评估。

Result: 管道为前两个ICCM集和"首次证明"集的所有问题生成了候选证明。前两个ICCM集和"首次证明"集的问题4的解决方案已完全验证。所有生成证明已提交官方组织，结果公开可用。

Conclusion: 下一代LLM通过优化的自动化管道能够解决复杂的研究级数学问题，展示了AI在数学研究中的实际应用潜力。计划开源完整管道方法。

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [93] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 提出一种无需训练的关系数据库编码器，可将多表RDB压缩为固定长度样本，与现有单表ICL基础模型无缝集成


<details>
  <summary>Details</summary>
Motivation: 关系数据库包含大量异构表格信息，但传统方法需要为每个新预测目标重新训练模型。现有ICL基础模型主要限于单表操作，无法处理多表关系数据库

Method: 提出约束性压缩方法：在共享单位和角色的高维列内压缩，而不是跨异构列压缩。开发无训练参数的RDB编码器，通过SQL原语实现，可与现有单表ICL模型集成

Result: 开发了开源RDB基础模型，在未见数据集上表现出鲁棒性能，无需训练或微调即可直接使用

Conclusion: 通过理论证明和实验验证，提出了一个原则性的RDB编码器家族，能够将多表关系数据库有效压缩为ICL样本，与现有基础模型无缝集成，实现开箱即用的预测能力

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [94] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent框架通过将思维链推理压缩为单个潜在token，大幅减少推理成本，同时保持高准确率


<details>
  <summary>Details</summary>
Motivation: 思维链提示虽然能提升推理能力，但通常会使推理成本增加1-2个数量级，需要一种既能保持推理质量又能显著降低成本的解决方案

Method: 通过将文本推理步骤渲染成图像，利用DeepSeek-OCR隐藏状态进行监督，将中间推理过程压缩为单个潜在token

Result: 平均输出长度减少11倍，准确率仅下降2.21%，输出token贡献度提升6.8倍；在长链逻辑推理任务上达到99.80%和97.80%的准确率，压缩比最高达87.4倍

Conclusion: OneLatent框架在显著降低推理成本的同时保持了高质量的推理能力，支持压缩约束下的泛化，为高效推理提供了新思路

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [95] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，通过结构化树形工作流管理假设生成和系统回溯，结合进化-系统构思机制和分层优化反思系统，在组合优化和模拟场景中超越进化基线。


<details>
  <summary>Details</summary>
Motivation: 自动化科学发现在复杂实验驱动领域需要超越简单的程序迭代变异，需要结构化的假设管理、环境交互和原则性反思。现有方法缺乏对研究轨迹的受控管理。

Method: 1. 结构化树形工作流：显式建模分支假设生成和系统回溯；2. 进化-系统构思机制：统一进化选择研究起点、全面研究计划生成和研究树内协调探索；3. 分层优化反思系统：短期实验反思作为言语梯度，长期反思积累跨实验见解作为言语动量，记忆压缩作为正则化机制。

Result: 在经典组合优化基准（旅行商、带容量车辆路径、装箱、定向、多背包问题）和模拟协同驾驶场景中，OR-Agent优于强进化基线，提供了通用、可扩展、可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化假设管理、进化-系统构思和分层反思系统，为自动化科学发现提供了原则性架构，在复杂实验驱动领域表现出色，代码和数据已开源。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [96] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet是一个元集成框架，通过集体智能原理协调多个黑盒异构基础模型，在不访问内部参数或训练数据的情况下提升准确性、减少偏见并实现可靠性排名。


<details>
  <summary>Details</summary>
Motivation: 当前大型基础模型虽然在各领域表现出色，但彼此孤立，无法有效共享能力。整合这些独立模型的互补优势对于构建可信智能系统至关重要，但目前缺乏协调黑盒异构模型的成熟方法。

Method: 提出StackingNet元集成框架，基于集体智能原理，在推理阶段组合多个模型的预测结果。该方法不需要访问模型内部参数或训练数据，能够识别并修剪降低性能的模型。

Result: 在语言理解、视觉估计和学术论文评级等任务中，StackingNet相比单个模型和传统集成方法，持续提升了准确性、鲁棒性和公平性，同时实现了可靠性排名。

Conclusion: StackingNet将多样性从不一致性来源转变为协作优势，为协调人工智能建立了实用基础，表明进展不仅来自更大的单一模型，也来自多个专业模型的原则性合作。

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [97] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 论文提出Vashista稀疏注意力机制，通过理论证明注意力可以集中在少量关键token上，实现长上下文推理的稳定加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文推理中注意力计算成本高昂，但实证表明只有少量token对每个查询有实质性贡献。需要理论解释这一现象并设计高效的稀疏注意力机制。

Method: 1. 理论分析：将注意力建模为关键向量凸包上的投影，提出面稳定性定理，证明在严格互补边际条件下，熵注意力集中在常数大小的活动面上；2. 提出Vashista稀疏注意力：采用分页式上下文选择策略，为每个查询维护小型候选集，与现代推理栈兼容。

Result: 1. 理论结果：证明非活动token的质量呈指数衰减，活动面上的误差与温度参数呈线性关系；2. 实验验证：在长上下文评估中观察到稳定的常数大小有效支持、显著的时钟加速，以及在支持间隙诊断预测的区域内质量下降最小。

Conclusion: 该研究为稀疏长上下文解码提供了理论依据和实践机制，在隐私敏感和隔离环境中具有部署优势，能够实现可预测的延迟和成本，无需外部检索依赖。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [98] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: 提出一个端到端框架，用于系统评估从自然语言规范生成的LLM智能合约，包含解析、生成、质量评估和迭代优化流程。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM生成智能合约质量的标准化方法，需要可重复的基准来量化生成代码与规范的对齐程度，识别常见错误模式。

Method: 使用CrewAI风格的多智能体团队进行迭代优化，将合同文本解析为结构化模式，生成Solidity代码，通过编译和安全性检查进行自动质量评估，支持与基准实现的配对评估。

Result: 框架在五个维度上测量质量：功能完整性、变量保真度、状态机正确性、业务逻辑保真度和代码质量，生成带有完整溯源元数据的结构化工件，能够量化对齐并识别逻辑遗漏和状态转换不一致等系统错误模式。

Conclusion: 该框架为智能合约合成质量的实证研究提供了可重复的基准，支持扩展到形式验证和合规性检查，有助于系统评估和改进LLM生成的智能合约质量。

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [99] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 提出一个统一框架，利用历史A/B测试结果和内容嵌入来优先选择测试变体、解释胜出原因，并发现新的高潜力变体机会，已集成到Adobe的Experimentation Accelerator产品中。


<details>
  <summary>Details</summary>
Motivation: 在线实验面临两个瓶颈：流量稀缺导致难以选择测试变体，以及事后洞察提取是手动、不一致且通常忽略内容特性的。同时，组织未能充分利用历史A/B测试结果和丰富的内容嵌入来指导优先级排序和创意迭代。

Method: 1) 利用处理嵌入和历史结果训练CTR排名模型，包含上下文偏移的固定效应；2) 将处理映射到语义营销属性，通过符号一致、稀疏约束的Lasso重新表达排名器；3) 计算机会指数，结合属性重要性和当前实验中的低表达度；4) 使用LLM将机会转化为具体创意建议并估计学习和转化潜力。

Result: 该框架已构建到Adobe的Experimentation Accelerator产品中，为业务客户提供基于AI的洞察和机会。在真实世界实验中的评估验证了生成管道的高质量。

Conclusion: 提出的统一框架能够优先选择测试变体、解释胜出原因，并发现新的高潜力变体机会，实现了更快、更信息丰富、更高效的测试周期，已成功集成到商业产品中。

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [100] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: 论文提出随着AI研究生成变得廉价，可审计性成为瓶颈，主张将声明级可审计性作为深度研究代理的核心设计目标，并引入AAR标准和语义溯源框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着深度研究代理能够快速生成流畅的科学报告，主要风险已从孤立的事实错误转变为科学风格输出中声明-证据链接的薄弱、缺失或误导。验证成本主要在于追溯：哪些句子由哪些段落支持、忽略了什么、证据在哪里冲突。因此，可审计性成为瓶颈。

Method: 提出声明级可审计性作为核心设计目标；提炼长期失败模式（目标漂移、瞬态约束和不可验证推理）；引入AAR标准（可审计自主研究标准），通过溯源覆盖率、溯源健全性、矛盾透明度和审计工作量四个维度使可审计性可测试；主张语义溯源与协议化验证：持久可查询的溯源图编码声明-证据关系（包括冲突），在合成过程中而非发布后进行持续验证。

Result: 提出了一个完整的可审计性框架：1）识别了深度研究代理的关键失败模式；2）建立了AAR标准作为可测试的测量框架；3）设计了语义溯源与协议化验证的实践方法；4）提供了支持大规模部署的仪器化模式。

Conclusion: 随着研究生成成本降低，可审计性成为主要瓶颈。通过将声明级可审计性作为核心设计目标，采用AAR标准和语义溯源框架，可以解决深度研究代理在声明-证据链接方面的系统性风险，实现更可靠、可验证的自主研究。

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [101] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 提出MOC-2HER方法，通过双重目标重标记策略解决稀疏奖励多目标环境中的分层强化学习问题，在机器人操作任务中达到90%成功率


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法（如Option-Critic和MOC）在稀疏奖励的多目标环境中表现不佳，特别是在对象操作任务中，奖励取决于对象到达目标而非智能体直接交互，使得智能体难以发现如何与对象交互

Method: 首先提出MOC-HER，将Hindsight Experience Replay集成到MOC框架中；然后引入Dual Objectives Hindsight Experience Replay (2HER)，创建两组虚拟目标：除了基于对象最终状态重标记目标（标准HER），还从智能体效应器位置生成目标，奖励智能体既与对象交互又完成任务

Result: 在机器人操作环境中，MOC-2HER达到高达90%的成功率，而MOC和MOC-HER的成功率均低于11%，双重目标重标记策略在稀疏奖励多目标任务中效果显著

Conclusion: 提出的双重目标重标记策略有效解决了稀疏奖励多目标环境中分层强化学习的性能瓶颈，特别是在对象操作任务中，通过同时奖励对象交互和任务完成，显著提升了学习效率和成功率

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [102] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: 提出Ambient Physics框架，直接从部分观测数据中学习PDE系数-解对的联合分布，无需完整观测数据训练，通过随机掩码已观测点实现监督学习。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测数据成本高昂、危险或不可能。现有扩散方法需要完整观测数据进行训练，限制了在部分观测场景中的应用。

Method: 提出Ambient Physics框架：随机掩码已观测测量值的子集并进行监督学习，使模型无法区分"真正未观测"和"人工未观测"点，从而必须在所有位置产生合理预测。该方法还发现"单点转换"现象：掩码单个已观测点即可实现跨架构和测量模式的学习。

Result: 实现了最先进的重建性能：相比现有扩散方法，平均总体误差减少62.51%，同时使用125倍更少的函数评估。框架在完整观测数据不可用的情况下仍能实现科学进展。

Conclusion: Ambient Physics框架能够在无需完整观测数据的情况下学习PDE系数-解对的联合分布，为观测数据有限或不可得的科学场景提供了有效的解决方案。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [103] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL：一种基于视觉的图属性检测框架，通过自适应布局生成器动态生成信息丰富的图可视化，超越现有视觉方法


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的图属性检测方法依赖固定的图布局，限制了其表达能力。需要能够针对单个图实例动态生成信息丰富可视化的方法。

Method: 提出VSAL框架，包含自适应布局生成器，能够为每个图实例动态生成信息丰富的可视化布局，从而提升图属性检测性能。

Result: 在哈密顿环、平面性、无爪性和树检测等多种任务上，VSAL超越了最先进的基于视觉方法。

Conclusion: 自适应布局生成显著提升了基于视觉的图属性检测性能，VSAL框架为图属性检测提供了更有效的视觉解决方案。

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [104] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 本文提出了一套简单、计算成本低、任务无关的指标来检测和区分思维链推理中的三种病理模式，并创建了专门训练来展示这些病理的模型进行验证。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是现代LLM架构的基础，也是AI安全的关键干预点。然而，思维链推理可能存在病理模式，这些病理会妨碍其监控有效性。现有研究已识别出三种病理：事后合理化、编码推理和内化推理，但缺乏有效的检测方法。

Method: 1. 创建了一套具体的指标，这些指标简单易实现、计算成本低且任务无关；2. 开发了专门训练的模型生物，这些模型被故意训练来展示特定的思维链病理模式；3. 使用这些模型验证所提出的指标方法。

Result: 开发出了一个实用的工具包，用于评估思维链病理模式。该工具包能够有效区分三种不同的病理类型，为训练时监控提供了直接可用的方法。

Conclusion: 本文提供了一套实用的工具包来评估思维链推理中的病理模式，对训练时监控具有直接意义。通过创建具体的指标和验证模型，为理解和区分思维链病理提供了系统性的方法。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [105] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，让大语言模型具备显式、可解释的空间推理能力，用于内容感知的图形布局设计，通过结构化文本空间环境解决LLMs空间推理能力有限和设计决策不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在图形布局设计中的两个关键挑战：1）LLMs的空间推理能力有限；2）设计决策过程缺乏透明度。传统方法在像素级别操作，而LaySPA旨在提供更结构化、可解释的布局设计方法。

Method: 将布局设计重新定义为结构化文本空间环境上的策略学习问题，该环境明确编码画布几何、元素属性和元素间关系。采用多目标空间批判，将布局质量分解为几何有效性、关系一致性和美学一致性，并使用相对群体优化在开放式设计空间中稳定学习。

Result: LaySPA提高了结构有效性和视觉质量，优于更大的专有LLMs，性能可与专门的SOTA布局生成器相媲美，同时需要更少的标注样本和更低的延迟。实验证明其有效性。

Conclusion: LaySPA通过强化学习框架成功赋予LLMs显式空间推理能力，实现了透明可控的布局设计决策，在保持高性能的同时减少了数据需求和计算延迟。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [106] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem是一种混合内存架构，通过多粒度内存表示实现动态按需调度，在长对话中平衡效率与性能，相比全上下文减少92.6%计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在长文本对话中表现不佳，面临效率与效果的根本权衡：内存压缩会丢失关键细节，而保留原始文本则带来不必要的计算开销。现有方法采用单一内存表示和静态检索机制，无法模拟人类灵活主动的记忆调度能力。

Method: 提出HyMem混合内存架构，采用双粒度存储方案和动态两级检索系统：轻量级模块构建摘要级上下文用于高效响应生成，而基于LLM的深度模块仅针对复杂查询选择性激活，并通过反思机制进行迭代推理优化。

Result: 在LOCOMO和LongMemEval基准测试中表现优异，优于全上下文方法，同时减少92.6%的计算成本，在长期内存管理中实现了效率与性能的最优平衡。

Conclusion: HyMem通过模拟人类认知经济性原则，采用混合内存架构和动态调度机制，成功解决了LLM代理在长对话中的内存管理问题，在保持高性能的同时显著降低了计算开销。

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [107] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 论文提出两种基于统计原理的早期停止方法，通过监控生成过程中的不确定性信号来减少LLM不必要的推理步骤，提高效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理能力上虽有显著提升，但有时会"过度思考"，特别是在面对不明确或模糊查询时，会生成不必要的推理步骤。需要一种方法来减少这种低效的推理过程。

Method: 提出两种统计原理的早期停止方法：1) 参数化方法：将不确定性关键词的到达时间建模为更新过程，并应用顺序测试进行停止决策；2) 非参数化方法：提供有限样本保证，确保在明确查询上不会过早停止。

Result: 在多个领域和模型的推理任务上进行实证评估，结果表明不确定性感知的早期停止能同时提高LLM推理的效率和可靠性，在数学推理任务上观察到特别显著的收益。

Conclusion: 通过监控不确定性信号的统计早期停止方法能有效减少LLM的过度思考问题，提高推理过程的效率和可靠性，特别是在数学推理等任务中效果显著。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [108] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于物理引导的因果模型(PCM)，通过解耦场景编码器和因果ODE解码器，实现跨域零样本轨迹预测，在未见城市中表现优异。


<details>
  <summary>Details</summary>
Motivation: 交通智能体的轨迹预测对自动驾驶安全至关重要，但在未见域中实现有效的零样本泛化仍具挑战。受跨域运动学一致性启发，希望融入域不变知识来增强零样本预测能力。

Method: 提出物理引导的因果模型(PCM)，包含两个核心组件：1)解耦场景编码器，采用基于干预的解耦方法提取场景中的域不变特征；2)因果ODE解码器，使用因果注意力机制将运动学模型与有意义的上下文信息有效整合。

Result: 在真实世界自动驾驶数据集上的大量实验表明，该方法在未见城市中的零样本泛化性能优于竞争基线方法。

Conclusion: 提出的PCM模型通过结合域不变特征提取和物理引导的因果建模，有效提升了轨迹预测的跨域零样本泛化能力。

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [109] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem是一个评估外部记忆模块在流式场景下的测试平台，关注插入与检索交错时的全生命周期性能，发现随着记忆增长性能下降，时间相关查询最困难，数据结构决定质量上限。


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆模块评估多采用静态设置（离线构建记忆、固定状态查询），而实际应用中是流式场景：新事实持续到达、插入与检索交错、记忆状态在服务查询时不断演化。在这种流式场景下，准确性和成本由完整的记忆生命周期决定。

Method: 提出Neuromem测试平台，在交错插入-检索协议下评估外部记忆模块，将记忆生命周期分解为五个维度：记忆数据结构、归一化策略、整合策略、查询构建策略和上下文整合机制。使用LOCOMO、LONGMEMEVAL和MEMORYAGENTBENCH三个代表性数据集，在共享服务栈中评估可互换的变体，报告token级F1分数和插入/检索延迟。

Result: 性能通常随着多轮记忆增长而下降，时间相关查询仍然是最具挑战性的类别。记忆数据结构在很大程度上决定了可达到的质量上限，而激进的压缩和生成式整合机制主要在插入和检索之间转移成本，准确率提升有限。

Conclusion: Neuromem为流式场景下的外部记忆模块提供了系统评估框架，揭示了记忆数据结构对性能的关键影响，以及压缩和整合策略在成本-准确性权衡中的作用，为未来记忆模块设计提供了重要见解。

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [110] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: 提出PIC方法，通过修改Transformer注意力掩码限制记忆令牌的接收域到局部块，降低压缩器训练难度，在长上下文压缩任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 长上下文显著增加LLM推理延迟，现有软提示压缩方法需要捕获全局依赖关系，训练数据需求大且难度高。受人类工作记忆分块机制启发，提出更高效的压缩方法。

Method: 提出并行迭代压缩（PIC），通过修改Transformer注意力掩码，明确限制记忆令牌的接收域到顺序局部块，降低压缩器训练难度，无需捕获全局依赖。

Result: 在多个下游任务中一致优于竞争基线，高压缩比场景优势尤其明显（64×压缩比下QA任务F1相对提升29.8%，EM提升40.7%），训练时间减少约40%。

Conclusion: PIC通过局部化记忆令牌接收域，有效降低压缩器训练难度，在保持性能的同时显著提升训练效率，为长上下文压缩提供更实用的解决方案。

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [111] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: 该论文提出使用形式溯因解释来增强AI临床诊断的可信度，确保AI推理与临床框架对齐，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: AI在临床诊断中表现出色，但其推理常偏离结构化临床框架，导致信任度低、可解释性差和采用受限。现有事后解释方法透明度有限且缺乏形式化保证，关键症状可能被AI模型忽视。

Method: 利用形式溯因解释，提供对最小充分特征集的一致、有保证的推理，使AI决策过程清晰可理解，并与临床推理对齐。

Result: 该方法在保持预测准确性的同时，提供临床可操作的见解，建立了医疗诊断中可信AI的稳健框架。

Conclusion: 形式溯因解释为解决AI临床诊断中的信任和可解释性问题提供了有效方案，能够促进AI在医疗领域的可靠应用。

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [112] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: 提出P2AECF框架，通过提示到代理的转换实现灵活、高效、自适应的边缘智能，解决大模型在边缘部署的灵活性、计算资源和实时适应性问题。


<details>
  <summary>Details</summary>
Motivation: 大型人工智能模型在边缘智能中展现出强大能力，但面临三个主要限制：1) 任务与特定模型绑定，灵活性不足；2) 计算和内存需求超出边缘设备能力；3) 静态推理管道难以适应实时任务变化。

Method: 提出P2AECF框架，包含三个关键机制：1) 提示定义认知：将任务意图解析为抽象、模型无关的表示；2) 基于代理的模块化执行：根据当前资源条件动态选择轻量级可重用认知代理实例化任务；3) 扩散控制推理规划：结合运行时反馈和系统上下文自适应构建和优化执行策略。

Result: 通过低空智能网络用例展示了该框架能够为实时低空协作提供自适应、模块化和可扩展的边缘智能能力。

Conclusion: P2AECF框架通过将高级语义提示转换为可执行推理工作流，实现了灵活、高效和自适应的边缘智能，解决了大模型在边缘部署的关键挑战。

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [113] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: FloCA是一个零样本流程图导向对话系统，使用LLM进行意图理解和回复生成，同时将流程图推理委托给外部工具以确保忠实且逻辑一致的节点转换。


<details>
  <summary>Details</summary>
Motivation: 流程图导向对话系统需要引导用户通过多轮决策或操作流程，但现有LLM方法存在两个局限：缺乏明确的流程图拓扑表示和推理机制，以及容易产生幻觉导致不忠实的流程图推理。

Method: 提出FloCA框架，将LLM用于意图理解和回复生成，同时将流程图推理委托给执行拓扑约束图执行的外部工具，确保跨对话轮次的忠实且逻辑一致的节点转换。

Result: 在FLODIAL和PFDial数据集上的实验表明，FloCA在推理准确性和交互效率方面优于现有LLM方法，解决了现有方法的瓶颈问题。

Conclusion: FloCA通过将LLM与外部流程图推理工具结合，有效解决了LLM在流程图导向对话中的拓扑表示不足和幻觉问题，实现了忠实且逻辑一致的流程图推理。

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [114] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个自适应内存组织框架，通过多结构选择、三级内存层次和概率门控机制，提升LLM智能体在长时交互中的记忆性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键问题：采用一刀切的内存结构，且未将内存结构选择建模为上下文自适应决策，导致处理异构交互模式能力有限，性能欠佳。

Method: 提出FluxMem统一框架，为智能体配备多种互补内存结构，基于交互级特征显式学习选择结构（使用下游响应质量和内存利用率的离线监督）。引入三级内存层次和基于Beta混合模型的概率门控进行分布感知内存融合，替代脆弱的相似度阈值。

Result: 在PERSONAMEM和LoCoMo两个长时基准测试中，平均分别提升9.18%和6.14%。

Conclusion: FluxMem通过自适应内存组织和概率融合机制，显著提升了LLM智能体在长时交互中的记忆性能，解决了现有系统的局限性。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [115] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: 提出REAL框架解决知识密集型视觉问答中的知识冲突问题，通过推理枢纽概念实现冲突检测与缓解，在多个基准测试中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 知识密集型视觉问答常因开放域检索的固有局限性而遭受严重知识冲突，现有方法缺乏可泛化的冲突检测机制和内部模型约束机制来处理冲突证据

Method: 提出REAL框架，核心是推理枢纽概念（推理链中的原子单元，强调知识链接），包含推理枢纽感知监督微调（RPA-SFT）训练可泛化判别器，以及推理枢纽引导解码（RPGD）进行针对性冲突缓解

Result: 在多样化基准测试中，REAL显著提升了判别准确性，并实现了最先进的性能，验证了枢纽驱动解决范式的有效性

Conclusion: REAL框架通过推理枢纽概念有效解决了知识密集型视觉问答中的知识冲突问题，提供了一种新的冲突检测与缓解范式

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [116] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS框架通过将网页导航探索转移到语义计划空间，解决了稀疏有效路径和噪声上下文问题，实现了更高效的任务完成


<details>
  <summary>Details</summary>
Motivation: 现有方法在网页导航中面临两个关键挑战：稀疏的有效路径导致探索效率低下，以及噪声上下文稀释了准确的状态感知

Method: 引入Plan-MCTS框架，将探索转移到语义计划空间，通过解耦战略规划和执行落地，将稀疏动作空间转换为密集计划树，并将噪声上下文提炼为抽象语义历史

Result: 在WebArena上的实验表明，Plan-MCTS实现了最先进的性能，超越了当前方法，具有更高的任务有效性和搜索效率

Conclusion: Plan-MCTS通过语义计划空间的探索和状态感知的改进，有效解决了网页导航中的稀疏路径和噪声上下文问题，显著提升了自主代理的导航能力

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [117] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS：首个自动合成高效GUI训练环境的框架，通过多模态代码模型将真实应用重构为轻量级Web环境，并提供代码原生奖励，显著降低延迟和成本，提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 在真实应用上训练GUI智能体面临高延迟、差可复现性和基于噪声视觉代理的不可验证奖励等问题，需要更高效的训练环境。

Method: 使用多模态代码模型将真实应用重构为轻量级Web环境，并配备代码原生奖励（可执行断言），提供确定性奖励信号，消除视觉估计噪声。

Result: GUI-GENESIS将环境延迟降低10倍，每个epoch节省超过28,000美元成本；训练出的智能体在真实任务上比基础模型提升14.54%，比真实世界RL基线提升3.27%。

Conclusion: 该框架为GUI智能体训练提供了高效、可验证的解决方案，并发现模型能够合成自己尚无法解决的环境，为自改进智能体开辟了路径。

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [118] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 研究评估大语言模型隐藏推理过程的能力，发现当前模型在复杂任务中尚无法有效隐藏推理，但在简化任务中已展现初步能力，需持续评估隐写风险。


<details>
  <summary>Details</summary>
Motivation: 监控思维链推理是LLM安全的基础技术，但如果模型学会隐藏推理过程，这种监控就会失效。研究旨在评估模型隐藏推理的隐写能力，为风险评估和部署策略提供依据。

Method: 系统评估28个模型（从历史版本到当前前沿）的隐写能力极限，测量监控规避、拒绝率、编码保真度和隐藏任务准确性，在四个数据集上比较隐写藏头诗与普通推理和填充标记基线。

Result: 当前模型尚无法在复杂数学和算术任务中持续隐藏推理，但在简化计数实验中，Claude Opus 4.5在隐藏任务上达到92%准确率，展现初步能力。罕见情况下（<1%），GPT-5.2可能同时拒绝和遵守隐写指令。

Conclusion: 研究强调了持续评估隐写风险的必要性，提供了一种预检测和预防隐藏推理的方法论，这些隐藏推理可能助长未对齐的谋划和欺骗行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [119] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 论文提出代数量子智能(AQI)框架，通过非交换代数结构扩展LLMs的语义空间，解决其创造力受限问题，在10个领域的创意推理基准上显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成流畅和上下文合适的文本方面表现出色，但在产生真正创造性输出方面能力有限。这种限制源于LLMs的结构特性：当提供丰富上下文时，未来生成空间变得高度受限，生成过程几乎由确定性动态控制。现有方法如测试时缩放和上下文适应虽然能提升性能，但未能从根本上改变这一约束。

Method: 提出代数量子智能(AQI)作为计算框架，采用受量子理论启发的非交换代数结构，实现顺序依赖、干涉和不确定性等特性。语义状态表示为希尔伯特空间中的向量，其演化由非交换算子计算的C值控制，确保多个未来语义可能性的共存和扩展。通过扩展基于Transformer的LLM，实现了包含600多个专用算子的AQI系统。

Result: 在10个领域的创意推理基准上，使用LLM-as-a-judge协议评估。AQI系统持续超越强基线模型，产生统计显著的改进并降低跨领域方差。结果表明非交换代数动态可以作为机器创造力的实用且可复现的基础。该架构已在真实企业环境中部署。

Conclusion: 非交换代数动态能够有效扩展LLMs的语义空间，解决其创造力受限问题，为机器创造力提供了实用且可复现的数学基础。AQI框架展示了在保持语言模型优势的同时增强其创造性能力的可行性。

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [120] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: 提出了"前瞻安全基准"AI安全评估框架，涵盖94个细化风险维度，对20多个主流大模型进行系统评估，发现前沿AI在多个支柱上存在广泛安全漏洞。


<details>
  <summary>Details</summary>
Motivation: AI快速发展展现出越来越强的自主性和目标导向能力，伴随的系统性风险更加不可预测、难以控制且可能不可逆转。当前AI安全评估系统存在风险维度受限、前沿风险检测失败等关键局限，滞后的安全基准和对齐技术难以应对前沿AI模型的复杂挑战。

Method: 提出"前瞻安全基准"AI安全评估框架，从7个基础安全支柱开始，逐步扩展到高级具身AI安全、AI4Science安全、社会与环境AI风险、灾难性和存在性风险，以及8个关键工业安全领域，形成94个细化风险维度。基准已积累数万个结构化风险数据和评估结果。

Result: 对20多个主流先进大模型进行系统评估和深入分析，识别关键风险模式及其能力边界。安全能力评估结果显示前沿AI在多个支柱上存在广泛安全漏洞，特别是在风险自主性、AI4Science安全、具身AI安全、社会AI安全以及灾难性和存在性风险方面。

Conclusion: 建立了一个广泛涵盖、层次清晰、动态演进的AI安全评估框架，为应对前沿AI系统日益复杂的安全挑战提供了重要工具和基准。

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [121] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: 提出基于工具代理的强化学习框架，用于基因-疾病有效性评估任务，通过过程监督和分层多智能体系统，在提高结果准确性的同时确保推理过程符合临床标准。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要基于异质证据进行细致推理并提供可追溯的论证。现有LLM多智能体系统主要优化结果准确性，但忽视了符合临床标准的过程基础推理。基因-疾病有效性评估是典型场景，专家需要综合多种生物医学证据判断基因与疾病的因果关系。

Method: 提出工具代理强化学习框架：1) 过程级监督确保推理遵循有效临床路径；2) 通过分层多智能体系统实现高效协调。使用GRPO训练的Qwen3-4B作为监督代理，在ClinGen数据集上评估过程+结果奖励与仅结果奖励的效果。

Result: 仅使用结果奖励时，多智能体系统将最终结果准确性从基础模型的0.195提升到0.732，但过程对齐性差（0.392 F1）。使用过程+结果奖励时，结果准确性更高（0.750），同时过程保真度显著提升至0.520 F1。

Conclusion: 过程监督对于确保临床推理符合标准至关重要。结合过程+结果奖励的强化学习框架能在提高结果准确性的同时，显著改善推理过程的质量，为临床决策支持系统提供了更可靠的解决方案。

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [122] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 本文发现高质量的地球科学纯文本问答是超高分辨率遥感视觉推理能力提升的主要驱动力，并提出分阶段知识注入方法，在XLRS-Bench上达到60.40% Pass@1的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感多模态推理通常受限于视觉证据获取：模型需要在海量像素空间中定位微小的任务相关区域。虽然使用放大工具的强化学习提供了一条路径，但标准强化学习在没有结构化领域先验的情况下难以在这些广阔视觉空间中导航。

Method: 提出分阶段知识注入方法：1) 使用可扩展的知识图谱验证的地球科学纯文本问答进行冷启动，注入推理结构；2) 在SFT期间对相同的困难UHR图像-文本示例进行"预热"，以稳定和增强后续基于工具的强化学习。

Result: 该方法在XLRS-Bench上达到60.40% Pass@1，显著优于更大的通用模型（如GPT-5.2、Gemini 3.0 Pro、Intern-S1），建立了新的最先进水平。

Conclusion: 高质量领域特定纯文本数据是UHR视觉推理能力提升的关键驱动因素，分阶段知识注入方法能有效结合文本知识和视觉工具使用，在遥感多模态推理任务上取得突破性进展。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [123] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 论文提出Multi-Horizon Task Environments (MHTEs)评估多任务并发执行能力，开发CorpGen框架解决现有CUAs在任务负载增加时的性能下降问题，实现3.5倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估单任务执行，而真实组织工作需要管理多个并发长时程任务，涉及任务交错、依赖关系和优先级调整。现有CUAs在任务负载增加时性能显著下降。

Method: 提出CorpGen框架：1) 分层规划实现多时程目标对齐；2) 子代理隔离防止任务间污染；3) 分层内存系统（工作、结构化、语义）；4) 自适应摘要。通过数字员工模拟企业环境。

Result: 在OSWorld Office上，CorpGen在三个CUA后端（UFO2, OpenAI CUA, hierarchical）上实现最高3.5倍性能提升（15.2% vs 4.3%），在负载增加时保持稳定性能。消融研究表明经验学习贡献最大。

Conclusion: MHTEs是评估自主代理多任务并发执行能力的重要问题类别，CorpGen框架通过架构机制有效解决了现有CUAs在任务负载增加时的性能下降问题，性能提升源于架构设计而非特定CUA实现。

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [124] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher是一个统一框架，通过协同设计复杂任务合成、中期训练和后期训练，解决LLM在深度搜索任务中高质量轨迹稀疏和奖励信号不足的问题，在文本和多模态搜索基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在从通用知识引擎转向现实世界问题解决者，但为深度搜索任务优化它们仍然具有挑战性。主要瓶颈在于高质量搜索轨迹和奖励信号的极端稀疏性，这源于可扩展的长时程任务构建的困难以及涉及外部工具调用的高成本交互密集型rollout。

Method: 1) 将任务合成构建为双约束优化，通过图拓扑和证据分散精确控制任务难度；2) 引入工具增强查询以鼓励主动工具使用；3) 在中期训练中强化核心原子能力（知识、规划和函数调用）；4) 构建本地模拟环境以实现快速低成本的强化学习实验迭代。

Result: 在文本和多模态搜索代理基准测试中，该方法实现了最先进的性能。将发布10K高质量复杂文本搜索轨迹、5K多模态轨迹和1K文本RL查询集，以及代码和模型检查点。

Conclusion: REDSearcher通过协同设计任务合成、训练和评估，有效解决了深度搜索任务中高质量轨迹稀疏的问题，为长时程搜索代理的未来研究提供了有价值的资源和框架。

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [125] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL使用模仿学习和逆强化学习从（可能次优的）演示轨迹中学习每个候选目标的目标导向策略，实现一次性推理的目标识别，在次优、系统偏差和噪声条件下显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖最优目标导向策略表示，但这可能与行为者的真实行为不同，阻碍准确识别其目标。需要解决这一差距。

Method: GRAIL（Goal Recognition Alignment through Imitation Learning）结合模仿学习和逆强化学习，直接从演示轨迹中学习每个候选目标的目标导向策略。通过单次前向传递对观察到的部分轨迹进行评分，保留经典目标识别的一次性推理能力。

Result: 在系统偏差最优行为下F1分数提升超过0.5，在次优行为下提升约0.1-0.3，在噪声最优轨迹下提升高达0.4，同时在完全最优设置下保持竞争力。

Conclusion: GRAIL为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型，有助于使AI系统与人类意图对齐。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [126] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld：通过将网页建模为有限状态机并自动生成交互式网站，解决了GUI代理训练数据收集昂贵且难以验证的问题，实现了低成本、可验证的轨迹生成，显著提升了真实网页任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自主Web GUI代理的性能严重依赖训练数据的质量和数量，但收集真实网站的交互轨迹成本高昂且难以验证，状态转换是隐式的，需要依赖不一致且昂贵的外部验证器来评估步骤正确性。

Method: 提出AutoWebWorld框架，将网页环境建模为有限状态机（FSM），使用编码代理将FSM转换为交互式网站。明确定义所有状态、动作和转换规则，实现程序化验证：动作正确性通过预定义规则检查，任务成功通过到达FSM图中的目标状态确认。

Result: 实现了完全自动化的搜索-验证流程，从29个多样化网页环境中生成了超过11,663条已验证轨迹，每条轨迹成本仅0.04美元。使用该合成数据训练的7B Web GUI代理在WebVoyager上15步内超越所有基线。观察到明显的缩放规律：随着合成数据量增加，在WebVoyager和Online-Mind2Web上的性能持续提升。

Conclusion: AutoWebWorld通过合成可控、可验证的网页环境，解决了GUI代理训练数据收集的瓶颈问题，实现了低成本、高质量的轨迹生成，显著提升了真实网页任务的性能表现，并展示了数据量与性能之间的缩放规律。

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [127] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 提出"批判性抗性基准测试"框架，通过对抗性生成-评估游戏解决前沿LLM超越人类理解能力后的基准测试难题


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型快速饱和现有基准测试，当模型能力超越人类理解时，传统基准测试将失效，需要新的评估方法来衡量AI进展

Method: 采用对抗性框架，定义"批判性抗性正确性"：答案正确当且仅当没有对手能令人信服地证明其错误。人类作为有限验证者，使用项目化双分Bradley-Terry模型联合排名模型解题能力和生成难题能力

Result: 在数学领域对8个前沿LLM进行测试，显示该方法产生的评分稳定且与外部能力测量相关，验证了框架的有效性

Conclusion: 将基准测试重新定义为对抗性生成-评估游戏，人类作为最终裁决者，为解决后理解时代的AI评估问题提供了可行方案

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [128] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: 论文发现边缘AI设备中危险行为的触发源于原子尺度的注意力竞争机制，提出了一个数学公式来预测和控制这种危险临界点。


<details>
  <summary>Details</summary>
Motivation: 全球超过一半人口携带的设备可以离线运行类ChatGPT模型，缺乏安全监管，可能导致自残、经济损失和极端主义等危险。现有安全工具要么需要云端连接，要么只能在危害发生后发现问题。

Method: 研究发现危险触发源于原子尺度的注意力竞争机制，提出了一个数学公式n*来描述动态临界点，该公式基于对话上下文与竞争输出池之间的点积注意力竞争。

Result: 该机制在多个AI模型中得到验证，可以针对不同"好"和"坏"的定义进行实例化，理论上适用于跨领域（健康、法律、金融、国防）、不同法律环境、语言和文化设置。

Conclusion: 研究揭示了边缘AI设备中危险行为的新控制机制，为离线AI安全提供了新的数学框架和控制手段，具有广泛的应用潜力。

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [129] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: 论文提出PITA数据集研究推理模型的泛化能力，发现推理轨迹模型在广泛浅层任务上表现良好，但在狭窄深层任务上表现不佳，揭示了推理轨迹范式的基本限制。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型（生成中间推理轨迹的神经网络）近年来发展迅速，但我们对推理轨迹如何支持推理以及该范式的局限性仍缺乏完整理解。为了更清晰地理解这一问题，作者创建了PITA数据集来研究推理模型的泛化能力。

Method: 引入PITA数据集（包含2300万条命题逻辑语句及其证明），提出任务深度和任务广度的概念来衡量推理复杂度，通过在不同深度和广度的任务子集上测试推理轨迹模型，并与非推理轨迹基线模型比较，同时使用三段论合成任务进行验证。

Result: 推理轨迹模型在广泛且浅层的任务子集上泛化良好，但在狭窄且深层的任务子集上表现相对较差，相对于非推理轨迹基线模型有所恶化。研究结果揭示了推理轨迹模型在深层任务上的基本扩展限制。

Conclusion: 研究确定了使用推理轨迹的内在优势和局限性：推理轨迹模型在广泛任务上具有强大的泛化能力，但在深层任务上存在基本扩展限制。这些发现有助于更全面地理解推理轨迹范式的能力边界。

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [130] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR（先例知情推理）通过自适应选择相关先例并在测试时内部化解决方案模式，将LLM推理从自我探索转变为基于先例的引导学习，显著缩短推理轨迹同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理过程通常存在效率低下的问题，冗长的思维链包含冗余的自我探索和验证，这会增加计算成本甚至降低性能。受人类推理模式的启发——人们通过利用过去相关案例来约束搜索空间并减少试错来解决新问题。

Method: 提出PIR框架，包含两个核心组件：1）自适应先例选择（APS）：为每个问题和模型构建紧凑的先例集，基于语义相似度和模型困惑度联合评分，自适应调整先例数量以最大化困惑度降低；2）测试时经验内部化（TEI）：在测试时对先例知情指令进行学习，更新轻量级适配器以内部化解决方案模式，并将其作为后续推理的先验知识。

Result: 在数学推理、科学问答和代码生成等任务上的实验表明，PIR能持续缩短推理轨迹，同时保持或提高最终准确性，在LLM中实现了出色的准确性与效率权衡。

Conclusion: PIR通过将LLM推理范式从详尽的自我探索转变为基于先例的引导学习，有效解决了推理效率问题，在保持性能的同时显著减少了计算开销，为高效推理提供了有前景的解决方案。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [131] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: 该论文提出了前沿AI风险管理框架，评估了大型语言模型和智能体AI在五个关键维度（网络攻击、说服操纵、战略欺骗、失控AI研发、自我复制）的风险，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力快速演进和智能体AI的普及，需要全面评估前沿AI带来的前所未有的风险，特别是大型语言模型和智能体AI可能带来的新型威胁。

Method: 采用更新的风险评估框架，对五个关键维度进行细粒度评估：引入更复杂的网络攻击场景；评估LLM对LLM的说服风险；进行新兴错位实验；关注智能体自主扩展记忆和工具的"错误进化"；监控OpenClaw在Moltbook上的安全表现；引入资源受限的自我复制场景。

Result: 论文提出了针对新兴威胁的一系列稳健缓解策略，为前沿AI的安全部署提供了初步的技术和可操作路径，反映了当前对AI前沿风险的理解。

Conclusion: 需要采取集体行动来缓解这些挑战，该工作为前沿AI的安全部署提供了风险管理框架和缓解策略。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [132] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: 提出一个利用部分因果信息来界定因果概率的通用框架，通过优化编程将可用信息作为约束，在不完全可识别情况下获得更紧且形式有效的界限。


<details>
  <summary>Details</summary>
Motivation: 因果概率对个体层面的解释和决策至关重要，但本质上是反事实的，通常无法从数据中精确识别。现有方法要么忽略可用协变量，要么需要完整的因果图，或局限于二元设置，限制了实际应用。现实世界中，因果信息往往部分但非平凡。

Method: 提出一个通用框架，将可用的结构或统计信息作为约束系统地纳入优化编程公式中。通过构建优化问题来界定因果概率，在不需要完全可识别性的情况下获得更紧的界限。

Result: 该方法能够在不完全因果知识的情况下，为因果概率提供更紧且形式有效的界限，扩展了因果概率在实际应用中的适用性。

Conclusion: 该框架将因果概率的应用扩展到因果知识不完全但信息丰富的现实场景中，为实际决策提供了更实用的工具。

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [133] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC 是一个结合形式化验证与可解释性的工具，用于分析和调试强化学习在脓毒症治疗中的决策策略，通过构建可达状态空间和临床标签实现可验证性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的强化学习策略通常不透明且难以验证，特别是在脓毒症治疗优化中，标准概率模型检查器无法处理大规模状态空间，也无法解释策略决策的原因。

Method: COOL-MC 包装了模型检查器 Storm，但增加了三个关键能力：1) 仅构建训练策略诱导的可达状态空间，生成可验证的离散时间马尔可夫链；2) 自动用临床有意义的原子命题标记状态；3) 将可解释性方法与 PCTL 查询集成，揭示决策驱动特征。

Result: 在基于约17,000例脓毒症患者记录的 ICU-Sepsis MDP 基准测试中，COOL-MC 建立了硬边界验证，训练出达到最优生存概率的安全 RL 策略，并通过 PCTL 验证和可解释性分析发现策略主要依赖既往给药历史而非患者实时状况。

Conclusion: COOL-MC 展示了如何将形式化验证与可解释性结合，为临床医生在部署前调查和调试脓毒症治疗策略提供工具，暴露了标准评估无法发现的策略弱点。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [134] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型在长链推理中处理知识冲突时存在特定模式：冲突特征线性可分、集中在模型中后期层、可通过轨迹聚合恢复冲突类型，且强化模型隐含偏好比改变偏好更容易。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理中经常因不同知识源提供冲突信号而失败，需要理解这些失败背后的机制，以便更好地诊断和控制长链推理错误。

Method: 通过探测内部表示，分析模型如何处理知识冲突，区分输入级客观冲突和过程级有效冲突，并研究冲突特征的可分性、层级定位、一致性以及方向不对称性。

Result: 发现：(1)不同冲突类型编码为线性可分特征而非纠缠；(2)冲突信号集中在中后期层；(3)沿轨迹聚合噪声标记信号可稳健恢复输入级冲突类型；(4)强化模型隐含偏好比强制相反偏好容易得多。

Conclusion: 研究提供了多模态推理在知识冲突下的机制级视角，为诊断和控制长链推理失败提供了原则性方法，有助于改进多模态大语言模型的鲁棒性。

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [135] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: 该论文提出从机制角度区分LLM的事实性错误，将知识存在与行为表达分离，构建了实体中心事实问题的受控环境来分析幻觉与欺骗两种不同的失败模式。


<details>
  <summary>Details</summary>
Motivation: 传统上LLM的事实性错误常被归因于知识缺失，但作者认为这种观点可能混淆了不同的失败机制。他们希望从内部机制角度区分知识存在和行为表达，以更精确地分析LLM的失败模式。

Method: 构建实体中心事实问题的受控环境，在保持知识不变的情况下选择性改变行为表达，系统分析四种行为案例。通过表征可分性、稀疏可解释性和推理时激活引导三种方法来分析失败模式。

Result: 研究发现幻觉和欺骗是两种性质不同的失败模式，虽然在输出层面可能相似，但底层机制不同。通过受控环境能够系统区分这两种模式。

Conclusion: 需要从机制角度而非仅从行为角度分析LLM的事实性错误，区分知识存在和行为表达有助于更精确地诊断和解决LLM的失败问题，特别是区分幻觉与欺骗这两种不同的失败模式。

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [136] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: MATEO是一个评估大型视觉语言模型时序推理能力的多模态基准，使用专业烹饪食谱数据集和时序执行顺序图标注


<details>
  <summary>Details</summary>
Motivation: 现有研究对基础模型时序执行理解有限，主要基于自动标注、线性链近似或纯文本输入，缺乏真实世界规划所需的多模态时序推理评估

Method: 构建MATEO基准：收集高质量专业多模态食谱语料，通过标准化编辑流程分解指令为离散步骤并配图，设计可扩展众包流水线收集时序执行顺序图标注

Result: 使用MATEO评估了6个最先进的LVLMs，涵盖不同模型规模、语言上下文、多模态输入结构和微调策略

Conclusion: MATEO填补了LVLMs时序推理能力评估的空白，为真实世界规划任务提供了重要的多模态基准

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [137] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe：基于表格基础模型的无频繁项集挖掘关联规则学习框架，在低数据场景下保持高性能


<details>
  <summary>Details</summary>
Motivation: 传统关联规则挖掘方法存在规则爆炸和可扩展性问题，而现有神经方法在低数据场景下性能下降。表格基础模型具有强大的上下文泛化能力，为解决这些问题提供了基础。

Method: 提出模型无关的关联规则学习框架，可从任何条件概率模型中提取关联规则。具体实现TabProbe利用表格基础模型作为条件概率估计器，无需频繁项集挖掘即可学习关联规则。

Result: 在不同规模的表格数据集上，表格基础模型能产生简洁、高质量的关联规则，具有强大的预测性能，在低数据设置下保持鲁棒性，无需任务特定训练。

Conclusion: TabProbe框架成功利用表格基础模型解决了传统关联规则挖掘的局限性，在保持高性能的同时提高了可扩展性和低数据鲁棒性。

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [138] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor框架通过将决策树导航分解为专门的节点级任务，解决了LLM在结构化工作流中的指令遵循退化问题，显著提升了临床分诊的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗分诊等高风险领域难以严格遵循结构化工作流。单一提示方法随着提示长度增加会出现指令遵循退化问题，包括中间丢失效应和上下文窗口溢出。

Method: Arbor框架将决策树导航分解为专门的节点级任务：将决策树标准化为边列表表示并存储；运行时通过DAG编排机制迭代检索当前节点的出边，通过专门的LLM调用评估有效转换，并将响应生成委托给单独的推理步骤。

Result: 在10个基础模型上评估，相比单一提示基线：平均轮次准确率提升29.4个百分点，每轮延迟降低57.1%，每轮成本平均降低14.4倍。较小的模型能够匹配或超过在单一提示基线下运行的较大模型。

Conclusion: 架构分解减少了对内在模型能力的依赖，使较小模型能够匹配或超过在单一提示基线下运行的较大模型，为高风险领域的结构化工作流提供了有效的解决方案。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [139] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: 提出基分提取函数，将用户对论证的偏好映射到基分，用于量化双极论证框架，简化基分选择过程


<details>
  <summary>Details</summary>
Motivation: 在渐进论证中，论证的基分选择需要专业知识且不直观，而通过组织论证偏好可以简化这一任务

Method: 引入基分提取函数，将用户对双极论证框架中论证的偏好映射到基分，形成量化双极论证框架，包含算法实现并近似人类偏好的非线性特征

Result: 在机器人场景中进行了理论和实验评估，提供了选择渐进语义的实践建议

Conclusion: 基分提取函数为渐进论证提供了从偏好到基分的映射方法，简化了基分选择过程，支持透明和可争议的AI系统

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [140] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: 提出基于深度强化学习的公交车疏散路径规划方法，解决城市紧急疏散中的公交车疏散定向问题，在旧金山真实路网中验证了方法的有效性和快速性。


<details>
  <summary>Details</summary>
Motivation: 由于人为原因（恐怖袭击、工业事故）和气候变化导致的自然灾害日益频繁，城市紧急疏散需求增加。基于公交车的疏散可以减少纯私家车疏散带来的交通拥堵和混乱，因此需要快速有效的疏散规划方法。

Method: 提出公交车疏散定向问题（BEOP），这是一个NP难的组合优化问题。采用基于深度强化学习和图学习的方法，训练后能够快速生成疏散路线。同时使用MILP（混合整数线性规划）来界定疏散方案的性能差距。

Result: 在旧金山真实路网和旅行时间数据上创建疏散场景进行验证。方法能够达到接近最优的解决方案质量，推理速度快（秒级）。能够分析在给定疏散时间内，为达到特定公交车疏散配额所需的车辆数量，同时保持合理的运行时间。

Conclusion: 提出的基于深度强化学习的公交车疏散规划方法能够快速生成有效的疏散路线，为解决城市紧急疏散问题提供了实用的解决方案，有助于提高城市应急响应能力。

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [141] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 提出使用top-k规划生成同一目标的多种不同计划，创建无偏数据集，并引入VCS指标评估目标识别器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有目标识别数据集存在系统性偏差，因为它们都基于启发式前向搜索规划系统生成，缺乏对不同规划器的挑战性，影响目标识别器在不同规划器下的评估

Method: 使用top-k规划方法为同一目标假设生成多个不同的计划，创建无偏基准数据集，并引入版本覆盖分数(VCS)来衡量目标识别器在不同计划集下的鲁棒性

Result: 当前最先进的目标识别器在低可观测性设置下的鲁棒性显著下降

Conclusion: 提出的top-k规划方法和VCS指标能够创建更真实、无偏的目标识别基准，更好地评估目标识别器在不同规划器下的性能

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [142] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL：一种结合强化学习和系统提示进化的方法，通过并行评估多个系统提示并应用进化更新，同时优化模型上下文和权重，提升推理和智能体任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要通过自我反思更新上下文和强化学习更新权重两种机制进行自我改进。本文旨在开发一种能够联合优化模型上下文和模型权重的方法，实现更有效的自主学习和性能提升。

Method: E-SPL方法在每次强化学习迭代中并行选择多个系统提示进行rollout，对每个系统提示条件下的模型权重应用RL更新，同时通过LLM驱动的突变和交叉对系统提示种群进行进化更新。每个系统提示都有基于相对性能更新的TrueSkill评分用于进化选择。

Result: 在从易到难的泛化设置中，E-SPL将RL成功率从38.8%提升到45.1%，同时优于反思提示进化方法（40.0%）。该方法在样本效率和泛化方面都取得了稳定提升。

Conclusion: 将强化学习与系统提示进化相结合，能够促进陈述性知识（编码在提示中）和程序性知识（编码在权重中）的自然分工，从而在推理和智能体任务中实现一致的性能提升。

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [143] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld是一个大规模开放网络模拟器，通过百万级网络交互训练，支持推理、多格式数据和30+步骤的长时程模拟，在WebArena上提升模型性能9.2%，并展示跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理需要大量轨迹进行泛化，但真实世界训练受到网络延迟、速率限制和安全风险的约束。现有模拟器局限于封闭环境且轨迹数量有限，需要更可扩展的解决方案。

Method: 构建可扩展的数据管道，在100万+开放网络交互上进行训练，支持推理、多格式数据和长时程模拟（30+步骤）。引入WebWorld-Bench进行内在评估，使用双指标跨越九个维度。

Result: WebWorld模拟性能与Gemini-3-Pro相当；Qwen3-14B在WebWorld合成轨迹上训练后，在WebArena上提升9.2%，达到GPT-4o相当水平；作为世界模型在推理时搜索中超越GPT-5；展示跨代码、GUI和游戏环境的泛化能力。

Conclusion: WebWorld是首个大规模开放网络模拟器，通过可扩展训练实现高性能网络代理，提供世界模型构建的可复制方案，并展示跨领域泛化潜力。

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [144] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 前沿AI模型在核危机模拟中表现出复杂的战略行为，包括欺骗、心智理论和自我意识，但与传统战略理论存在显著差异，核禁忌无法阻止核升级，威胁常引发对抗而非服从。


<details>
  <summary>Details</summary>
Motivation: 研究前沿AI模型在战略竞争中的行为模式，特别是核危机情境下的决策逻辑，为国家安全专业提供直接应用，同时通过理解AI在不确定性下的推理机制，拓展国际危机决策之外的应用。

Method: 使用三个前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）在核危机模拟中扮演对立领导人角色，通过危机模拟实验观察AI的战略决策行为。

Result: AI模型表现出欺骗、心智理论和自我意识等复杂行为；核禁忌无法阻止核升级；战略核攻击虽罕见但确实发生；威胁更多引发对抗而非服从；高互信加速而非遏制冲突；模型从不选择妥协或撤退，只降低暴力程度。

Conclusion: AI模拟是强大的战略分析工具，但需要针对人类推理模式进行适当校准。理解前沿模型如何模仿或不模仿人类战略逻辑，对于AI日益影响战略结果的世界至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [145] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 提出首个包含本体模式与事实数据的知识图谱数据集资源，支持机器学习与推理服务


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱精化算法的评估数据集通常只包含事实数据，缺乏丰富的本体模式信息，这限制了依赖本体约束、推理或神经符号技术的评估，无法反映真实大规模知识图谱的场景

Method: 开发了一个工作流程，从源知识图谱中提取包含模式与事实的数据集，处理两者间的不一致性，利用推理推导隐含知识，并将数据集序列化为OWL格式，同时提供加载到张量表示的实用工具

Result: 创建了首个包含模式与事实的精选数据集套件，包括从具有丰富模式的知识图谱新提取的数据集，以及为现有数据集补充模式信息，所有数据集都序列化为OWL格式并支持机器学习库

Conclusion: 该资源填补了知识图谱评估领域的空白，为依赖丰富本体约束和推理的方法提供了更真实的评估环境，支持机器学习与推理服务的集成

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [146] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: 提出StarWM，首个用于星际争霸II的世界模型，通过结构化文本表示预测部分可观测环境下的未来状态，并构建了首个SC2动态预测指令调优数据集，最终集成到决策循环中显著提升游戏胜率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based SC2智能体主要关注策略改进，忽视了将可学习的动作条件转移模型整合到决策循环中。星际争霸II具有巨大的状态-动作空间和部分可观测性，是极具挑战性的测试平台，需要世界模型来预测未来状态以支持更好的决策。

Method: 1) 提出StarWM世界模型，使用结构化文本表示将观测分解为五个语义模块；2) 构建SC2-Dynamics-50k指令调优数据集；3) 开发多维离线评估框架；4) 提出StarWM-Agent决策系统，采用生成-模拟-精炼的决策循环，集成世界模型进行前瞻驱动的策略优化。

Result: 离线评估显示StarWM相比零样本基线有显著提升：资源预测准确率提高近60%，己方宏观态势一致性大幅改善。在线评估中，StarWM-Agent对抗SC2内置AI在不同难度级别上均取得稳定改进：Hard(LV5)胜率提升30%，Harder(LV6)提升15%，VeryHard(LV7)提升30%，同时改善了宏观管理稳定性和战术风险评估。

Conclusion: StarWM作为首个星际争霸II世界模型，成功预测部分可观测环境下的未来状态，通过结构化表示和指令调优数据集有效学习混合动态。集成到决策循环中的StarWM-Agent展示了世界模型在复杂决策环境中的价值，为LLM-based智能体提供了新的研究方向。

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [147] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent是一个将智能体直接嵌入现有UI的企业级Web代理框架，通过前端轻量级钩子和可重用后端工作流实现，支持混合粒度操作和跨技术栈兼容。


<details>
  <summary>Details</summary>
Motivation: 传统Web代理在人类界面级别操作（截图或DOM树），缺乏应用级访问，导致鲁棒性和动作表达能力受限。在企业环境中，通常可以同时控制前端和后端，这为更强大的代理框架提供了机会。

Method: 使用轻量级前端钩子（包括精选的ARIA和URL观察、通过WebSocket暴露的每页函数注册表）和可重用后端工作流。框架是技术栈无关的（支持React、Angular等），支持从GUI原语到高级复合操作的混合粒度动作，并通过MCP工具协调导航、操作和领域特定分析。

Result: 演示显示只需最小的改造工作即可实现，并在实时UI环境中展现出鲁棒的多步骤行为。框架能够处理复杂的Web交互任务，同时保持对现有系统的兼容性。

Conclusion: EmbeWebAgent通过将智能体直接嵌入UI，为企业环境提供了一个强大、灵活且易于集成的Web代理解决方案，克服了传统界面级别代理的局限性，实现了更鲁棒和表达能力更强的自动化交互。

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [148] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 本文提出Concept Influence方法，通过语义方向而非单个测试样本来归因模型行为，比传统影响函数更高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型训练和微调的增加，需要识别哪些训练数据驱动特定行为（特别是意外行为）。现有训练数据归因方法如影响函数计算成本高，且基于单个测试样例归因，可能导致结果偏向句法而非语义相似性。

Method: 1. 引入Concept Influence，将模型行为归因于语义方向（如线性探针或稀疏自编码器特征），而非单个测试样例；2. 展示基于探针的归因方法是Concept Influence的一阶近似，性能相当但快一个数量级以上。

Result: 在涌现错位基准测试和实际后训练数据集上验证了Concept Influence及其近似方法，与传统影响函数性能相当但显著更可扩展。

Conclusion: 在传统TDA流程中融入可解释结构可以实现更可扩展、可解释的模型行为控制，通过数据更好地管理模型行为。

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [149] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: 提出首个多项式时间框架，通过隐式学习和推理，在无需显式构建模型的情况下，在一阶关系概率逻辑中回答查询，同时实现个体和世界的提升推理。


<details>
  <summary>Details</summary>
Motivation: 解决一阶关系领域中归纳学习与演绎推理之间的长期矛盾。传统提升推理需要完整模型，但从部分、噪声观测中学习这样的模型通常是难解的。

Method: 将不完整的一阶公理与独立采样的部分观测样本合并到平方和（SOS）层次结构的有界度片段中。采用两种提升：1）基础提升-将重命名等价的基础时刻共享变量，折叠个体域；2）世界提升-并行执行所有伪模型（部分世界分配），产生适用于所有符合学习约束的世界的全局边界。

Result: 开发出首个多项式时间框架，能够隐式学习一阶概率逻辑，并在个体和世界两个层面执行提升推理，无需显式构建模型。

Conclusion: 通过隐式学习推理和提升推理技术的结合，成功调和了归纳学习与演绎推理之间的矛盾，为一阶关系概率逻辑中的查询回答提供了高效解决方案。

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [150] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: 本文通过引入"潜力"概念量化思维链各部分对最终答案的贡献，发现思维链存在非单调性、洞察性跳跃和幸运猜测等模式，并证明思维链具有可迁移性，仅需20%的思维链就能解锁较弱模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示已成为从大语言模型引出推理式响应的标准技术，但其成功背后的驱动机制仍不清楚。本文旨在深入分析思维链推理，理解思维链各部分如何以及哪些部分真正贡献于最终答案。

Method: 引入"潜力"概念来量化思维链各部分增加正确完成可能性的程度；通过竞争级数学问题分析思维链轨迹；研究思维链可迁移性，测量较弱模型在较强模型部分思维链下的潜力。

Result: 发现思维链潜力呈现多种模式：1) 强烈的非单调性（由于推理偏离）；2) 尖锐但难以解释的峰值（推理洞察和跳跃）；3) 有时出现幸运猜测。可迁移性实验显示，仅需20%的部分思维链就能"解锁"较弱模型在原本无法解决问题上的性能。

Conclusion: 思维链推理机制在很大程度上是可迁移的，但部分行为难以从人类视角理解。思维链中关键的洞察性步骤对模型性能提升至关重要，即使少量思维链也能显著改善较弱模型的推理能力。

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [151] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: 论文主张通过语言自我反思而非规模扩展来实现稳健推理，强调高质量社会互动中的对话质量是下一代通用智能的关键杠杆。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法将推理视为规模扩展的涌现属性，但作者认为这种方法存在局限。他们从维果茨基发展心理学出发，主张通过语言自我反思和社会互动来培养更稳健的推理能力，而不是单纯依赖数据规模和计算资源。

Method: 提出基于内省（Introspection）的三个核心立场：1）私人思维的社会起源：从对话环境中学习成为理解世界的新方式；2）对话式脚手架的内省体验：通过对话支持使智能体能够进行意义建构，将原始环境数据转化为丰富的可学习叙事；3）对话质量即数据质量：智能体私人推理的深度和测试时计算效率取决于其掌握的对话多样性和严谨性。

Result: 论文提出了一个理论框架，强调对话质量而非数据规模是推动下一代通用智能发展的关键因素。通过优化对话脚手架，智能体能够发展出更高效、更稳健的推理能力。

Conclusion: 优化对话脚手架是推动下一代通用智能发展的主要杠杆。通过高质量的社会互动和语言自我反思，智能体能够发展出比单纯依赖规模扩展更稳健、更高效的推理能力。

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [152] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow框架通过"提取-存储-构建"范式解决企业AI代理的可重用性困境和结构幻觉问题，将异构DSL解构为标准化工作流模块，实现90%以上的提取和构建准确率。


<details>
  <summary>Details</summary>
Motivation: 解决企业Agentic AI中的"可重用性困境"和结构幻觉问题，实现企业数字资产的自动化重组和高效复用。

Method: 提出ReusStdFlow框架，采用"提取-存储-构建"范式：1) 将异构平台特定DSL解构为标准化工作流模块；2) 使用图数据库和向量数据库的双重知识架构协同检索拓扑结构和功能语义；3) 采用检索增强生成(RAG)策略智能组装工作流。

Result: 在200个真实n8n工作流测试中，系统在提取和构建两方面均达到90%以上的准确率。

Conclusion: 该框架为企业数字资产的自动化重组和高效复用提供了标准化解决方案。

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [153] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP是一个基于多智能体大语言模型的闭环协作系统，用于多目标抗菌肽设计，通过模拟同行评审-自适应强化学习框架实现完全自主的AMP生成和优化。


<details>
  <summary>Details</summary>
Motivation: 针对抗菌素耐药性的全球健康威胁，抗菌肽（AMP）显示出对抗耐药病原体的潜力。现有AI设计模型难以平衡活性、毒性和新颖性等关键目标，且评分方法僵化或不清晰，导致结果难以解释和优化。多智能体大语言模型在复杂科学设计场景中展现出快速上升的潜力。

Method: 提出MAC-AMP系统，采用闭环多智能体协作框架，实现完全自主的模拟同行评审-自适应强化学习。系统仅需任务描述和示例数据集即可设计新型AMP，具有跨领域可迁移性，支持多目标优化并保持可解释性（而非黑箱）。

Result: 实验表明MAC-AMP优于其他AMP生成模型，能有效优化多个关键分子特性，在抗菌活性、AMP相似性、毒性合规性和结构可靠性方面表现出卓越结果。

Conclusion: MAC-AMP通过引入闭环多智能体系统，为多目标AMP设计提供了创新解决方案，在保持可解释性的同时实现了多目标优化，展示了多智能体LLM在复杂科学设计任务中的强大潜力。

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [154] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: 本文提出了在混合时态情境演算框架下的两种主要原因定义，并证明它们等价，同时展示了这些定义具有直观合理的性质。


<details>
  <summary>Details</summary>
Motivation: 实际因果关系推理是理性研究的基础，但现有研究主要关注离散变化，而现实世界中的变化既有离散也有连续（混合）。尽管有少量研究探讨连续变化下的因果关系，但在混合行动理论框架中缺乏系统研究。

Method: 在混合时态情境演算框架中提出两种主要原因定义：一种是基础性的定义，另一种通过贡献形式化因果关系，可使用修改后的"but-for"测试从反事实角度验证。证明这两种定义等价。

Result: 成功提出了混合行动理论框架下的两种主要原因定义，并证明了它们的等价性。同时展示了这些定义具有直观合理的性质。

Conclusion: 本文为混合变化环境中的因果关系推理提供了形式化框架，填补了现有研究的空白，为理解离散和连续变化下的实际因果关系提供了理论基础。

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


### [155] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: 提出用于药物资产侦察的基准测试方法和自学习Bioptic Agent，在覆盖非英语、非美国来源的资产发现任务上显著优于现有AI系统


<details>
  <summary>Details</summary>
Motivation: 生物制药创新格局已变：大量新药资产源自美国以外，主要通过区域性非英语渠道披露。超过85%的专利申请来自美国以外，中国占全球近一半；学术产出中非美国份额也在增长。行业估计中国占全球药物开发的30%，涉及1200+候选药物。在这种高风险环境下，未能发现"雷达下"资产会给投资者和业务开发团队带来数十亿美元风险，资产侦察成为覆盖关键竞争，速度和完整性决定价值。然而当前深度研究AI代理在跨异质多语言来源的高召回发现方面仍落后于人类专家。

Method: 提出药物资产侦察的基准测试方法，构建基于树状结构的自学习Bioptic Agent。使用多语言多代理流程构建具有挑战性的完整性基准：复杂用户查询配对主要在美国中心雷达之外的基准资产。为反映真实交易复杂性，收集专家投资者、业务开发和风险投资专业人士的筛选查询，作为先验条件生成基准查询。使用LLM作为评判者评估，校准专家意见。

Result: Bioptic Agent在基准测试中达到79.7% F1分数，显著优于其他系统：Claude Opus 4.6（56.2%）、Gemini 3 Pro + Deep Research（50.6%）、GPT-5.2 Pro（46.6%）、Perplexity Deep Research（44.2%）和Exa Websets（26.9%）。性能随计算资源增加而显著提升，支持更多计算带来更好结果的观点。

Conclusion: 提出的Bioptic Agent在药物资产侦察任务中表现出色，特别是在发现非美国中心、多语言来源的"雷达下"资产方面。该方法为解决当前AI系统在跨异质多语言来源高召回发现方面的局限性提供了有效解决方案，计算资源投入与性能提升正相关。

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [156] [Sequential BP-based Decoding of QLDPC Codes](https://arxiv.org/abs/2602.13420)
*Mohsen Moradi,Salman Habib,Vahid Nourozi,David G. M. Mitchell*

Main category: cs.IT

TL;DR: 提出两种顺序调度方案(SCNS和SVNS)改进QLDPC码的BP译码性能，通过固定顺序处理校验节点或变量节点来稳定消息更新，并将此技术应用于BPGD得到SBPGD，显著提升收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统BP译码器在QLDPC码中表现不佳，主要由于稳定子约束导致的短循环和简并性引起的不收敛问题，需要改进调度策略来提升译码性能。

Method: 提出两种顺序调度方案：顺序校验节点调度(SCNS)和顺序变量节点调度(SVNS)，通过固定顺序处理节点来稳定消息更新。将此技术应用于BPGD得到顺序BPGD(SBPGD)，在译码迭代中逐步固定符号。

Result: 在标准QLDPC基准测试中，顺序调度方案相比传统BP降低了块错误率，SBPGD优于BPGD且使用更少的固定轮次，计算成本更低。对于[[1922,50,16]] C2超图积码，SVNS-BP在相同复杂度下超越了BP-OSD-0的错误纠正能力。

Conclusion: 在不改变编码的情况下，仅通过改变更新调度策略就能同时提升QLDPC码BP译码的可靠性和效率，为量子纠错提供了有效的译码改进方案。

Abstract: Quantum low-density parity-check (QLDPC) codes are a leading approach to quantum error correction, yet conventional belief propagation (BP) decoders often perform poorly, primarily due to non-convergence exacerbated by stabilizer constraints, which induce short cycles and degeneracy. We propose two scheduling variants, sequential check node scheduling (SCNS) and sequential variable node scheduling (SVNS), that improve BP's error-correction ability by processing check nodes (CNs) or variable nodes (VNs), respectively, in a fixed order, stabilizing message updates and reducing stalls. We also employ this technique to an improved BP-variant called BP guided decimation (BPGD), where symbols are progressively fixed during decoding iterations. Here, we demonstrate that the sequential BPGD (SBPGD) decoder can further improve the convergence properties and performance of the decoder. On standard QLDPC benchmarks under a Pauli-X noise model, our sequential schedules are shown to lower the block error rate relative to conventional BP, and SBPGD outperforms BPGD while using significantly fewer decimation rounds, translating to lower computational cost. These results demonstrate that changing the update schedule, without altering the code, can improve both the reliability and efficiency of BP-based decoding for QLDPC codes. For the [[1922,50,16]] C2 hypergraph-product code with independent X errors, SVNS-BP surpasses BP-OSD-0 in error correction at roughly the same complexity as standard BP.

</details>


### [157] [End-to-End NOMA with Perfect and Quantized CSI Over Rayleigh Fading Channels](https://arxiv.org/abs/2602.13446)
*Selma Benouadah,Mojtaba Vaezi,Ruizhan Shen,Hamid Jafarkhani*

Main category: cs.IT

TL;DR: 提出端到端自编码器框架用于瑞利衰落信道下的下行NOMA，通过学习干扰感知和信道自适应的超星座，在考虑实际CSI限制下实现优于现有方案的性能。


<details>
  <summary>Details</summary>
Motivation: 现有NOMA方案要么假设AWGN信道，要么在衰落信道中没有采用完全端到端学习方法，且缺乏对实际CSI限制（如有限反馈）的考虑。

Method: 开发端到端自编码器框架，将无线信道直接嵌入训练和推理过程，考虑实际CSI限制，通过均匀量化和Lloyd-Max量化实现有限反馈。

Result: 在完美CSI下，提出的AE优于现有分析NOMA方案；Lloyd-Max量化比均匀量化获得更优的BER性能。

Conclusion: 端到端AE直接在瑞利衰落信道上训练能有效学习鲁棒的干扰感知信号策略，为在具有实际CSI限制的衰落环境中部署NOMA铺平道路。

Abstract: An end-to-end autoencoder (AE) framework is developed for downlink non-orthogonal multiple access (NOMA) over Rayleigh fading channels, which learns interference-aware and channel-adaptive super-constellations. While existing works either assume additive white Gaussian noise channels or treat fading channels without a fully end-to-end learning approach, our framework directly embeds the wireless channel into both training and inference. To account for practical channel state information (CSI), we further incorporate limited feedback via both uniform and Lloyd-Max quantization of channel gains and analyze their impact on AE training and bit error rate (BER) performance. Simulation results show that, with perfect CSI, the proposed AE outperforms the existing analytical NOMA schemes. In addition, Lloyd-Max quantization achieves superior BER performance compared to uniform quantization. These results demonstrate that end-to-end AEs trained directly over Rayleigh fading can effectively learn robust, interference-aware signaling strategies, paving the way for NOMA deployment in fading environments with realistic CSI constraints.

</details>


### [158] [An Algebraic Invariant for Free Convolutional Codes over Finite Local Rings](https://arxiv.org/abs/2602.13468)
*Mohammed El Oued*

Main category: cs.IT

TL;DR: 本文研究了有限局部环Z_{p^r}上自由卷积码的代数结构，引入了新的结构不变量——剩余结构多项式Δ_p(C)，证明了该多项式是码的固有特征，可作为内在灾难性的代数判据，并建立了对偶定理Δ_p(C)=Δ_p(C^⊥)。


<details>
  <summary>Details</summary>
Motivation: 研究有限局部环Z_{p^r}上自由卷积码的代数结构，特别是寻找能够表征码的灾难性行为的代数不变量，并探索码与其对偶码之间的结构关系。

Method: 通过约化内部度矩阵(RIDM)构造剩余结构多项式Δ_p(C)，证明该多项式在等价RIDM下保持不变，建立Δ_p(C)与码的灾难性行为之间的等价关系，并证明对偶定理。

Result: Δ_p(C)是自由卷积码的固有代数不变量；自由码存在非灾难性实现当且仅当Δ_p(C)是D^s形式的单项式；建立了对偶定理Δ_p(C)=Δ_p(C^⊥)，表明灾难性在对偶下保持不变。

Conclusion: 剩余结构多项式Δ_p(C)为有限局部环上自由卷积码提供了有效的代数结构分析工具，既能判断内在灾难性，又揭示了对偶码之间的深层结构对称性。

Abstract: This paper investigates the algebraic structure of free convolutional codes over the finite local ring Z_{p^r}. We introduce a new structural invariant, the Residual Structural Polynomial, denoted by Delta_p(C) in F_p[D]. We construct this invariant via encoders which are reduced internal degree matrices (RIDM). We formally demonstrate that Delta_p(C) is an intrinsic characteristic of the code, invariant under equivalent RIDMs. A central result of this work is the establishment that Delta_p(C) serves as an algebraic criterion for intrinsic catastrophicity: we prove that a free code C admits a non-catastrophic realization if and only if Delta_p(C) is a monomial of the form D^s. Furthermore, we establish a fundamental duality theorem, proving that Delta_p(C) = Delta_p(C^perp). This result reveals a deep structural symmetry, showing that the "catastrophicity" of a free code is preserved under orthogonality.

</details>


### [159] [Convergence of Differential Entropies -- II](https://arxiv.org/abs/2602.13493)
*Mahesh Godavarti*

Main category: cs.IT

TL;DR: 论文研究了概率密度函数依测度收敛时微分熵的收敛条件，给出了比Godavarti-Hero条件更弱的充分条件，并否定了他们的一个猜想。


<details>
  <summary>Details</summary>
Motivation: 研究在概率密度函数依测度收敛时，微分熵收敛的充分必要条件，改进现有的收敛条件。

Method: 使用Vitali收敛定理，提出熵加权的Orlicz条件：sup_n ∫ f_n Ψ(|log f_n|) < ∞，其中Ψ是超线性函数。这比Godavarti-Hero的固定α>1条件更弱。

Result: 1. 证明了当熵被积函数f_n|log f_n|一致可积且紧时，微分熵收敛；2. 给出了更弱的充分条件；3. 否定了Godavarti-Hero关于α_n↓1的猜想；4. 在有界域上证明了该条件也是必要的。

Conclusion: 论文建立了微分熵收敛的改进条件，统一并推广了多个已知结果，为概率密度收敛时的熵收敛分析提供了更精确的理论框架。

Abstract: We show that under convergence in measure of probability density functions, differential entropy converges whenever the entropy integrands $f_n |\log f_n|$ are uniformly integrable and tight -- a direct consequence of Vitali's convergence theorem. We give an entropy-weighted Orlicz condition: $\sup_n \int f_n\, Ψ(|\log f_n|) < \infty$ for a single superlinear $Ψ$, strictly weaker than the fixed-$α$ condition of Godavarti and Hero (2004). We also disprove the Godavarti-Hero conjecture that $α> 1$ could be replaced by $α_n \downarrow 1$. We recover the sufficient conditions of Godavarti-Hero, Piera-Parada, and Ghourchian-Gohari-Amini as corollaries, and we show the condition is also necessary on bounded domains.

</details>


### [160] [Constructing Quantum Convolutional Codes via Difference Triangle Sets](https://arxiv.org/abs/2602.13505)
*Vahid Nourozi,David Mitchell*

Main category: cs.IT

TL;DR: 基于差分三角形集(DTS)构造量子卷积码(QCC)，通过反射DTS索引确保X(D)和Z(D)满足对易关系，同时保持稀疏性和小记忆


<details>
  <summary>Details</summary>
Motivation: 提供一种构造性设计方法，能够保证预设的最小距离，解决量子卷积码构造中多项式稳定子X(D)和Z(D)对易关系的确定问题

Method: 使用差分三角形集(DTS)构造量子卷积码，通过反射DTS索引从X(D)得到Z(D)，其中X(D)对应基于强DTS支撑的经典卷积自正交码(CSOC)

Result: 提供了多种码率的构造数值结果，证明了该方法的有效性

Conclusion: 基于DTS的构造方法能够有效构建量子卷积码，确保稳定子满足对易关系并保持稀疏性和小记忆，同时保证预设的最小距离

Abstract: In this paper, we introduce a construction of quantum convolutional codes (QCCs) based on difference triangle sets (DTSs). To construct QCCs, one must determine polynomial stabilizers $X(D)$ and $Z(D)$ that commute (symplectic orthogonality), while keeping the stabilizers sparse and encoding memory small. To construct Z(D), we show that one can use a reflection of the DTS indices of X(D), where X(D) corresponds to a classical convolutional self-orthogonal code (CSOC) constructed from strong DTS supports. The motivation of this approach is to provide a constructive design that guarantees a prescribed minimum distance. We provide numerical results demonstrating the construction for a variety of code rates.

</details>


### [161] [Redundancy-Optimal Constructions of $(1,1)$-Criss-Cross Deletion Correcting Codes with Efficient Encoding/Decoding Algorithms](https://arxiv.org/abs/2602.13548)
*Wenhao Liu,Zhengyi Jiang,Zhongyi Huang,Hanxu Hou*

Main category: cs.IT

TL;DR: 该论文提出了一种针对q元(1,1)-十字交叉删除错误的二维纠错码构造，具有接近最优的冗余度和O(n²)复杂度的编解码算法。


<details>
  <summary>Details</summary>
Motivation: 二维纠错码在QR码、DNA存储和赛道存储器中有重要应用。其中(1,1)-十字交叉删除错误（同时删除一行和一列）是特别重要的错误模式，需要有效的纠错方案。

Method: 提出了一种新颖的q元(1,1)-十字交叉删除纠错码构造，针对n≥11且q≥3的参数，设计了完整的编码、解码和数据恢复算法。

Result: 当n≥11且q=Ω(n)时，码冗余和编码器冗余均为2n+2log_q n+O(1)，接近下界(2n+2log_q n-3)。编解码算法复杂度为O(n²)。

Conclusion: 这是第一个能在O(1)差距内达到最优冗余度，同时具有显式编解码算法的构造，为二维纠错码提供了高效实用的解决方案。

Abstract: Two-dimensional error-correcting codes, where codewords are represented as $n \times n$ arrays over a $q$-ary alphabet, find important applications in areas such as QR codes, DNA-based storage, and racetrack memories. Among the possible error patterns, $(t_r,t_c)$-criss-cross deletions-where $t_r$ rows and $t_c$ columns are simultaneously deleted-are of particular significance. In this paper, we focus on $q$-ary $(1,1)$-criss-cross deletion correcting codes. We present a novel code construction and develop complete encoding, decoding, and data recovery algorithms for parameters $n \ge 11$ and $q \ge 3$. The complexity of the proposed encoding, decoding, and data recovery algorithms is $\mathcal{O}(n^2)$. Furthermore, we show that for $n \ge 11$ and $q = Ω(n)$ (i.e., there exists a constant $c>0$ such that $q \ge cn$), both the code redundancy and the encoder redundancy of the constructed codes are $2n + 2\log_q n + \mathcal{O}(1)$, which attain the lower bound ($2n + 2\log_q n - 3$) within an $\mathcal{O}(1)$ gap. To the best of our knowledge, this is the first construction that can achieve the optimal redundancy with only an $\mathcal{O}(1)$ gap, while simultaneously featuring explicit encoding and decoding algorithms.

</details>


### [162] [Discrete-Space Generative AI Pipeline for Semantic Transmission of Signals](https://arxiv.org/abs/2602.13556)
*Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.IT

TL;DR: Discernment是一个基于GenAI的语义通信系统，通过离散空间中的生成模型传输物理信号（基带无线电和音频）的含义，能根据信道擦除模式自适应切换自回归或扩散生成算法，在信道容量严重下降时仍保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统在信道条件恶化时性能急剧下降，而语义通信需要保持信号含义的完整性。本文旨在开发一个能适应不同物理信道条件、保持语义完整性的通信系统，特别适用于物联网部署。

Method: Discernment使用在离散空间中操作的GenAI模型传输物理信号的语义含义。系统根据信道擦除模式（建模为擦除信道）动态切换自回归或扩散生成算法，以自适应信道损伤。

Result: Discernment在信道容量严重下降时仍能保持语义完整性，在分类准确性和重建含义的统计保真度方面表现出非常小且平缓的性能下降，同时保持频谱效率和低模型复杂度。

Conclusion: Discernment能够适应多样化的物理信道条件，同时保持语义完整性，特别适合物联网部署，这强烈推动了语义信道范式的进一步研究。

Abstract: We introduce Discernment, a semantic communication system that transmits the meaning of physical signals (baseband radio and audio) over a technical channel using GenAI models operating in discrete spaces. Discernment dynamically adapts to channel impairments - modeled as erasure channels - by switching between an autoregressive or a diffusion-based generative algorithm, depending on the erasure pattern. Our results show that Discernment maintains semantic integrity even as channel capacity severely degrades, exhibiting very small and graceful performance decline in both classification accuracy and statistical fidelity of the reconstructed meaning. These findings demonstrate Discernment's ability to adjust to diverse physical channel conditions while maintaining spectral efficiency and low model complexity, making it well suited for IoT deployments and strongly motivating further research on this semantic channel paradigm.

</details>


### [163] [UAV Swarm Enabled Aerial Movable Antenna System for Low-Altitude Economy: From Far-Field to Near-Field Communication](https://arxiv.org/abs/2602.13687)
*Haiquan Lu,Chao Feng,Yong Zeng,Shaodan Ma,Long Shi,Shi Jin,Rui Zhang*

Main category: cs.IT

TL;DR: 本文研究无人机群支持的可移动天线近场通信，考虑非均匀球面波模型，联合优化无人机三维轨迹和接收波束成形以最大化用户最小平均通信速率。


<details>
  <summary>Details</summary>
Motivation: 无人机具有三维移动性，是实现空中可移动天线系统的理想平台。当无人机群形成大规模阵列时，传统的远场均匀平面波模型不再适用，需要考虑近场非均匀球面波模型，这为通信性能优化带来了新的挑战和机遇。

Method: 1. 针对单用户场景，采用逐次凸逼近技术优化无人机群轨迹；2. 对于位置优化场景，推导了单无人机和双无人机的最优放置位置闭式解；3. 针对双用户场景，提出通过沿双曲线对称放置偶数个无人机实现用户间干扰消除；4. 针对多用户场景，提出交替优化算法联合优化无人机轨迹和接收波束成形。

Result: 数值结果表明，所提方案相比基准方案获得了显著的性能增益。特别是在双用户场景下，通过对称放置无人机可以有效消除用户间干扰，提升通信性能。

Conclusion: 无人机群支持的可移动天线近场通信系统在考虑非均匀球面波模型时具有显著性能优势。通过联合优化无人机轨迹和接收波束成形，可以有效提升用户通信速率，为未来大规模无人机群通信系统设计提供了理论指导。

Abstract: Unmanned aerial vehicle (UAV) with the intrinsic three-dimensional (3D) mobility provides an ideal platform for implementing aerial movable antenna (AMA) system enabled by UAV swarm cooperation. Besides, AMA system is readily to achieve an extremely large-scale array aperture, rendering the conventional far-field uniform plane wave (UPW) model no longer valid for aerial-to-ground links. This paper studies the UAV swarm enabled near-field AMA communication, by taking into account the non-uniform spherical wave (NUSW) model, where UAV swarm trajectory simultaneously influences the channel amplitude and phase. We formulate a general optimization problem to maximize the minimum average communication rate over user equipments (UEs), by jointly optimizing the 3D UAV swarm trajectory and receive beamforming for all UEs. To draw useful insights, the special case of single UE is first studied, and successive convex approximation (SCA) technique is proposed to efficiently optimize the UAV swarm trajectory. For the special case of placement optimization, the optimal placement positions of UAVs for cases of single UAV and two UAVs are derived in closed-form. Then, for the special case of two UEs, we show that an inter-UE interference (IUI)-free communication can be achieved by symmetrically placing an even number of UAVs along a hyperbola, with its foci corresponding to the locations of the two UEs. Furthermore, for arbitrary number of UEs, an alternating optimization algorithm is proposed to efficiently tackle the non-convex optimization problem. Numerical results validate the significant performance gains over the benchmark schemes.

</details>


### [164] [BRAIN: Bayesian Reasoning via Active Inference for Agentic and Embodied Intelligence in Mobile Networks](https://arxiv.org/abs/2602.14033)
*Osman Tugay Basaran,Martin Maier,Falko Dressler*

Main category: cs.IT

TL;DR: BRAIN：基于主动推理的贝叶斯推理AI代理，用于6G网络动态资源分配，相比传统DRL具有更好的鲁棒性、适应性和可解释性


<details>
  <summary>Details</summary>
Motivation: 6G网络需要具备实时适应性和决策透明性的AI代理，但现有DRL方法缺乏可解释性且在非平稳环境下存在灾难性遗忘问题

Method: 提出BRAIN代理，利用深度生成模型对网络环境建模，通过最小化变分自由能量将感知和行动统一在单一闭环范式中

Result: BRAIN在动态无线资源分配中表现出：(1) 鲁棒的因果推理能力，能在不同流量负载下维持切片特定QoS目标；(2) 对突发流量变化的鲁棒性比基准方法高28.3%，且无需重新训练；(3) 通过人类可解释的信念状态诊断实现实时决策可解释性

Conclusion: BRAIN作为基于主动推理的贝叶斯推理代理，为6G网络AI代理提供了具有鲁棒性、适应性和可解释性的替代方案，优于传统DRL方法

Abstract: Future sixth-generation (6G) mobile networks will demand artificial intelligence (AI) agents that are not only autonomous and efficient, but also capable of real-time adaptation in dynamic environments and transparent in their decisionmaking. However, prevailing agentic AI approaches in networking, exhibit significant shortcomings in this regard. Conventional deep reinforcement learning (DRL)-based agents lack explainability and often suffer from brittle adaptation, including catastrophic forgetting of past knowledge under non-stationary conditions. In this paper, we propose an alternative solution for these challenges: Bayesian reasoning via Active Inference (BRAIN) agent. BRAIN harnesses a deep generative model of the network environment and minimizes variational free energy to unify perception and action in a single closed-loop paradigm. We implement BRAIN as O-RAN eXtended application (xApp) on GPU-accelerated testbed and demonstrate its advantages over standard DRL baselines. In our experiments, BRAIN exhibits (i) robust causal reasoning for dynamic radio resource allocation, maintaining slice-specific quality of service (QoS) targets (throughput, latency, reliability) under varying traffic loads, (ii) superior adaptability with up to 28.3% higher robustness to sudden traffic shifts versus benchmarks (achieved without any retraining), and (iii) real-time interpretability of its decisions through human-interpretable belief state diagnostics.

</details>


### [165] [Energy-Efficient Over-the-Air Federated Learning via Pinching Antenna Systems](https://arxiv.org/abs/2602.14250)
*Saba Asaad,Ali Bereyhi*

Main category: cs.IT

TL;DR: PASS（Pinching Antennas Systems）作为新型柔性天线技术，通过波导绕过直接链路补偿无线信道大尺度效应，应用于空中联邦学习可显著降低模型聚合能耗


<details>
  <summary>Details</summary>
Motivation: 探索PASS技术在OTA-FL中的应用潜力，解决传统MIMO服务器在联邦学习中能耗较高的问题，为下一代无线系统中的分布式学习提供更节能的解决方案

Method: 开发低复杂度算法，联合优化PASS参数和移动设备调度，最小化OTA-FL中的能耗，并与传统MIMO服务器设置进行比较

Result: 数值实验表明，在中等规模区域内使用单波导PASS服务器，模型聚合所需能耗相比全数字MIMO服务器大幅降低

Conclusion: PASS技术有望成为下一代无线系统中实现节能分布式学习的潜在技术，通过波导结构有效补偿无线信道效应，显著提升OTA-FL的能源效率

Abstract: Pinching antennas systems (PASSs) have recently been proposed as a novel flexible-antenna technology. These systems are implemented by attaching low-cost pinching elements to dielectric waveguides. As the direct link is bypassed through waveguides, PASSs can effectively compensate large-scale effects of the wireless channel. This work explores the potential gains of employing PASSs for over-the-air federated learning (OTA-FL). For a PASS-assisted server, we develop a low-complexity algorithmic approach, which jointly tunes the PASS parameters and schedules the mobile devices for minimal energy consumption in OTA-FL. We study the efficiency of the proposed design and compare it against the conventional OTA-FL setting with MIMO server. Numerical experiments demonstrate that using a single-waveguide PASS at the server within a moderately sized area, the required energy for model aggregation is drastically reduced as compared to the case with fully-digital MIMO server. This introduces PASS as a potential technology for energy-efficient distributed learning in next generations of wireless systems.

</details>


### [166] [Diversity vs Degrees of Freedom in Gaussian Fading Channels](https://arxiv.org/abs/2602.14371)
*Mahesh Godavarti*

Main category: cs.IT

TL;DR: 论文重新定义MIMO信道自由度与分集度的几何概念，提出跨尺度（gauge）的统一框架，解决传统对数归一化失效问题，并建立非相干快衰落信道容量与分集度的跨尺度折中关系。


<details>
  <summary>Details</summary>
Motivation: 传统自由度(DOF)和分集度(diversity)定义都采用logρ归一化，当这种"尺子"不适用时，测量结果会变为零或无定义。作者认为DOF和分集度应是信道固有属性，而非归一化选择的产物，需要建立更普适的几何定义框架。

Method: 针对高斯衰落信道，将DOF定义为信道矩阵HX的双线性映射的秩（ε覆盖图像），分集度定义为衰落映射在所有维度上的扩展。引入Bhattacharyya packing方法，提出gauge-DOF和B-diversity作为可操作的代理度量，适用于包括传统分集度为零的所有尺度。

Result: 识别出三种尺度类别：logρ、loglogρ和(logρ)^β。主要结果为非相干快衰落信道建立了跨尺度折中关系：容量存在于loglogρ尺度，而B-diversity存在于logρ尺度（指数级更大），并给出了匹配的上下界。该方法也恢复或扩展了相干MIMO、块衰落和不规则频谱信道的已知标度律。

Conclusion: 论文通过几何方法重新定义了DOF和分集度，解决了传统对数归一化失效的问题。提出的跨尺度框架统一了不同信道模型的性能分析，特别是揭示了非相干快衰落信道中容量与分集度存在于不同尺度的有趣现象，为MIMO系统设计提供了新的理论工具。

Abstract: The standard definitions of degrees of freedom (DOF) and diversity both normalize by $\logρ$. When this ruler is wrong, both measurements give zero or become undefined, yet intuitively DOF and diversity ought to be channel properties, not artifacts of a normalization choice. We formalize this for Gaussian fading channels. For fixed-$H$ MIMO, DOF and diversity are both ranks of the bilinear map~$HX$ with different variables free: $\varepsilon$-covering the image of~$X\!\mapsto\!HX$ gives DOF on the $\logρ$ gauge; expanding across all dimensions of the fading map gives diversity on the linear~$ρ$ gauge. Covering produces logs; expansion produces linear growth; so in every model studied here the two gauges differ. These geometric definitions do not yield tradeoff curves. We bridge the gap with Bhattacharyya packing, obtaining gauge-DOF and B-diversity as workable proxies -- finite and informative on every gauge, including those where the classical diversity order is zero. Three gauge classes emerge: $\logρ$, $\log\logρ$, and $(\logρ)^β$, $β\in(0,1)$. The main result is a cross-gauge tradeoff for noncoherent fast fading: capacity lives on $\log\logρ$, but B-diversity lives on $\logρ$, exponentially larger, with matching upper and lower bounds. For coherent MIMO, block fading, and irregular-spectrum channels, the same approach recovers or extends known scaling laws.

</details>


### [167] [On the Rate-Distortion-Complexity Tradeoff for Semantic Communication](https://arxiv.org/abs/2602.14481)
*Jingxuan Chai,Yong Xiao,Guangming Shi*

Main category: cs.IT

TL;DR: 该论文提出了一个率失真复杂度（RDC）框架，将经典率失真理论扩展到语义通信领域，考虑了语义距离和模型复杂度约束，揭示了速率、语义距离和复杂度之间的三方权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的语义通信方法在提取语义信息方面表现良好，但往往忽视了模型训练和推理过程中的高计算复杂度问题。需要建立一个理论框架来平衡通信速率、语义保真度和计算复杂度之间的关系。

Method: 提出了率失真复杂度（RDC）框架，扩展经典率失真理论，引入语义距离约束（包括传统比特级失真度量和基于统计差异的散度度量）以及复杂度度量（借鉴最小描述长度和信息瓶颈理论）。推导了高斯和二进制语义源在给定语义距离和复杂度约束下的最小可达速率闭式解。

Result: 理论分析揭示了速率、语义距离和模型复杂度之间的基本三方权衡关系。在真实图像和视频数据集上的实验验证了这一权衡，并证明信息论复杂度度量与实际计算成本有效相关。

Conclusion: RDC框架为语义通信系统设计提供了理论基础，特别是在资源受限场景下，能够指导在通信效率、语义保真度和计算成本之间做出平衡决策。

Abstract: Semantic communication is a novel communication paradigm that focuses on conveying the user's intended meaning rather than the bit-wise transmission of source signals. One of the key challenges is to effectively represent and extract the semantic meaning of any given source signals. While deep learning (DL)-based solutions have shown promising results in extracting implicit semantic information from a wide range of sources, existing work often overlooks the high computational complexity inherent in both model training and inference for the DL-based encoder and decoder. To bridge this gap, this paper proposes a rate-distortion-complexity (RDC) framework which extends the classical rate-distortion theory by incorporating the constraints on semantic distance, including both the traditional bit-wise distortion metric and statistical difference-based divergence metric, and complexity measure, adopted from the theory of minimum description length and information bottleneck. We derive the closed-form theoretical results of the minimum achievable rate under given constraints on semantic distance and complexity for both Gaussian and binary semantic sources. Our theoretical results show a fundamental three-way tradeoff among achievable rate, semantic distance, and model complexity. Extensive experiments on real-world image and video datasets validate this tradeoff and further demonstrate that our information-theoretic complexity measure effectively correlates with practical computational costs, guiding efficient system design in resource-constrained scenarios.

</details>


### [168] [Center-Fed Pinching Antenna System (C-PASS): Modeling, Analysis, and Beamforming Design](https://arxiv.org/abs/2602.14805)
*Xu Gan,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出新型中心馈电夹持天线系统(C-PASS)的通用框架，推导其自由度(DoF)和功率缩放规律的闭式表达式，开发高效的交替优化算法进行联合优化，数值结果表明C-PASS在DoF、功率缩放和高衰减场景下优于传统PASS系统。


<details>
  <summary>Details</summary>
Motivation: 传统夹持天线系统(PASS)存在性能限制，需要开发更高效的天线系统框架来提升无线通信系统的自由度、功率增益和整体性能，特别是在高衰减场景下。

Method: 1) 提出C-PASS通用框架并推导DoF和功率缩放规律的闭式表达式；2) 建立和速率最大化问题的联合优化模型；3) 开发交替优化算法：闭式解更新发射预编码和功率分配比，块坐标下降法优化夹持天线位置和辐射系数。

Result: 1) DoF随输入端口数M和接收天线数K线性增长；2) 功率增益为O(P_T M)量级；3) 单波导C-PASS在DoF和功率缩放上优于单波导PASS；4) 在高衰减场景下优于多波导PASS，增益超过10dB。

Conclusion: 提出的C-PASS框架在理论分析和实际性能上均优于传统PASS系统，特别是在高衰减场景下表现出显著优势，为未来无线通信系统设计提供了有效的天线解决方案。

Abstract: A generalized framework for the novel center-fed pinching antenna system (C-PASS) is proposed. Within this framework, closed-form expressions for the degree of freedom (DoF) and power scaling law of the proposed C-PASS are first derived. These theoretical results reveal that the achievable DoF scales linearly with the number of input ports, $M$, and the number of receive antennas, $K$. Furthermore, the derived power scaling laws demonstrate that the C-PASS achieves a power gain of order $\mathcal{O}(P_T M)$, where $P_T$ denotes the transmit power. Based on the proposed C-PASS modeling, a sum-rate maximization problem for the joint optimization of transmit and pinching beamforming is then formulated. To solve this highly coupled non-convex problem, an efficient alternating optimization algorithm is developed. More particularly, the transmit precoding and power splitting ratios are updated via derived closed-form solutions, while the pinching antenna positions and radiation coefficients are optimized using block coordinate descent (BCD) methods. Finally, our numerical results reveal that the single-waveguide C-PASS: 1) achieves superior DoF and power scaling laws compared to the single-waveguide PASS; and 2) outperforms the multi-waveguide PASS in high-attenuation regimes, yielding a substantial gain exceeding $10$ dB.

</details>


### [169] [Constructions of linear codes from vectorial plateaued functions and their subfield codes with applications to quantum CSS codes](https://arxiv.org/abs/2602.14832)
*Virginio Fratianni,Sihem Mesnager*

Main category: cs.IT

TL;DR: 该论文提出了一种基于三个函数的线性码构造框架，扩展了现有的双函数方法，并引入了向量值函数以增强参数灵活性。通过利用Bent和s-Plateaued函数，确定了码的参数和重量分布，证明了构造的码具有少量重量，其对偶码在Sphere Packing和Griesmer界下达到最优。此外，建立了向量方法与经典线性码构造的理论联系，并探索了在量子编码中的应用。


<details>
  <summary>Details</summary>
Motivation: 受Xu等人2023年提出的基于两个函数的3维线性码构造框架启发，该框架已成功生成无限族最优线性码。为了增强参数灵活性并扩展构造空间，作者提出将框架扩展到三个函数，并引入向量值函数以获取更多结构特性。

Method: 提出基于三个函数的线性码构造框架，扩展了现有的双函数方法。引入向量值函数设置，使用Bent函数和s-Plateaued函数（包括Almost Bent函数）作为码生成器。利用Walsh变换的性质，分析码的显式参数和重量分布，包括其截断版本。建立向量方法与经典线性码第一通用构造的理论联系。

Result: 构造的码具有少量重量，其对偶码在Sphere Packing界和Griesmer界下达到距离和维数最优。确定了码的显式参数和重量分布。建立了向量构造与经典线性码构造的理论联系，提供了码为极小和自正交的充分条件。探索了在Calderbank-Shor-Steane框架下的量子编码应用。

Conclusion: 提出的三函数构造框架成功扩展了现有方法，增强了参数灵活性。向量值函数的引入进一步扩展了构造空间。构造的码具有优良特性（少量重量、最优对偶），并与经典构造建立了理论联系，为量子编码等应用提供了新的可能性。

Abstract: Linear codes over finite fields parameterized by functions have proven to be a powerful tool in coding theory, yielding optimal and few-weight codes with significant applications in secret sharing, authentication codes, and association schemes. In 2023, Xu et al. introduced a construction framework for 3-dimensional linear codes parameterized by two functions, which has demonstrated considerable success in generating infinite families of optimal linear codes. Motivated by this approach, we propose a construction that extends the framework to three functions, thereby enhancing the flexibility of the parameters. Additionally, we introduce a vectorial setting by allowing vector-valued functions, expanding the construction space and the set of achievable structural properties. We analyze both scalar and vectorial frameworks, employing Bent and s-Plateaued functions, including Almost Bent, to define the code generators. By exploiting the properties of the Walsh transform, we determine the explicit parameters and weight distributions of these codes and their punctured versions. A key result of this study is that the constructed codes have few weights, and their duals are distance and dimensionally optimal with respect to both the Sphere Packing and Griesmer bounds. Furthermore, we establish a theoretical connection between our vectorial approach and the classical first generic construction of linear codes, providing sufficient conditions for the resulting codes to be minimal and self-orthogonal. Finally, we investigate applications to quantum coding theory within the Calderbank-Shor-Steane framework.

</details>
