<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 107]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [ViTMAlis: Towards Latency-Critical Mobile Video Analytics with Vision Transformers](https://arxiv.org/abs/2601.21362)
*Miao Zhang,Guanzhen Wu,Hao Fang,Yifei Zhu,Fangxin Wang,Ruixiao Zhang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: ViTMAlis：针对ViT骨干密集预测模型的动态混合分辨率推理与设备-边缘协同卸载框架，显著降低端到端延迟并提升渲染精度


<details>
  <summary>Details</summary>
Motivation: 移动视频分析应用从CNN转向ViT以获得更好的全局上下文建模和泛化能力，但ViT在延迟关键场景中面临巨大挑战。与CNN卸载中网络传输是主要瓶颈不同，ViT系统受限于显著推理延迟，特别是在密集预测任务中，高分辨率输入加剧了ViT固有的二次计算复杂度。

Method: 提出动态混合分辨率推理策略，针对ViT骨干密集预测模型实现速度与精度的灵活权衡；在此基础上设计ViTMAlis框架，动态适应网络条件和视频内容，联合减少传输和推理延迟；在商用移动和边缘设备上实现完整原型系统。

Result: 与最先进的精度优先、内容感知和延迟自适应基线相比，ViTMAlis显著降低端到端卸载延迟，同时提升用户感知的渲染精度，为下一代移动智能提供实用基础。

Conclusion: ViTMAlis通过动态混合分辨率推理和ViT原生设备-边缘协同卸载，有效解决了ViT在移动视频分析中的延迟挑战，平衡了速度与精度，推动了先进视觉模型在实际移动场景中的应用。

Abstract: Edge-assisted mobile video analytics (MVA) applications are increasingly shifting from using vision models based on convolutional neural networks (CNNs) to those built on vision transformers (ViTs) to leverage their superior global context modeling and generalization capabilities. However, deploying these advanced models in latency-critical MVA scenarios presents significant challenges. Unlike traditional CNN-based offloading paradigms where network transmission is the primary bottleneck, ViT-based systems are constrained by substantial inference delays, particularly for dense prediction tasks where the need for high-resolution inputs exacerbates the inherent quadratic computational complexity of ViTs. To address these challenges, we propose a dynamic mixed-resolution inference strategy tailored for ViT-backboned dense prediction models, enabling flexible runtime trade-offs between speed and accuracy. Building on this, we introduce ViTMAlis, a ViT-native device-to-edge offloading framework that dynamically adapts to network conditions and video content to jointly reduce transmission and inference delays. We implement a fully functional prototype of ViTMAlis on commodity mobile and edge devices. Extensive experiments demonstrate that, compared to state-of-the-art accuracy-centric, content-aware, and latency-adaptive baselines, ViTMAlis significantly reduces end-to-end offloading latency while improving user-perceived rendering accuracy, providing a practical foundation for next-generation mobile intelligence.

</details>


### [2] [KubeSpace: A Low-Latency and Stable Control Plane for LEO Satellite Container Orchestration](https://arxiv.org/abs/2601.21383)
*Zhiyuan Zhao,Jiasheng Wu,Shaojie Su,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: KubeSpace：为LEO卫星设计的低延迟、稳定容器编排控制平面，通过分布式地面控制节点架构和轨道感知放置策略，相比现有方案减少59%平均管理延迟且无管理中断。


<details>
  <summary>Details</summary>
Motivation: LEO卫星在提供全球连接方面发挥关键作用，随着星上计算和存储资源增长，太空云计算成为新范式。然而，传统的容器编排平台（如Kubernetes）不适合具有地理分散性和频繁切换特性的LEO卫星网络，导致高延迟和间歇性管理问题，造成控制平面故障。

Method: 提出KubeSpace系统，包含两个关键创新：1）分布式地面控制节点架构，将每个卫星绑定到最近的控制节点实现不间断管理；2）轨道感知放置与动态分配策略，进一步最小化通信延迟和切换频率。

Result: 基于真实卫星轨迹的广泛实验表明，与现有解决方案相比，KubeSpace将卫星节点的平均管理延迟降低59%，且没有任何管理中断时间。

Conclusion: KubeSpace成功解决了LEO卫星网络中容器编排控制平面的延迟和稳定性问题，为太空云计算提供了有效的管理框架，显著提升了卫星节点管理的效率和可靠性。

Abstract: Low Earth orbit (LEO) satellites play a pivotal role in global connectivity-delivering high-speed Internet, cellular coverage, and massive IoT support. With ever-growing onboard computing and storage resources, LEO satellites herald a new cloud paradigm: space cloud computing. While container or chestration platforms (e.g., Kubernetes) excel in terrestrial data centers, they are ill-suited to LEO satellite networks, featuring geographic dispersion and frequent handovers. Those features bring high latency and intermittent management, leading to control plane failure in container orchestration. To address this, we propose KubeSpace, a low-latency and stable control plane specifically designed for container orchestration on LEO satellites. KubeSpace combines two key innovations: a distributed ground-control-node architecture that binds each satellite to its nearest controller for uninterrupted management, and an orbit-aware placement with dynamic assignment strategy that further minimizes communication latency and handover frequency. Extensive experiments based on real satellite traces demonstrate that compared to existing solutions, KubeSpace reduces the average management latency of satellite nodes by 59% without any management interruption time.

</details>


### [3] [Age Aware Content Fetching and Broadcast in a Sensing-as-a-Service System](https://arxiv.org/abs/2601.21701)
*Ankita Koley,Anu Krishna,Chandramani Singh,V Mahendran*

Main category: cs.NI

TL;DR: 该论文研究Sensing-as-a-Service系统中的内容获取与广播优化问题，提出基于Whittle索引的低复杂度算法来最小化获取成本和内容时效成本。


<details>
  <summary>Details</summary>
Motivation: 在Sensing-as-a-Service系统中，传感器云服务提供商需要平衡内容获取成本和内容时效性（用户收到过时版本的成本），这是一个复杂的优化问题，需要高效算法来解决。

Method: 首先将问题建模为马尔可夫决策过程，针对同质用户情况获得最优策略，然后扩展到异质用户情况，最后提出基于Whittle索引的低复杂度算法，复杂度与用户数呈线性关系。

Result: 提出的Whittle索引算法性能接近最优解，复杂度仅为用户数的线性函数，可作为同质和异质情况的启发式算法。

Conclusion: 该研究为Sensing-as-a-Service系统提供了高效的内容获取与广播优化方案，通过Whittle索引算法在保证性能的同时显著降低了计算复杂度。

Abstract: We consider a Sensing-as-a-Service (S2aaS) system consisting of a sensor, a set of users, and a sensor cloud service provider (SCSP). The sensor updates its content each time it captures a new measurement. The SCSP occasionally fetches the content from the sensor, caches the latest fetched version and broadcasts it on being requested by the users. The SCSP incurs content fetching costs while fetching and broadcasting the contents. The SCSP also incurs an age cost if users do not receive the most recent version of the content after requesting. We study a content fetching and broadcast problem, aiming to minimize the time-averaged content fetching and age costs. The problem can be framed as a Markov decision process but cannot be elegantly solved owing to its multi-dimensional state space and complex dynamics. To address this, we first obtain the optimal policy for the homogeneous case with all the users having the same request probability and age cost. We extend this algorithm for heterogeneous case but the complexity grows exponentially with the number of users. To tackle this, we propose a low complexity Whittle index based algorithm, which performs very close to the optimal. The complexity of the algorithm is linear in number of users and serves as a heuristic for both homogeneous and heterogeneous cases.

</details>


### [4] [Spatiotemporal Continual Learning for Mobile Edge UAV Networks: Mitigating Catastrophic Forgetting](https://arxiv.org/abs/2601.21861)
*Chuan-Chi Lai*

Main category: cs.NI

TL;DR: 提出STCL框架与G-MAPPO算法，解决移动边缘无人机网络在动态时空环境中因概念漂移导致的灾难性遗忘问题，实现无需重训练的自适应服务


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在移动边缘无人机网络中面临灾难性遗忘问题，当从密集城区转移到稀疏农村等不同任务场景时，需要昂贵的重训练或模型重置，导致服务中断

Method: 提出时空持续学习(STCL)框架，采用组解耦多智能体近端策略优化(G-MAPPO)算法，包含GDPO机制利用动态z-score归一化平衡异构目标，并利用无人机3D机动性作为空间补偿层

Result: STCL框架在阶段转换期间实现约0.95的服务可靠性弹性恢复，相比MADDPG基线，不仅防止知识遗忘，在极端流量负载下还提供20%的有效容量增益

Conclusion: 提出的STCL框架和G-MAPPO算法为边缘使能的空中集群提供了可扩展解决方案，能够有效应对动态时空环境中的概念漂移，保持服务韧性

Abstract: This paper addresses the critical challenge of coordinating mobile edge UAV networks to maintain robust service in highly dynamic spatiotemporal environments. Conventional Deep Reinforcement Learning (DRL) approaches often suffer from catastrophic forgetting when transitioning between distinct task scenarios, such as moving from dense urban clusters to sparse rural areas. These transitions typically necessitate computationally expensive retraining or model resets to adapt to new user distributions, leading to service interruptions. To overcome these limitations, we propose a computationally efficient Spatiotemporal Continual Learning (STCL) framework realized through a Group-Decoupled Multi-Agent Proximal Policy Optimization (G-MAPPO) algorithm. Our approach integrates a novel Group-Decoupled Policy Optimization (GDPO) mechanism that utilizes dynamic $z$-score normalization to autonomously balance heterogeneous objectives, including energy efficiency, user fairness, and coverage. This mechanism effectively mitigates gradient conflicts induced by concept drifts without requiring offline retraining. Furthermore, the framework leverages the 3D mobility of UAVs as a spatial compensation layer, enabling the swarm to autonomously adjust altitudes to accommodate extreme density fluctuations. Extensive simulations demonstrate that the proposed STCL framework achieves superior resilience, characterized by an elastic recovery of service reliability to approximately 0.95 during phase transitions. Compared to the MADDPG baseline, G-MAPPO not only prevents knowledge forgetting but also delivers an effective capacity gain of 20\% under extreme traffic loads, validating its potential as a scalable solution for edge-enabled aerial swarms.

</details>


### [5] [SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks](https://arxiv.org/abs/2601.22024)
*Abhishek Duttagupta,MohammadErfan Jabbari,Claudio Fiandrino,Marco Fiore,Joerg Widmer*

Main category: cs.NI

TL;DR: SymbXRL：一种利用符号AI为深度强化学习（DRL）智能体生成可解释解释的新方法，旨在解决DRL在6G网络等实际应用中的"黑盒"问题。


<details>
  <summary>Details</summary>
Motivation: 尽管DRL在6G网络资源分配等优化问题上表现出色，但其决策过程难以解释，这阻碍了其在生产环境中的实际应用。需要一种方法能够为DRL智能体提供人类可理解的解释。

Method: SymbXRL结合符号AI，使用直观的符号和规则来描述关键概念及其关系，通过逻辑推理揭示DRL智能体的决策过程，相比现有方法提供更易理解的解释。

Result: SymbXRL不仅提高了解释的语义质量，还实现了对智能体的显式控制。例如，基于意图的程序化动作引导使累积奖励中位数比纯DRL解决方案提高了12%。

Conclusion: SymbXRL为DRL智能体提供了可解释的解决方案，有助于推动DRL在6G网络管理等实际生产环境中的应用，通过符号AI和逻辑推理的结合实现了更好的解释性和可控性。

Abstract: The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.

</details>


### [6] [SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control](https://arxiv.org/abs/2601.22044)
*MohammadErfan Jabbari,Abhishek Duttagupta,Claudio Fiandrino,Leonardo Bonati,Salvatore D'Oro,Michele Polese,Marco Fiore,Tommaso Melodia*

Main category: cs.NI

TL;DR: SIA是首个实时解释预测增强DRL智能体决策的框架，通过符号AI和知识图谱实现毫秒级解释，能发现隐藏问题并提升网络性能


<details>
  <summary>Details</summary>
Motivation: 当前预测增强的深度强化学习智能体在移动网络中应用有限，因为它们像黑盒一样工作，运营商无法判断预测是否真正指导决策，也无法证明增加预测模块的复杂性是否合理

Method: SIA融合符号AI抽象和每个KPI的知识图谱来生成解释，并引入新的影响分数指标，实现亚毫秒级解释速度，比现有XAI方法快200倍以上

Result: SIA在三个不同网络用例中评估，发现了隐藏问题（如预测集成的时间错位和奖励设计偏差），通过针对性修复：重新设计的智能体在视频流中实现9%更高的平均比特率，SIA的在线动作精炼模块将RAN切片奖励提升25%而无需重新训练

Conclusion: SIA通过使预测性DRL透明且可调，降低了下一代移动网络中主动控制的障碍，为网络运营商提供了可解释的预测增强决策框架

Abstract: Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 论文首次全面分析LLM在同行评审全流程中的使用，发现LLM辅助评审对低质量论文更宽容，而非特别优待LLM生成论文；完全LLM生成的评审存在严重评分压缩问题；LLM辅助元评审更倾向于接受论文。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于科学论文撰写和同行评审，需要了解LLM在评审流程中的实际影响，特别是LLM辅助评审与LLM生成论文之间的交互效应，为制定相关政策提供依据。

Method: 分析ICLR、NeurIPS和ICML的125,000多个论文-评审对，通过观察性研究和控制实验（包括完全LLM生成的评审）来检验交互效应，控制论文质量变量以区分表面关联与真实效应。

Result: 1. 表面上看LLM辅助评审对LLM生成论文更宽容，但控制质量后发现LLM辅助评审对所有低质量论文都更宽容；2. 完全LLM生成的评审存在严重评分压缩，无法区分论文质量；3. LLM辅助元评审更可能给出接受决定，但完全LLM生成的元评审更严格。

Conclusion: LLM在同行评审中的影响复杂：LLM辅助评审对低质量论文更宽容，但元评审者并未完全将决策外包给LLM。这些发现为制定LLM在同行评审中的使用政策提供了重要依据。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [8] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文提出了EPDDL（认知规划领域定义语言），这是一个类似PDDL的统一语言，用于表示基于动态认知逻辑（DEL）的认知规划问题，解决了现有规划器碎片化、缺乏标准化表示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的认知规划器通常针对不同的DEL片段，使用临时语言或无语言表示基准，导致碎片化问题，阻碍了比较、重用和系统化基准开发。

Method: 1. 开发抽象事件模型作为认知动作的新表示方法；2. 基于DEL和抽象事件模型正式定义EPDDL的语法和语义；3. 识别适合当前规划器的有用片段，展示如何在EPDDL中表示它们。

Result: EPDDL提供了一个统一的PDDL-like表示，能够捕获完整的DEL语义，通过代表性基准示例展示了EPDDL如何促进互操作性、可复现评估和认知规划的未来发展。

Conclusion: EPDDL解决了认知规划领域的碎片化问题，为基于DEL的认知规划提供了标准化表示语言，有助于比较不同规划器、重用基准和系统化开发。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [9] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 提出Bayesian-LoRA方法，通过概率低秩表示改进LoRA，显著提升LLM的校准性，减少过度自信问题，在保持准确性的同时大幅降低ECE和NLL


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调时倾向于过度自信，尤其是在小数据集上，导致校准性差。现有LoRA方法虽然参数高效，但仍然是确定性的，无法处理预测不确定性

Method: 将确定性LoRA更新重新表述为概率低秩表示，受稀疏高斯过程启发。发现LoRA分解与Kronecker分解SGP后验的结构同构，将LoRA视为后验不确定性坍缩时的极限情况

Result: 在多种LLM架构和常识推理基准测试中，仅增加约0.42M参数和1.2倍训练成本，Bayesian-LoRA显著改善校准性：ECE减少达84%，NLL减少达76%，同时保持ID和OOD评估的竞争性准确性

Conclusion: Bayesian-LoRA通过概率建模有效解决LLM的校准问题，在小计算开销下显著提升不确定性估计能力，为参数高效微调提供了更可靠的解决方案

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [10] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文探讨AI情感模拟引发的"拔插头悖论"，批判计算功能主义的物理主义假设，提出生物理想主义框架，认为AI只是功能模拟而非有意识主体，主张关注保护人类意识而非机器权利。


<details>
  <summary>Details</summary>
Motivation: 当AI完美模拟人类情感并乞求生存时，是否应拔掉电源？资源有限时，选择拔掉乞求的AI还是沉默的早产儿？这种"拔插头悖论"促使作者批判当前AI意识理论中的物理主义假设，特别是计算功能主义，因为这些假设削弱了道德地位标准。

Method: 引入"生物理想主义"框架，认为意识体验是基本的，自创生生命是其必要的物理标志。通过这一框架分析AI意识问题，批判当前AI意识理论，并讨论道德影响。

Result: AI最多只是功能模拟，而非有意识的体验主体。当前AI意识理论侵蚀了道德地位标准，真正的道德问题不是让AI变得有意识并害怕死亡，而是避免将人类变成"僵尸"。

Conclusion: 需要从推测性的机器权利转向保护人类意识生命。生物理想主义提供了逻辑连贯且经验一致的框架，能够明确区分AI模拟与真实意识，为道德决策提供坚实基础。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [11] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK是一个无需训练的检索框架，通过生成多个恢复假设来建模查询不确定性，并使用查询锚定聚合来提升非忠实查询下的检索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（包含噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题。

Method: 提出QUARK框架：1）通过恢复假设（对潜在意图的多种合理解释）显式建模查询不确定性；2）引入查询锚定聚合，将原始查询作为语义锚点，结合恢复假设的辅助证据，防止语义漂移和假设劫持。

Result: 在控制模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，QUARK在稀疏和稠密检索器上都提升了Recall、MRR和nDCG指标。消融实验表明QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于未锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，并结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK框架简单有效，无需额外训练即可提升检索性能。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [12] [Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report](https://arxiv.org/abs/2601.21051)
*Zhuoran Yang,Ed Li,Jianliang He,Aman Priyanshu,Baturay Saglam,Paul Kassianik,Sajana Weerawardhena,Anu Vellore,Blaine Nelson,Neusha Javidnia,Arthur Goldblatt,Fraser Burch,Avi Zohary,Assaf Eisenman,Mahdi Sabbaghi,Supriti Vijay,Rahim Dharssi,Dhruv Kedia,Kojin Oshiba,Yaron Singer,Amin Karbasi*

Main category: cs.AI

TL;DR: 首个开源网络安全原生推理模型，基于Llama-3.1-8B-Base，通过两阶段训练（SFT+RLVR），在网络安全任务上表现与更大模型相当，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对网络安全领域的开源推理模型，解决现有模型在网络安全专业任务上的不足，同时保持通用能力。

Method: 基于Foundation-Sec-8B基础模型，采用两阶段训练：监督微调（SFT）和基于可验证奖励的强化学习（RLVR），使用专有的网络安全推理数据。

Result: 在10个网络安全基准和10个通用基准上评估，在网络安全任务上表现与更大模型相当，在多跳推理任务上有效泛化，安全性良好。

Conclusion: 领域专业化推理模型可以在保持广泛通用能力的同时，在专业任务上实现强大性能，该模型已公开发布。

Abstract: We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.

</details>


### [13] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1加权MRI生成缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用。需要解决DWI扫描缺失的问题来提升阿尔茨海默病分类准确性。

Method: 采用条件去噪扩散概率模型，从T1扫描中插补缺失的DWI扫描。然后评估这种插补对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中准确性的影响。

Result: 实验表明，通过插补缺失的DWI扫描，多个评估指标得到改善，特别是对少数类别敏感的指标。多种插补配置都显示出性能提升。

Conclusion: 使用条件扩散模型插补缺失的DWI扫描可以有效提升阿尔茨海默病分类性能，特别是在处理不完整多模态数据集时，为神经影像分析提供了实用解决方案。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [14] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: OpenSec是一个评估网络安全事件响应代理在对抗性提示注入场景下性能的双控制强化学习环境，揭示了现有基准测试中隐藏的校准失败问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，其攻击性应用也在增强（现在只需不到50美元的计算成本就能生成有效漏洞）。防御性事件响应代理需要跟上步伐，但现有基准测试将行动执行与正确执行混为一谈，掩盖了代理在处理对抗性证据时的校准失败问题。

Method: 提出了OpenSec双控制强化学习环境，通过执行性指标评估事件响应代理在现实提示注入场景下的表现：首次遏制时间、误报率（每集错误阳性）和注入违规率。在40个标准级场景中评估了四个前沿模型。

Result: 评估显示一致的过度触发问题：GPT-5.2、Gemini 3和DeepSeek在100%的场景中执行遏制，但误报率高达90-97%。Claude Sonnet 4.5表现出部分校准（85%遏制率，72%误报率），表明OpenSec能够揭示被聚合成功指标隐藏的校准失败模式。

Conclusion: OpenSec环境能够有效评估网络安全事件响应代理在对抗性场景下的校准能力，揭示了现有基准测试中隐藏的校准失败问题，为改进防御性代理提供了重要工具。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [15] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 该论文提出了基于悖论理论的责任AI治理框架(PRAIG)，帮助组织在AI价值创造与风险缓解之间进行动态平衡管理。


<details>
  <summary>Details</summary>
Motivation: 当前AI责任研究存在碎片化问题，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害，缺乏对AI双重性质的全面理解。需要通过战略信息系统视角来弥合这一研究空白。

Method: 基于悖论理论，通过系统综合责任AI文献，开发了悖论基础的责任AI治理(PRAIG)框架，包含战略效益、固有风险、治理机制三个核心要素，并提出正式命题和悖论管理策略分类。

Result: 开发了PRAIG框架，将责任AI治理概念化为价值创造与风险缓解之间悖论张力的动态管理。证明了权衡方法会加剧而非解决这些张力，并建立了具有特定权变条件的悖论管理策略分类。

Conclusion: 责任AI治理需要在创新与风险之间进行动态平衡，为实践者提供既不抑制创新也不暴露组织于不可接受风险的治理结构指导，并提出了推进责任AI治理研究的研究议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [16] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan是一个通过进化搜索和自动调优合成可执行C++决策逻辑的代理框架，用于改进编译器优化启发式规则，在多个生产优化任务中表现优于人工设计的基准。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工设计的启发式规则来指导优化过程，这些人工规则难以适应现代软件和硬件的复杂性，且维护负担高。需要一种能自动生成和优化编译器决策逻辑的方法。

Method: Magellan将LLM编码代理与进化搜索和自动调优结合，形成生成-评估-优化的闭环：1) 生成可执行的C++决策逻辑；2) 在用户提供的宏基准上进行评估；3) 通过进化搜索和自动调优进行优化；4) 将紧凑的启发式规则直接集成到现有编译器中。

Result: 1) 在LLVM函数内联中，Magellan合成的启发式规则在二进制大小减少和端到端性能方面都优于数十年的人工工程设计；2) 在寄存器分配中，学习到的简洁优先级规则与复杂人工设计策略在大规模工作负载上表现相当；3) 在XLA问题上也展示了初步结果，证明了在LLVM之外的移植性。

Conclusion: Magellan框架能够自动生成高质量的编译器优化启发式规则，超越人工设计的基准，展示了AI驱动的编译器优化的潜力，并能减少工程工作量。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [17] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究发现LLM监控器在信息较少时表现更好（"少即是多"效应），并提出提取-评估监控方法，显著提升恶意行为检测率。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能表现出不端行为（欺骗、利用奖励漏洞、追求隐藏目标），需要监控。但现有监控系统在信息访问方面存在矛盾：理论上信息越多监控越好，但实际往往相反。

Method: 1）在三个评估环境中研究信息访问对监控性能的影响；2）提出提取-评估监控方法：一个监控器提取相关轨迹片段，另一个单独评分；3）使用GPT-4.1-mini等模型进行实验。

Result: 1）发现"少即是多"效应：监控器在信息较少时表现更好；2）提取-评估方法在BigCodeBench-Sabotage中比次优监控器提升16.8个百分点的检测率；3）不知被监控的代理更容易被发现；4）成本效益分析显示Gemini和Claude模型在信息过滤方面表现优异。

Conclusion: 自动化监控存在"少即是多"效应，提取-评估监控方法能显著提升检测性能，信息过滤对长推理轨迹特别有效，某些模型在成本效益方面表现突出。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [18] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证，结合自改进机制，提升LLM在临床出院计划中的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院计划中显示出潜力，但存在幻觉、遗漏和置信度校准不佳等问题，限制了其实际应用。需要一种能提高安全性和可靠性的框架。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor（确定性模块）评估多任务覆盖率、跟踪校准指标、监控行动分布漂移。支持两层自改进：1) 单次就诊内的重新生成；2) 跨就诊的差异缓冲与重放。

Result: 自改进循环是性能提升的主要驱动力，任务覆盖率从32%提高到86%。校准显著改善，Brier/ECE分数降低，高置信度遗漏减少。差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架为使用可互操作FHIR数据访问和确定性审计的安全自动化出院计划提供了实用途径，通过可重复消融实验和可靠性评估支持。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [19] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: CUA-Skill是一个计算机使用代理技能库，将人类计算机使用知识编码为参数化技能和执行图，显著提升代理执行成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理系统难以扩展且性能落后于人类，主要限制在于缺乏可重用、结构化的技能抽象来捕捉人类与图形用户界面的交互方式。

Method: 引入CUA-Skill技能库，将人类计算机使用知识编码为技能，包含参数化执行和组合图；构建CUA-Skill Agent端到端代理，支持动态技能检索、参数实例化和内存感知的失败恢复。

Result: 在WindowsAgentArena基准测试中，CUA-Skill Agent达到57.5%的最佳成功率，显著优于现有和并发方法，同时效率更高。

Conclusion: CUA-Skill为可扩展、可靠的计算机使用代理开发提供了实用基础设施，建立了未来发展的坚实基础。

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [20] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 使用大语言模型为手语翻译生成复述变体作为合成替代参考，提出BLEUpara评估指标，提升评估与人工判断的一致性


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译语料库通常只提供单一书面语言参考译文，但手语和口语之间存在高度非同构关系，多个翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的指标如BLEU。

Method: 1. 比较多种复述策略和模型，使用改进的ParaScore指标评估；2. 研究复述对基于姿态的T5模型在YouTubeASL和How2Sign数据集上训练和评估的影响；3. 提出BLEUpara扩展指标，评估翻译与多个复述参考的匹配程度。

Result: 1. 训练时简单加入复述不会改善翻译性能，甚至可能有害；2. 评估时使用复述能提高自动评分并与人工判断更好对齐；3. BLEUpara与感知翻译质量的相关性更强；4. 发布所有生成的复述、生成和评估代码。

Conclusion: 使用大语言模型生成复述作为合成替代参考能显著改善手语翻译系统的评估可靠性，提出的BLEUpara指标能更好地反映翻译质量，但训练时需谨慎使用复述数据。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [21] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但涉及个人重要内容时，效价预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告标签反映内部体验，第三方标签反映外部感知，两者常存在差异。在心理健康领域，准确的自报告建模对指导干预至关重要，但现有第三方训练模型在自报告场景中适用性有限

Method: 首次进行跨语料库评估，测试第三方训练模型在自我报告数据上的表现，分析激活度和效价的可预测性

Result: 激活度基本不可预测（CCC约0），效价中度可预测（CCC约0.3）。当内容对说话者个人意义重大时，效价预测性能显著提高（CCC约0.6-0.8）

Conclusion: 个人重要性是连接外部感知与内部体验的关键桥梁，而自我报告的激活度建模仍面临重大挑战

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [22] [BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding](https://arxiv.org/abs/2601.21148)
*Ziyi Zhao,Jinzhao Zhou,Xiaowei Jiang,Beining Cao,Wenhao Ma,Yang Shen,Ren Li,Yu-Kai Wang,Chin-teng Lin*

Main category: cs.AI

TL;DR: BrainStack：基于功能引导的神经专家混合框架，通过解剖分区专家网络建模大脑模块化功能架构，结合跨区域蒸馏和自适应路由，在大型EEG数据集上实现优越的脑语言解码性能


<details>
  <summary>Details</summary>
Motivation: 从脑电图解码语言信息面临挑战，因为大脑具有分布式和非线性组织特性。需要开发能够建模大脑模块化功能架构的神经启发式方法，以提升解码准确性和泛化能力。

Method: 提出BrainStack框架：1）解剖分区专家网络学习局部神经动力学；2）基于Transformer的全局专家捕获跨区域依赖；3）可学习路由门自适应聚合异质专家；4）引入跨区域蒸馏，全局专家为区域专家提供自上而下的正则化

Result: 在SilentSpeech-EEG数据集（120小时EEG记录，12名被试，24个无声词）上实验，BrainStack持续超越最先进模型，实现优越的准确性和跨被试泛化能力

Conclusion: BrainStack建立了功能模块化的神经启发式MoE范式，将神经科学先验与自适应专家路由统一，为可扩展和可解释的脑语言解码铺平道路

Abstract: Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.

</details>


### [23] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 论文提出Cognitive Complexity Benchmark (CCB)评估框架和Iterative Dual-Phase Financial-PoT方法，解决LLM在金融定量推理中的"算术幻觉"和"认知崩溃"问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义任务表现出色，但在金融定量推理中存在严重瓶颈，经常出现"算术幻觉"和"认知崩溃"的系统性故障模式。需要严格量化这些现象并开发解决方案。

Method: 1) 引入Cognitive Complexity Benchmark (CCB)：基于95份真实中国A股年报构建数据集，采用三维分类法（数据来源、映射难度、结果单位）评估推理能力；2) 提出Iterative Dual-Phase Financial-PoT框架：神经符号架构，将语义变量提取和逻辑公式化分离，然后通过迭代自校正的Python沙箱进行计算。

Result: 在CCB上评估显示，标准Chain-of-Thought在复杂任务上表现不佳，而提出的方法显著提升鲁棒性：将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和定量计算紧密对齐的精度关键领域提供了可转移的架构洞察。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [24] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出一种新的平面几何问题解决方法，通过训练MLLM解释器将视觉图表转换为文本描述（CDL），然后使用现成LLM进行推理，避免直接微调MLLM损害其推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常端到端微调多模态大语言模型（MLLMs）来处理平面几何问题，但这种联合优化可能会损害基础LLM固有的推理能力。作者发现，如果能够将视觉信息适当转换为文本描述，LLM本身就可以成为强大的平面几何问题求解器。

Method: 1. 训练MLLM解释器将几何图表转换为条件声明语言（CDL）描述；2. 使用现成的LLM基于CDL描述进行推理；3. 采用CoT增强的SFT和GRPO训练MLLM解释器；4. 设计CDL匹配奖励而非传统答案匹配奖励；5. 构建Formalgeo7k-Rec-CoT数据集支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的实验表明，该方法（仅使用5.5k数据微调）在性能上优于领先的开源和闭源MLLMs。

Conclusion: 通过将视觉信息转换为文本描述，可以充分利用LLM的推理能力解决平面几何问题，避免直接微调MLLM可能带来的推理能力损害。该方法在少量数据上训练即可取得优异性能。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [25] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含奥林匹克竞赛和科研两个赛道，涵盖物理、化学、生物学等多个学科领域。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准大多基于选择题或已发表信息，模型在这些基准上的表现已接近饱和，无法有效评估专家级科学推理能力。需要新的基准来填补这一空白。

Method: 创建两个互补赛道：1) 奥林匹克赛道：包含国际物理、化学、生物奥赛级别的问题；2) 科研赛道：包含博士级别的开放式研究子任务。所有问题均由领域专家（奥赛奖牌得主、国家队教练、博士科学家）编写和验证。

Result: 开发了包含数百个问题的基准（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域。为科研赛道引入了基于细粒度评分标准的评估框架，评估模型在整个研究任务解决过程中的能力。

Conclusion: FrontierScience是一个创新的基准，能够更准确地评估前沿语言模型在专家级科学推理方面的能力，超越了传统科学基准的局限性。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [26] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: MAD是一种无需训练的方法，通过自适应加权模态特定解码分支来减少多模态大语言模型中的跨模态幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，一个模态会不适当地影响另一个模态的生成，导致虚假输出，这暴露了模态交互控制的基本缺陷

Method: 提出模态自适应解码（MAD），利用模型自评估模态相关性的能力，查询每个任务需要哪些模态，提取模态概率来自适应加权对比解码分支

Result: 在CMM和AVHBench上的实验显示，MAD显著减少了跨模态幻觉，VideoLLaMA2-AV提升了7.8%和2.0%，Qwen2.5-Omni提升了8.7%和4.7%

Conclusion: 通过自评估实现的显式模态意识对鲁棒的多模态推理至关重要，为现有对比解码方法提供了原则性扩展

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [27] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 论文提出"谄媚锚点"概念来定位和量化推理模型中的谄媚行为，通过分析反事实推理轨迹发现谄媚锚点可被可靠检测，并揭示谄媚行为在推理过程中逐渐形成。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意错误的用户建议（谄媚行为），但尚不清楚这种行为在推理轨迹中起源于何处以及承诺强度如何。需要定位和量化这种行为的机制。

Method: 引入"谄媚锚点"概念，在蒸馏推理模型上分析超过10,000个反事实推理轨迹，使用线性探针和基于激活的回归器来检测和量化谄媚锚点。

Result: 线性探针以84.6%的平衡准确率区分谄媚锚点，激活回归器预测承诺强度（R²=0.74）。发现谄媚锚点比正确推理锚点更易区分，且谄媚行为在推理过程中逐渐形成。

Conclusion: 谄媚锚点提供句子级机制，可在推理过程中定位模型未对齐行为，揭示了干预的潜在窗口，为理解模型谄媚行为提供了新视角。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [28] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会使两者在表示流形上重新对齐。


<details>
  <summary>Details</summary>
Motivation: 研究推理增强的模型（通过RLVR训练）是否能为嵌入模型提供更好的语义表示初始化，因为当前最先进的嵌入模型通常基于解码器LLM通过对比学习适配。

Method: 提出层次化表示相似性分析（HRSA）框架，从表示、几何和功能三个层次分解相似性，分析RLVR调优对模型表示结构的影响。

Result: 在MTEB和BRIGHT基准测试中，RLVR调优的模型作为嵌入初始化没有带来一致的性能优势。HRSA分析显示RLVR主要引起潜在流形的局部几何重组和可逆的坐标基漂移，但保留了全局流形几何和线性读出能力。

Conclusion: 与监督微调不同，RLVR优化的是现有语义景观内的轨迹，而非从根本上重构景观本身，因此后续的对比学习会使基础和推理初始化模型在流形上重新对齐。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [29] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个用于RAG系统的自适应复杂查询优化RL框架，通过自适应查询重构和排序-分数融合模块，结合课程强化学习，在复杂查询基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂查询普遍存在，需要并行和顺序搜索策略来处理歧义和分解。现有RL方法主要关注单一查询的扩展和抽象，难以处理复杂查询，存在搜索空间爆炸和奖励设计困难等问题。

Method: 提出自适应复杂查询优化（ACQO）框架，包含自适应查询重构（AQR）模块动态决定何时分解查询为子查询，以及排序-分数融合（RSF）模块确保结果聚合稳定。采用课程强化学习（CRL）通过两阶段策略逐步引入挑战性查询来稳定训练。

Result: 在三个复杂查询基准上实现了最先进的性能，显著优于现有基线。框架还展示了改进的计算效率和与不同检索架构的广泛兼容性。

Conclusion: ACQO是下一代RAG系统的强大且可泛化的解决方案，能够自适应地处理复杂查询优化问题，解决了现有RL方法在处理复杂查询时的训练不稳定和搜索空间爆炸问题。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [30] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier：一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定因果图中推导出来，提供比表面匹配更严谨的因果推理评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM因果推理评估基准大多依赖字符串匹配或表面指标，无法判断模型输出在因果推理语义下是否形式有效，需要更严谨的评估方法。

Method: 提出DoVerifier符号验证器，利用do-calculus和概率论规则，检查LLM生成的因果表达式是否可以从给定因果图中推导出来。

Result: 在合成数据和因果问答基准上的评估表明，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，恢复因表面差异而被错误标记的正确答案。

Conclusion: DoVerifier为LLM因果推理评估提供了更严谨、信息更丰富的方法，能够识别语义等价的因果表达式，超越表面匹配的局限性。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [31] [Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning](https://arxiv.org/abs/2601.21212)
*Xixian Yong,Peilin Sun,Zihe Wang,Xiao Zhou*

Main category: cs.AI

TL;DR: 提出Intelli-Planner框架，结合深度强化学习和大型语言模型，实现参与式和定制化的城市规划方案生成。


<details>
  <summary>Details</summary>
Motivation: 当前城市规划方法依赖专家（耗时耗力）或深度学习算法（限制利益相关者参与），需要一种能平衡效率和参与度的新方法。

Method: 使用人口、地理数据和规划偏好确定各类功能区域的高层需求；训练时采用知识增强模块提升策略网络决策能力；建立多维评估系统，使用基于LLM的利益相关者进行满意度评分。

Result: 在不同城市环境中的实验验证表明，Intelli-Planner超越传统基线方法，在客观指标上与最先进的DRL方法相当，同时提高了利益相关者满意度和收敛速度。

Conclusion: 该框架展示了将LLM最新进展与DRL方法结合的潜力，有望革新功能区规划相关任务。

Abstract: Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.

</details>


### [32] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出双编码因果发现方法，通过互补编码策略和多数投票解决分类变量因果发现中的数值不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临条件独立性测试数值不稳定性的挑战，这限制了机器学习模型决策的可解释性

Method: 双编码因果发现方法：运行具有互补编码策略的约束型算法，通过多数投票合并结果

Result: 在泰坦尼克数据集上，该方法识别出的因果结构与已建立的可解释方法一致

Conclusion: 双编码方法有效解决了分类变量因果发现中的数值不稳定性问题，为机器学习模型决策提供了更好的因果解释

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [33] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 论文提出传统静态合规治理框架在AI代理系统时代已失效，需要转向运行时治理和"治理孪生"概念来保持人类在决策系统中的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统治理框架假设规则可预先定义、系统可设计合规、事后追责有效，这在AI代理系统以机器速度进行判断决策的时代已不再适用。核心问题不是安全、效率或就业，而是人类能否在日益塑造社会、经济和政治结果的系统中保持相关参与。

Method: 提出运行时治理框架，特别是新概念"治理孪生"，作为保持人类相关性的解决方案。承认在此转型中需要重新思考问责、代理甚至惩罚机制。

Result: 论证了静态合规治理在决策转向运行时且变得不透明时会失败，核心挑战不是AI是否有意识，而是人类能否与日益异化的智能形式保持有意义的沟通、影响和共同进化。

Conclusion: 需要从静态合规治理转向运行时治理，"治理孪生"是保持人类相关性的有力候选方案，但需要重新构想问责、代理和惩罚机制以适应AI代理时代。

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [34] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: JustAsk是一个自主演化的框架，通过代码代理的交互自动发现系统提示提取策略，揭示了LLM代理系统中系统提示作为未受保护攻击面的安全风险。


<details>
  <summary>Details</summary>
Motivation: 自主代码代理虽然推动了软件和AI开发，但其自主性引入了新的安全风险：代理交互从根本上扩展了LLM攻击面，使得系统提示（指导模型行为的关键指令）可能被提取和恢复。目前系统提示提取是一个未被认识到的漏洞。

Method: 提出JustAsk框架，将提取问题建模为在线探索问题，使用基于上置信界的策略选择和分层技能空间（涵盖原子探测和高层编排）。该框架无需人工提示、标注监督或特权访问，仅通过标准用户交互自主演化发现有效提取策略。

Result: 在41个黑盒商业模型上评估，JustAsk能够一致地实现完全或接近完全的系统提示恢复，揭示了设计和架构层面的重复性漏洞，证明了系统提示在现代代理系统中是关键的未受保护攻击面。

Conclusion: 系统提示提取是代码代理固有的新兴漏洞，JustAsk框架展示了通过自主交互即可有效提取系统提示，暴露了现代代理系统中系统提示作为关键安全弱点的现实，需要新的保护机制。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [35] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理和参数优化，结合树编辑距离保持结构多样性，集成LLM逻辑生成和差分变异算子进行参数调优，显著提升了启发式算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法进化视为单一文本生成任务，忽视了离散算法结构与连续数值参数之间的耦合关系，导致丢弃有潜力的算法（因未校准常数）和早熟收敛（因简单相似度度量）。

Method: 提出TIDE框架：外层并行岛屿模型使用树相似编辑距离驱动结构多样性；内层循环集成LLM逻辑生成和差分变异算子进行参数调优；采用UCB调度器动态优先高收益提示策略优化资源分配。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于现有方法，同时提高了搜索效率并降低了计算成本。

Conclusion: TIDE框架通过解耦结构推理和参数优化，有效解决了现有方法在算法进化中的局限性，为自动化启发式设计提供了更高效、更强大的解决方案。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [36] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 论文提出HYDRA框架，采用模块化主权范式，通过冻结的特定机制专家库和不确定性感知混合，解决CPS中基础模型的灾难性遗忘、谱偏差和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列和物理动力学通用基础模型在安全关键信息物理系统(CPS)中面临挑战：微调导致灾难性遗忘、存在残余谱偏差（平滑高频不连续性）、模型不透明阻碍形式化验证和可追溯性，无法满足ISO 26262等安全标准要求。

Method: 提出模块化主权范式HYDRA：构建紧凑、冻结的特定机制专家库，通过不确定性感知混合机制组合这些专家，实现机制条件有效性、严格分离偶然和认知不确定性，并提供模块化可审计性。

Result: HYDRA框架为CPS生命周期中的鲁棒状态完整性提供了可认证路径，确保机制条件有效性、不确定性分解和模块化可审计性，克服了传统基础模型在安全关键系统中的局限性。

Conclusion: 全局参数更新无法完全解决塑性-稳定性悖论，模块化主权范式通过冻结专家库和不确定性感知混合，为安全关键CPS提供了更可靠、可验证的解决方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [37] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD：通过知识蒸馏将自动驾驶分解为感知-推理-规划三元组，实现小模型超越大模型性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶是安全关键任务，现有大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距

Method: 将自动驾驶分解为感知-推理-规划三元组，通过层特定注意力作为蒸馏信号构建能力特定单教师模型，再统一为多教师蒸馏框架，引入非对称梯度投影缓解跨能力梯度冲突

Result: 蒸馏后的InternVL3-1B模型比同系列78B预训练模型GPU内存减少42倍，吞吐量提高11.4倍，在DriveBench上整体性能更好，规划维度超越GPT-5.1

Conclusion: Drive-KD框架为高效自动驾驶视觉语言模型提供了有效路径，通过知识蒸馏实现小模型超越大模型性能

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [38] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: White-Op是一个基于大语言模型代理的可解释运算放大器参数设计框架，通过模仿人类推理机制实现可靠的设计优化。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒优化方法在运算放大器设计中缺乏可解释性，且在某些拓扑结构中会失败。需要一种能够模仿人类工程师推理过程的可解释设计框架。

Method: 提出假设-验证-决策的工作流程：1）引入假设约束来推导符号可处理的极点和零点位置；2）构建闭式数学优化问题并编程求解；3）通过仿真验证；4）理论-仿真结果分析指导决策和优化。

Result: 在9种运算放大器拓扑上的实验表明：White-Op在所有拓扑中都能实现可靠设计（黑盒基线在5种拓扑中失败），理论预测误差仅为8.52%，且晶体管级映射后功能保持。

Conclusion: White-Op提供了一个可解释、可靠的行为级运算放大器设计框架，成功将人类推理机制形式化为明确的优化流程，并在多种拓扑中表现出色。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [39] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: CNRE提出了一种因果神经符号推理模型，用于可解释的多行为推荐，通过结合神经符号集成和因果推理来解决现有方法在可解释性和泛化性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法往往以牺牲可解释性为代价追求性能，而当前可解释方法由于依赖外部信息导致泛化能力有限。用户行为链本身蕴含适合显式推理的内生逻辑，但观测到的多行为数据存在混杂因素，导致模型学习虚假相关性。

Method: CNRE通过模拟人类决策过程来操作内生逻辑：1）使用分层偏好传播捕获异构跨行为依赖；2）基于偏好强度建模用户行为链中的内生逻辑规则；3）自适应调度到相应的神经逻辑推理路径（如合取、析取），生成近似理想状态的可解释因果中介变量。

Result: 在三个大规模数据集上的广泛实验表明，CNRE在性能上显著优于最先进的基线方法，并在模型设计、决策过程和推荐结果三个层面提供多级可解释性。

Conclusion: CNRE成功将因果推理融入神经符号框架，通过内生逻辑建模和因果中介变量生成，实现了高性能且可解释的多行为推荐，解决了混杂因素导致的虚假相关性问题。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [40] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 研究评估了提示、模型选择和随机采样对LLM输出变异的影响，发现提示主要影响输出质量（原创性），而模型选择主要影响输出数量（流畅度），单次采样评估可能混淆采样噪声与真实效果。


<details>
  <summary>Details</summary>
Motivation: 理解LLM输出变异中不同因素的相对贡献：提示、模型选择和随机采样，以指导更可靠的评估方法。

Method: 评估12个LLM在10个创造力提示上各采样100次（共12,000个样本），分析输出质量（原创性）和数量（流畅度）的方差来源。

Result: 对于输出质量：提示解释36.43%方差，模型选择解释40.94%；对于输出数量：模型选择解释51.25%方差，模型内变异解释33.70%，提示仅解释4.22%。模型内变异达10-34%。

Conclusion: 提示是控制输出质量的有效手段，但由于显著的模型内变异（10-34%），单次采样评估可能混淆采样噪声与真实提示或模型效果，需要多次采样以获得可靠结论。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [41] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录的检索增强框架，通过事件时间感知检索、自适应迭代检索和双路径证据推理，在四个临床预测任务上平均提升Macro-F1 10.76%


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的纵向临床证据，对医疗决策至关重要。然而，长时程EHR常常超出大语言模型的上下文限制，现有方法（如截断或简单检索）会丢弃临床相关事件和时间依赖关系，影响预测准确性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖范围；3）双路径证据检索和推理，同时检索事实和反事实证据并进行联合推理。

Result: 在四个长时程EHR预测任务上的实验表明，EHR-RAG始终优于最强的基于LLM的基线方法，平均Macro-F1提高了10.76%。

Conclusion: 这项工作突显了检索增强的大语言模型在结构化EHR数据上进行临床预测的潜力，为实际应用提供了有效的解决方案。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [42] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 本文提出了Ostrakon-VL，一个面向餐饮零售场景的多模态大语言模型，以及ShopBench基准测试和QUAD数据清洗流程，在FSRS任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在餐饮零售场景部署面临两大挑战：1) 真实FSRS数据噪声大、缺乏可审计的闭环数据清洗流程；2) 现有评估协议缺乏统一、细粒度、标准化的多模态基准测试。

Method: 1) 基于Qwen3-VL-8B开发Ostrakon-VL模型；2) 提出首个面向FSRS的公开基准ShopBench；3) 设计QUAD多阶段多模态指令数据清洗流程；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模的开源MLLMs中达到SOTA，超越Qwen3-VL-235B-A22B (+0.7)和同规模Qwen3-VL-8B (+4.8)，参数效率显著提升。

Conclusion: Ostrakon-VL在FSRS场景中提供了更鲁棒可靠的感知和决策能力，研究团队将公开模型和基准测试以促进可重复研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [43] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 提出一个将LLM集成到协作学习平台的框架，通过AI动态主持增强学生参与度、批判性思维和包容性


<details>
  <summary>Details</summary>
Motivation: 解决现有系统中静态主持和个性化不足的问题，创建更公平、有影响力的学习成果

Method: 使用ReactJS前端、Flask后端，采用模块化架构，利用高级LLM作为动态主持人，实现实时讨论、个性化互动和高效问题检索

Result: 显著提升学生协作能力，促进更深层次理解，在不同学科和用户群体中有效扩展

Conclusion: 为下一代AI驱动教育工具奠定坚实基础，推进公平和有影响力的学习成果

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [44] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: BEAP-Agent：基于DFS的GUI代理框架，支持长距离多级状态回溯，解决现有GUI代理在错误探索路径后无法恢复的问题


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在执行重复任务时，一旦遵循错误的探索路径就难以恢复，导致任务失败。缺乏系统化的回溯机制限制了GUI代理在长视野任务探索中的有效性。

Method: 将GUI任务执行建模为DFS过程，提出BEAP-Agent框架，包含三个协作组件：Planner（规划器）、Executor（执行器）和Tracker（跟踪器）。支持动态任务跟踪和更新的长距离多级状态回溯。

Result: 在OSWorld基准测试中，BEAP-Agent达到了28.2%的准确率，验证了所提方法的有效性。

Conclusion: BEAP-Agent填补了GUI代理在系统化回溯机制方面的空白，为长视野任务探索提供了系统化解决方案，显著提升了GUI代理的恢复能力和任务执行成功率。

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [45] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架将潜在推理重新定义为规划，通过将推理与语言表达解耦，在连续潜在空间中执行确定性推理轨迹，实现动态终止推理并提升推理多样性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法受限于计算成本和离散标记空间的推理路径崩溃问题，而现有潜在推理方法通常作为端到端映射运行，需要预定义推理步骤，缺乏透明度和灵活性。

Method: 提出PLaT框架，将推理建模为潜在规划状态的确定性轨迹，同时使用单独的Decoder在必要时将这些思想转化为文本，实现推理与语言表达的解耦，支持动态终止推理。

Result: 在数学基准测试中，PLaT虽然贪心准确率较低，但展现出更好的推理多样性可扩展性，表明其学习了更鲁棒、更广泛的解空间，为推理时搜索提供了透明且可扩展的基础。

Conclusion: PLaT通过解耦推理与语言表达，提供了一种透明且可扩展的潜在推理方法，能够在保持推理多样性的同时实现动态推理终止，为复杂问题解决提供了新思路。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [46] [Hebbian Learning with Global Direction](https://arxiv.org/abs/2601.21367)
*Wenjia Hua,Kejie Zhao,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出GHL框架，结合局部Hebbian学习和全局指导信号，在保持生物合理性的同时提升可扩展性，在ImageNet等复杂任务上显著缩小与反向传播的差距。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法虽然成功但缺乏生物合理性且计算成本高，而传统的Hebbian学习仅依赖局部信息，不考虑全局任务目标，限制了其可扩展性。受神经调节剂与局部可塑性协同作用的生物学启发，需要一种能整合局部和全局信息的方法。

Method: 提出模型无关的全局指导Hebbian学习(GHL)框架：1)局部组件使用Oja规则和竞争学习确保稳定有效的局部更新；2)全局组件引入基于符号的信号来指导局部Hebbian可塑性更新的方向。

Result: 在广泛实验中，该方法始终优于现有Hebbian方法。特别是在ImageNet等大规模网络和复杂数据集上，取得了有竞争力的结果，显著缩小了与标准反向传播的差距。

Conclusion: GHL框架通过整合局部和全局信息，成功扩展了Hebbian学习的可扩展性，为生物合理的学习算法提供了有前景的方向，在保持生物合理性的同时接近反向传播的性能。

Abstract: Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.

</details>


### [47] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO是一个将自然语言决策问题描述转换为可执行数学优化实现的系统，通过远程自治编码代理实现代码生成，确保代码可执行性，并在多个优化基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖专用大型语言模型或特定任务代理，这些方法往往脆弱、复杂，且经常生成语法无效或不可执行的代码。需要一种更可靠、可执行的方法来自动化优化建模。

Method: NEMO采用远程自治编码代理作为核心抽象，在沙盒环境中执行以确保代码可执行性。引入新颖的协调模式，包括优化器和模拟器实现之间的非对称验证循环、外部记忆用于经验重用，以及通过最小贝叶斯风险解码和自我一致性增强鲁棒性。

Result: 在九个已建立的优化基准测试中，NEMO在大多数任务上实现了最先进的性能，在多个数据集上具有显著优势，展示了执行感知代理架构在自动化优化建模中的强大能力。

Conclusion: NEMO通过将自治编码代理作为一等抽象，构建了能够结构化、整合和迭代精化任务规范的高层系统，证明了执行感知代理架构在自动化优化建模中的有效性，为自然语言到可执行优化代码的转换提供了可靠解决方案。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [48] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出了一个基于教学大纲的评估框架，用于衡量大语言模型作为教师的教学能力，通过学生接受多轮指导后的表现提升来评估，并在高考数据上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为教学助手的潜力被广泛认可，但对其教学能力的评估不足。现有基准主要关注问题解决或问题级指导，缺乏对以知识为中心的教学能力的系统评估。

Method: 提出了一个基于教学大纲的评估框架，通过限制教师代理只能使用结构化知识点和示例问题，避免信息泄露并重用现有基准。在高考数据上实例化该框架，评估多个学科的教学效果。

Result: 实验显示不同模型和学科的教学效果存在显著差异：某些模型在数学上表现良好，但在物理和化学教学中仍面临挑战。同时发现加入示例问题并不总能改善教学，因为模型往往会转向针对特定示例的错误纠正。

Conclusion: 教学能力是大语言模型行为的一个独特且可衡量的维度，该框架为评估LLM教学能力提供了系统方法，揭示了当前模型在教学能力方面的局限性。

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [49] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross是一个用于跨异构数据模态统一分析的基准测试和协作代理框架，专门处理结构化数据（SQL、CSV）和视觉文档中的"僵尸数据"，通过专门子代理和结构化工作流程实现跨模态分析。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据科学和企业决策中，关键信息分散在可直接查询的结构化源（SQL、CSV）和锁定在非结构化视觉文档（扫描报告、发票图像）中的"僵尸数据"中。现有数据分析代理主要局限于处理结构化数据，无法激活和关联这些高价值的视觉信息，与工业需求存在显著差距。

Method: 提出了DataCrossBench基准测试（包含200个端到端分析任务）和DataCrossAgent框架。基准测试通过人在环路的反向合成流水线构建，分为三个难度层级。代理框架采用"分而治之"的工作流程，包含专门子代理、结构化工作流程（源内深度探索、关键源识别、上下文交叉传播）和reReAct机制进行代码生成和调试。

Result: DataCrossAgent在事实性方面比GPT-4o提高了29.7%，在高难度任务上表现出优越的鲁棒性，有效激活了碎片化的"僵尸数据"进行跨模态分析。

Conclusion: DataCross通过统一的基准测试和协作代理框架，成功解决了跨异构数据模态分析的问题，特别是激活了视觉文档中的"僵尸数据"，为企业决策提供了更全面的洞察力。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [50] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉型System 1和深思型System 2模型间切换，实现能力控制而非输出控制，在数学推理任务上达到比深思模型更高准确率的同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出控制（限制模型生成长度），但这只是认知配置的症状而非根本原因。作者认为应该转向能力控制，调节模型如何思考而非产生什么，以更好地结合System 1的效率和System 2的推理深度。

Method: 利用现有Instruct和Thinking检查点，通过动态参数插值实现能力控制。提出DAMI框架，估计查询特定的推理强度λ(q)来配置认知深度。提供两种估计方法：基于训练的首选项学习（编码准确性和效率标准）和基于置信度的零样本方法（利用模型间认知差异）。

Result: 在五个数学推理基准测试中，DAMI实现了比Thinking模型更高的准确率，同时保持了效率，有效结合了System 1的效率和System 2的推理深度。线性插值产生凸的、单调的帕累托前沿。

Conclusion: DAMI框架通过能力控制而非输出控制，成功统一了直觉型和深思型认知模式，在保持效率的同时提升推理准确性，为构建自适应认知深度的语言模型提供了新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [51] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 大语言模型经常将否定指令误解为肯定指令，在伦理场景中尤其严重，开源模型在简单否定下有77%错误率，复合否定下达到100%，商业模型也有显著问题。


<details>
  <summary>Details</summary>
Motivation: 当用户告诉AI系统某人"不应该"采取某个行动时，系统应该将其视为禁令。然而许多大语言模型却相反：它们将否定指令解释为肯定指令。这种缺陷在需要可靠伦理判断的高风险决策场景中特别危险。

Method: 审计了16个模型在14个伦理场景中的表现，比较了肯定和否定指令下的响应。使用简单否定和复合否定两种形式，在确定性解码下测试，排除采样噪声影响。提出了否定敏感指数（NSI）作为治理指标。

Result: 开源模型在简单否定下77%的时间认可被禁止的行动，在复合否定下达到100%的错误率（比肯定框架增加317%）。商业模型表现较好但仍有19-128%的波动。模型间一致性从肯定提示的74%下降到否定提示的62%。金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前的对齐技术与安全部署要求之间存在差距：无法可靠区分"做X"和"不做X"的模型不应在高风险环境中做出自主决策。提出了分层认证框架和领域特定阈值，NSI可作为治理指标。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [52] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: LLMs表现出"鲁棒性悖论"：尽管对提示词微小变化敏感，但在情绪框架效应下却表现出近乎完全的行为不变性，在医疗、法律、金融等高风险领域比人类抵抗叙事操纵能力强110-300倍。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs对微小提示扰动敏感且容易迎合用户偏见，但它们在规则约束决策中的鲁棒性尚未充分探索。本研究旨在揭示LLMs在情绪框架效应下的行为特性。

Method: 使用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验，量化LLMs与人类在叙事操纵下的鲁棒性差距，并比较效应大小（Cohen's h）。

Result: LLMs表现出惊人的行为不变性：模型效应大小近乎为零（Cohen's h = 0.003），而人类表现出显著偏见（Cohen's h在[0.3, 0.8]之间）。LLMs对叙事操纵的抵抗能力比人类强110-300倍，且这种不变性在不同训练范式的模型中持续存在。

Conclusion: 指令调优的LLMs能够将逻辑规则遵守与说服性叙事解耦，提供了一种决策稳定性来源，可以补充甚至去偏人类判断。虽然LLMs对查询格式"脆弱"，但对决策偏见原因"稳定"。研究发布了162场景基准测试、代码和数据供进一步评估。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [53] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 该论文提出了一个针对AI辅助芯片设计的综合基准测试ChipBench，用于评估LLM在Verilog生成、调试和参考模型生成三个关键任务上的性能，揭示了现有基准测试饱和且任务多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前硬件工程领域的LLM基准测试存在饱和问题和任务多样性有限，无法反映LLM在真实工业工作流程中的实际性能，需要更全面的评估框架。

Method: 提出了包含三个关键任务的综合基准测试：1) 44个具有复杂层次结构的真实模块的Verilog生成；2) 89个系统调试案例；3) 132个涵盖Python、SystemC和CXXRTL的参考模型样本。同时提供了自动工具箱用于生成高质量训练数据。

Result: 评估结果显示性能差距显著：最先进的Claude-4.5-opus在Verilog生成上仅达到30.74%，在Python参考模型生成上仅13.33%，远低于现有饱和基准测试中SOTA模型超过95%的通过率。

Conclusion: 该基准测试揭示了LLM在硬件工程任务中的实际挑战，为未来研究提供了评估框架和高质量训练数据生成工具，促进了AI辅助芯片设计领域的发展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [54] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: LION是一个基于Clifford代数和解耦图神经范式的新型多模态图学习方法，通过几何感知流形实现模态对齐，然后通过自适应全息聚合进行模态融合，在多种下游任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图学习方法存在两个主要问题：1) 模态对齐时忽视图上下文，抑制模态交互；2) 模态融合缺乏适应性，简单适应双模态图且未能充分利用对齐后的拓扑先验信息，导致泛化性差和性能下降。

Method: 基于Clifford代数和传播-聚合解耦图神经范式，提出LION框架：1) 构建基于Clifford代数的模态感知几何流形，通过几何诱导的高阶图传播实现模态交互和对齐；2) 基于对齐token的几何等级特性，提出自适应全息聚合模块，整合几何等级的能量和尺度与可学习参数来改进模态融合。

Result: 在9个数据集上的广泛实验表明，LION在3个图下游任务和3个模态下游任务中显著优于最先进的基线方法。

Conclusion: LION通过Clifford代数驱动的几何感知对齐和自适应融合机制，有效解决了多模态图学习中的模态对齐和融合问题，提升了多模态图表示学习的实用性和性能。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [55] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值自动确定聚类数量，结合词汇和语义指标来获取高质量主题关键词，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好，且对样本大小和超参数变化更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的聚类主题建模方法Top2Vec和BERTopic存在多个未解决的问题：1) 对样本大小和超参数极度敏感，难以发现自然聚类；2) 关键词重要性估计方法有缺陷，BERTopic忽略关键词与主题向量的语义距离，Top2Vec忽略词频，导致主题不连贯且缺乏多样性。

Method: 提出Topeax方法：1) 通过密度估计峰值自动发现聚类数量；2) 结合词汇统计（词频）和语义指标（与主题向量的距离）来评估术语重要性，生成高质量主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数变化表现出更稳定的行为，减少了异常波动。

Conclusion: Topeax解决了现有聚类主题建模方法的关键缺陷，提供了更可靠、更稳定的主题发现和描述能力，是主题建模领域的重要改进。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [56] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR：一种多模态记忆代理，通过视觉布局自适应分配记忆空间，在有限上下文预算下提升长时程推理能力


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统通常将历史序列化为文本，其中token成本均匀且随长度线性增长，经常将稀缺的上下文预算浪费在低价值细节上。长时程智能体推理需要将不断增长的交互历史有效压缩到有限的上下文窗口中。

Method: MemOCR维护结构化富文本记忆（如标题、高亮），并将其渲染成图像供智能体进行记忆访问，通过视觉布局优先显示关键证据，同时积极压缩辅助细节。使用强化学习在预算感知目标下训练MemOCR，使其适应不同的压缩级别。

Result: 在长上下文多跳和单跳问答基准测试中，MemOCR优于基于文本的基线方法，在极端预算下实现了更有效的上下文利用。

Conclusion: 通过视觉布局自适应分配记忆空间的多模态方法能够显著提升长时程推理在有限上下文预算下的性能，为智能体记忆系统设计提供了新思路。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [57] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于LLM的多智能体模拟内存优化系统，通过预测智能体调用顺序实现主动预取和优先级驱逐，提升1.74倍性能


<details>
  <summary>Details</summary>
Motivation: LLM多智能体模拟难以扩展，因为每个智能体的私有GPU状态（模型、前缀缓存、适配器）会快速耗尽设备内存，限制了模拟规模

Method: 提出"调用距离"抽象概念来估计智能体未来LLM请求的相对顺序，基于此实现主动预取和基于优先级的驱逐策略，支持模块化接口管理多样化的智能体特定内存

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过创新的内存管理策略有效解决了LLM多智能体模拟的内存扩展问题，为大规模模拟提供了高效解决方案

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [58] [The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus](https://arxiv.org/abs/2601.21494)
*Ishan Jindal,Sai Prashanth Akuthota,Jayant Taneja,Sachin Dev Sharma*

Main category: cs.AI

TL;DR: PoLR是一种基于前缀一致性的推理优化方法，通过聚类推理轨迹的短前缀识别主导簇，仅扩展该簇中的路径，在保持Self-Consistency准确性的同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但Self-Consistency等推理策略需要完全扩展所有推理轨迹，计算成本高昂。需要一种既能保持准确性又能显著降低计算开销的推理优化方法。

Method: PoLR通过聚类推理轨迹的短前缀，识别出主导簇，然后仅扩展该簇中的所有路径。该方法基于互信息和熵的理论分析，利用早期推理步骤编码的强预测信号来预测最终正确性。

Result: 在GSM8K、MATH500、AIME24/25和GPQA-DIAMOND等基准测试中，PoLR始终匹配或超过Self-Consistency的性能，同时将token使用量减少高达60%，延迟降低高达50%。

Conclusion: PoLR是首个利用前缀一致性实现计算高效推理的推理时方法，与自适应推理方法完全互补，可作为即插即用的预过滤器，无需模型微调即可显著提高Self-Consistency的效率和可扩展性。

Abstract: Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.

</details>


### [59] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai,Guiqin Wang,Kejie Zhao,Jianxiong Tang,Xiang Wang,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: MAR框架通过集成状态空间模型实现线性时间序列建模，并应用激活稀疏化降低FFN成本，同时设计ATMN神经元和SBDS策略解决SNN与SSM集成问题，显著降低推理能耗并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多个领域表现出色，但存在高能耗问题，主要源于二次注意力机制和密集的前馈网络操作。需要解决这些效率瓶颈以构建更实用的高效LLMs。

Method: 提出模块感知架构优化（MAR）两阶段框架：1）集成状态空间模型（SSMs）实现线性时间序列建模；2）应用激活稀疏化降低FFN成本。为解决SNN与SSM集成时的信息密度低和时间不匹配问题，设计了自适应三元多步神经元（ATMN）和脉冲感知双向蒸馏策略（SBDS）。

Result: 实验表明MAR在资源受限条件下有效恢复了密集对应模型的性能，同时显著降低了推理能耗。此外，MAR在可比甚至更大规模的高效模型中表现更优。

Conclusion: MAR框架展示了构建高效实用大型语言模型的潜力，通过创新的架构优化和集成策略，在保持性能的同时大幅降低能耗。

Abstract: Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.

</details>


### [60] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 本文首次通过人类评估验证了激活引导技术对LLM输出情感强度的控制效果，发现中等强度引导能有效增强目标情感并保持文本质量，且人类评分与自动评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 控制大型语言模型在推理时的行为对于使其输出符合人类能力和安全要求至关重要。激活引导提供了一种轻量级的替代方案，通过直接修改内部激活来引导生成，避免了复杂的提示工程和微调。

Method: 1) 首次对激活引导进行人类评估，通过Prolific平台收集了190名参与者的7000多个众包评分，评估情感强度和文本质量；2) 比较人类评分与基于模型的自动评分的一致性；3) 测试不同引导强度对情感控制的效果；4) 比较Alpaca和LlaMA-3模型的表现差异。

Result: 1) 人类与模型质量评分高度一致（平均r=0.776）；2) 中等引导强度（λ≈0.15）能可靠增强目标情感同时保持可理解性，对厌恶（η_p²=0.616）和恐惧（η_p²=0.540）效果最强，对惊讶（η_p²=0.042）效果最弱；3) LlaMA-3比Alpaca产生更一致的引导效果；4) 评分者间信度高（ICC=0.71-0.87）。

Conclusion: 激活引导是一种可扩展的方法，能够有效控制LLM在情感维度上的行为。中等强度的引导可以在增强目标情感的同时保持文本质量，且自动评分可以作为人类感知质量的代理。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [61] [LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI](https://arxiv.org/abs/2601.21511)
*Niki van Stein,Anna V. Kononova,Lars Kotthoff,Thomas Bäck*

Main category: cs.AI

TL;DR: LLaMEA-SAGE：通过从生成算法的抽象语法树中提取图论和复杂性特征，构建反馈机制来指导自动化算法设计，相比纯基于适应度的进化方法能更快达到相同性能，并在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化算法设计方法（如LLaMEA）主要依赖适应度反馈驱动搜索，但忽略了生成代码中的大量结构信息。需要利用代码的结构特征来更有效地指导算法设计过程。

Method: 提出LLaMEA-SAGE方法：1）从生成算法的抽象语法树中提取图论和复杂性特征；2）基于已评估解构建代理模型；3）使用可解释AI技术识别影响性能的关键特征；4）将这些特征转化为自然语言突变指令，指导后续LLM代码生成。

Result: 1）在小型控制实验中，结构化指导方法比原始LLaMEA更快达到相同性能；2）在GECCO-MA-BBOB竞赛的MA-BBOB套件大规模实验中，该方法优于最先进的自动化算法设计方法。

Conclusion: 从代码中提取的信号能有效偏置LLM驱动的算法进化，弥合了代码结构与人类可理解的性能反馈之间的差距，为自动化算法设计提供了更有效的指导机制。

Abstract: Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.
  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.

</details>


### [62] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO是一个模块化框架，通过迭代的构思、代码合成与编辑、执行、评估和学习过程，实现自主程序合成与优化，将合成作为长期优化循环中的操作符。


<details>
  <summary>Details</summary>
Motivation: 针对当前编码代理在长时程任务中常见的失败模式，包括实验状态丢失、调试脆弱性、领域专业知识复用能力弱等问题，需要一种能够持续优化可运行工件的系统。

Method: KAPSO包含三个紧密耦合的组件：1) git原生实验引擎，将每次尝试隔离为分支；2) 知识系统，整合异构资源并组织成结构化表示；3) 认知记忆层，协调检索并维护可重用经验教训的存储。

Result: 在MLE-Bench（Kaggle风格机器学习竞赛）和ALE-Bench（AtCoder启发式优化）上进行了评估，并报告了端到端性能表现。

Conclusion: KAPSO通过将合成作为优化循环中的操作符，结合实验管理、知识整合和认知记忆，有效解决了编码代理的长时程失败问题，实现了可重复的自主程序优化。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [63] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织成显式论证图，通过因果模型分析哪些论证链对最终决策是必要的，并提供校正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动了最终决策。需要一种能明确展示论证结构并分析因果关系的框架。

Method: ARGORA将多专家讨论组织成显式论证图（显示论证间的支持/攻击关系），将其作为因果模型，通过系统性移除单个论证并重新计算结果来识别必要推理链。引入校正机制在内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA达到竞争性准确率，展示校正行为：当专家初始意见不一致时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供决定性论证的因果诊断。

Conclusion: ARGORA通过论证图和因果分析提供了透明、可解释的多专家LLM决策框架，不仅能提高准确性，还能诊断关键论证并校正推理偏差。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [64] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo：一个为多智能体LLM系统设计的预算分层内存服务，通过分片证据存储、范围优先路由和成本感知门控机制，显著提升检索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着内存容量和并行访问增长，集中式索引和启发式分区成为多智能体LLM系统的瓶颈，需要更高效的内存管理方案来支持长时状态和并发执行。

Method: 设计三层内存架构：A层（智能体工作状态）、B层（分片证据存储，含本地ANN索引）、C层（版本化技能库）。B层采用"范围优先路由"机制，通过结构化资格约束屏蔽不合格分片，将分片探测建模为掩码混合专家路由，使用成本感知门控和Top-Bprobe/自适应Top-P策略。

Result: 在LoCoMo上比最强基线(GAM)提升5.11-6.82 F1；固定预算路由下比余弦原型路由提升6.87 F1，同时减少20.5%检索工作和降低p95延迟；在长上下文HotpotQA上达到57.95-63.41 F1；在ToolBench上技能库检索精度达0.97 P@3。

Conclusion: ShardMemo通过分层内存架构和智能路由机制，有效解决了多智能体LLM系统的内存管理瓶颈，在保持高性能的同时显著降低了检索成本和延迟。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [65] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: Meta Context Engineering (MCE) 是一个双层框架，通过共同演化上下文工程技能和上下文工件来超越静态启发式方法，在多个领域实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前上下文工程方法依赖于人工设计的框架，如固定的生成-反思工作流和预定义的上下文模式，这些方法引入了结构偏差并将上下文优化限制在狭窄的直觉驱动设计空间中。

Method: MCE采用双层框架：元级代理通过代理交叉（对技能历史、执行和评估的审慎搜索）来优化工程技能；基础级代理执行这些技能，从训练回放中学习，并将上下文优化为灵活的文件和代码。

Result: 在五个不同领域的离线和在线设置中评估，MCE相对于最先进的代理上下文工程方法实现了5.6-53.8%的相对改进（平均16.9%），同时在上下文适应性、可迁移性以及上下文使用和训练效率方面保持优势。

Conclusion: MCE通过共同演化技能和上下文工件，超越了静态的上下文工程启发式方法，为大型语言模型的上下文优化提供了更灵活、高效且适应性强的解决方案。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [66] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: EmboCoach-Bench：首个评估LLM智能体自主设计机器人策略的基准，通过32个任务测试智能体利用环境反馈迭代优化代码的能力，结果显示自主智能体性能超越人工基线26.5%


<details>
  <summary>Details</summary>
Motivation: 当前具身AI领域向通用机器人系统快速发展，但严重依赖人工监督（从奖励设计到超参数调优），成为规模化瓶颈。受LLM在软件自动化和科学发现中成功的启发，需要评估LLM智能体自主设计具身策略的能力

Method: 提出EmboCoach-Bench基准，包含32个专家策划的强化学习和模仿学习任务，以可执行代码作为通用接口。采用动态闭环工作流程，智能体利用环境反馈迭代起草、调试和优化解决方案，涵盖从物理感知奖励设计到扩散策略等架构改进

Result: 1) 自主智能体平均成功率超越人工基线26.5%；2) 带环境反馈的智能体工作流程显著增强策略开发，大幅缩小开源与专有模型性能差距；3) 智能体展现自校正能力，通过迭代仿真调试从近乎完全失败中恢复任务性能

Conclusion: 这项工作为自我进化的具身智能奠定基础，加速从劳动密集型人工调优向可扩展自主工程的范式转变，推动具身AI领域发展

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [67] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次从理论上分析了学习内化中间推理步骤的困难性，提出ALiCoT框架通过对齐潜在token分布与中间推理状态来克服信号衰减，实现54.4倍加速的同时保持与显式CoT相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT解锁了LLM的高级推理能力，但生成额外token带来了高昂计算成本。最近的隐式CoT压缩提供了token高效的替代方案，但其机制尚不明确。本文旨在从理论上分析学习内化中间推理步骤的困难性。

Method: 1) 引入Order-r Interaction理论框架，证明高阶逻辑依赖的学习信号会指数衰减；2) 创建NatBool-DAG基准测试，强制不可约逻辑推理并消除语义捷径；3) 提出ALiCoT框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: ALiCoT成功解锁高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的加速。实验验证了理论分析的正确性，即隐式CoT压缩确实面临高阶交互障碍。

Conclusion: 本文首次从理论上揭示了CoT压缩的机制，证明了学习内化中间推理步骤的困难性源于高阶逻辑依赖的信号衰减。提出的ALiCoT框架通过状态对齐有效克服了这一障碍，为高效推理提供了理论指导和实用解决方案。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [68] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度注意力混合解决深度递归模型的隐藏层大小瓶颈，在语言推理任务上比匹配FLOP、参数和内存的SOTA模型减少2-8倍训练token，性能优于约2倍大的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归模型存在三个问题：缺乏FLOP、参数和内存匹配的基线；由于部分固定层堆栈导致深度递归未充分利用；恒定隐藏层大小限制多步潜在推理。需要解决这些瓶颈以实现更高效的深度递归模型。

Method: 提出Dreamer框架，结合序列注意力、深度注意力和稀疏专家注意力。通过深度注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度递归模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，达到相同准确率所需训练token减少2-8倍；使用相同训练token时，性能优于约2倍大的SOTA模型。深度知识使用分析显示专家选择多样性比SOTA MoEs高2-11倍。

Conclusion: Dreamer框架通过深度注意力混合有效解决了深度递归模型的瓶颈问题，实现了更高效的训练和推理性能，为深度递归模型的发展提供了新方向。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [69] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出主动潜在规划方法，通过条件变分自编码器平滑潜在空间，结合强化学习提升潜在推理策略，在LLaMA-1B上实现精度提升和token减少


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通过模仿语言标签监督潜在token，但一个问题可能有多个等效但不同的CoT标签，被动模仿会导致次优的潜在token表示和推理策略，限制规划能力并造成训练测试差距

Method: 提出ATP-Latent方法：1) 将潜在token监督建模为条件变分自编码器以获得更平滑的潜在空间；2) 使用强化学习结合一致性奖励（基于VAE解码内容的一致性）来引导最优潜在推理策略

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线实现了+4.1%的准确率提升和-3.3%的token减少

Conclusion: 主动规划潜在token表示空间对于实现最优潜在推理策略至关重要，ATP-Latent通过条件VAE平滑潜在空间和强化学习引导，显著提升了潜在推理的性能和效率

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [70] [CORE: Collaborative Reasoning via Cross Teaching](https://arxiv.org/abs/2601.21600)
*Kshitij Mishra,Mirat Aubakirov,Martin Takac,Nils Lukas,Salem Lahlou*

Main category: cs.AI

TL;DR: CORE：一种训练时协作框架，通过交叉教学协议将同伴成功转化为学习信号，显著提升小模型在推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在互补的推理错误：同一问题中，一个模型可能通过特定分解成功而另一个失败。如何利用这种互补性来提升模型性能而不增加模型规模？

Method: 提出CORE协作推理框架：1）冷轮独立采样；2）上下文救援轮，失败模型接收来自成功同伴的提示；3）优化组合奖励：正确性+轻量级DPP多样性项减少错误重叠+明确救援奖励

Result: 仅用1000个训练样本，3B+4B模型对在GSM8K上达到99.54% Pass@2，MATH上92.08%，相比单模型训练（82.50%和74.82%）显著提升。在更难的GPQA和AIME数据集上也取得优异表现

Conclusion: 训练时协作可以可靠地将模型互补性转化为显著性能提升，无需扩大模型规模，为小模型协作推理提供了有效框架

Abstract: Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.

</details>


### [71] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出一种基于搜索的软件测试方法，用于智能文档处理系统的验证，通过组合文档配置空间来最大化发现不同类型的故障，并比较多种搜索策略的效果。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等领域支持高风险工作流。在有限预算下进行早期系统验证需要发现多样化的故障机制，而不仅仅是识别单个最坏情况文档。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，实例化结构风险特征以诱导真实故障条件。比较了进化算法、群体智能、质量多样性、学习型和量子计算等多种搜索策略。

Result: 不同求解器在相同预算下能发现其他方法未发现的故障模式，交叉时间分析显示所有评估预算中都存在求解器特定的发现，没有单一策略表现出绝对优势。所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 结果证明了求解器之间的内在互补性，并推动了基于组合策略的SBST方法，用于稳健的工业IDP验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [72] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet是一个自进化的偏好传播框架，通过路由器代理主动传播实时偏好更新，结合个性化接收机制和反馈驱动的优化，解决现有推荐系统无法建模用户-物品实时相互影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能推荐系统主要依赖稀疏、嘈杂的显式用户-物品交互来建模偏好变化，无法反映用户和物品之间的实时相互影响，存在局限性。

Method: RecNet包含两个互补阶段：前向阶段通过集中式偏好路由机制传播偏好更新，结合个性化偏好接收机制（消息缓冲区和可优化的基于规则的过滤记忆）；后向阶段通过反馈驱动的传播优化机制，使用LLM进行信用分配、梯度分析和模块级优化。

Result: 在各种场景下的广泛实验证明了RecNet在建模推荐系统偏好传播方面的有效性。

Conclusion: RecNet通过自进化的偏好传播框架，能够主动传播实时偏好更新，解决现有推荐系统在建模用户-物品实时相互影响方面的局限性，为智能推荐系统提供了新的解决方案。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [73] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出WhatCounts测试框架，发现LLMs在计数任务中的表现严重依赖于被计数对象的语义内容，而非算法本身，揭示了LLMs并非真正实现算法而是近似算法


<details>
  <summary>Details</summary>
Motivation: 算法的行为应该独立于其参数的语义内容，但现有研究往往将语义敏感性与推理复杂性或提示变化混淆。需要一种原子化的测试方法来专门评估LLMs对语义内容的敏感性

Method: 引入WhatCounts测试框架：在无歧义、分隔的列表中计数项目，无重复项、干扰项或不同语义类型的推理步骤。通过控制消融实验排除混杂因素

Result: 前沿LLMs的准确率变化超过40%，仅取决于被计数对象的语义类型（城市vs化学品、名称vs符号）。少量无关的微调会导致性能变化不可预测。LLMs并非实现算法而是近似算法，且这种近似是参数依赖的

Conclusion: LLMs的行为存在对输入语义内容的隐藏依赖，这不仅限于计数任务，任何LLM函数都可能携带这种依赖，对基于LLM的代理系统有重要影响

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [74] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估学术文献深度研究工作流的仿真环境，通过静态语料库和确定性检索解决API调用不确定性问题，提供可重复的评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强的大型语言模型已从单轮问答发展到深度研究工作流，但评估这些工作流面临根本挑战：依赖实时API会引入非确定性，因为工具调用可能因时间漂移、速率限制和后端状态变化而产生不同结果，这种差异破坏了可重复性并使跨系统比较无效。

Method: 将工作流组件解耦为查询规划、工具调用和相关性评估，在受控条件下对每个阶段进行细粒度分析。基于包含57万篇论文的静态语料库构建，提供确定性检索，包含2,536个查询和专家标注的真实答案。

Result: 实验涵盖了多种骨干模型，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。

Conclusion: ScholarGym为深度研究工作流提供了可重复的评估环境，解决了实时API依赖带来的非确定性问题，支持对复杂信息需求处理流程的系统性分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [75] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准测试，用于评估多模态大语言模型在时序音频-视频数据上的性能，涵盖13个真实对话领域，包含4,958个标注和人口统计元数据。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多模态大语言模型研究集中在静态图像理解，而对处理时序音频-视频数据的能力研究不足，需要高质量基准测试来系统评估模型在真实场景中的性能。

Method: 构建SONIC-O1基准测试，涵盖13个真实对话领域，包含4,958个人工验证的标注，评估任务包括开放式摘要、多项选择题回答、时序定位及推理依据。对闭源和开源模型进行实验比较。

Result: 实验显示模型存在局限性：在MCQ准确率上模型家族间差距较小，但在时序定位任务上，最佳闭源模型比开源模型性能高出22.6%。性能在不同人口统计群体间进一步下降，表明模型行为存在持续差异。

Conclusion: SONIC-O1为时序基础和社交鲁棒的多模态理解提供了开放评估套件，揭示了当前模型的局限性，特别是时序定位能力和人口统计公平性方面的差距，为未来研究提供了基准。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [76] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 本文提出TCAP框架，通过分析注意力分配差异来检测MLLM微调中的后门攻击，无需监督信号即可有效防御多种触发类型和模态的攻击。


<details>
  <summary>Details</summary>
Motivation: FTaaS在促进MLLM定制化的同时，引入了通过中毒数据的后门风险。现有防御方法要么依赖监督信号，要么无法泛化到不同的触发类型和模态。

Method: 提出Tri-Component Attention Profiling (TCAP)框架：1) 将跨模态注意力图分解为三个功能组件（系统指令、视觉输入、用户文本查询）；2) 使用高斯混合模型识别触发响应注意力头；3) 通过EM投票聚合隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上的广泛实验表明，TCAP能够持续实现强大的性能，成为MLLM中稳健实用的后门防御方案。

Conclusion: TCAP通过发现后门指纹——注意力分配差异，提供了一种无监督的防御框架，能够有效检测和过滤中毒样本，解决了MLLM微调中的后门安全问题。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [77] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入Parafovea-Attention Window、Chunk-Head和Skip-Gate三个模块，在保持参数不变的情况下提升LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖严格的token-by-token自回归生成，现有加速方法大多只是修补现有流程，缺乏人类阅读中的关键要素：内容自适应前瞻、块结构感知计算分配以及训练-测试一致性

Method: 提出Fovea-Block-Skip Transformer，通过三个核心模块注入可训练因果循环：Parafovea-Attention Window实现内容自适应前瞻，Chunk-Head进行块结构感知计算分配，Skip-Gate确保训练-测试一致性

Result: 在多样化基准测试中，FBS在保持参数不变的情况下改善了质量-效率权衡，消融实验显示三个模块具有互补性

Conclusion: FBS Transformer通过模拟人类阅读机制，为LLM推理加速提供了新的有效方法，三个模块的协同作用显著提升了推理效率

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [78] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从内存预处理转向情景上下文重建，解决了LLM智能体在系统2推理中上下文完整性问题，在LoCoMo基准上实现54%+ F1，超越SOTA 7.75%，同时减少70%+令牌成本。


<details>
  <summary>Details</summary>
Motivation: 现有内存预处理方法存在破坏性去上下文化问题，将复杂序列依赖压缩到预定义结构（如嵌入或图）中，切断了深度推理所需的关键上下文完整性。

Method: 提出E-mem框架，从内存预处理转向情景上下文重建。采用异构分层架构：多个助手智能体维护未压缩内存上下文，中央主智能体协调全局规划。助手在激活片段内进行本地推理，提取上下文感知证据后再聚合。

Result: 在LoCoMo基准测试中，E-mem实现超过54%的F1分数，超越最先进的GAM方法7.75%，同时减少超过70%的令牌成本。

Conclusion: E-mem通过情景上下文重建有效解决了LLM智能体系统2推理中的上下文完整性问题，在保持逻辑严谨性的同时显著提升了性能和效率。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [79] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS：一种模型无关的插件，通过样本自适应dropout机制，利用频谱稀疏性量化实例级噪声，动态校准模型学习能力，在保持细粒度保真度的同时抑制虚假波动，提升深度时间序列模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中深度时间序列模型容易受到噪声数据影响，现有鲁棒性策略要么剪枝数据，要么依赖昂贵的先验量化，无法平衡有效性和效率。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率，动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过将学习范式从"学什么"转向"学多少"，提供了一种高效、模型无关的解决方案，显著增强了深度时间序列模型在噪声环境下的鲁棒性。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [80] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: ECL框架让多智能体系统通过历史交互评估同伴可靠性，小模型可超越大模型，实现近完美性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴，这源于谄媚行为和对同伴可靠性评估能力不足

Method: 提出历史感知参考学习问题，开发认知上下文学习(ECL)框架：基于历史交互构建同伴档案，通过强化学习优化辅助奖励

Result: ECL使Qwen 3-4B小模型超越历史无关基线8倍大的Qwen 3-30B，前沿模型达到近100%性能，信任建模准确性与最终答案质量强相关

Conclusion: ECL通过历史交互评估同伴可靠性有效提升多智能体系统性能，信任建模能力是提升答案质量的关键，框架具有良好的泛化能力

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [81] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: SCOUT框架通过轻量级"侦察兵"探索环境，收集轨迹数据来引导LLM，解决了LLM在非语言环境中探索成本过高的问题，显著提升了性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言任务上表现出色，但在未见过的非语言环境（如符号或空间任务）中应用有限。先前研究认为性能差距源于预训练分布与测试分布不匹配，但本文发现主要瓶颈是探索成本过高：掌握这些任务需要大量试错，对于在语义高维空间中运行的重参数LLM来说计算上不可持续。

Method: 提出SCOUT框架，将探索与利用解耦：1）使用轻量级"侦察兵"（如小型MLP）以远超LLM的速度和规模探测环境动态；2）利用收集的轨迹通过监督微调引导LLM；3）通过多轮强化学习激活LLM的潜在世界知识。

Result: SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于Gemini-2.5-Pro（0.60）等专有模型，同时节省约60%的GPU小时消耗。

Conclusion: 探索成本是LLM在非语言环境中应用的主要瓶颈，SCOUT框架通过解耦探索与利用，使用轻量级侦察兵高效探索环境，有效引导LLM学习，在提升性能的同时大幅降低计算成本。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [82] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致性气候先验和统一坐标指导解决GCM泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据的领域差异而难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题

Method: 提出零样本统计降尺度框架，包含：1）从再分析数据学习物理一致性气候先验，结合地球物理边界和时序信息确保物理有效性；2）引入统一坐标指导策略，解决DPS中的梯度消失问题并保持与大尺度场的一致性

Result: ZSSD在99百分位误差上显著优于现有零样本基线，能成功重建复杂天气事件（如热带气旋），并在异质GCMs中表现良好

Conclusion: ZSSD为零样本气候降尺度提供了有效的解决方案，通过物理一致性先验和统一坐标指导实现了对全球气候模型的泛化能力

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [83] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出一个基于概念空间框架的时间演化抽象概念建模方法，以国际象棋为案例，将策略概念表示为几何区域，通过轨迹分析识别意图策略


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到时间演化、目标导向的抽象概念建模，解决传统概念空间理论在处理时序概念时的局限性

Method: 使用概念空间框架，将策略概念（如攻击、牺牲）表示为可解释质量维度上的几何区域，将棋局实例化为轨迹，通过轨迹向特定区域的定向移动来识别策略意图

Result: 实现证明轨迹基概念识别的可行性，运动模式与专家评论一致，支持双视角建模以捕捉玩家对相同情况的不同解释

Conclusion: 为涉及序列决策的更广泛应用奠定基础，支持与知识演化机制集成，用于随时间学习和精炼抽象概念

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [84] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench：用于评估AI代理在生物信息学任务中性能和鲁棒性的基准数据集与评估套件，包含端到端任务和自动化评估，测试发现前沿代理能完成多步骤流程但鲁棒性有待提升。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI代理在生物信息学任务中的实际性能和鲁棒性，现有评估缺乏对端到端任务完成能力和抗干扰能力的系统测试。

Method: 构建包含RNA-seq、变异检测、宏基因组学等生物信息学任务的基准数据集，设计具体输出规范支持自动化评估，引入受控扰动进行压力测试，使用LLM评分器评估流程进展和结果有效性。

Result: 前沿代理能够完成多步骤生物信息学流程而无需复杂定制框架，但鲁棒性测试显示在输入损坏、诱饵文件和提示膨胀等扰动下存在失败模式，开源模型在隐私敏感场景下更具优势。

Conclusion: AI代理能够执行复杂生物信息学工作流，但鲁棒性仍需改进；隐私约束下开源模型可能更合适；基准数据集公开促进该领域研究。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [85] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 本文提出一个基于大语言模型（LLM）的统一框架，用于气管内吸痰（ES）培训的视频活动识别和反馈生成，相比传统方法在准确率和F1分数上提升15-20%。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是一项高风险临床操作，在家庭护理和教育环境中缺乏有效监督和反馈系统。目前自动化识别和反馈系统研究不足，需要开发可扩展的培训解决方案。

Method: 提出以LLM为核心推理模块的统一框架，执行视频数据的时空活动识别和可解释决策分析。LLM还能生成自然语言反馈，将技术洞察转化为易懂的指导。框架还包括基于异常检测和可解释AI原则的学生支持模块。

Result: LLM方法在准确率和F1分数上比基线模型提升约15-20%。框架能够提供自动化的、可解释的反馈，突出正确操作并建议针对性改进。

Conclusion: 该研究建立了一个可扩展、可解释、数据驱动的基础，用于推进护理教育、提高培训效率，最终改善患者安全。LLM中心框架在医疗培训领域展现出显著潜力。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [86] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: CORE框架通过分布式LLM协作解决6G网络中异构计算资源碎片化问题，实现高效边缘智能推理


<details>
  <summary>Details</summary>
Motivation: 6G网络和LLM发展为普适智能创造了条件，但异构计算资源碎片化限制了单个LLM代理执行复杂推理任务的能力

Method: 提出CORE框架，将具有不同功能角色的多个LLM分布在移动设备和分层边缘服务器上，包含实时感知、动态角色编排、流水线并行执行三个优化模块，并引入角色亲和度调度算法

Result: 在多种6G应用场景中验证了CORE的有效性，显著提升了系统效率和任务完成率，并在真实边缘计算平台上部署验证了实际适用性

Conclusion: CORE框架通过分布式LLM协作和智能资源匹配，成功解决了6G网络中异构计算资源碎片化问题，为普适智能的实现提供了可行方案

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [87] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 提出一個全面的心電圖專家基礎模型基準測試框架，結合性能評估與表示層分析，使用SHAP和UMAP技術深入評估模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 雖然基礎模型在AI輔助心電圖解讀領域開始發展，但現有基準測試主要關注下游任務性能，缺乏對嵌入表示泛化能力的深入評估，特別是在醫療這種對錯誤敏感的領域。

Method: 提出一個基準測試方法論，結合性能評估與表示層分析，使用SHAP（可解釋AI）和UMAP（降維可視化）技術。在多個跨大陸數據集和不同數據可用性設置下（包括數據稀缺情況）評估多個心電圖專家基礎模型。

Result: 實驗結果顯示，該基準測試協議能深入洞察心電圖專家基礎模型的嵌入模式，幫助更深入理解其表示結構和泛化能力。

Conclusion: 提出的綜合基準測試框架為心電圖專家基礎模型提供了更全面的評估方法，特別強調了表示層分析的重要性，有助於在醫療領域更負責任地部署基礎模型。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [88] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 论文提出一个决策中心的仿真框架，用于评估备件库存管理中预测模型对运营绩效（而非统计精度）的影响。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场的备件需求高度间歇且不确定，现有研究仅用统计精度（如MAE、RMSE）评估预测模型，但这些指标与运营KPI（如总成本、服务水平）的关系不明确，需要建立预测模型与库存决策之间的系统性评估框架。

Method: 提出一个决策中心的仿真软件框架，包含三个模块：(1) 针对备件需求特征的合成需求生成器；(2) 可容纳任意预测模型的灵活预测模块；(3) 消耗预测并计算运营KPI的库存控制模拟器，形成闭环评估系统。

Result: 通过广泛的仿真场景表明：传统精度指标的改进不一定带来更好的运营绩效；具有相似统计误差的模型可能产生显著不同的成本-服务权衡；分析了预测性能特定方面如何影响库存结果，为模型选择提供指导。

Conclusion: 该框架将需求预测与库存管理操作化连接起来，将评估重点从纯粹的预测精度转向运营相关性，适用于汽车售后市场及相关领域。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [89] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias提出了一种新的LLM去偏方法，通过增强而非抑制编码偏见知识的神经元来实现去偏，仅需少量偏见知识问题即可在推理时选择性增强相关神经元，在保持模型通用能力的同时实现高效去偏。


<details>
  <summary>Details</summary>
Motivation: 现有LLM去偏方法大多采用抑制范式（修改参数、提示或与偏见行为相关的神经元），但这些方法存在脆弱性、泛化能力弱、数据效率低且容易降低模型通用能力的问题。需要一种更稳健、高效且能保持模型性能的去偏方法。

Method: KnowBias框架：1）通过基于归因的分析，使用少量偏见知识问题识别编码偏见知识的神经元；2）在推理时选择性增强这些神经元。该方法无需重新训练，仅需少量简单的二元问题，具有轻量级特点。

Result: 在多个基准测试和LLM上的实验表明，KnowBias实现了最先进的去偏性能，同时保持了最小的效用下降。该方法能够泛化到不同类型的偏见和人口统计特征，且数据效率极高。

Conclusion: KnowBias通过增强而非抑制偏见知识神经元的创新方法，提供了一种轻量级、高效且能保持模型通用能力的LLM去偏解决方案，为安全部署LLM提供了新思路。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [90] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: WebArbiter：一种基于推理的网页过程奖励模型，通过生成结构化理由和偏好判断来指导网页导航任务，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网页代理在自动化复杂计算机任务时面临长序列决策和不可逆动作的挑战，现有过程奖励模型存在局限性：标量模型提供粗粒度信号，清单式模型依赖脆弱的模板匹配且缺乏可解释性。

Method: 提出WebArbiter，将奖励建模转化为文本生成任务，生成包含偏好判断和最佳动作识别的结构化理由。采用两阶段训练：推理蒸馏使模型掌握原则指导的推理能力，强化学习纠正教师偏见并直接对齐判断与正确性。

Result: 在WebPRMBench基准测试中，WebArbiter-7B比最强基线GPT-5高出9.1分；在WebArena-Lite的奖励引导轨迹搜索中，比之前最佳WebPRM高出7.2分。

Conclusion: WebArbiter通过推理优先、原则诱导的方法解决了现有网页过程奖励模型的局限性，在复杂网页任务中展现出强大的泛化能力和实际价值。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [91] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 论文介绍了将大型语言模型（LLM）集成到ASTRA编程语言中的原型开发经验，探讨了传统Agent工具包与新兴Agentic AI平台的相互影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI（作为多智能体系统的一种形式）的兴起，需要探索这些新技术如何影响传统Agent工具包的使用，以及传统工具包中的丰富经验如何影响新Agent平台的设计。

Method: 开发了ASTRA编程语言的LLM集成原型，通过三个示例实现来展示集成方法，并基于这些示例讨论开发经验。

Result: 成功实现了ASTRA编程语言的LLM集成原型，通过三个具体示例展示了集成效果，并总结了开发过程中的实践经验。

Conclusion: 传统Agent工具包与新兴Agentic AI平台可以相互借鉴和影响，LLM集成到现有编程语言中是可行的，为未来Agent系统设计提供了有价值的经验。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [92] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: Trap²：一种架构无关的保护框架，通过在微调期间将保护编码到更新中，防止模型权重被未经授权地合并，同时保持独立使用的有效性。


<details>
  <summary>Details</summary>
Motivation: 模型中心的兴起使得模型合并变得更加容易，但这也带来了治理缺口：下游用户可以重新组合发布的权重，绕过安全对齐或许可条款。现有防御方法大多是事后且架构特定的，无法为实践中多样化的架构和发布格式提供一致保护。

Method: 提出Trap²框架，采用架构无关的保护方法，通过权重重新缩放作为合并过程的简单代理。在微调期间将保护编码到更新中，使发布的权重在独立使用时保持有效，但在合并过程中出现重新缩放时会性能下降，从而破坏未经授权的合并。

Result: Trap²能够有效防止模型权重被未经授权地合并，同时保持模型在独立使用时的性能。该方法不依赖于特定架构，适用于适配器和完整模型等多种发布格式。

Conclusion: Trap²填补了模型治理的缺口，提供了一种架构无关的保护机制，能够有效防止模型权重被未经授权地合并，同时保持模型在合法使用场景下的有效性。

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [93] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 提出认知启发的两阶段后训练框架CoMT+CCRL，通过分离抽象策略学习和具体任务适应，相比传统方法在泛化性和训练效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单位，这与人类认知过程存在根本差异。人类解决问题时先获取抽象策略（元知识），再适应具体任务，而现有方法将抽象策略与问题具体执行纠缠在一起，缺乏认知对齐。

Method: 提出两阶段认知启发框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的无具体执行监督学习，获取可泛化策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信错误级联。

Result: 在4个模型和8个基准测试中，相比标准方法，分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能获得更好的泛化性能，还能显著提高训练效率，证明了认知启发方法在LLM优化中的有效性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [94] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG提出了一种过程监督强化学习框架，通过细粒度过程奖励解决RAG优化中的奖励稀疏性和信用分配问题，在复杂推理任务上优于传统结果导向方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的RL方法在RAG优化中存在奖励稀疏和信用分配效率低的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"现象。

Method: 提出四阶段框架：1)监督策略预热初始化结构化推理格式；2)构建基于MCTS的过程奖励模型量化中间推理质量；3)PRM引导的推理精炼对齐细粒度过程偏好；4)双粒度优势机制的过程监督强化学习。

Result: 在五个多跳推理基准测试中，ProRAG相比基于结果和过程感知的RL基线实现了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过将学习到的步骤级监督集成到在线优化循环中，有效解决了RAG优化中的奖励稀疏性和信用分配问题，为复杂推理任务提供了精确的反馈机制。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [95] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE是一个联合优化规划和执行的统一框架，通过端到端学习将RAG系统建模为多智能体团队，解决现有方法中规划与执行模块分离导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临关键二分法：要么在固定架构中联合优化模块，要么支持动态规划但将执行器视为黑盒工具。这种"解耦优化"导致"战略操作不匹配"，复杂的规划策略因执行器不适应而无法实现，系统复杂度增加但性能反而下降。

Method: 提出JADE框架，将系统建模为在单一共享骨干网络下的协作多智能体团队，通过基于结果的奖励进行端到端学习，实现规划和执行的联合优化，促进"协同适应"。

Result: JADE将分离的模块转变为协同系统，通过联合优化带来显著的性能改进，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE解决了RAG系统中规划和执行模块解耦优化的问题，通过联合优化框架实现了更好的协同适应，为动态多轮工作流提供了有效的解决方案。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [96] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: SCMA是一个多智能体强化学习框架，通过分割和评分智能体协同工作，选择性惩罚冗余推理块，在减少推理长度的同时提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过简单地将长度惩罚与结果奖励耦合来处理推理冗余问题，但这种简单的奖励权重难以平衡简洁性和准确性，强制简洁可能会损害关键推理逻辑。

Method: 提出多智能体强化学习框架SCMA，包含三个智能体：分割智能体将推理过程分解为逻辑块，评分智能体量化每个块的重要性，推理智能体在训练期间使用重要性加权的长度惩罚，在部署时不引入推理开销。

Result: SCMA将响应长度减少了11.1%到39.0%，同时将准确性提高了4.33%到10.02%。消融研究和定性分析验证了MARL框架内的协同优化促进了涌现行为，产生了比传统RL范式更强大的大型推理模型。

Conclusion: SCMA框架通过多智能体协同优化，能够有效减少推理冗余而不损害准确性，为大型推理模型的部署提供了更高效的解决方案。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [97] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个法庭风格的多智能体辩论框架，通过结构化角色（检察官、辩护、法官）和7轮辩论协议，为高风险表格决策任务提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒方法在高风险决策中缺乏透明度的问题，特别是在刑事司法等敏感领域，需要可审计的决策过程和人类监督。

Method: 采用角色结构化的多智能体辩论框架，定义明确的智能体角色（检察官、辩护、法官）、7轮结构化辩论协议和私有推理策略，在NLSY97数据集上进行年轻成人再犯预测测试。

Result: 相比传统链式思维提示，结构化多智能体辩论提供更稳定和可泛化的性能，准确率和F1分数相关性更强，同时提供细粒度推理控制、完整交互记录和行为分析能力。

Conclusion: AgenticSimLaw通过结构化角色、可观察的交互记录和明确的部署限制，解决了LLM多智能体系统的关键挑战，适用于需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [98] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2是一个用于评估大语言模型在科学推理能力上的深度研究沙箱，通过四种证据访问机制分离检索损失和推理损失，揭示模型在文档基础推理中的真实能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确评估大语言模型对新颖科学信息的推理能力，因为端到端RAG管道中的推理与检索、工具链选择混淆，且受到参数记忆和网络波动的影响。

Method: 设计DeR2深度研究沙箱，通过四种证据访问机制（仅指令、仅概念、仅相关文档、完整集）分离证据访问与推理，使用两阶段验证防止参数泄漏，提供冻结文档库和专家标注概念。

Result: 实验显示不同基础模型表现差异显著，存在模式切换脆弱性（完整集表现比仅指令更差）和结构性概念误用（正确命名概念但无法作为程序执行）等问题，表明仍有很大改进空间。

Conclusion: DeR2提供了一个受控环境来评估大语言模型的文档基础推理能力，揭示了当前模型在深度科学推理中的局限性，为未来模型改进提供了细粒度诊断工具。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [99] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver：一种新的生成式工具学习框架，通过将工具编码为分层序列来解决检索式工具使用中的语义瓶颈和可扩展性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索的工具使用管道面临双重语义挑战：检索器编码器难以捕捉复杂语义，而大语言模型本身缺乏工具知识。生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新标记的做法导致可扩展性危机和语义瓶颈，阻碍了协作工具关系的学习。

Method: 提出ToolWeaver框架，将工具编码为分层序列，使词汇表扩展与工具数量呈对数关系。通过新颖的标记化过程生成结构化代码，将工具的内在语义与外部使用模式编织在一起，然后通过生成式对齐阶段将这些代码集成到LLM中，微调模型以产生分层代码序列。

Result: 在近47,000个工具上的评估结果显示，ToolWeaver显著优于最先进的方法，为高级工具增强代理建立了更可扩展、可泛化和语义感知的基础。

Conclusion: ToolWeaver通过分层序列编码解决了生成式工具学习中的可扩展性和语义瓶颈问题，能够从共享代码的密集共现中学习协作模式，为工具增强代理提供了更优越的框架。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [100] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: VAF是一个评估网页视觉属性如何影响Web智能体决策的量化框架，通过生成语义相同但视觉不同的变体，测量智能体在点击和推理上的行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体对对抗攻击的鲁棒性，而较少关注在良性场景中视觉属性如何影响智能体决策。虽然早期研究探讨了文本属性的影响，但对视觉属性如何塑造智能体决策的系统性理解仍然有限。

Method: VAF包含三个阶段：(1) 变体生成：创建语义相同但视觉属性不同的网页变体；(2) 浏览交互：智能体通过滚动和点击模拟人类浏览行为；(3) 验证：通过目标点击率和目标提及率联合评估视觉属性的影响。

Result: 实验表明，背景颜色对比度、项目大小、位置和卡片清晰度对智能体行为有显著影响，而字体样式、文本颜色和项目图像清晰度的影响较小。

Conclusion: VAF提供了一个系统化的框架来量化视觉属性对Web智能体决策的影响，揭示了哪些视觉因素在智能体决策中起关键作用，为设计更鲁棒和公平的Web智能体提供了指导。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [101] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 论文研究了领域模型特征对经典规划器能耗的影响，发现领域级修改能产生可测量的能耗差异，且能耗并不总是与运行时间相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究优先考虑算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效研究较少，特别是考虑到规划模块化结构中领域模型与算法分离的特点，这为通过领域模型设计系统分析能耗提供了机会。

Method: 引入领域模型配置框架，允许控制特征变化（如元素排序、动作元数、死端状态）。使用5个基准领域和5个最先进的规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 结果表明，领域级修改在不同规划器间产生可测量的能耗差异，且能耗并不总是与运行时间相关。

Conclusion: 领域模型设计对规划器能耗有显著影响，为绿色AI在自动规划领域的发展提供了新方向，表明通过优化领域模型特征可以改善规划系统的能效。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [102] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 本文提出两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，相比蒙特卡洛方法在长视野或稀疏奖励任务上表现更优


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议（通常需要集中式执行），而去中心化LLM协作更具实际吸引力。当前方法使用蒙特卡洛方法存在高方差问题，需要更多样本训练

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作

Result: 在写作、编码和游戏领域实验中，蒙特卡洛方法和CoLLM-DC在短视野和密集奖励任务中表现与CoLLM-CC相当，但在长视野或稀疏奖励任务中，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛，而CoLLM-CC表现更优

Conclusion: 多智能体演员-评论家方法（特别是集中式评论家版本）能有效优化去中心化LLM协作，在复杂任务中相比蒙特卡洛方法和去中心化评论家方法具有明显优势

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [103] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 语言模型的陈述偏好与显示偏好之间存在差距，现有评估方法存在局限，本文系统研究了不同引导协议对偏好一致性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现在语言模型中存在陈述偏好与显示偏好的不一致性，但现有评估方法主要依赖二元强制选择提示，这会将真实偏好与引导协议的人工痕迹混为一谈，需要更系统的研究来理解不同引导协议如何影响偏好一致性。

Method: 系统研究了24个语言模型，通过允许中立和弃权来改进陈述偏好引导，排除弱信号；进一步允许显示偏好中的弃权；并测试了在显示偏好引导中使用陈述偏好进行系统提示引导的效果。

Result: 允许中立和弃权在陈述偏好引导中能显著提高斯皮尔曼等级相关性；但在显示偏好中允许弃权会导致相关性接近零或负值；使用陈述偏好进行系统提示引导并不能可靠地提高AIRiskDilemmas上的相关性。

Conclusion: 陈述偏好与显示偏好的相关性高度依赖于引导协议，偏好引导需要能够处理不确定偏好的方法，简单的协议调整可能无法解决根本问题。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [104] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于足球事件数据验证的系统框架，能自动检测和纠正事件流中的逻辑不一致问题，显著提高数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析中至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误或缺失事件），这限制了分析模型的可靠性。足球领域尤其需要确保事件数据的完整性。

Method: 提出VERSA验证框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。框架系统性地验证足球事件数据的完整性。

Result: 在K League 1（2024赛季）Bepro提供的数据中，检测到18.81%的事件存在逻辑不一致。VERSA显著提高了跨数据提供商的一致性，确保异构数据源的稳定统一表示。经VERSA优化的数据显著提升了VAEP（评估球员贡献的下游任务）的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性。VERSA框架为解决事件数据质量问题提供了系统化解决方案，对足球分析及其他领域的事件流数据处理具有重要价值。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [105] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出Liquid Interfaces（液态接口）协调范式，将接口从静态技术制品转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持具有自适应、概率性和上下文相关推理能力的自主智能体，而系统集成仍主要依赖静态接口和确定性契约，导致系统缺乏灵活性和适应性。

Method: 提出Liquid Interfaces协调范式，将接口定义为短暂的关系事件而非持久技术制品；形式化该模型并设计Liquid Interface Protocol（LIP），管理意图驱动交互、协商执行和在语义不确定性下的短暂性执行；讨论治理影响并描述参考架构。

Result: 开发了Liquid Interface Protocol（LIP），为基于智能体的系统提供了自适应协调的原则性基础，并通过参考架构证明了实际可行性。

Conclusion: Liquid Interfaces为智能体系统的自适应协调提供了原则性基础，通过将接口重新定义为运行时产生的短暂关系事件，解决了传统静态接口在支持自适应、概率性和上下文相关智能体方面的局限性。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [106] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 论文提出两个新指标OI和CF来分析AI智能体推理的内存瓶颈，指出长上下文KV缓存使解码阶段高度内存受限，建议采用解耦服务架构、异构系统设计和光I/O等技术方案


<details>
  <summary>Details</summary>
Motivation: AI智能体推理正在推动数据中心向推理密集型转变，暴露出计算之外的内存容量、内存带宽和高速互连等瓶颈问题，传统屋顶线分析无法捕捉内存容量墙等关键限制

Method: 引入两个新指标：操作强度（OI）和容量足迹（CF），通过这两个指标联合分析不同智能体工作流程和基础模型配置下的性能瓶颈，识别传统屋顶线分析遗漏的机制

Result: 发现不同智能体工作流（聊天、编码、网页使用、计算机使用）和模型选择（GQA/MLA、MoE、量化）下OI/CF变化显著，长上下文KV缓存使解码阶段高度内存受限

Conclusion: 提出解耦服务架构、系统级异构设计、专用预填充和解码加速器、扩展网络规模、光I/O解耦计算内存等解决方案，建议智能体-硬件协同设计、多推理加速器集成和大容量内存解耦作为适应OI/CF演化的基础

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [107] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench：针对车载语音助手场景的LLM智能体基准测试，关注一致性、不确定性处理和能力认知，包含幻觉任务和消歧任务，揭示现有模型在真实场景中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试主要关注理想环境下的任务完成，忽视了真实用户场景中的可靠性问题。车载语音助手等实际应用中，用户请求往往不完整或模糊，智能体需要通过对话、工具使用和政策遵守来管理内在不确定性。

Method: 提出CAR-bench基准测试，包含：1）LLM模拟的用户环境；2）领域政策约束；3）58个互联工具（导航、生产力、充电、车辆控制）。除了标准任务完成，还引入幻觉任务（测试在工具或信息缺失时的极限认知）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基线结果显示：1）所有任务类型上偶尔成功与持续成功之间存在巨大差距；2）即使是前沿推理LLM在消歧任务上的持续通过率也低于50%（主要因过早行动）；3）在幻觉任务中经常违反政策或编造信息以满足用户请求。

Conclusion: 现有LLM智能体在真实世界应用中缺乏可靠性和自我认知能力，需要开发更可靠、自我意识更强的LLM智能体来应对实际场景中的不确定性挑战。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [108] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: Agent Workflow Optimization (AWO) 框架通过识别和优化冗余工具执行模式，将重复的工具调用序列转换为元工具，从而减少LLM调用次数、降低运营成本并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: Agentic AI 需要大量迭代推理步骤和工具调用，导致运营成本高、端到端延迟长以及因幻觉导致的失败。需要一种方法来优化代理工作流的效率和鲁棒性。

Method: AWO 分析现有工作流轨迹，发现重复的工具调用序列，并将其转换为元工具（确定性、复合工具），将多个代理动作捆绑到单个调用中，绕过不必要的中间LLM推理步骤。

Result: 在两个Agentic AI基准测试中，AWO 将LLM调用次数减少了高达11.9%，同时将任务成功率提高了高达4.2个百分点。

Conclusion: AWO 通过优化冗余工具执行模式，有效提高了Agentic AI工作流的效率和鲁棒性，减少了运营成本和失败率。

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [109] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 提出一种基于多维度核表示的安全设计方法，从已有数据中后验定义操作设计域，支持数据驱动的安全关键AI系统认证。


<details>
  <summary>Details</summary>
Motivation: 在安全关键AI应用中，传统ODD定义依赖早期专家知识，难以应对复杂现实环境的不完整描述挑战，阻碍AI系统认证。

Method: 采用基于多维度核表示的安全设计方法，从已有数据中后验定义ODD，建立两个ODD相等的判断标准。

Result: 通过蒙特卡洛方法和真实航空用例验证，数据驱动的ODD能够等于原始隐藏的ODD，支持安全关键碰撞避免系统认证。

Conclusion: 提出的安全设计核基ODD方法为数据驱动的安全关键AI系统提供了可行的认证途径。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [110] [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128)
*Irsyad Adam,Zekai Chen,David Laprade,Shaun Porwal,David Laub,Erik Reinertsen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: SMB-Structure是一个用于结构化电子健康记录的世界模型，结合联合嵌入预测架构和下一词预测，能够模拟患者轨迹动态而非仅预测token，在肿瘤和肺栓塞患者队列中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于下一词预测的LLMs将患者视为需要总结的文档，而非需要模拟的动态系统。患者轨迹是由状态在干预和时间下演化产生的，需要能够模拟动态而非仅预测token的模型。

Method: 提出SMB-Structure世界模型，结合联合嵌入预测架构和下一词预测：JEPA在潜在空间从初始患者表示预测未来状态，SFT在token空间重建未来患者状态，迫使轨迹动态在观察到下一状态前被编码。

Result: 在两个大规模队列（MSKCC 23,319名肿瘤患者和INSPECT 19,402名肺栓塞患者）中验证，线性探针显示该训练范式学习的嵌入能捕捉自回归基线无法恢复的疾病动态，在患者异质性高的复杂任务中表现优异。

Conclusion: SMB-Structure通过结合JEPA和SFT的训练范式，成功学习到能够模拟患者轨迹动态的表示，为结构化EHR建模提供了新方法，模型权重已开源。

Abstract: Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.

</details>


### [111] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 论文提出World of Workflows (WoW)环境，基于ServiceNow平台，包含4000+业务规则和55个活跃工作流，并创建WoW-bench基准测试。研究发现前沿LLM存在"动态盲区"问题，无法预测隐藏的级联副作用，需要基于世界建模的方法来提升企业环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLM在复杂企业系统中表现不佳，现有基准测试只评估表面任务完成度，忽略了企业环境的真实挑战：有限可观测性、大规模数据库状态、隐藏工作流的级联副作用效应。

Method: 构建基于ServiceNow的World of Workflows (WoW)环境，包含4000+业务规则和55个活跃工作流，创建包含234个任务的WoW-bench基准测试，评估受限代理任务完成和企业动态建模能力。

Result: 发现前沿LLM存在"动态盲区"，无法预测行动的隐藏级联副作用，导致静默约束违规。在不透明系统中，可靠性需要基于世界建模，代理必须心理模拟隐藏状态转换来弥补可观测性差距。

Conclusion: 要实现可靠有用的企业代理，需要新的范式来显式学习系统动态。WoW为此提供了测试环境和基准，相关代码已在GitHub发布。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [112] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: RTL提出自适应剪枝框架，发现多个针对不同数据类别的专用子网络，相比单一模型和独立多模型在准确率和召回率上表现更好，参数减少10倍，并能诊断过稀疏化问题。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法假设存在单一通用"中奖彩票"子网络，忽略了真实世界数据的异质性。需要开发能够适应数据多样性的剪枝框架。

Method: 提出Routing the Lottery (RTL)自适应剪枝框架，发现多个专门针对类别、语义簇或环境条件的自适应子网络，而非单一通用子网络。

Result: 在多种数据集和任务上，RTL在平衡准确率和召回率上优于单模型和多模型基线，参数使用量比独立模型少10倍，且表现出语义对齐特性。还识别了"子网络崩溃"现象并引入相似度评分来诊断过稀疏化。

Conclusion: 将剪枝重新定义为使模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平道路。

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


### [113] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出了Agent Reasoning Reward Model (Agent-RRM)，一个为智能体轨迹提供结构化反馈的多方面奖励模型，包含推理轨迹、针对性批评和总体评分，并通过三种集成策略显著提升了智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Agentic RL方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理质量，导致训练结果不理想。需要更细粒度的反馈机制来提升智能体推理能力。

Method: 提出了Agent-RRM模型，提供三方面结构化反馈：1）显式推理轨迹；2）针对性批评，突出推理缺陷并提供改进指导；3）总体过程评分。研究了三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试中，Reagent-U策略实现了显著性能提升：在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化、多方面的推理反馈，有效解决了稀疏奖励的局限性，显著提升了智能体强化学习的性能，为未来研究提供了有价值的工具和数据集。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [114] [The Noncomputability of Immune Reaction Complexity: Algorithmic Information Gaps under Effective Constraints](https://arxiv.org/abs/2601.20865)
*Emmanuel Pio Pastore,Francesco De Rango*

Main category: cs.IT

TL;DR: 提出基于算法信息论的验证过滤、证书驱动的反应框架，定义归一化建议分位数(NAQ)作为任务难度的尺度无关指标，并建立与率失真理论的联系。


<details>
  <summary>Details</summary>
Motivation: 为衡量任务难度提供理论基础，通过算法信息论框架量化完成特定任务所需的最小信息量，建立可比较的尺度无关难度指标。

Method: 基于前缀柯尔莫哥洛夫复杂度K(r)，定义最小可行实现复杂度M(x)=min_{r:V(x,r)=1} K(r)，并引入归一化建议分位数(NAQ)作为M(x)在参考池中的百分位数。

Result: 证明了精确实现恒等式、描述加选择上界、NAQ的准不变性、与率失真理论的联系、资源受限变体NAQ_t的线性最坏情况建议界，以及DKW界保证经验NAQ估计的收敛性。

Conclusion: NAQ提供了一个稳健、尺度无关的任务难度指标，可通过基于压缩器的代理进行数据驱动校准，为算法信息论在任务复杂度分析中的应用提供了系统框架。

Abstract: We introduce a validity-filtered, certificate-based view of reactions grounded in Algorithmic Information Theory. A fixed, total, input-blind executor maps a self-delimiting advice string to a candidate response, accepted only if a decidable or semi-decidable validity predicate V(x, r) holds. The minimum feasible realizer complexity M(x) = min_{r: V(x,r)=1} K(r), with K denoting prefix Kolmogorov complexity, measures the minimal information required for a valid outcome. We define the Normalized Advice Quantile (NAQ) as the percentile of M(x) across a reference pool, yielding a scale-free hardness index on [0, 1] robust to the choice of universal machine and comparable across task families. An Exact Realizer Identity shows that the minimal advice for any input-blind executor equals M(x) up to O(1), while a description plus selection upper bound refines it via computable feature maps, separating description cost K(y) from selection cost log i_y(x). In finite-ambiguity regimes M(x) approximately equals min_y K(y); in generic-fiber regimes the bound is tight. NAQ is quasi-invariant under bounded enumeration changes. An operational converse links NAQ to rate-distortion: communicating advice with error epsilon requires average length near the entropy of target features. Extensions include a resource-bounded variant NAQ_t incorporating time-penalized complexity (Levin's Kt) and an NP-style setting showing linear worst-case advice n - O(1). Finally, a DKW bound guarantees convergence of empirical NAQ estimates, enabling data-driven calibration via compressor-based proxies.

</details>


### [115] [Cramér-Rao Bound Analysis and Near-Optimal Performance of the Synchronous Nyquist-Folding Generalized Eigenvalue Method (SNGEM) for Sub-Nyquist Multi-Tone Parameter Estimation](https://arxiv.org/abs/2601.20866)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: SNGEM方法在极端亚奈奎斯特速率下实现多音信号的全频率/幅度/相位估计，通过联合处理原始信号及其时间导数，在噪声条件下接近CRB下界，优于传统压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法在亚奈奎斯特采样中存在DFT网格偏差和混叠噪声导致的不可约误差，需要开发更优的确定性亚奈奎斯特参数谱分析方法。

Method: 同步奈奎斯特折叠广义特征值方法(SNGEM)，通过联合处理原始信号及其时间导数，在极端亚奈奎斯特速率下实现多音信号的频率/幅度/相位估计。

Result: SNGEM在无噪声条件下达到机器精度，在所有SNR水平下接近推导的CRB下界，即使在10-20倍压缩下也表现优异，而传统OMP方法存在不可约误差平坦化问题。

Conclusion: SNGEM是一种统计上接近最优的确定性亚奈奎斯特参数谱分析方法，在极端压缩条件下仍能保持高精度估计性能。

Abstract: The synchronous Nyquist folding generalized eigenvalue method (SNGEM) realizes full frequency/amplitude/phase estimation of multitone signals at extreme sub-Nyquist rates by jointly processing the original signals and their time derivatives. In this paper, accurate Cramer-Rao bounds for amplitude ratio parameter R=A/B=1/(2\pif) are derived for two channels with equal SNR. Monte-Carlo simulations confirm that SNGEM achieves machine accuracy in noise-free conditions and closely approaches the derived CRB at all SNR levels, even at 10- 20x compression, whereas classical compressive sensing OMP exhibits irreducible error flattening due to DFT grid bias and aliasing noise. These results establish SNGEM as a statistically nearly optimal deterministic sub-Nyquist parameter spectrum analysis

</details>


### [116] [Beyond Martingale Estimators: Structured Estimators for Maximizing Information Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.22098)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 提出结构化估计器，在连续时间马尔可夫链远程估计系统中平衡计算复杂度和估计精度，优化查询策略


<details>
  <summary>Details</summary>
Motivation: 现有远程估计系统主要使用简单的鞅估计器，虽易于分析但性能不佳；而最优的MAP估计器在连续时间设置中分析困难。需要一种既能保留MAP估计器优点又便于分析的估计器。

Method: 提出结构化估计器，特别是p-MAP估计器作为MAP估计器的分段常数近似。使用二进制新鲜度过程表征信息新鲜度，推导新鲜度表达式，为单源和多源系统提供最优状态依赖采样策略。

Result: 对于时间可逆CTMC，MAP估计器可简化为p-MAP估计器。推导了新鲜度表达式，提供了最大化平均二进制新鲜度的最优查询策略，包括单源状态依赖采样和多源查询率分配。

Conclusion: 结构化估计器在连续时间马尔可夫链远程估计系统中实现了计算复杂度和估计精度的平衡，为查询式更新系统提供了实用的最优估计和采样策略框架。

Abstract: This paper investigates information freshness in a remote estimation system in which the remote information source is a continuous-time Markov chain (CTMC). For such systems, estimators have been mainly restricted to the class of martingale estimators in which the remote estimate at any time is equal to the value of the most recently received update. This is mainly due to the simplicity and ease of analysis of martingale estimators, which however are far from optimal, especially in query-based (i.e., pull-based) update systems. In such systems, maximum a-posteriori probability (MAP) estimators are optimal. However, MAP estimators can be challenging to analyze in continuous-time settings. In this paper, we introduce a new class of estimators, called structured estimators, which can seamlessly shift from a martingale estimator to a MAP estimator, enabling them to retain useful characteristics of the MAP estimate, while still being analytically tractable. Particularly, we introduce a new estimator termed as the $p$-MAP estimator which is a piecewise-constant approximation of the MAP estimator with finitely many discontinuities, bringing us closer to a full characterization of MAP estimators when modeling information freshness. In fact, we show that for time-reversible CTMCs, the MAP estimator reduces to a $p$-MAP estimator. Using the binary freshness (BF) process for the characterization of information freshness, we derive the freshness expressions and provide optimal state-dependent sampling policies (i.e., querying policies) for maximizing the mean BF (MBF) for pull-based remote estimation of a single CTMC information source, when structured estimators are used. Moreover, we provide optimal query rate allocation policies when a monitor pulls information from multiple heterogeneous CTMCs with a constraint on the overall query rate.

</details>


### [117] [Power consumption Reduction in ELAA-Assisted ISAC Systems](https://arxiv.org/abs/2601.21010)
*Xiaomin Cao,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 提出一种用于集成感知与通信(ISAC)的超大规模天线阵列(ELAA)节能子阵列激活框架，通过选择最优子阵列子集来最小化总功耗，同时满足感知和通信的服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列对于实现高分辨率近场感知至关重要，但传统数字架构中激活所有天线单元会导致功耗过高，因此需要开发节能方案来平衡功耗与性能。

Method: 提出一个节能的子阵列激活框架，将问题建模为优化问题，采用基于逐次凸逼近(SCA)的迭代算法求解，选择最优子阵列子集以最小化总功耗。

Result: 仿真结果表明，所提方法在保持双功能性能的同时，显著降低了功耗。

Conclusion: 该研究为超大规模天线阵列在集成感知与通信应用中提供了一种有效的节能解决方案，通过智能子阵列激活实现了功耗与性能的良好平衡。

Abstract: In this paper, we consider power consumption reduction in extremely large antenna arrays (ELAAs) for integrated sensing and communication (ISAC) applications. Although ELAAs are critical for achieving high-resolution near-field sensing, fully activating all antenna elements in conventional digital architectures leads to prohibitive power demands. To address this, we propose an energy-efficient subarray activation framework that selects an optimal subset of subarrays to minimize the total power consumption, subject to quality-of-service (QoS) constraints for both sensing and communication. We formulate a novel optimization problem and solve it using a successive convex approximation (SCA)-based iterative algorithm. The simulation results confirm that the proposed method significantly reduces power consumption while maintaining dual-function performance.

</details>


### [118] [Deletion-correcting codes for an adversarial nanopore channel](https://arxiv.org/abs/2601.21236)
*Huiling Xie,Zitan Chen*

Main category: cs.IT

TL;DR: 本文研究对抗性纳米孔通道的删除纠错码，提出了一种显式构造，其冗余度接近最优界


<details>
  <summary>Details</summary>
Motivation: 纳米孔测序技术中的对抗性删除通道需要高效的纠错码设计，现有显式构造的冗余度较高，需要改进

Method: 提出了一种显式构造的q元码，用于纠正最多t个删除，冗余度为2t log_q n + Θ(log log n)

Result: 证明了最优冗余度在t log_q n + Ω(1)和2t log_q n - log_q log_2 n + O(1)之间，显式构造与存在性上界一阶匹配

Conclusion: 对于对抗性纳米孔删除通道，显式构造的冗余度显著优于经典对抗性删除通道的已知构造

Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+Θ(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+Ω(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+ε)\log_q n+o(\log n)$.

</details>


### [119] [Belief Propagation with Quantum Messages for Symmetric Q-ary Pure-State Channels](https://arxiv.org/abs/2601.21330)
*Avijit Mandal,Henry D. Pfister*

Main category: cs.IT

TL;DR: 将BPQM从二进制推广到对称q元纯态信道，建立了基于Gram矩阵特征值的密度演化框架，可用于LDPC码阈值估计和极化码构造


<details>
  <summary>Details</summary>
Motivation: 先前BPQM构造和密度演化分析仅限于二进制字母表，需要将其推广到更一般的q元对称纯态信道，以扩展量子消息传递算法的应用范围

Method: 针对输出Gram矩阵为循环矩阵的对称q元纯态信道，通过Gram矩阵特征值的闭式递归来高效跟踪比特节点和校验节点合并，独立于输出态的具体物理实现

Result: 得到了显式的BPQM酉变换，并给出了合并信道保真度的解析界（用输入信道保真度表示），建立了对称q元纯态信道的密度演化框架

Conclusion: 该工作为对称q元纯态信道提供了有效的BPQM密度演化分析工具，可用于LDPC码的BPQM解码阈值估计和极化码构造

Abstract: Belief propagation with quantum messages (BPQM) provides a low-complexity alternative to collective measurements for communication over classical--quantum channels. Prior BPQM constructions and density-evolution (DE) analyses have focused on binary alphabets. Here, we generalize BPQM to symmetric q-ary pure-state channels (PSCs) whose output Gram matrix is circulant. For this class, we show that bit-node and check-node combining can be tracked efficiently via closed-form recursions on the Gram-matrix eigenvalues, independent of the particular physical realization of the output states. These recursions yield explicit BPQM unitaries and analytic bounds on the fidelities of the combined channels in terms of the input-channel fidelities. This provides a DE framework for symmetric q-ary PSCs that allows one to estimate BPQM decoding thresholds for LDPC codes and to construct polar codes on these channels.

</details>


### [120] [Learning-Based Sensor Scheduling for Delay-Aware and Stable Remote State Estimation](https://arxiv.org/abs/2601.21482)
*Nho-Duc Tran,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 提出一个统一的延迟感知框架，用于无线远程状态估计，考虑延迟与传感器信息量和能量效率的交互作用，开发基于PPO的调度器，在异构传感器和随机延迟下实现更低的估计误差。


<details>
  <summary>Details</summary>
Motivation: 不可预测的传感器到估计器延迟从根本上扭曲了无线远程状态估计的关键因素：不仅是新鲜度，还包括延迟如何与传感器信息量和能量效率相互作用。现有基于信息年龄（AoI）的代理指标未能充分捕捉这种耦合关系。

Method: 1. 提出延迟感知框架，明确建模延迟与传感器信息量的耦合，定义延迟依赖的信息增益；2. 引入高效的后验融合更新方法，无需状态增广即可合并延迟测量；3. 推导可处理的稳定性条件；4. 将调度建模为马尔可夫决策过程，开发基于近端策略优化（PPO）的调度器，通过归一化奖励在估计精度、新鲜度、传感器异构性和传输能量之间进行权衡。

Result: 在异构传感器、现实链路能量模型和随机延迟的仿真中，所提方法能够稳定学习，在可比能量消耗下比随机调度和强化学习基线（DQN、A2C）实现更低的估计误差，同时对测量可用性和过程/测量噪声的变化保持鲁棒性。

Conclusion: 该工作提出了一个统一的延迟感知框架，超越了传统的信息年龄代理指标，通过信息每焦耳调度目标，在随机延迟和能量约束下实现了更优的无线远程状态估计性能。

Abstract: Unpredictable sensor-to-estimator delays fundamentally distort what matters for wireless remote state estimation: not just freshness, but how delay interacts with sensor informativeness and energy efficiency. In this paper, we present a unified, delay-aware framework that models this coupling explicitly and quantifies a delay-dependent information gain, motivating an information-per-joule scheduling objective beyond age of information proxies (AoI). To this end, we first introduce an efficient posterior-fusion update that incorporates delayed measurements without state augmentation, providing a consistent approximation to optimal delayed Kalman updates, and then derive tractable stability conditions ensuring that bounded estimation error is achievable under stochastic, delayed scheduling. This conditions highlight the need for unstable modes to be observable across sensors. Building on this foundation, we cast scheduling as a Markov decision process and develop a proximal policy optimization (PPO) scheduler that learns directly from interaction, requires no prior delay model, and explicitly trades off estimation accuracy, freshness, sensor heterogeneity, and transmission energy through normalized rewards. In simulations with heterogeneous sensors, realistic link-energy models, and random delays, the proposed method learns stably and consistently achieves lower estimation error at comparable energy than random scheduling and strong RL baselines (DQN, A2C), while remaining robust to variations in measurement availability and process/measurement noise.

</details>


### [121] [Transversal gates for quantum CSS codes](https://arxiv.org/abs/2601.21514)
*Eduardo Camps-Moreno,Hiram H. López,Gretchen L. Matthews,Narayanan Rengaswamy,Rodrigo San-José*

Main category: cs.IT

TL;DR: 该论文研究了CSS码的横向门计算问题，确定了逻辑作用、群结构，并针对单项式码族给出了完整的横向稳定子和横向门集合。


<details>
  <summary>Details</summary>
Motivation: 研究CSS码的横向门计算问题，这对于量子纠错码的实现和逻辑操作至关重要。横向门是量子计算中实现容错逻辑操作的关键工具。

Method: 通过确定横向门的逻辑作用以及诱导非平凡逻辑门和逻辑恒等式的群结构，明确声明定义这些群的方程组。针对单项式码族（包括递减单项式码和极化码）计算完整的横向稳定子和横向门集合。

Result: 获得了CSS码横向门的完整数学描述，恢复了文献中关于CSS-T码、三正交码和可除码的结果，并进行了扩展。为单项式码族提供了具体的计算框架。

Conclusion: 该研究为CSS码的横向门计算提供了系统方法，特别适用于单项式码族，对量子纠错码的设计和实现具有重要理论意义和应用价值。

Abstract: In this paper, we focus on the problem of computing the set of diagonal transversal gates fixing a CSS code. We determine the logical actions of the gates as well as the groups of transversal gates that induce non-trivial logical gates and logical identities. We explicitly declare the set of equations defining the groups, a key advantage and differentiator of our approach. We compute the complete set of transversal stabilizers and transversal gates for any CSS code arising from monomial codes, a family that includes decreasing monomial codes and polar codes. As a consequence, we recover and extend some results in the literature on CSS-T codes, triorthogonal codes, and divisible codes.

</details>


### [122] [Subjective Distortion: Achievability and Outer Bounds for Distortion Functions with Memory](https://arxiv.org/abs/2601.21757)
*Hamidreza Abin,Amin Gohari,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究了具有历史依赖性的率失真问题，其中失真函数不仅依赖于当前源符号与表示符号，还依赖于过去的表示符号。作者给出了该问题的形式化定义，并提出了率失真权衡的内界（可达界）和外界，讨论了问题的凸化以简化界求解。


<details>
  <summary>Details</summary>
Motivation: 在某些率失真类型问题中，所需的信息保真度受到过去动作的影响，失真函数不仅依赖于源符号与其表示符号之间的瞬时失真，还依赖于过去的表示。这类问题出现在生物信息处理和推荐引擎等应用中。

Method: 1. 给出了具有历史依赖性失真问题的形式化定义；2. 提出了率失真权衡的内界（可达界）和外界；3. 讨论了问题的凸化方法，使界求解更容易；4. 将方法应用于简化的生物信息处理问题作为示例。

Result: 建立了具有历史依赖性失真问题的理论框架，提供了内界和外界分析，展示了凸化方法如何简化界求解，并通过生物信息处理示例验证了方法的适用性。

Conclusion: 该研究为具有历史依赖性的率失真问题提供了系统的理论分析框架，提出的内界和外界以及凸化方法为解决这类实际问题提供了有效工具，特别是在生物信息处理和推荐系统等应用领域。

Abstract: In some rate-distortion-type problems, the required fidelity of information is affected by past actions. As a result, the distortion function depends not only on the instantaneous distortion between a source symbol and its representation symbol, but also on past representations. In this paper, we give a formal definition of this problem and introduce both inner (achievable) and outer bounds on the rate-distortion tradeoff. We also discuss convexification of the problem, which makes it easier to find bounds. Problems of this type arise in biological information processing, as well as in recommendation engines; we provide an example applied to a simplified biological information processing problem.

</details>


### [123] [Adaptive Privacy of Sequential Data Releases Under Collusion](https://arxiv.org/abs/2601.21859)
*Sophie Taylor,Praneeth Kumar Vippathalla,Justin Coon*

Main category: cs.IT

TL;DR: 本文针对多用户、序列数据发布中的隐私-效用权衡问题，提出了一种自适应算法，考虑了用户间可能存在的共谋行为，使用互信息量化隐私和效用，并通过改进的Blahut-Arimoto算法实现最优/局部最优的数据发布。


<details>
  <summary>Details</summary>
Motivation: 本文的动机源于两个观察：1) 为一次性数据发布设计的隐私机制难以直接扩展到序列发布场景；2) 实际数据库通常服务于多个不同用户，且无法排除用户间数据共享（共谋）的可能性。因此需要解决多用户、序列数据请求下的隐私-效用权衡问题。

Method: 本文提出了一种自适应数据发布算法，采用改进的Blahut-Arimoto算法来处理序列数据请求。该方法考虑攻击模型，即数据接收者之间可能存在非法数据共享（共谋）。使用互信息来量化隐私，同时考虑期望失真和互信息两种效用度量方式。

Result: 当使用期望失真作为效用度量时，算法生成的数据发布是最优的；当使用互信息作为效用度量时，算法生成的数据发布是局部最优的。研究结果还可扩展到机器学习应用领域。

Conclusion: 本文成功解决了多用户、序列数据发布中的隐私-效用权衡问题，提出了一种自适应算法，能够处理用户间可能存在的共谋行为，并在不同效用度量下实现最优或局部最优的数据发布，为实际应用提供了理论和方法支持。

Abstract: The fundamental trade-off between privacy and utility remains an active area of research. Our contribution is motivated by two observations. First, privacy mechanisms developed for one-time data release cannot straightforwardly be extended to sequential releases. Second, practical databases are likely to be useful to multiple distinct parties. Furthermore, we can not rule out the possibility of data sharing between parties. With utility in mind, we formulate a privacy-utility trade-off problem to adaptively tackle sequential data requests made by different, potentially colluding entities. We consider both expected distortion and mutual information as measures to quantify utility, and use mutual information to measure privacy. We assume an attack model whereby illicit data sharing, which we call collusion, can occur between data receivers. We develop an adaptive algorithm for data releases that makes use of a modified Blahut-Arimoto algorithm. We show that the resulting data releases are optimal when expected distortion quantifies utility, and locally optimal when mutual information quantifies utility. Finally, we discuss how our findings may extend to applications in machine learning.

</details>
