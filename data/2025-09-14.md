<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: dMAP是一个远程测量框架，通过行为指纹识别来区分和聚类DPI设备，解决了DPI设备难以测量的问题。


<details>
  <summary>Details</summary>
Motivation: 全球用户面临日益严重的网络干扰（审查、限速、拦截），DPI设备商品化使其广泛可用，但我们对DPI设备及其部署了解有限，传统扫描工具无法检测网络中间件，DPI厂商也故意隐藏产品信息。

Method: 基于差分模糊测试，dMAP系统地发现、选择并部署专门探针，将DPI内部解析行为转化为外部可观察的指纹。利用协议解析中的模糊性和RFC解释差异来创建可测量的DPI实现差异。

Result: 应用dMAP到全球DPI部署，证明其实用可行性，仅需20-40个判别性探针就能可靠区分各种DPI实现，包括主要国家审查基础设施和商业DPI产品。

Conclusion: 该指纹识别方法可推广到审查之外的其他形式针对性干扰，为互联网上DPI的主动侦察提供了第一步。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [2] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 本综述论文探讨了推理赋能AI在无线通信网络中的应用，重点分析大型语言模型(LLMs)和其他先进推理范式如何解决传统深度学习方法在复杂多步决策问题中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在无线通信网络中缺乏结构化推理能力，无法有效处理复杂的多步决策问题，需要引入具有推理能力的AI技术来优化网络性能。

Method: 建立分类系统分析无线网络任务，逐层(物理层、数据链路层、网络层、传输层、应用层)研究AI推理技术，重点探讨LLM-based agents如何结合推理、长期规划、记忆、工具利用和自主跨层控制。

Result: 论文系统梳理了AI推理技术在无线通信各层的应用潜力，识别了关键挑战，并展示了推理方法如何提升基于AI的无线通信性能。

Conclusion: 通过结合通信和AI领域的见解，为下一代无线网络集成推理技术指明了研究方向和发展路径，推动无线网络向更智能、自主的方向发展。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [3] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于机器学习的框架，用于在O-RAN网络中同时优化负载均衡和能源效率，避免传统动态睡眠技术导致的负载不均衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AI/ML基于动态睡眠技术对能源效率有显著改善，但通过卸载用户设备来实现睡眠会导致网络负载不均衡，影响速率性能。需要一种方案能同时保持PRB分配和提高能源效率。

Method: 提出一种多类分类框架，通过预测性评估潜在的RU配置来优化能源效率。将网络条件映射到三个负载均衡类别，采用多阈值方法（保守、中等、攻击性）来适应不同的运营优先级。使用随机森林模型进行实现。

Result: 实验评估使用426万个真实网络测量数据，随机森林模型达到了98.3%的F1-macro性能，相比传统基准策略提高了195%。

Conclusion: 该研究成功开发了一种高效的机器学习框架，能够在O-RAN环境中同时优化负载均衡和能源效率，解决了传统动态睡眠技术导致的性能问题。

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [4] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于软件定义网络(SDN)的量子密码分配(QKD)网络架构，通过虚拟化密钥管理系统(vKMS)和量子安全控制器(QuSeC)解决QKD网络的可扩展性、路由路径发现和密钥管理挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展对通信网络安全构成了新挑战，QKD技术虽然能够生成无条件安全的密钥，但当前QKD网络在可扩展性和长距离实现方面仍面临挑战，特别是信任中继节点路径确定问题缺乏有效解决方案。

Method: 整合SDN原理，在每个节点中建立高层次的虚拟密钥管理系统(vKMS)，并创建新的量子安全控制器(QuSeC)实体。vKMS处理终端用户的密钥请求，QuSeC基于网络拓扑和状态的全局视图计算终到终的中继路径和应用安全策略。

Result: 该方案能够有效解决QKD网络的可扩展性问题，完成了路由路径发现和密钥管理系络识别的挑战。论文还提供了对建议方案的安全分析，识别了架构的安全等级并分析了核心网络安全属性。

Conclusion: 通过SDN化的架构设计，该方案为QKD网络提供了一种可扩展、灵活且安全的解决方案，有效地解决了长距离量子密码分配网络中的关键技术挑战。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [5] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: PARROT是一个可复现的Android应用流量采集系统，用于系统化收集应用流量数据，支持自动化环境设置、流量记录管理和SSL/TLS解密。通过对比2021年和2025年数据集，发现TLSv1.3协议使用率从6.7%增长到90.0%，QUIC协议采用率大幅提升，DNS通信从非加密Do53转向加密DoT协议。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议的快速演进和当前数据集的有限可用性限制了应用流量分析研究，需要开发可复现的流量采集系统来支持相关研究。

Method: 开发PARROT系统，使用Android虚拟设备进行自动化应用流量采集，集成mitmproxy进行可选流量解密，支持SSL/TLS密钥自动提取，收集80个应用的流量数据并进行对比分析。

Result: 对比分析显示：TLSv1.3在TCP加密流量中占比从6.7%(2021)增长到90.0%(2025)；QUIC协议采用率显著提升；DNS通信从91.0%非加密Do53(2021)转变为81.1%加密DoT(2025)。

Conclusion: PARROT系统为研究社区提供了可复现的应用流量采集能力，揭示了移动应用安全协议的显著演进趋势，特别是向TLSv1.3、QUIC和加密DNS协议的迁移。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值产生精确输出，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理处理区间不确定性。

Method: 开发了区间二型贝叶斯定理版本，使用保守方法避免输入不一致性；提出新颖算法将专家提供的区间编码为二型模糊隶属函数。

Result: 该方法能够处理区间输入概率，避免产生无效输出结果，扩展了贝叶斯定理在不确定性环境下的应用能力。

Conclusion: 区间二型贝叶斯定理为处理专家提供的区间估计信息提供了有效框架，扩展了传统贝叶斯方法在现实不确定环境中的应用范围。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [7] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出基于NLP和多模态LLM的端到端框架，将游戏设计文档自动转换为Unity游戏原型，显著提升代码生成质量和设计规范遵循度


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中的关键空白，利用LLM技术简化从游戏设计到实现的过程，提高开发效率

Method: 结合微调的LLaMA-3模型（专门用于Unity代码生成）和自定义Unity集成包，解析GDD、提取结构化游戏规范并合成Unity兼容的C#代码

Result: 微调模型在编译成功率、GDD遵循度、最佳实践采用和代码模块化等指标上表现优异（平均得分4.8/5.0），优于现有最先进LLM

Conclusion: 该系统有效证明了LLM作为从游戏设计过渡到实现的有价值工具，能够生成高度遵循GDD规范的多类型游戏模板

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [8] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 使用多个专门的LLM模型代理分解MiniZinc建模任务，通过不同代理处理不同类型的全局约束，最终组装成完整模型


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这一过程很具挑战性

Method: 采用多代理框架，每个专门的LLM代理负责检测和生成特定类型的全局约束代码，最后由组装代理整合成完整模型

Result: 初步实验显示该方法在多个LLM上都表现更好，超过了一次性提示和思维链提示等基准方法

Conclusion: 通过任务分解和专门化代理的方式可以有效降低MiniZinc建模的复杂度，并为未来工作提供了全面的发展路线图

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [9] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出一种截断交叉熵(TCE)损失函数，通过降低模型对自生成数据的高置信度预测权重，有效延缓生成式AI模型在合成数据递归训练中的模型崩溃现象


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型产生合成数据的比例不断增加，重复训练合成数据会导致模型崩溃现象，现有缓解策略有限，需要新的解决方案

Method: 识别模型对自生成数据的过度自信是崩溃关键驱动因素，提出置信度感知的截断交叉熵(TCE)损失函数，降低高置信度预测的权重

Result: TCE显著延迟模型崩溃，可将模型保真度区间延长2.3倍以上，方法具有模型无关性并在多模态场景中验证有效

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [10] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究解释性AI中的不确定性解释和全局解释，探索如何通过直观可视化提高用户满意度和人类可解释性


<details>
  <summary>Details</summary>
Motivation: 虽然解释性AI已成为常见术语，但不确定性解释和全局解释方面的研究还比较缺失，需要构建通用的XAI指南

Method: 选择了一种结合了不确定性、稳健性和全局XAI多个概念的算法，测试其调节信任的能力，并检查直观可视化方法对用户满意度和可解释性的影响

Result: 算法能够有效调节用户信任，虽然复杂难懂，但通过直观的可视化方式可以提高用户满意度和人类可解释性

Conclusion: 直观的可视化解释方法对于提高XAI的可接受性和信任度至关重要，尽管它们可能在技术上更复杂

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [11] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来解决推荐系统中的冷启动用户问题，通过优化指令提示和少量样本学习，在低数据设置下显著提升了LLM推荐模型的性能。


<details>
  <summary>Details</summary>
Motivation: 冷启动用户问题严重影响了推荐系统的效果，因为缺乏历史行为信息。需要一种有效的方法来利用大语言模型在少量样本情况下进行推荐任务。

Method: 提出了上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户档案，Ds是精选支持集，R̂是预测的排序项目列表。使用基于transformer的自回归LLM（BioGPT、LLaMA-2、GPT-4），采用标记级对齐和嵌入空间正则化技术。

Result: 实验证明最优示例注入和指令结构可以显著提高precision@k和NDCG分数，特别是在低数据设置下。提示的及时组合不仅影响语法，还直接控制注意力规模和推理过程中的解码器行为。

Conclusion: 基于提示的适应方法可以视为解决基于LLM的推荐管道中冷启动问题的一种有效途径，展示了语义保真度的重要性。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [12] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 人工智能代理在动态协调任务中的性能评估，比较了人类、大语言模型和贝叶斯模型在动态协商中的表现和行为差异


<details>
  <summary>Details</summary>
Motivation: 随着协调任务趋向由自主代理执行，需要评估不仅代理的性能结果，还要考察其在多代理环境中的协商过程和行为动态

Method: 在动态协商环境中对人类(N=216)、LLMs(GPT-4o, Gemini 1.5 Pro)和贝叶斯代理进行直接可比较的实验，捕获结果和行为动态数据

Result: 贝叶斯代理通过敏锐优化获得最高剩余，但抱死率高；人类和LLMs总体剩余相似，但行为差异：LLMs偏向保守透协交易，人类更具战略性、冒险性和公平性

Conclusion: 纯粹的性能相等指标可能隐藏了代理在协调过程和对齐性方面的根本差异，这对实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [13] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱风险识别的机器学习管道，在竞赛中获得了第二名，AUROC达到0.961


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构的优先事项，机器学习在此领域具有巨大潜力，需要开发系统化的方法来识别高风险银行客户

Method: 采用16步设计和统计分析流程，构建SQLite数据库，开发SQL特征工程算法，连接预训练模型，并提供可解释AI模块

Result: 管道在包含195,789个客户ID的数据集上实现了0.961的平均AUROC（标准差0.005），在竞赛中获得第二名

Conclusion: 提出的综合系统化方法能够有效开发稳健的机器学习管道，用于反洗钱高风险客户识别，具有实际应用价值

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [14] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于神经科学原理的计算框架，用于提升人工智能系统的空间推理能力，包含六个核心模块以模仿人类空间智能的生物学机制。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统在空间推理能力方面存在显著缺口，主要限于符号和序列处理，而人类空间智能则基于多感官知觉、空间记忆和认知地图，能够在非结构化环境中进行灵活的上下文感知决策。缩小这个差距对于提升人工智能与物理世界的交互能力至关重要。

Method: 研究从计算神经科学中的空间神经模型出发，提出了一种基于神经科学原理的计算框架。该框架将核心生物功能映射到六个重要计算模块：生物启发的多模态感知、多感官整合、自我中心-客观中心坐标转换、人工认知地图、空间记忆和空间推理。这些模块共同构成了一个空间推理能力的全局视角。

Result: 研究进行了框架导向的方法分析，评估了最新方法与每个模块的相关性，并识别了阻碍更多神经科学基础空间推理模块发展的关键缺口。还检验了新兴的测试标准和数据集，探讨了从虚拟到体现系统（如机器人学）的潜在应用领域。

Conclusion: 这项工作为研究社区提供了一个基于神经科学的视角和结构化路径，询划了一条有前景的研究路线图，能够在动态或非结构化环境中实现空间推理能力的普适化。这个框架有望推动人工智能系统在空间智能方面的进一步发展。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [15] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和渐进多尺度解码策略，解决多智能体运动预测中交互关系动态演变的问题，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体联合预测方法忽略了智能体间交互关系的动态演变特性，需要新的方法来显式捕捉未来场景中不断变化的社会交互

Method: 使用动态异构图进行场景建模，设计渐进式建模策略处理时空依赖关系，结合多尺度解码逐步消除未来运动的不确定性

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准上也达到最先进性能

Conclusion: ProgD方法通过动态异构图和渐进多尺度解码有效捕捉了多智能体交互的动态演变特性，显著提升了运动预测的准确性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [16] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多代理系统监管架构，通过行为追踪、动态声誉和恶意行为预测模块实现可靠监管


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自主代理在重要领域带来机遇，但其不可预测性和异构能力造成了监管和责任制挑战

Method: 提出三层架构：代理层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 建立了一个系统性的可信、弹性和可扩展的监管机制基础

Conclusion: 该框架为大规模代理生态系统提供了可靠监管解决方案，并提出了区块链在多代理系统监管中的未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [17] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了NbQA数据集和Jupiter框架，通过从真实Jupyter笔记本提取高质量数据科学任务和解决方案，结合MCTS搜索算法提升LLM在多步推理和工具使用方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数据科学工作流中仍难以处理多步推理和工具使用，限制了复杂数据分析任务的效果。

Method: 1) 从真实Jupyter笔记本提取标准化任务-解决方案对构建NbQA数据集；2) 提出Jupiter框架，将数据分析建模为搜索问题，使用MCTS生成多样化解决方案轨迹；3) 结合价值模型和节点访问计数进行高效推理。

Result: Qwen2.5-7B和14B模型在NbQA上分别解决77.82%和86.38%的任务，性能匹配或超越GPT-4o和先进代理框架，在多步推理任务上展现出更好的泛化能力和工具使用推理。

Conclusion: 通过构建高质量数据集和引入搜索框架，显著提升了LLM在数据科学多步推理和工具使用方面的能力，为自动化数据分析提供了有效解决方案。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [18] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 知识图构建与LLM整合的技术比较研究，评估spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种方法在问答系统中的效果、可行性和适应性


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG方法在处理复杂长文本时的主题性和整体理解限制，需要更深入的文本与语境分析

Method: 对spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种开源知识图构建方法进行综合技术比较，分析其能力、发展状态并集成到LLM问答系统中

Result: 实验结果显示OpenIE提供最全面的三元组覆盖，而GraphRAG在推理能力方面表现最优

Conclusion: 讨论了各方法的优势与限制，并提出了改进知识图基问答系统的未来发展方向

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [19] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹用于改进基于偏好的强化学习中的策略优化，提出分阶段GRPO训练范式，通过树结构优势估计获得丰富的前缀条件奖励信号


<details>
  <summary>Details</summary>
Motivation: 受MCTS在LLM推理中生成高质量中间轨迹的启发，探索如何将传统用于训练价值/奖励模型的MCTS轨迹重新用于改进偏好强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树结构优势估计方法，获得前缀条件奖励信号

Result: 结构化优势估计可以稳定更新并更好地反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [20] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得显著进展，但在设计通用、鲁棒和高效的智能体部署平台方面仍存在重大挑战。

Method: 提出了LightAgent框架，集成了Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级的结构，是一个完全开源的解决方案。

Result: LightAgent能够无缝集成到主流聊天平台，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了一个轻量级但功能强大的部署平台。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [21] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究锦标赛中获胜者的认证解释问题，通过识别最小支持子锦标赛来证明候选者为何获胜，为多种锦标赛规则提供可解释AI的解决方案。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选者之间的成对优势关系，但缺乏对获胜原因的正式解释。研究旨在为各种锦标赛规则提供认证解释，回答"为什么获胜者会赢"这一核心问题。

Method: 提出最小支持概念——候选者必然获胜的最小子锦标赛，无论锦标赛其余部分如何完成。针对多种常见锦标赛解决方案（顶级循环、无覆盖集、Copeland规则、Borda规则、极大极小规则、加权无覆盖集）进行分析。

Result: 确定了每种规则的最小支持规模，提出了多项式时间算法来计算除加权无覆盖集外的所有规则的最小支持（该问题被证明是NP完全的）。

Conclusion: 最小支持能够产生紧凑、认证且直观的解释，为锦标赛获胜者的可解释性提供了有效的数学框架和计算方法。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [22] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究空间协调三个维度（探索多样性、移动专业化、自适应空间接近度）对限制显性沟通的团队协作搜救任务绩效的影响


<details>
  <summary>Details</summary>
Motivation: 许多团队（消防员、军事、执法、应急响应）需要在没有视觉线索或广泛显性沟通的情况下协调物理空间中的移动，但现有研究主要关注共址同步团队或分布式知识工作协调

Method: 分析34个四人团队（136名参与者）在搜救任务中的空间接近度、分布模式和移动对齐等空间协调指标数据

Result: 空间专业化正向预测绩效，自适应空间接近度呈现边际倒U型关系（适度适应水平最优），这些指标的时序动态能区分高低绩效团队

Conclusion: 研究揭示了基于角色的团队工作中隐性空间协调机制，强调了平衡自适应策略的重要性，对培训和AI辅助团队支持系统具有启示意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [23] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的端到端机器学习代理的多样化、真实结构化基准测试，包含150个AutoML任务，具有浏览器自动化采集、难度建模和多维度评估框架三大创新。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实场景中的端到端ML工作流自动化能力。

Method: 开发了基于浏览器自动化和LLM的任务采集系统，从Kaggle等平台自动收集结构化ML挑战；采用排行榜驱动的难度建模机制；构建多维度评估框架。

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个版本，覆盖多种数据模态和难度级别。

Conclusion: TAM Bench为解决现有基准测试局限性提供了全面解决方案，为评估LLM-based ML代理在真实端到端任务中的能力提供了实用测试平台。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [24] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型和层状奖励函数的新题DRL算法，实现了资源效率高的语义探索，提升了物体发现率和语义导向行为


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习方法在语义探索中认知能力有限、探索效率与语义理解不平衡的挑战

Method: 集成视觉-语言模型通过层状奖励函数，将VLM查询模型为专门动作，结合课程学习策略指导不同复杂度层次的学习

Result: 实验结果显示代理实现了显著提升的物体发现率，形成了向语义丰富区域导航的能力，并掌握了战略性主动查询外部信息的能力

Conclusion: 该研究提供了一种将常识性语义推理嵌入自主代理的可扩展方法，为实现全面智能自主探索提供了新方向

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [25] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLM利用内在推理能力生成响应，无需手动构建few-shot示例，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型内在推理能力的利用，且任务特定的提示构建成本高、跨任务不一致

Method: 提出Template-Oriented Reasoning (TORSO)方法，引导模型利用内部推理能力生成适当响应，无需手动构建few-shot示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强劲性能，并生成合理的推理过程

Conclusion: TORSO提供了一种有效的方法来激发LLM的内在推理能力，无需依赖成本高昂的手工few-shot提示，在多个任务上表现优异

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [26] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 这篇技术报告分析了大语言模型在法律领域的“幽灵问题”，探讨了RAG策略的有效性和局限性，建议采用以验证性和可追溯性为核心的“咨询式AI”范式，强调人类监督的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在法律应用中产生假信息（幽灵）的挑战，探索有效的缓解策略和其局限性。

Method: 分析幽灵现象的原因和表现形式，评估RAG（基于知识提取的生成）缓解策略的效果，提出整体优化方案。

Result: 发现RAG策略存在局限性，需要更全面的优化。人工监督在法律AI应用中具有不可替代的作用。

Conclusion: 解决方案不在于增量改进生成模型，而是应采用以验证性和可追溯性为核心的“咨询式AI”范式，作为增强专业判断的工具，而非替代人类专业能力。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [27] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化的分布式记忆框架，通过可验证写入、动态记忆调度和跨域知识扩散来解决多智能体系统中的记忆管理问题，提高推理准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和历史交互数据，现有记忆管理方法存在噪声累积、内存无限扩张和跨域泛化能力有限的问题，需要更高效可验证的记忆框架。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入机制、根据经验效用动态排序和整合记忆条目的自调度记忆控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散机制。

Result: 在基准数据集上的评估显示，SEDM相比强基线方法提高了推理准确性，同时降低了token开销，并且能够将从事实验证中提炼的知识用于增强多跳推理能力。

Conclusion: SEDM为开放式多智能体协作提供了一个可扩展且可持续的记忆机制，将记忆从被动存储库转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [28] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子变分电路在图像描述任务中实现组合泛化，相比经典组合模型表现更好，特别是在多热编码方面取得了良好的概念验证结果。


<details>
  <summary>Details</summary>
Motivation: 人类认知具有组合泛化的关键能力，但当前AI工具如视觉语言模型缺乏这种能力。量子模型训练效率更高，有望提升组合泛化任务的性能。

Method: 在希尔伯特空间中解释组合张量模型的表示，使用变分量子电路学习这些表示。采用两种图像编码技术：二进制图像向量的多热编码(MHE)和CLIP模型图像向量的角度/幅度编码。

Result: 使用噪声MHE编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍优于经典组合模型。

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在特定编码方式下表现优异，为量子AI在组合推理方面的发展提供了实证支持。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [29] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并实现管道并行化，显著提升具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统顺序计算模式在保证准确性的同时，难以满足具身AI系统对高频率"思考"的需求，限制了在动态环境中的实时应用。

Method: 解耦感知和生成模块，提供受控的管道并行化；建立共享公共上下文来解决并行化增加时的数据陈旧性问题。

Result: 平均吞吐量提升2.54倍，同时达到原始准确性的102.7%，实现了高吞吐量和高准确性的平衡。

Conclusion: Auras框架有效克服了顺序计算的限制，为具身AI系统提供了高吞吐量的推理能力，同时保证了系统准确性。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [30] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 论文研究发现，大语言模型在长任务执行中存在执行错误而非推理能力不足的问题，模型规模扩大能显著提升多步任务执行能力，但存在自我条件效应导致错误累积，而思维模型能避免此问题并执行更长任务。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型规模扩展是否带来边际收益递减，探索模型在长任务中失败的原因（执行错误而非推理能力不足），以及如何通过提供知识和计划来提升长视野任务的执行能力。

Method: 通过隔离执行能力，明确提供解决长视野任务所需的知识和计划，测试不同规模模型在多步任务中的表现，分析错误模式和自我条件效应，并对比前沿思维模型的单轮任务执行长度。

Result: 发现大模型即使在小模型单步准确率100%的情况下也能正确执行更多步骤；模型每步准确率随步骤增加而下降，存在自我条件效应（模型在包含先前错误的上下文中更容易犯错）；思维模型不会自我条件化且能单轮执行更长的任务。

Conclusion: 通过关注执行能力可以解释LLMs在复杂推理和简单长任务中的表现差异，强调模型规模和顺序测试时计算对长视野任务的重要价值，思维模型在避免错误累积方面表现优异。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [Improved Receiver Chain Performance via Error Location Inference](https://arxiv.org/abs/2509.08869)
*Michael Greenwood,Robert Hunter*

Main category: cs.IT

TL;DR: 提出一种在解码端使用机器学习模型估计字节级损坏概率的方法，通过标记擦除来增强RS解码能力，无需改变航天器硬件或编码标准，在信号条件恶化时实现0.3分贝的性能增益。


<details>
  <summary>Details</summary>
Motivation: 现代航天器通信系统依赖级联纠错方案（卷积码+RS码），需要在信号条件恶化时提高数据恢复能力，同时避免对现有硬件和标准的改动。

Method: 在解码端使用机器学习模型估计接收数据帧中字节级损坏的可能性，利用这些估计在RS解码前标记擦除，从而增强RS码的纠错能力。

Result: 该方法在信号条件恶化的情况下实现了0.3分贝的性能增益，提高了数据恢复能力。

Conclusion: 提出的机器学习辅助擦除标记方法能够有效提升航天器通信系统的纠错性能，且具有向后兼容性，无需修改现有硬件和编码标准。

Abstract: Modern spacecraft communication systems rely on concatenated error correction
schemes, typically combining convolutional and Reed-Solomon (RS) codes. This
paper presents a decoder-side method that uses a machine learning model to
estimate the likelihood of byte-level corruption in received data frames. These
estimates are used to mark erasures prior to RS decoding, enhancing its
correction capacity without requiring changes to spacecraft hardware or
encoding standards. The approach enables improved data recovery under degraded
signal conditions at a gain of 0.3 decibels.

</details>


### [32] [Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?](https://arxiv.org/abs/2509.09411)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Farshad Rostami Ghadi,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 这篇论文探讨在流体天线系统(FAS)中使用频幅相关矩阵比传统的系数相关矩阵更能准确评估中断性能能。研究通过完全相关Nakagami-m衰落渠道模型验证了该方法的优势，尤其在稀疏端口部署和低中断区域。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用Jake模型的通道系数相关矩阵来近似Gaussian copula中的协方差矩阵，但因为多元正态随机变量是通过变换相关频幅生成的，所以使用频幅相关矩阵可能更准确。本文要解决这个问题并验证哪种方法更优称。

Method: 在完全相关Nakagami-m衰落条件下，使用频幅相关矩阵来评估FAS性能。开发了一种生成这种相关衰落渠道的方法，用于Monte Carlo模拟作为理论结果的验证标准。

Result: 模拟结果证实了所提出的渠道建模方法的有效性，并证明使用频幅相关矩阵具有更高的准确性，尤其在稀疏端口部署和低中断概率区域。

Conclusion: 对于流体天线系统的中断性能能评估，使用频幅相关矩阵比传统的系数相关矩阵更为准确。这种方法在实际应用中尤其重要，特别是在稀疏端口配置和高可靠性要求的场景下。

Abstract: Gaussian copula has been employed to evaluate the outage performance of Fluid
Antenna Systems (FAS), with the covariance matrix reflecting the dependence
among multivariate normal random variables (RVs). While prior studies
approximate this matrix using the channel coefficient correlation matrix from
Jake's model, this work instead employs the channel envelope correlation
matrix, motivated by the fact that the multivariate normal RVs are generated by
transforming correlated channel envelopes. This raises an open question of
whether using the coefficient- or envelope-level correlation matrix yields
better accuracy in accessing FAS performance. Toward this end, this paper
explores the benefits of using the envelope-level correlation matrix under
fully correlated Nakagami-m fading, and develops a method for generating such
fading channels for Monte Carlo simulations, which serve as a benchmark for
validating the theoretical results. Simulation results confirm the
effectiveness of the proposed channel modeling approach and demonstrate the
superior accuracy of using the envelope-level correlation matrix, particularly
in sparse port deployment and low-outage regime.

</details>


### [33] [Mixture of Semantics Transmission for Generative AI-Enabled Semantic Communication Systems](https://arxiv.org/abs/2509.09499)
*Junjie Ni,Tong Wu,Zhiyong Chen,Yin Xu,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于生成式AI的语义混合传输策略(MoS)，通过ROI/RONI分区处理实现更高效的无线语义通信


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的语义通信方法在信道资源利用效率方面存在不足，需要在视觉保真度和语义相关性之间取得更好平衡

Method: 在发送端将图像分为感兴趣区域(ROI)和非感兴趣区域(RONI)，分别提取语义信息并分配不同带宽；在接收端使用扩散模型根据接收到的语义信息重建完整图像

Result: 实验结果表明适当的ROI-RONI分配至关重要，MoS在ROI的PSNR和RONI的CLIP分数方面取得了显著的性能提升

Conclusion: MoS策略通过语义信息的分区处理和差异化传输，实现了更高效的信道资源利用，在保持语义相关性的同时提升了视觉质量

Abstract: In this paper, we propose a mixture of semantics (MoS) transmission strategy
for wireless semantic communication systems based on generative artificial
intelligence (AI). At the transmitter, we divide an image into regions of
interest (ROI) and reigons of non-interest (RONI) to extract their semantic
information respectively. Semantic information of ROI can be allocated more
bandwidth, while RONI can be represented in a compact form for transmission. At
the receiver, a diffusion model reconstructs the full image using the received
semantic information of ROI and RONI. Compared to existing generative AI-based
methods, MoS enables more efficient use of channel resources by balancing
visual fidelity and semantic relevance. Experimental results demonstrate that
appropriate ROI-RONI allocation is critical. The MoS achieves notable
performance gains in peak signal-to-noise ratio (PSNR) of ROI and CLIP score of
RONI.

</details>


### [34] [Fast Polarisation-Aware Decoder for Non-Binary Polar Codes](https://arxiv.org/abs/2509.09554)
*Joseph Jabbour,Ali Chamas Al-Ghouwayel,Emmanuel Boutillon*

Main category: cs.IT

TL;DR: 提出FSC-PA算法，通过定制化非二进制极化码解码器的每个核，显著降低解码复杂度，在BPSK和CCSK调制下实现60%域加法和30%实数加法减少，性能损失仅0.2dB


<details>
  <summary>Details</summary>
Motivation: 研究低复杂度非二进制极化码解码器，通过分析输入极化程度相同的校验节点来降低整体解码计算负载

Method: 开发FSC-PA（快速连续消除-极化感知）方案，对非二进制极化码解码器的每个核进行离线定制化分析，最小化相同输入极化级别校验节点的计算复杂度

Result: 相比最先进的扩展最小和算法，FSC-PA算法实现60%域加法和30%实数加法减少，性能退化小于0.2dB

Conclusion: FSC-PA算法通过核级定制化有效降低了非二进制极化码解码器的复杂度，在保持接近最优性能的同时显著减少计算资源需求

Abstract: The paper investigates the emerging field of low-complexity non-binary polar
code (NB-PC) decoders. It shows that customizing each kernel of an NB-PC
decoder through offline analysis can significantly reduce the overall decoding
complexity. The proposed decoder, referred to as the Fast Successive
Cancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing
the computational load of parity-check nodes that share the same level of input
polarization. The NB polar decoder is developed for both BPSK and CCSK
modulations. Compared to the state-of-the-art extended min-sum algorithm, the
FSC-PA algorithm achieves an overall reduction of 60 percents in field
additions and 30 percents in real additions, while incurring only a negligible
performance loss (less than 0.2 dB degradation).

</details>


### [35] [RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems](https://arxiv.org/abs/2509.09644)
*Chunjie Wang,Xuhui Zhang,Wenchao Liu,Jinke Ren,Shuqiang Wang,Yanyan Shen,Kejiang Ye,Kim Fung Tsang*

Main category: cs.IT

TL;DR: 提出RIS赋能的智能交通系统数据收集增强框架，通过联合优化RIS相移、功率分配、计算资源和时隙分配，最大化RSU对的最小处理数据量


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中数据收集和处理效率问题，利用RIS技术增强通信性能，提高交通数据的处理能力

Method: 采用混合RSMA和TDMA协议，提出迭代算法结合交替优化和序列秩一约束松弛方法解决非凸优化问题

Result: 仿真结果表明所提算法在不同场景下显著优于基线方案，有效提升了智能交通应用的数据处理性能

Conclusion: 该框架为RIS赋能的智能交通系统提供了有效的数据收集和处理解决方案，具有实际应用价值

Abstract: This paper investigates the data collection enhancement problem in a
reconfigurable intelligent surface (RIS)-empowered intelligent consumer
transportation system (ICTS). We propose a novel framework where a data center
(DC) provides energy to pre-configured roadside unit (RSU) pairs during the
downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid
rate-splitting multiple access (RSMA) and time-division multiple access (TDMA)
protocol to transmit the processed data to the DC, while simultaneously
performing local data processing using the harvested energy. Our objective is
to maximize the minimal processed data volume of the RSU pairs by jointly
optimizing the RIS downlink and uplink phase shifts, the transmit power of the
DC and RSUs, the RSU computation resource allocation, and the time slot
allocation. To address the formulated non-convex problem, we develop an
efficient iterative algorithm integrating alternating optimization and
sequential rank-one constraint relaxation methods. Extensive simulations
demonstrate that the proposed algorithm significantly outperforms baseline
schemes under diverse scenarios, validating its effectiveness in enhancing the
data processing performance for intelligent transportation applications.

</details>
