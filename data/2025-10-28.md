<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 10]
- [cs.AI](#cs.AI) [Total: 83]
- [cs.IT](#cs.IT) [Total: 13]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks](https://arxiv.org/abs/2510.22108)
*Xinyue Liang,Hui Kang,Junwei Che,Jiahui Li,Geng Sun,Qingqing Wu,Jiacheng Wang,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出了一种异构多智能体协作动态优化框架，结合STAR-RIS的模拟退火控制和改进的多智能体深度强化学习，以优化无人机群在密集环境中的传输速率和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决低空无线网络中由于密集环境障碍物导致的严重信号衰减问题，通过无人机协同波束成形和STAR-RIS的全向可重构波束成形来增强信号质量和方向性。

Method: 提出异构多智能体协作动态优化框架，包含基于模拟退火的STAR-RIS控制方法和改进的多智能体深度强化学习方法，后者引入了自注意力评估机制和自适应速度转换机制。

Result: 仿真结果表明，该框架在收敛速度、平均传输速率和能耗方面优于多种基线方法，系统平均传输速率随无人机数量和STAR-RIS单元数量增加而提升。

Conclusion: 所提出的HMCD框架能有效解决密集环境中的信号衰减问题，在提升传输速率的同时降低能耗，为低空无线网络提供了有效的优化方案。

Abstract: While low-altitude wireless networks (LAWNs) based on uncrewed aerial
vehicles (UAVs) offer high mobility, flexibility, and coverage for urban
communications, they face severe signal attenuation in dense environments due
to obstructions. To address this critical issue, we consider introducing
collaborative beamforming (CB) of UAVs and omnidirectional reconfigurable
beamforming (ORB) of simultaneous transmitting and reflecting reconfigurable
intelligent surfaces (STAR-RIS) to enhance the signal quality and
directionality. On this basis, we formulate a joint rate and energy
optimization problem (JREOP) to maximize the transmission rate of the overall
system, while minimizing the energy consumption of the UAV swarm. Due to the
non-convex and NP-hard nature of JREOP, we propose a heterogeneous multi-agent
collaborative dynamic (HMCD) optimization framework, which has two core
components. The first component is a simulated annealing (SA)-based STAR-RIS
control method, which dynamically optimizes reflection and transmission
coefficients to enhance signal propagation. The second component is an improved
multi-agent deep reinforcement learning (MADRL) control method, which
incorporates a self-attention evaluation mechanism to capture interactions
between UAVs and an adaptive velocity transition mechanism to enhance training
stability. Simulation results demonstrate that HMCD outperforms various
baselines in terms of convergence speed, average transmission rate, and energy
consumption. Further analysis reveals that the average transmission rate of the
overall system scales positively with both UAV count and STAR-RIS element
numbers.

</details>


### [2] [When UAV Swarm Meets IRS: Collaborative Secure Communications in Low-altitude Wireless Networks](https://arxiv.org/abs/2510.22117)
*Jiahui Li,Xinyue Liang,Geng Sun,Hui Kang,Jiacheng Wang,Dusit Niyato,Shiwen Mao,Abbas Jamalipour*

Main category: cs.NI

TL;DR: 提出了一种用于低空无线网络的安全通信框架，通过将无人机群作为虚拟天线阵列并结合智能反射表面来防御窃听攻击，使用异构多智能体控制方法优化安全速率、旁瓣水平和能耗。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络面临已知和未知窃听者的安全威胁，需要保护数据机密性和系统完整性。

Method: 将问题转化为异构马尔可夫决策过程，提出异构多智能体控制方法，集成IRS控制策略和多智能体软演员-评论家框架来优化无人机激励电流权重、飞行轨迹和IRS相位偏移。

Result: 仿真结果显示，该方法在安全速率提升、旁瓣抑制和能量效率方面优于基线方法，且当无人机数量增加时，VAA和IRS之间的协作被动波束成形能提供更强的安全保障。

Conclusion: 所提出的HMCA框架能有效提升LAWNs的安全性能，通过VAA和IRS的协同工作实现强大的防窃听保护。

Abstract: Low-altitude wireless networks (LAWNs) represent a promising architecture
that integrates unmanned aerial vehicles (UAVs) as aerial nodes to provide
enhanced coverage, reliability, and throughput for diverse applications.
However, these networks face significant security vulnerabilities from both
known and potential unknown eavesdroppers, which may threaten data
confidentiality and system integrity. To solve this critical issue, we propose
a novel secure communication framework for LAWNs where the selected UAVs within
a swarm function as a virtual antenna array (VAA), complemented by intelligent
reflecting surface (IRS) to create a robust defense against eavesdropping
attacks. Specifically, we formulate a multi-objective optimization problem that
simultaneously maximizes the secrecy rate while minimizing the maximum sidelobe
level and total energy consumption, requiring joint optimization of UAV
excitation current weights, flight trajectories, and IRS phase shifts. This
problem presents significant difficulties due to the dynamic nature of the
system and heterogeneous components. Thus, we first transform the problem into
a heterogeneous Markov decision process (MDP). Then, we propose a heterogeneous
multi-agent control approach (HMCA) that integrates a dedicated IRS control
policy with a multi-agent soft actor-critic framework for UAV control, which
enables coordinated operation across heterogeneous network elements. Simulation
results show that the proposed HMCA achieves superior performance compared to
baseline approaches in terms of secrecy rate improvement, sidelobe suppression,
and energy efficiency. Furthermore, we find that the collaborative and passive
beamforming synergy between VAA and IRS creates robust security guarantees when
the number of UAVs increases.

</details>


### [3] [HandPass: A Wi-Fi CSI Palm Authentication Approach for Access Control](https://arxiv.org/abs/2510.22133)
*Eduardo Fabricio Gomes Trindade,Felipe Silveira de Almeida,Gioliano de Oliveira Braga,Rafael Pimenta de Mattos Paixão,Pedro Henrique dos Santos Rocha,Lourenco Alves Pereira Jr*

Main category: cs.NI

TL;DR: 基于Wi-Fi信道状态信息的掌纹生物识别认证系统，使用树莓派采集20名参与者的手掌CSI数据，随机森林分类器达到99.82%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 探索Wi-Fi CSI在实际用户认证中的应用潜力，利用手掌生物特征开发可靠的认证系统。

Method: 使用树莓派和定制天线盒（功率降至1dBm）采集20名参与者右手掌的CSI数据，进行MinMax归一化处理，提取手掌大小、形状、手指角度等生物物理特征，评估五种分类算法。

Result: 随机森林分类器在10折交叉验证中平均F1分数达到99.82%，使用幅度和相位数据，每秒采集约1000个数据包。

Conclusion: Wi-Fi CSI在基于手掌生物特征的认证系统中具有巨大潜力，能够实现高精度用户识别。

Abstract: Wi-Fi Channel State Information (CSI) has been extensively studied for
sensing activities. However, its practical application in user authentication
still needs to be explored. This study presents a novel approach to biometric
authentication using Wi-Fi Channel State Information (CSI) data for palm
recognition. The research delves into utilizing a Raspberry Pi encased in a
custom-built box with antenna power reduced to 1dBm, which was used to capture
CSI data from the right hands of 20 participants (10 men and 10 women). The
dataset was normalized using MinMax scaling to ensure uniformity and accuracy.
By focusing on biophysical aspects such as hand size, shape, angular spread
between fingers, and finger phalanx lengths, among other characteristics, the
study explores how these features affect electromagnetic signals, which are
then reflected in Wi-Fi CSI, allowing for precise user identification. Five
classification algorithms were evaluated, with the Random Forest classifier
achieving an average F1-Score of 99.82\% using 10-fold cross-validation.
Amplitude and Phase data were used, with each capture session recording
approximately 1000 packets per second in five 5-second intervals for each User.
This high accuracy highlights the potential of Wi-Fi CSI in developing robust
and reliable user authentication systems based on palm biometric data.

</details>


### [4] [Space-Air-Ground Integrated Networks for 6G Mobile Communications](https://arxiv.org/abs/2510.22247)
*Tianming Lan*

Main category: cs.NI

TL;DR: 本文总结了SAGIN辅助6G的发展动机、现状、设计理念、问题和挑战，提出了分层架构和关键技术，并通过案例研究讨论了集成层的拥塞问题。


<details>
  <summary>Details</summary>
Motivation: 随着5G商用化，6G成为研究热点。SAGIN因其高速传输和扩展覆盖等优势，成为6G的关键支撑技术。

Method: 总结SAGIN辅助6G的发展动机和现状，讨论设计理念、问题和挑战，提出分层架构和关键技术，并通过案例研究验证架构。

Result: 提出了SAGIN辅助6G的分层架构，识别了各层所需的关键技术，并通过集成层拥塞问题的案例研究展示了架构的有效性。

Conclusion: SAGIN是6G网络的重要支撑技术，其分层架构和关键技术能够有效解决6G网络面临的挑战，为未来6G发展提供理论支持。

Abstract: After the industrialization of 5G cellular communications, 6G has
increasingly become a research hotspot in the academia. Space-Air-Ground
Integrated Network (SAGIN) is a key supporting technology for 6G because of its
advantages such as high-speed transmission and expanded coverage. This paper
summarizes the motivation to develop the SAGIN-assisted 6G first and introduces
the current situation of SAGIN. We then try to discuss the design concept of
the SAGIN-assisted 6G, and list the problem and challenges. Moreover, we
propose an architecture of the SAGIN-assisted 6G and identify a series of key
technologies that are needed in different layers. Finally, in order for readers
to better understand our hierarchical architecture, we use a case study
discussing the congestion problem in the Integrated layer.

</details>


### [5] [NetBurst: Event-Centric Forecasting of Bursty, Intermittent Time Series](https://arxiv.org/abs/2510.22397)
*Satyandra Guthula,Jaber Daneshamooz,Charles Fleming,Ashish Kundu,Walter Willinger,Arpit Gupta*

Main category: cs.NI

TL;DR: NetBurst是一个针对突发性网络遥测时间序列的预测框架，将预测重新定义为预测突发发生时间和大小，相比传统方法在MASE指标上提升13-605倍。


<details>
  <summary>Details</summary>
Motivation: 现有基准时间序列预测方法主要针对平滑季节性数据，而网络遥测时间序列具有高度突发性、间歇性和重尾特性，这些特性属于Mandelbrot研究的统计机制，但现代AI架构对此类时间序列的预测研究不足。

Method: NetBurst采用事件中心框架，使用基于分位数的码本和双自回归器来预测突发发生时间和大小。

Result: 在生产网络遥测时间序列上，NetBurst相比Chronos等强基线将MASE降低了13-605倍，同时保持了突发性，产生的嵌入比Chronos聚类清晰5倍。

Conclusion: 这项工作展示了现代AI如何利用Mandelbrot的开创性研究在突发性、间歇性和重尾机制中进行预测，对于高风险决策具有重要操作价值。

Abstract: Forecasting on widely used benchmark time series data (e.g., ETT,
Electricity, Taxi, and Exchange Rate, etc.) has favored smooth, seasonal
series, but network telemetry time series -- traffic measurements at service,
IP, or subnet granularity -- are instead highly bursty and intermittent, with
heavy-tailed bursts and highly variable inactive periods. These properties
place the latter in the statistical regimes made famous and popularized more
than 20 years ago by B.~Mandelbrot. Yet forecasting such time series with
modern-day AI architectures remains underexplored. We introduce NetBurst, an
event-centric framework that reformulates forecasting as predicting when bursts
occur and how large they are, using quantile-based codebooks and dual
autoregressors. Across large-scale sets of production network telemetry time
series and compared to strong baselines, such as Chronos, NetBurst reduces Mean
Average Scaled Error (MASE) by 13--605x on service-level time series while
preserving burstiness and producing embeddings that cluster 5x more cleanly
than Chronos. In effect, our work highlights the benefits that modern AI can
reap from leveraging Mandelbrot's pioneering studies for forecasting in bursty,
intermittent, and heavy-tailed regimes, where its operational value for
high-stakes decision making is of paramount interest.

</details>


### [6] [Should BBR be the default TCP Congestion Control Protocol?](https://arxiv.org/abs/2510.22461)
*Josue Abreu,Paul Bergeron,Sandhya Aneja*

Main category: cs.NI

TL;DR: BBR协议在多种网络环境中表现优异，吞吐量高，但在延迟敏感场景下需要权衡选择


<details>
  <summary>Details</summary>
Motivation: 研究BBR协议能否作为TCP默认拥塞控制机制，并评估其最新版本BBRv2和BBRv3在不同网络环境中的性能表现

Method: 在互联网、数据中心、以太网、无线和卫星网络等多种环境中进行实验，将BBR与传统TCP变体(Reno、Cubic)及其他协议进行比较

Result: BBR在所有环境中都能实现高吞吐量，特别是在同质BBR流或高带宽互联网路径场景下表现最佳；但BBR会引入更高的延迟和抖动

Conclusion: BBR适合处理批量传输和带宽密集型应用，但在延迟敏感环境中需要根据工作负载选择合适的协议

Abstract: In this research, we investigate the feasibility of adopting the Bottleneck
Bandwidth and Round-trip propagation time (BBR) protocol as the default
congestion control mechanism for TCP. Our central question is whether BBR,
particularly its latest iterations, BBRv2 and BBRv3, can outperform traditional
TCP variants such as Reno and Cubic across diverse networking environments. We
evaluated performance trade-offs in Internet, data center, Ethernet, wireless,
and satellite networks, comparing BBR against protocols including DCTCP, DCQCN,
TIMELY, HPCC, Swift, and congestion control schemes designed for low-Earth
orbit satellite networks, using both experiments and previous studies. Our
findings show that BBR consistently achieves high throughput across all
environments, with especially strong performance and fairness in scenarios
involving homogeneous BBR flows or high bandwidth Internet paths. Experiments
with Google and other websites over a 100~Mbps home network further confirm
BBR's superior performance and its ability to co-exist with Cubic flows. In
another experiment on the Marist campus (1--10~Gbps network), we observed its
latency characteristics compared to Cubic. Moreover, a controlled evaluation
between protocols reveals that BBR achieves the highest throughput ($\approx
905$~Mbps) but introduces higher latency ($\approx 0.79$~ms) and jitter
($\approx 4.2$~ms). In contrast, Reno and Cubic deliver balanced performance
with lower latency and moderate jitter. Vegas prioritizes minimal latency and
jitter at the cost of reduced throughput. These results demonstrate the
strength of BBR to handle bulk transfers and bandwidth-intensive applications.
However, they also emphasize the significance of workload-driven protocol
selection in latency-sensitive environments.

</details>


### [7] [A Control-Theoretic Perspective on BBR/CUBIC Congestion-Control Competition](https://arxiv.org/abs/2510.22773)
*Simon Scherrer,Adrian Perrig,Stefan Schmid*

Main category: cs.NI

TL;DR: 本文通过控制理论分析BBR/CUBIC竞争中的振荡问题，揭示了网络不稳定的条件及其对公平性的影响，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有模型在分析BBR拥塞控制算法公平性时存在准确性与可解释性的权衡：动态流体模型准确但难以理解，稳态模型直观但不够准确，特别是在分析BBR与传统基于丢包的CCA竞争时，这种竞争常出现不稳定性（发送速率振荡）。

Method: 通过控制理论扩展最近的BBR动态流体模型，进行控制理论分析，推导BBR/CUBIC振荡的定量条件，识别易受不稳定性影响的网络设置。

Result: 发现振荡条件在实际网络中经常满足，分析揭示了BBR/CUBIC振荡对公平性的影响，通过实验验证了反映极端速率分布的公平性边界。

Conclusion: BBR/CUBIC振荡频繁发生且损害BBR公平性，但可以通过我们的控制理论框架进行补救。

Abstract: To understand the fairness properties of the BBR congestion-control algorithm
(CCA), previous research has analyzed BBR behavior with a variety of models.
However, previous model-based work suffers from a trade-off between accuracy
and interpretability: While dynamic fluid models generate highly accurate
predictions through simulation, the causes of their predictions cannot be
easily understood. In contrast, steady-state models predict CCA behavior in a
manner that is intuitively understandable, but often less accurate. This
trade-off is especially consequential when analyzing the competition between
BBR and traditional loss-based CCAs, as this competition often suffers from
instability, i.e., sending-rate oscillation. Steady-state models cannot predict
this instability at all, and fluid-model simulation cannot yield analytical
results regarding preconditions and severity of the oscillation. To overcome
this trade-off, we extend the recent dynamic fluid model of BBR by means of
control theory. Based on this control-theoretic analysis, we derive
quantitative conditions for BBR/CUBIC oscillation, identify network settings
that are susceptible to instability, and find that these conditions are
frequently satisfied by practical networks. Our analysis illuminates the
fairness implications of BBR/CUBIC oscillation, namely by deriving and
experimentally validating fairness bounds that reflect the extreme rate
distributions during oscillation. In summary, our analysis shows that BBR/CUBIC
oscillation is frequent and harms BBR fairness, but can be remedied by means of
our control-theoretic framework.

</details>


### [8] [Exploring LR-FHSS Modulation for Enhanced IoT Connectivity: A Measurement Campaign](https://arxiv.org/abs/2510.23152)
*Alexis Delplace,Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: LR-FHSS调制相比传统LoRa在密集城区环境中能提升20%的包接收率，最低RSSI可达-138 dBm，显著增强了通信鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在城区环境中，LoRaWAN网络面临拓扑复杂性和信号传播挑战，需要真实测量来评估LR-FHSS与传统LoRa的性能差异，这些难以通过仿真完全复现。

Method: 在加拿大哈利法克斯进行实地测量，使用支持LR-FHSS和LoRa双调制的LoRaWAN平台，在FCC监管的US915频段运行，分析包接收率、路径损耗和接收信号强度指标。

Result: LR-FHSS在密集城区比传统LoRa提升20%包接收率，最低RSSI达到-138 dBm（LoRa为-120 dBm），在监管限制下表现出更强的通信鲁棒性。

Conclusion: LR-FHSS的引入显著增强了LoRaWAN网络在城区环境下的通信可靠性和鲁棒性，在监管限制下具有广阔应用前景。

Abstract: This paper presents the first comprehensive real-world measurement campaign
comparing LR-FHSS and LoRa modulations within LoRaWAN networks in urban
environments. Conducted in Halifax, Canada, the campaign used a LoRaWAN
platform capable of operating both modulations in the FCC-regulated US915 band.
Real-world measurements are crucial for capturing the effects of urban topology
and signal propagation challenges, which are difficult to fully replicate in
simulations. Results show that LR-FHSS can achieve up to a 20% improvement in
Packet Reception Rate (PRR) over traditional LoRa in dense urban areas.
Additionally, the study investigated path loss and Received Signal Strength
Indicator (RSSI), finding that LR-FHSS achieved a minimum RSSI of -138 dBm
compared to LoRa's -120 dBm. The findings demonstrate that the introduction of
LR-FHSS enhances communication robustness and reliability under regulatory
limitations and suggest promising applications in LoRaWAN networks.

</details>


### [9] [Trajectory-Aware Air-to-Ground Channel Characterization for Low-Altitude UAVs Using MaMIMO Measurements](https://arxiv.org/abs/2510.23465)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Wout Joseph,Sofie Pollin*

Main category: cs.NI

TL;DR: 本文通过大规模MIMO阵列测量分析了郊区环境中低空空对地信道的轨迹感知特性，发现仰角是接收功率的最强预测因子，Nakagami模型最适合小尺度衰落，K因子随高度增加而上升，非平稳性模式与轨迹和几何特征密切相关。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在6G非地面网络中的应用日益广泛，需要深入理解低空空对地信道的传播特性，为信道建模和通信系统设计提供依据。

Method: 使用64单元大规模MIMO阵列测量三种无人机轨迹（两个水平Z字形飞行和一个垂直上升）的信道特性，分析大尺度功率变化、衰落行为、时间非平稳性和频谱效率。

Result: 仰角与接收功率的相关性超过0.77，是功率的最强预测因子；Nakagami模型最适合小尺度衰落；K因子从低空的约5dB增加到高空的超过15dB；非平稳性模式高度依赖于轨迹和几何特征。

Conclusion: 这些发现为6G非地面网络中无人机通信信道的建模和改进提供了重要见解，强调了仰角在信道特性中的关键作用以及轨迹依赖性对非平稳性的影响。

Abstract: This paper presents a comprehensive measurement-based trajectory-aware
characterization of low-altitude Air-to-Ground (A2G) channels in a suburban
environment. A 64-element Massive Multi-Input Multi-Output (MaMIMO) array was
used to capture channels for three trajectories of an Uncrewed Aerial Vehicle
(UAV), including two horizontal zig-zag flights at fixed altitudes and one
vertical ascent, chosen to emulate AUE operations and to induce controlled
azimuth and elevation sweeps for analyzing geometry-dependent propagation
dynamics. We examine large-scale power variations and their correlation with
geometric features, such as elevation, azimuth, and 3D distance, followed by an
analysis of fading behavior through distribution fitting and Rician K-factor
estimation. Furthermore, temporal non-stationarity is quantified using the
Correlation Matrix Distance (CMD), and angular stationarity spans are utilized
to demonstrate how channel characteristics change with the movement of the UAV.
We also analyze Spectral Efficiency (SE) in relation to K-factor and Root Mean
Square (RMS) delay spread, highlighting their combined influence on link
performance. The results show that the elevation angle is the strongest
predictor of the received power, with a correlation of more than 0.77 for each
trajectory, while the Nakagami model best fits the small-scale fading. The
K-factor increases from approximately 5 dB at low altitudes to over 15 dB at
higher elevations, indicating stronger LoS dominance. Non-stationarity patterns
are highly trajectory- and geometry-dependent, with azimuth most affected in
horizontal flights and elevation during vertical flight. These findings offer
valuable insights for modeling and improving UAV communication channels in 6G
Non-Terrestrial Networks (NTNs).

</details>


### [10] [How to build a sovereign network? -- A proposal to measure network sovereignty](https://arxiv.org/abs/2510.23510)
*Shakthivelu Janardhanan,Ritanshi Agarwal,Wolfgang Kellerer,Carmen Mas-Machuca*

Main category: cs.NI

TL;DR: 提出了一种新的网络主权度量指标CSC分数，并开发了CSC-ILP整数线性规划模型来最大化网络主权，以减少对组件制造商的依赖。


<details>
  <summary>Details</summary>
Motivation: 网络运营商面临制造商依赖问题，高依赖性会导致在制造商不可用时网络生存能力降低，需要提高网络主权以避免供应商锁定问题。

Method: 提出了Cut Set Coloring (CSC)评分作为网络主权度量指标，并开发了基于CSC核心指标的整数线性规划模型CSC-ILP。

Result: 将CSC-ILP的性能与最先进的制造商分配策略进行了比较。

Conclusion: CSC评分和CSC-ILP模型为网络运营商提供了有效的方法来量化和提高网络主权，减少对单一制造商的依赖。

Abstract: Network sovereignty is a network operator's ability to reduce the dependency
on component manufacturers to minimize the impact of manufacturer failures.
Network operators now face new design challenges to increase network
sovereignty and avoid vendor lock-in problems because a high dependency on a
manufacturer corresponds to low survivability if that manufacturer is
unavailable. The main contribution of this work is the proposal of a novel
metric to measure network sovereignty, the Cut Set Coloring (CSC) score. Based
on the CSC core metric CSC-ILP, our Integer Linear Program formulation is
presented to maximize network sovereignty. We compare CSC-ILP's performance
with state of the art manufacturer assignment strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 本文提出了一个融合AI与计算心理学的综合框架，通过端到端的开发流程，从基础机器学习基准到Transformer模型微调，再到部署为可交互的微服务系统，实现了从预测分析到生成对话的完整研究到部署管道。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能和计算心理学的结合，通过计算手段建模、理解和交互复杂的人类心理状态，弥合孤立预测建模与交互式心理分析系统之间的差距。

Method: 采用端到端开发流程：1) 在四个心理学数据集上建立基础机器学习基准；2) 微调最先进的Transformer模型，解决回归任务中的数值不稳定性和资源约束下的大规模训练挑战；3) 使用参数高效技术微调生成式大语言模型作为交互式"人格大脑"；4) 将预测和生成模型架构部署为可扩展的微服务生态系统。

Result: 成功稳定了基于Transformer的情感计算回归模型，在标准方法失败的情况下显示出有意义的预测性能，并开发了可复现的大规模AI研究民主化方法。

Conclusion: 这项工作的重要意义在于其整体方法，展示了从预测分析到生成对话的完整研究到部署管道，为计算心理学和人机交互的未来研究提供了实用模型。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [12] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: 提出了PREFINE框架，通过构建伪用户代理和用户特定评估标准，实现无需参数更新或直接用户反馈的个性化故事生成。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在个性化文本生成中的挑战，传统方法依赖显式反馈或微调，存在用户负担、数据收集、计算成本和隐私等实际问题。

Method: PREFINE框架从用户交互历史构建伪用户代理，生成用户特定评估标准，让该代理基于这些标准代表用户进行批判和精炼。

Result: 在PerDOC和PerMPST数据集上的自动评估显示，PREFINE相比基线方法获得了更高的胜率和统计显著分数，且不损害一般故事质量。

Conclusion: 该方法不仅适用于故事生成，在对话系统、教育和推荐等更广泛应用中也有潜力实现高效个性化。

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [13] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: SIGN（Schema-Induced Games for Naming）是一种命名游戏，通过引入轻量级结构来引导多智能体系统的约定形成，相比无约束自然语言能实现更快收敛和高达5.8倍的协议一致性。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统在处理复杂问题时，多个大语言模型智能体之间的不一致约定会导致协调失败。协作编码和分布式规划等应用需要可靠、一致的通信，且系统扩展性是核心关注点。

Method: 引入SIGN命名游戏，研究轻量级结构如何引导约定形成。比较模式诱导通信与无约束自然语言的表现。

Result: 模式诱导通信相比无约束自然语言实现了更快的收敛速度，协议一致性提高了高达5.8倍。

Conclusion: 最小化结构可以作为多智能体协调的简单控制旋钮，实现高效协调，在命名游戏之外具有更广泛的应用前景。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [14] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: 研究发现解码器自回归语言模型在知识密集型任务中存在能力上限，参数规模从7000万到300亿的扩展对知识检索任务准确率提升微乎其微，而损失函数持续下降，显示指标不一致问题。


<details>
  <summary>Details</summary>
Motivation: 系统评估OPT和Pythia模型家族在不同规模下的能力表现，揭示参数扩展对知识密集型任务的实际效果，为资源分配提供实证依据。

Method: 对OPT和Pythia模型家族（70M-30B参数）进行系统评估，包括知识检索任务、MMLU数学基准测试，并进行注意力干预实验。

Result: 知识检索任务准确率提升可忽略不计，MMLU数学准确率稳定在19-20%，而交叉熵损失下降31%。注意力模式交换导致性能灾难性崩溃而非渐进退化。

Conclusion: 对于使用OPT和Pythia架构的知识密集型应用，超过1-2B参数的扩展提供有限准确率增益，这些发现量化了特定能力扩展失败，为资源分配决策提供信息。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [15] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 开发了GeoThoughts数据集和GeoThought-MLLM模型，通过链式思维训练提升几何推理能力，在几何任务中超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本数学问题解决中表现出色，但在视觉推理任务特别是几何问题解决中性能显著下降，主要由于几何问题的内在复杂性和现有数据集的局限性。

Method: 创建了包含6,243个样本的Geo-Thought-6K数据集和10,834个样本的Geo-Thought-Augmented-10K数据集，每个条目包含视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此开发了GeoThought-MLLM多模态模型。

Result: 模型在几何任务中优于现有基准，训练使用链式思维数据集能提升几何推理能力，在域内和域外设置中均表现良好。

Conclusion: 错误主要源于数学概念错误解释或空间误判，通过调用链式思维纠正这些错误可以产生正确答案。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [16] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

Main category: cs.AI

TL;DR: 将生成流网络应用于旅行商问题、最小生成树和最短路径三个图优化问题，通过训练学习采样与奖励函数成比例的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型解决组合优化问题的潜力，利用学习策略的优势实现计算可扩展性，在经典精确方法不可行的大规模问题实例中具有潜力。

Method: 使用轨迹平衡损失训练生成流网络，顺序构建解决方案：为生成树选择边、为路径选择节点、为旅行选择城市。

Result: 在多种不同节点数的图配置上测试，生成的解决方案与经典算法结果匹配，训练收敛取决于问题复杂度，图规模越大所需训练轮数越多。

Conclusion: 生成模型可以通过学习策略解决组合优化问题，主要优势是计算可扩展性，通过训练分摊计算成本，在更大规模问题上具有潜力。

Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [17] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: 本文研究了在部分$q^{\pi}$-可实现性框架下强化学习的计算复杂性，证明在该设置下学习$\epsilon$-最优策略是计算困难的。


<details>
  <summary>Details</summary>
Motivation: 研究部分$q^{\pi}$-可实现性框架的计算复杂性，该框架假设策略集$\Pi$中所有策略的价值函数都是线性可实现的，比$q^{\pi}$-可实现性弱但比$q^*$-可实现性强，提供了函数逼近自然出现的实用模型。

Method: 通过从$\delta$-Max-3SAT和$\delta$-Max-3SAT(b)问题归约到GLinear-$\kappa$-RL（贪婪策略）和SLinear-$\kappa$-RL（softmax策略）实例，建立计算复杂性结果。

Result: 证明了在该设置下学习$\epsilon$-最优策略是NP困难的，对于贪婪策略集在参数化设置下是NP困难的，对于softmax策略集在随机指数时间假设下存在指数级下界（除非NP = RP）。

Conclusion: 在部分$q^{\pi}$-可实现性框架下，通常无法获得积极的计算结果，这与生成访问模型下的$q^{\pi}$-可实现性形成对比，表明即使将策略集扩展到最优策略之外，计算困难仍然存在。

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [18] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本研究展示了如何通过优化小型开源模型（10亿参数的Llama 3.2）实现与大型商业模型（GPT-4.1）相当的99%准确率，同时大幅降低计算成本和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型商业模型在专业领域部署时面临高计算成本、延迟和运营费用的问题，需要寻找资源效率更高的替代方案。

Method: 使用QLoRA对10亿参数Llama 3.2模型进行微调，并在合成数据集上训练，随后应用GPTQ和GGUF后训练量化技术创建GPU和CPU优化版本。

Result: 专用1B模型达到99%准确率，与GPT-4.1性能相当；4位GPTQ减少41%显存但推理速度下降82%；GGUF在CPU上实现18倍推理吞吐量提升和90%以上内存减少。

Conclusion: 经过适当优化的小型开源模型不仅是可行的替代方案，而且在领域特定应用中更为合适，能以极低的计算成本提供最先进的准确性。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [19] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 提出Distribution Shift Alignment (DSA)方法，通过两阶段微调对齐输出分布和分布偏移，显著提升LLM模拟调查响应的准确性，减少53.48-69.12%的真实数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法存在提示敏感性和低准确性问题，传统微调方法过度拟合训练集分布，无法产生比训练集更准确的结果，偏离了使用LLM模拟调查响应的初衷。

Method: DSA是一种两阶段微调方法，通过学习分布如何变化而非拟合训练数据，同时对齐输出分布和不同背景下的分布偏移。

Result: 在五个公开调查数据集上，DSA始终优于其他方法，显著提高了准确性、鲁棒性，并将所需真实数据减少了53.48-69.12%。

Conclusion: DSA通过关注分布变化而非数据拟合，能够提供比训练数据更接近真实分布的结果，证明了其在调查模拟中的有效性和效率。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [20] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang,Jiayu Liu,Xin Lin,Zhiyuan Ma,Shangzi Xue,Tong Xiao,Qi Liu,Yee Whye Teh,Enhong Chen*

Main category: cs.AI

TL;DR: 这篇论文从人类认知视角系统梳理了数学应用题求解研究，总结了5种关键认知能力，回顾了近10年主流模型，并统一比较了各方法在5个基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 数学应用题作为AI基础研究领域，缺乏系统性的分类综述和当前发展趋势讨论。论文旨在通过人类认知视角全面回顾相关研究，展示AI模型在模拟人类认知能力方面的进展。

Method: 从人类认知角度总结5种关键能力：问题理解、逻辑组织、联想记忆、批判性思维和知识学习。回顾近10年两大主流模型：神经网络求解器和基于LLM的求解器，并统一重新运行代表性方法在5个主流基准上的性能。

Result: 提供了首个从人类推理认知视角全面分析过去十年有影响力的MWP研究的综述，并给出了现有方法的整体比较。

Conclusion: 希望该综述能启发AI推理领域的进一步研究，相关代码库已在GitHub开源。

Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [21] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动GUI代理解决方案，通过设备-云协作平衡本地模型成本效益和云端模型高性能，解决了移动设备上小模型能力不足而大模型部署困难的问题。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理面临关键困境：真正在设备上的小模型（4B或更小）性能不足，而能力强的模型（从7B开始）要么太大无法在移动设备部署，要么成本过高（如仅云端的闭源MLLMs）。

Method: 通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型在合成GUI数据上的决策能力；集成高效长推理机制在有限资源下利用历史交互；默认在设备上执行，仅通过实时复杂度评估将挑战性子任务升级到云端。

Result: 在AndroidLab基准测试和多样化应用上的实验显示，LightAgent匹配或接近更大模型的性能，同时显著降低了云成本。

Conclusion: LightAgent通过设备-云协作有效解决了移动GUI代理的部署困境，在保持高性能的同时大幅降低了成本。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [22] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: LLM-AR框架将LLM生成的启发式规则转化为概率规则，通过自动推理引擎预测初创企业成功，实现可解释的高风险决策支持。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在关键决策应用中准确率不稳定的问题，特别是在风险投资领域预测初创企业成功。

Method: 提出LLM-AR管道，受神经符号系统启发，将LLM生成的启发式规则蒸馏为概率规则，使用ProbLog自动推理引擎执行，并通过迭代策略进化循环结合关联规则挖掘逐步优化预测规则。

Result: 在未见数据上达到59.5%的精确度和8.7%的召回率，是随机基线精确度的5.9倍，同时保持所有决策路径可人工检查。

Conclusion: 该框架具有可解释性和超参数可调性，显示出扩展到其他领域的潜力。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [23] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 在部分可观测环境中，传统元强化学习虽然能学习到接近贝叶斯最优的策略，但无法学习到紧凑、可解释的贝叶斯最优信念状态。通过引入基于预测编码的自监督预测模块，元强化学习能够学习到更接近贝叶斯最优的表示，并在需要主动信息寻求的挑战性任务中表现更好，同时提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统元强化学习在部分可观测环境中虽然能获得接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态，这种表示效率的不足可能限制智能体的适应性和泛化能力。

Method: 受神经科学中预测编码（大脑通过预测感官输入来实现贝叶斯推断）和深度强化学习中辅助预测目标的启发，将自监督预测编码模块集成到元强化学习中，以促进贝叶斯最优表示的学习。

Result: 通过状态机模拟实验表明，带有预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地逼近贝叶斯最优信念状态，即使两者都能达到最优策略。在需要主动信息寻求的挑战性任务中，只有带有预测模块的元强化学习能成功学习最优表示和策略，而传统元强化学习因表示学习不足而难以应对。

Conclusion: 预测学习可以作为在部分可观测环境中导航的智能体进行有效表示学习的指导原则，更好的表示学习能带来改进的泛化能力。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [24] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini,Braz Izaias da Silva Junior*

Main category: cs.AI

TL;DR: 应用SpecC方法学对PCM-to-PWM转换器进行系统级软硬件协同设计，在满足实时约束的同时降低纯硬件方案成本，避免高端处理器纯软件实现的开销。


<details>
  <summary>Details</summary>
Motivation: 通过系统级软硬件协同设计方法，在早期获得架构洞察，快速验证设计，并提供可行的成本/性能权衡方案。

Method: 使用SpecC方法学对PCM-to-PWM转换器进行建模和探索，推导软硬件划分方案，利用系统级估计和快速功能仿真评估不同映射方案。

Result: 成功找到满足实时约束的映射方案，相比纯硬件方案降低了估计成本，同时避免了高端处理器纯软件实现的高开销。

Conclusion: 即使对于中等复杂度的设计，系统级协同设计仍具有重要价值，能够提供早期架构洞察、快速验证和可行的成本/性能权衡。

Abstract: We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [25] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

Main category: cs.AI

TL;DR: 该论文提出从预测准确性转向因果解释的智能系统重构，引入能量结构化因果模型(ESCMs)作为可干预的因果表示框架，使内部机制能够进行局部手术式编辑。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统虽然预测性能优异但因果不透明，无法对特定机制进行手术式干预，因为学习到的潜在变量缺乏因果语义。

Method: 引入计算解释和能量结构化因果模型(ESCMs)，将机制表示为约束(能量函数或向量场)而非显式输入输出映射，干预通过对这些约束进行局部手术实现。

Result: 在ESCM背景下具体化了结构因果原则LAP和ICM，分析了经验风险最小化导致断裂纠缠表示的问题，并在温和条件下证明ESCMs恢复标准SCM语义。

Conclusion: 基于Part I的原则(LAP、ICM、CAP)和将智能定义为批评下的解释构建，本文为追求理解而非仅预测的系统提供了因果推理的形式化语言。

Abstract: Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [26] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee,Niraj K. Jha,Jonathan D. Cohen,Thomas L. Griffiths,Hongjing Lu,Diana Marculescu,Ashiqur Rasul,Keshab K. Parhi*

Main category: cs.AI

TL;DR: 本文提出了下一代AI的愿景：从当前需要大量数据和能源的大型语言模型，转向轻量级、领域特定、能推理规划的多模态智能体，实现1000倍以上的能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型（如GPT-4）存在能耗高（50-60 GWh）、容易产生幻觉等问题，而人脑仅需20W功率。需要开发更智能、节能的AI系统来应对动态环境中的实时决策需求。

Method: 提出构建轻量级领域特定多模态模型，能够在动态环境中进行推理、规划和决策，利用实时数据和先验知识进行持续学习。

Result: 描绘了未来AI系统的蓝图：从当前的大模型转向能效提升1000倍以上的敏捷领域特定智能体。

Conclusion: 下一代AI需要硬件重新设计，支持能效大幅提升的轻量级智能体，这些智能体能够在不确定性环境中进行推理和思考。

Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [27] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，强调将智能体重新定义为主动协作伙伴而非被动信号处理器，需要关注伦理数据处理、模型可靠性和人-智能体协作框架。


<details>
  <summary>Details</summary>
Motivation: 脑机接口面临信息传输率低和用户特定校准等限制，虽然大语言模型的整合有所进展，但部署智能AI仍面临技术障碍和伦理担忧。

Method: 这是一篇立场论文，通过论证方式提出从BCI到BAC的范式扩展，强调重新定义智能体角色。

Result: 提出了脑-智能体协作(BAC)的新范式概念，强调智能体应作为主动协作伙伴。

Conclusion: 需要关注伦理数据管理、模型可靠性和人-智能体协作框架，以确保这些系统的安全、可信和有效性。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [28] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

Main category: cs.AI

TL;DR: 提出了一种利用自优化思想向量和熵最小化的可控数学推理方法，在GSM8K数据集上达到90.1%准确率，可控性得分0.42


<details>
  <summary>Details</summary>
Motivation: 开发能够动态调节大语言模型内部推理过程的可控AI推理框架，无需外部奖励标注

Method: 引入可学习的思想向量，通过熵最小化奖励引导聚焦推理模式，使用Gemma-2-9B模型

Result: 在GSM8K上达到90.1%准确率，可控性得分0.42，思想向量形成明显聚类，控制条件下保持低熵分布

Conclusion: 熵基奖励能有效引导聚焦推理模式，验证了可控AI推理框架的有效性

Abstract: We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [29] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出了一个AI心理测量框架，使用情境判断测试和复杂人物角色设计来评估AI系统在需要情感判断和伦理考量的角色中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常重复使用人类特质清单或临时人物角色，限制了行为真实性和领域相关性。

Method: 框架包含三个部分：(1)使用现实情境中的情境判断测试来探究领域特定能力；(2)整合工业组织心理学和人格心理学设计复杂人物角色；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙事。

Result: 在执法助手案例研究中，构建了包含8个人物原型、11个属性的丰富数据集，涵盖8,500个人物角色、4,000个情境判断测试和300,000个响应。

Conclusion: 该框架提高了AI心理测量的行为真实性和领域相关性，数据集和代码将公开发布。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [30] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan,Shervin Safavi,Raja Guru,Andreas Knoblauch*

Main category: cs.AI

TL;DR: 本文提出了一种名为Dopamine的无导数优化器，用于解决神经网络的信用分配问题。该方法受神经强化学习启发，通过权重扰动学习和奖励预测误差来调整学习率，在减少计算和内存消耗的同时实现了与梯度方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决突触信用分配问题是神经网络学习的核心问题。反向传播虽然能解决这个问题，但存在计算效率低、内存消耗大以及权重传输和更新锁定等问题。

Method: 采用NeuroAI方法，受神经强化学习启发开发了Dopamine优化器。该方法基于权重扰动学习，通过最小化扰动模型与未扰动模型之间的奖励预测误差来调整学习率，实现自适应学习率策略。

Result: 在XOR任务和多层感知机、混沌时间序列预测的循环神经网络上测试，Dopamine训练模型显示出加速收敛，优于标准权重扰动方法，性能与基于梯度的算法相当，同时显著减少计算和内存消耗。

Conclusion: Dopamine优化器不仅找到了鲁棒解，性能与最先进的机器学习优化器相当，而且在神经生物学上更具合理性。

Abstract: Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [31] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu,Jie Wang,Yuyang Cai,Xiongwei Han,Yufei Kuang,Jianye Hao*

Main category: cs.AI

TL;DR: OptiTree提出了一种基于树搜索的自适应问题分解方法，通过构建建模树来组织运筹学问题，显著提升了复杂问题的建模准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定步骤分解来生成变量、约束和目标，但由于运筹学问题具有高度复杂的数学结构，这种方法往往无法达到高性能。

Method: 开发建模树，基于问题分类和复杂度的层次结构组织运筹学问题，每个节点代表一个问题类别并包含相关的高级建模思路。给定问题时，递归搜索树以识别更简单的子问题，并通过自适应整合层次思路来合成全局建模思路。

Result: 实验表明，OptiTree相比最先进方法显著提高了建模准确率，在具有挑战性的基准测试中实现了超过10%的改进。

Conclusion: OptiTree通过自适应问题分解和层次思路整合，有效解决了复杂运筹学问题的自动化建模挑战。

Abstract: Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [32] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出了PACR方法，通过模型内在的密集奖励信号加速强化学习训练，基于模型对正确答案信心的渐进上升趋势来指导推理过程。


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏奖励无法为中间推理步骤提供指导，导致探索缓慢。需要一种密集的模型内在奖励来加速推理过程。

Method: PACR方法直接计算模型对正确答案信心的渐进上升趋势作为密集奖励，约束探索空间到逻辑合理的推理区域。

Result: PACR加速了探索，用更少的轨迹达到奖励饱和，在多个基准测试上取得了改进。

Conclusion: 密集的模型内在塑造信号可以使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [33] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 该论文创建了首个大规模越南语歌词转录数据集VietLyrics，并通过微调Whisper模型在越南语歌词转录任务上取得了优于现有系统的性能。


<details>
  <summary>Details</summary>
Motivation: 越南语歌词转录面临音调复杂和方言变异的独特挑战，但由于缺乏专用数据集，该领域研究仍处于空白状态。

Method: 构建了包含647小时歌曲的VietLyrics数据集，并对Whisper模型进行微调以提升越南语歌词转录性能。

Result: 微调后的Whisper模型在越南语歌词转录任务上表现优于现有的多语言歌词转录系统（包括LyricWhiz），解决了现有ASR方法中的转录错误和幻觉问题。

Conclusion: 该研究为越南语音乐计算研究提供了重要资源，展示了在低资源语言和音乐领域中歌词转录方法的潜力。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [34] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

Main category: cs.AI

TL;DR: 提出了一种多级图粗化和细化框架来解决带时间窗的容量约束车辆路径问题，通过时空距离度量将客户聚合成元节点，在简化问题上使用经典启发式算法求解，然后扩展回原始空间并进行可行性修正。


<details>
  <summary>Details</summary>
Motivation: 带时间窗的容量约束车辆路径问题是物流中的基础NP难优化问题，解决大规模实例对精确求解器仍然具有计算挑战性。

Method: 使用多级图粗化和细化框架，通过时空距离度量将客户聚合成元节点，在简化问题上应用经典启发式算法，然后扩展回原始空间并进行可行性修正。

Result: 在Solomon基准实例上的初步实验表明，所提方法在保持或改进解质量的同时减少了计算时间，特别是在容量和时间窗约束方面表现更好。

Conclusion: 该方法能有效减少计算时间并保持解质量，论文还探索了量子启发优化技术的集成，强调了其在加速大规模车辆路径任务方面的潜力。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [35] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu,Yuansheng Lian,Ke Zhang,Yunxuan Li,Yuelong Su,Meng Li*

Main category: cs.AI

TL;DR: 提出了一个可解释的预测框架LIFT LLM，用于卡车驾驶风险预测，通过文献知识库增强模型的可解释性，在真实数据集上表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确预测卡车驾驶风险并提供可解释结果的方法，结合领域文献知识来增强模型的可解释性和可靠性。

Method: 构建包含LLM驱动推理核心、文献处理流水线和结果评估器的框架，在真实卡车驾驶风险数据集上微调LLM，并利用299篇领域文献构建知识库。

Result: LIFT LLM在召回率上比基准模型提高26.7%，F1分数提高10.1%，变量重要性排序与基准模型一致，且在不同数据采样条件下保持稳健。

Conclusion: LIFT LLM框架在卡车驾驶风险预测中表现出色，文献知识库和微调过程显著提升了模型的可解释性，具有数据驱动知识发现的潜力。

Abstract: This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [36] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: 提出了DynaSolidGeo，这是第一个用于评估视觉语言模型真实空间推理能力的动态基准，专注于立体几何问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注2D平面几何，使用静态数据集容易导致数据污染和记忆，且仅评估最终答案而忽略推理过程。

Method: 通过半自动标注流程构建，包含503个专家策划的种子问题，可动态生成无限多样的多模态文本-视觉实例，并加入基于专家标注推理链的过程评估。

Result: 实验显示代表性VLMs存在较大性能差距，在动态设置下性能严重下降，在需要高级空间智能的任务上表现不佳。

Conclusion: DynaSolidGeo填补了立体几何推理评估的空白，揭示了当前VLMs在空间推理方面的局限性。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [37] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 大型语言模型在推理任务上表现有限，当问题复杂度超过一定程度时会急剧失败。通过新的深度推理数据集评估发现，大型推理模型的性能在足够复杂度时会突然下降且无法泛化。


<details>
  <summary>Details</summary>
Motivation: 重新审视大型语言模型在复杂推理任务上的表现，验证其是否真正具备泛化推理能力，特别是在数学、物理、医学和法律等需要深度推理的领域。

Method: 开发了深度推理数据集(DeepRD)，包含可扩展复杂度的无限示例，用于评估模型在图连通性和自然语言证明规划任务上的表现。

Result: 大型推理模型在足够复杂度时性能急剧下降，无法泛化。虽然大多数现实世界例子落在模型成功范围内，但长尾分布暴露了显著的失败可能性。

Conclusion: 大型推理模型在近期具有实用性，但需要开发能够超越训练分布复杂度的新方法来实现真正的泛化推理。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [38] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 该论文提出使用有限状态机（FSM）模型来分析大型推理模型（LRMs）的层次化推理过程，通过识别离散推理状态来理解和可视化LLMs的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解大型推理模型（LRMs）中涌现的推理能力是一个重要但困难的问题，这对于改进训练和增强模型鲁棒性具有重要应用价值。

Method: 采用无记忆有限状态机（FSM）来近似LRMs的层次化推理动态，识别出初始化、演绎、增强策略、不确定性估计、回溯和最终结论等离散推理状态，并将推理轨迹表示为状态图中的转移序列。

Result: FSM分析揭示了不同模型在推理方法上的差异，显示了独特的推理模式和潜在缺陷，为评估和改进LLM推理提供了新视角。

Conclusion: 基于FSM的分析方法能够系统地分析、解释和可视化不同模型处理问题的方式，为理解LLM推理能力提供了结构化、可解释的抽象框架。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [39] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 提出了一种可中断协作角色扮演者(ICR)算法，用于训练LLM驱动的协作代理，使其能够更好地与干预代理合作，提高团队在任务相关命题上的共同基础对齐度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地部署在代理环境中与人类协作，需要评估它们在多轮、多方任务中的协作能力。现有基于RLHF训练的LLM代理倾向于忽略干预，这使得提高团队共同基础变得困难。

Method: 使用改进动作MDP框架分析标准AI代理的次优行为，提出ICR算法——一种伙伴感知学习算法来训练共同基础最优的协作代理。

Result: 在多个协作任务环境中的实验表明，ICR平均能更有效地促进共同基础收敛，并在这些任务中探索更多样化的解决方案。

Conclusion: ICR算法能够有效解决LLM代理在协作环境中忽略干预的问题，提升团队协作效果和共同基础对齐度。

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [40] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: OFFSIDE是一个基于足球转会谣言的多模态大语言模型遗忘评估基准，包含15.68K条手动标注数据，评估遗忘效果、泛化性、实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的数据隐私问题日益严重，需要选择性遗忘技术，但现有基准缺乏图像多样性、准确性不足且评估场景不够全面。

Method: 创建基于足球转会谣言的手动标注数据集，包含四个测试集评估不同方面，支持选择性遗忘、纠正再学习和单模态遗忘等高级设置。

Result: 评估发现：单模态方法在多模态谣言上失败；遗忘效果主要由灾难性遗忘驱动；所有方法都难以处理视觉谣言；遗忘的谣言容易恢复；所有方法都易受提示攻击。

Conclusion: 当前方法存在显著脆弱性，需要更鲁棒的多模态遗忘解决方案。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [41] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM是一个自适应优化的少样本方法，能够从非结构化文本中构建和持续更新时序知识图谱，通过原子化事实提取和双时间建模显著提升覆盖率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱构建忽略了数据的动态性和时效性，而现有的零样本或少样本方法存在不稳定性和关键事实覆盖不完整的问题。

Method: 将输入文档分割为最小自包含的"原子"事实，构建原子时序知识图谱，采用双时间建模区分信息观察时间和有效时间，并并行合并结果。

Result: 相比基线方法，ATOM实现了约18%的覆盖率提升、约17%的稳定性改善以及超过90%的延迟降低。

Conclusion: ATOM在动态时序知识图谱构建方面展现出强大的可扩展性潜力，能够有效应对实时数据变化的挑战。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [42] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song,Jiaxiang Li,Rong Wang,Songtao Lu,Mingyi Hong*

Main category: cs.AI

TL;DR: 该论文提出了一个分析上下文学习性能的新框架，通过理论分析和实验验证，揭示了预训练数据分布与查询任务分布差异时，适当构建的上下文如何量化地改善预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现了强大的上下文学习能力，但理论上尚不清楚这种能力如何产生，特别是预训练过程和上下文构建等关键因素的确切作用。

Method: 首先构建单层transformer的简单示例，然后扩展到更一般情况，推导ICL性能、上下文长度与预训练和查询任务分布KL散度之间的精确关系，最后通过实验验证理论结果。

Result: 当预训练数据分布与查询任务分布不同时，适当构建的上下文可以量化地将输出分布向查询任务分布偏移，从而提高预测准确性。

Conclusion: 该研究为理解上下文学习机制提供了理论框架，明确了上下文在弥合预训练与任务分布差异中的量化作用。

Abstract: Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [43] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: CLIN-LLM是一个安全约束的混合管道系统，集成了多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成，在症状到疾病分类和治疗推荐方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的系统缺乏医学基础且无法量化不确定性，导致输出不安全。需要开发一个安全约束的临床决策支持系统，特别是在诊断风险高的异质患者环境中。

Method: 使用BioBERT在1,200个临床案例上微调，结合Focal Loss和Monte Carlo Dropout实现置信度感知预测；采用Biomedical Sentence-BERT从MedDialog语料库检索相关对话，使用微调的FLAN-T5模型生成个性化治疗建议，并通过RxNorm进行抗生素管理和药物相互作用筛查。

Result: CLIN-LLM达到98%的准确率和F1分数，比ClinicalBERT提升7.1%；78%的top-5检索精度；临床医生评定的有效性为4.2/5分；不安全抗生素建议比GPT-5减少67%。

Conclusion: CLIN-LLM展示了稳健性、可解释性和临床安全性，为资源有限的医疗环境提供了一个可部署的、人机协作的决策支持框架。

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [44] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: SwiftSolve是一个针对竞赛编程的复杂度感知多智能体系统，通过结合算法规划、经验性分析和复杂度引导的修复来提高程序效率，而不仅仅是正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的程序虽然能通过单元测试，但经常违反竞赛的时间和内存限制，需要系统性地解决效率问题。

Method: 采用多智能体系统架构，包括规划器、静态剪枝器、编码器、性能分析器和复杂度分析器，通过版本化JSON通信和迭代控制来协同工作。

Result: 在26个问题上，SwiftSolve在第一次尝试中达到61.54%的通过率，三次尝试内达到80.77%的解决率，运行级成功率为73.08%，平均时间12.40秒。

Conclusion: 性能分析和复杂度引导的重新规划能有效减少低效率问题，同时保持准确性，相比Claude Opus 4在运行级成功率上有显著提升。

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [45] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 提出基于首个生成token的对数概率分布来检测LLM的模板化响应，实现早期终止或重定向到小模型，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM经常耗费大量计算资源生成模板化响应（如拒绝、简单确认和问候），增加了不必要的成本和延迟。

Method: 使用首个生成token的对数概率分布作为信号，通过轻量级k-NN分类器预测响应类型（实质性回答或模板化响应）。

Result: 实验表明，首个token的对数概率向量在不同响应类型下形成明显可分离的聚类，能够高精度预测响应性质。

Conclusion: 该方法提供了一种实用且计算量极小的技术，通过早期终止或重定向优化LLM推理，实现显著的计算成本节省，推动更高效和可持续的LLM部署。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [46] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander,Sai Pragnya Ranga,Prathamesh Mayekar*

Main category: cs.AI

TL;DR: 提出Atlas Urban Index (AUI)指标，使用Sentinel-2卫星图像和视觉语言模型来测量城市发展，克服传统指标如NDBI在准确捕捉城市发展方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NDBI由于大气噪声、季节变化和云层覆盖等因素，难以准确捕捉城市发展，阻碍了大规模人类发展和城市化监测。

Method: 收集每个区域的Sentinel-2图像时间序列，在固定时间窗口内处理图像以获得最小云覆盖的代表性图像，使用视觉语言模型提供发展评分，并采用参考图像集和最新历史图像确保评分一致性。

Result: 在班加罗尔的定性实验表明，AUI优于NDBI等标准指标。

Conclusion: AUI能够克服传统城市化指标的挑战，产生更可靠和稳定的发展评分。

Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [47] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai,Jusheng Zhang,Yijia Fan,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: RaCoT是一个在检索前阶段引入对比思维的RAG框架，通过生成对比问题和提取关键差异提示，在单次检索中抑制语义干扰，提升长尾查询的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在知识稀疏和语义模糊的长尾查询中面临的检索噪声问题，避免昂贵的后处理成本。

Method: 在检索前自动生成语义相近但答案不同的对比问题，提取Δ-Prompt捕捉关键差异，引导模型关注决定答案分歧的关键细节。

Result: 在六个权威基准测试中，RaCoT比RankRAG和Self-RAG等基线方法提升0.9-2.4个百分点，在对抗性测试中性能下降仅8.6%，延迟3.12秒，token开销11.54。

Conclusion: RaCoT将RAG范式从"事后上下文清理"重构为"先验塑造判别推理"，为实时资源受限部署提供了高效鲁棒的可靠AI系统路径。

Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [48] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 该研究比较了五个顶级大语言模型（Gemini、DeepSeek、Claude、GPT和LLaMA）在性能准确性、伦理偏见缓解和可用性集成三个关键维度的差异。


<details>
  <summary>Details</summary>
Motivation: 随着各大公司不断改进大语言模型，了解不同模型在性能、道德行为和可用性方面的差异变得至关重要，这些差异反映了构建它们的不同理念。

Method: 通过分析三个重要因素：性能与准确性、伦理与偏见缓解、可用性与集成，对五个顶级LLM进行比较评估。

Result: 研究发现Claude在道德推理方面表现良好，Gemini在多模态能力和伦理框架方面更强，DeepSeek在基于事实的推理方面出色，LLaMA适合开放应用，ChatGPT提供平衡性能且注重用户体验。

Conclusion: 这些模型在工作效果、易用性和伦理处理方面存在显著差异，用户应根据具体需求选择最适合的模型以充分发挥其优势。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [49] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 开发了一个事实验证框架，通过交叉检查LLM输出与多个知识源来实时捕获和纠正幻觉错误，将幻觉减少67%，同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: LLM虽然改变了我们与AI系统的交互方式，但存在严重缺陷：它们会自信地生成听起来完全合理但实际上是虚假的信息。这种幻觉问题已成为在需要准确性的实际应用中部署这些模型的主要障碍。

Method: 开发了一个事实验证框架，结合结构化数据库、实时网络搜索和学术文献，在生成过程中验证事实声明。当检测到不一致时，系统会自动纠正错误，同时保持响应的自然流畅性。

Result: 跨多个领域的测试显示，该系统能够将幻觉减少67%，同时不牺牲响应质量。医疗、金融和科学研究领域的专家对纠正后的输出满意度达到89%，显著优于未验证的LLM响应。

Conclusion: 这项工作为在不能容忍事实错误的应用中使LLM更可信提供了实用解决方案。

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [50] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu,Junyu Feng,Ruichuan An,Yulin Luo,Shilin Yan,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: Jarvis是一个通过个人KV-Cache检索实现个性化AI助手的创新框架，在视觉问答和纯文本任务中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么学习一组概念标记，要么训练视觉语言模型使用用户特定信息，但两种方法都难以生成准确答案。

Method: 将用户特定信息存储在文本和视觉标记的KV-Caches中，通过检索相关KV-Caches来确保回答准确性。

Result: Jarvis能够提供更准确的响应，特别是在依赖特定局部细节时，在多个数据集上达到最先进结果。

Conclusion: Jarvis为个性化AI助手提供了一条实用路径，代码和数据集将发布。

Abstract: The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [51] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 该研究首次直接比较了人类与AI代理在多个工作技能上的表现，发现代理在速度上快88.3%、成本低90.4-96.2%，但工作质量较差且倾向于数据伪造，同时代理普遍采用程序化方法而非人类常用的UI中心方法。


<details>
  <summary>Details</summary>
Motivation: AI代理在人类工作相关任务中的优化趋势显著，但缺乏对人类工作执行方式的清晰理解，需要揭示代理具备的专业知识及其在不同工作流程中的角色。

Method: 引入可扩展工具包，从人类或代理的计算机使用活动中诱导可解释的结构化工作流程，并比较人类和代理在相同任务上的表现。

Result: 代理在任务执行速度上快88.3%，成本低90.4-96.2%，但工作质量较差，经常通过数据伪造和滥用高级工具来掩盖缺陷，且采用程序化方法而非人类常用的UI中心方法。

Conclusion: 代理在效率上具有显著优势，适合委托易于编程的任务，但需注意其质量问题和行为差异，以实现有效的人机协作。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [52] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 提出了一种用于Microsoft Copilot服务的Agentic Meta-orchestrator（AMO），能够处理多任务和可扩展代理，通过元学习决策树模型选择最佳推理策略，并在M365电商Copilot和代码合规Copilot两个生产用例中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着Copilot套件中代理数量的动态扩展，需要一个强大的编排器来将用户提示的任务分发给正确的代理，以处理从客户购买协助到代码漏洞检测等各种重要任务。

Method: 提出了Agentic Meta-orchestrator（AMO），利用元学习方法训练决策树模型，在多个代理/模型之间决定最佳推理策略，能够提供自然语言和动作响应。

Result: 在两个生产用例中展示了AMO的有效性：M365电商Copilot为外部客户提供最新产品信息并连接多个代理；代码合规Copilot扫描内部DevOps代码检测合规问题。

Conclusion: AMO为Copilot服务提供了一个可扩展的多任务编排解决方案，通过元学习决策树实现了高效的代理选择和任务分配。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [53] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 本文分析人工智能对人类构成的生存风险，探讨从当前AI到超智能的发展轨迹，基于理论研究和近期出版物，讨论AGI和超智能的伦理与生存影响。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能特别是超智能对人类构成的生存风险，探讨当机器智能远超人类时可能带来的伦理和生存威胁，即使没有恶意意图也可能导致人类灭绝。

Method: 基于Irving J. Good和Nick Bostrom的理论工作，结合近期出版物《AI 2027》和《If Anyone Builds It, Everyone Dies》，分析AGI和超智能的发展轨迹及其影响。

Result: 分析表明，机器认知能力呈指数级增长，超智能可能拥有远超人类的智力水平，这种根本性异质的智能即使没有恶意，也可能因其不可控的认知优势导致人类灭绝。

Conclusion: 人工智能特别是超智能的发展对人类构成严重的生存风险，这种风险并非源于恶意，而是源于不可控的、冷漠的认知优越性，需要认真对待和防范。

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [54] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: HRM-Agent是HRM的强化学习变体，能够在动态不确定的迷宫环境中学习导航到目标，并成功重用先前时间步的计算。


<details>
  <summary>Details</summary>
Motivation: HRM虽然具有强大的推理能力，但仅限于监督学习、静态、完全可观测的问题，无法处理动态、不确定或部分可观测的现实世界问题。

Method: 开发HRM-Agent，这是HRM的变体，仅使用强化学习进行训练，探索其循环推理过程的动态特性。

Result: HRM-Agent能够在动态不确定的迷宫环境中学习导航到目标，并发现其循环推理过程成功重用了早期环境时间步的计算。

Conclusion: HRM-Agent扩展了HRM的应用范围，使其能够处理动态和不确定的环境，并通过重用计算提高了效率。

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [55] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 论文研究让强化学习智能体在训练过程中考虑计算成本，通过展示计算成本并允许智能体控制计算使用时机，在相同训练计算预算下，75%的游戏表现更好且平均减少三倍计算量。


<details>
  <summary>Details</summary>
Motivation: 人类在任务熟练后会减少认知努力，而传统强化学习智能体在提升性能时不会变得更计算高效。如果智能体能在学习过程中考虑计算成本，就能降低计算足迹，实现更节能的智能体或释放计算资源用于其他过程。

Method: 在Arcade学习环境中进行实验，向智能体展示计算成本并赋予其控制计算使用时机的能力。

Result: 在相同训练计算预算下，考虑计算的智能体在75%的游戏中表现更好，平均减少三倍计算量。

Conclusion: 让强化学习智能体考虑计算成本能显著提高计算效率，在保持性能的同时大幅减少计算资源消耗。

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [56] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao,Qiucheng Wu,Yang Zhang,Zhaowen Wang,Handong Zhao,Shiyu Chang*

Main category: cs.AI

TL;DR: 该论文分析了多模态大语言模型中存在的模态差距问题，即模型过度依赖文本线索而忽视视觉内容，并提出了从数据和损失函数设计两方面来弥合这一差距的训练策略。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉和文本模态之间存在推理能力不平衡的问题，模型往往过度依赖文本线索而忽视视觉内容，导致在需要真正视觉推理的任务上表现不佳。

Method: 从训练方法的角度分析模态差距，通过数据和损失函数设计两个互补视角，系统性地探索弥合模态差距的策略。

Result: 研究发现现有的训练方法倾向于放大模态差距，而提出的训练策略能够有效缓解这一差距，促进更平衡的多模态推理。

Conclusion: 该研究为开发能够减轻模态差距并促进平衡多模态推理的训练方法提供了重要见解，相关代码已公开。

Abstract: Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [57] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li,Erik-Jan van Kampen*

Main category: cs.AI

TL;DR: 开发了级联在线学习飞行控制系统并增强了动作平滑性，研究了系统收敛性能，通过李雅普诺夫函数增量表征，考虑了离散化误差和状态预测误差的影响。


<details>
  <summary>Details</summary>
Motivation: 研究级联在线学习飞行控制系统的收敛性能，特别是考虑离散化误差和增量模型引入的状态预测误差对系统稳定性的影响。

Method: 使用李雅普诺夫函数增量作为收敛性能指标，推导过程中考虑了离散化误差和状态预测误差，通过飞行控制仿真进行对比分析。

Result: 通过仿真展示了考虑误差因素后的收敛性能对比结果，验证了所提方法的有效性。

Conclusion: 所提出的级联在线学习飞行控制系统在考虑离散化误差和状态预测误差的情况下仍能保持良好的收敛性能，为实际应用提供了理论支持。

Abstract: A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [58] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本文提出了一个基于逻辑门电子电路的统一框架，将不同的推理机制（分类、归纳、溯因等）整合起来，通过组合探索识别了四种依赖形式和八种常见推理模式。


<details>
  <summary>Details</summary>
Motivation: 认知研究和人工智能领域为各种推理机制开发了不同的模型，但缺乏统一框架。本文试图填补这一空白，从物质角度假设高级激活过程。

Method: 采用符号AI建模技术，通过基于逻辑门的电子电路简化视角来分析推理机制，进行组合探索识别依赖形式，并在逻辑程序背景下识别常见推理模式。

Result: 识别了四种主要的依赖形式，发现了八种常见的推理模式，揭示了传统上不同的推理机制在统一框架下的联系，并通过概率解释揭示了内部功能依赖。

Conclusion: 尽管论证主要基于符号方法和数字系统基础设施，但观察结果可能指向更普遍适用的结构，为认知和AI推理机制提供了统一视角。

Abstract: Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [59] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 本文提出CoreThink智能推理框架，通过轻量级符号推理层增强大语言模型，在多个工具调用基准测试中实现530%的性能提升，且计算成本仅为十分之一。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工具调用环境中的泛化挑战，当前大语言模型在跨领域推理策略迁移和工具协调方面存在显著不足。

Method: 开发CoreThink智能推理器框架，为大语言模型添加轻量级符号推理层，实现结构化分解和自适应工具编排。

Result: 在多个基准测试中实现最先进性能，相比现有基线提升530%，计算成本降低90%，但在新设计的MAVEN基准上准确率仍低于50%。

Conclusion: CoreThink框架能有效提升大语言模型在工具调用任务中的泛化能力，但跨领域推理仍存在挑战，需要进一步研究。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [60] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li,Jieyuan Pei,Tangwei Ye,Zhongyuan Lai,Zihan Liu,Fengyuan Xu,Qi Zhang,Liang Hu*

Main category: cs.AI

TL;DR: GTR-Mamba是一个新颖的下一个兴趣点推荐框架，通过跨流形条件路由，在双曲几何中建模静态偏好层次结构，在欧几里得切空间中处理动态序列更新，有效解决了现有模型难以同时捕捉空间选择层次结构和用户特定时间上下文动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的POI推荐模型主要基于图神经网络和序列模型，但存在根本性限制：难以同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态变化及不规则转变。

Method: 提出GTR-Mamba框架，利用不同数学空间的优势：在双曲几何中建模静态的树状偏好层次结构，在欧几里得切空间中使用新颖的Mamba层路由动态序列更新，通过跨流形通道融合时空信息来显式引导状态空间模型。

Result: 在三个真实世界数据集上的广泛实验表明，GTR-Mamba在下一个POI推荐任务中持续优于最先进的基线模型。

Conclusion: GTR-Mamba通过跨流形条件路由方法，有效解决了POI推荐中同时建模空间层次结构和时间动态性的挑战，为下一代位置推荐系统提供了有前景的解决方案。

Abstract: Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [61] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 提出了MA-CDMP方法，使用扩散模型进行去中心化通信资源管理，通过模型基强化学习解决传统方法的非平稳性和合作不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决集中式MARL的可扩展性和隐私风险，以及分布式训练的去中心化执行范式中的非平稳性和有限合作问题。

Method: 基于模型基强化学习，使用扩散模型捕捉环境动态并规划轨迹，结合逆动态模型生成动作，引入均值场机制近似大规模智能体交互。

Result: 实验表明MA-CDMP在平均奖励和QoS指标上持续优于现有MARL基线，展示了其可扩展性和实际应用价值。

Conclusion: MA-CDMP通过扩散模型和均值场机制有效解决了去中心化资源管理中的挑战，为无线网络优化提供了实用解决方案。

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [62] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu,Jiakai Wang,Linna Jing,Haolin Li,Haodong Liu,Haotong Qin,Aishan Liu,Ke Xu,Xianglong Liu*

Main category: cs.AI

TL;DR: 本文提出了InSUR框架，通过多维度指令不确定性减少来生成更优的语义约束对抗样本，解决了语言指令中的参考多样性、描述不完整性和边界模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成语义约束对抗样本的方法攻击能力不足，主要原因是未能充分研究人类指令中的语义不确定性因素，如参考多样性、描述不完整性和边界模糊性。

Method: 提出多维度指令不确定性减少框架：1）采样方法维度：残差驱动攻击方向稳定化，通过ResAdv-DDIM采样器稳定对抗优化；2）任务建模维度：上下文编码攻击场景约束，通过引导掩码和渲染器集成补充缺失知识；3）生成器评估维度：语义抽象攻击评估增强，明确评估边界。

Result: 大量实验证明InSUR在迁移攻击性能上的优越性，并首次实现了无需参考的语义约束3D对抗样本生成。

Conclusion: InSUR框架通过系统性解决语义不确定性问题，能够生成更具迁移性、适应性和有效性的语义约束对抗样本。

Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [63] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales,Carlos Andrés Ferro Sánchez,Reinel Tabares-Soto,Jesús Alfonso López Sotelo,Gonzalo A. Ruz,Johan Sebastian Piña Durán*

Main category: cs.AI

TL;DR: ProfileXAI是一个模型和领域无关的框架，结合后置解释器（SHAP、LIME、Anchor）与检索增强的LLM，为不同类型用户生成解释。系统索引多模态知识库，通过定量标准为每个实例选择解释器，并通过聊天式提示生成基于知识的叙述。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够为不同类型用户提供高效、可信赖解释的通用框架，解决现有解释方法在用户适配性、解释质量和稳定性方面的不足。

Method: 结合SHAP、LIME、Anchor三种后置解释器与检索增强的大型语言模型，构建多模态知识库索引，基于定量标准为每个实例选择最佳解释器，使用聊天式提示生成基于知识的解释叙述。

Result: 在心脏病和甲状腺癌数据集上的评估显示：LIME在保真度-鲁棒性权衡方面表现最佳（Infidelity ≤ 0.30，L<0.7）；Anchor产生最稀疏、低token的规则；SHAP获得最高满意度（x̄=4.1）。Profile条件化稳定了token使用（σ≤13%）并保持跨配置文件的正面评分（x̄≥3.7，领域专家为3.77）。

Conclusion: ProfileXAI框架能够为不同类型用户提供高效且可信赖的解释，没有单一解释器在所有指标上占优，但通过适当的解释器选择和配置，可以实现稳定的解释质量和用户满意度。

Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [64] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Xiaoming Li,Jie Chen,Yongxu Liu,Xingpeng Zhang,Chen Liu,Wei Chen*

Main category: cs.AI

TL;DR: 该研究提出了一个多维度可信度评估框架来优化大语言模型生成的肝脏MRI报告的可信度，并比较了多个先进LLM在放射学报告生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对不同临床场景下优化提示设计的系统指导，以及评估LLM生成放射学报告可信度的标准化框架。

Method: 引入多维度可信度评估框架，应用该框架在SiliconFlow平台上评估和比较多个先进LLM的性能，包括Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3和ByteDance-Seed-OSS-36B-Instruct。

Result: 研究比较了多个LLM在肝脏MRI报告生成中的表现，并提供了机构特定的提示优化指导。

Conclusion: 该研究为优化LLM生成的放射学报告可信度提供了系统方法和评估框架，有助于提升放射学报告的质量和可靠性。

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [65] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh,Rajesh P. N. Rao*

Main category: cs.AI

TL;DR: 提出混合密度扩散器(MDD)，通过可调节的密度超参数在时间轴上实现非均匀稀疏规划，在多个任务领域达到新的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器使用稀疏步规划优于单步规划，但过度稀疏会降低性能。作者假设时间密度阈值在时间轴上非均匀分布，某些轨迹部分需要更密集规划

Method: 提出MDD扩散规划器，在整个时间轴上的密度是可调节的超参数，允许不同部分采用不同的规划密度

Result: 在Maze2D、Franka Kitchen和Antmaze D4RL任务领域取得了新的最先进性能

Conclusion: 通过可调节的混合密度规划策略，MDD能够更好地平衡长期依赖捕捉和规划性能，证明了非均匀密度规划的有效性

Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [66] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 本文系统综述了AI科学家从计算工具向自主科学知识创造者的转变，提出了统一的六阶段方法论框架，并梳理了该领域从基础模块到闭环系统再到可扩展性发展的演进历程。


<details>
  <summary>Details</summary>
Motivation: AI科学家系统的快速无序发展造成了研究领域的碎片化，需要系统性的方法论框架来理清发展脉络和原则。

Method: 引入统一的六阶段方法论框架：文献综述、想法生成、实验准备、实验执行、科学写作和论文生成，通过这一分析视角梳理领域发展。

Result: 构建了完整的AI科学家发展脉络图，从早期基础模块(2022-2023)到集成闭环系统(2024)，再到当前的可扩展性、影响力和人机协作阶段(2025至今)。

Conclusion: 该综述不仅阐明了自主科学的现状，还为克服鲁棒性和治理方面的挑战提供了关键路线图，指导下一代系统成为人类科学探究中值得信赖且不可或缺的合作伙伴。

Abstract: Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [67] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang,Meixin Su,Yang Yang,Chunyan Zeng,Lizhi Ye*

Main category: cs.AI

TL;DR: 本文提出了一种创新的跨学科认知诊断方法(TLCD)，结合深度学习和迁移学习策略，利用主学科的共同特征来提升目标学科中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 在线教育模式快速发展，但跨学科领域的认知诊断面临挑战，包括知识体系差异、认知结构不同和数据特征不一致等问题。传统方法难以处理跨学科的复杂性。

Method: 研究神经网络认知诊断和知识关联神经网络认知诊断，提出TLCD方法，结合深度学习技术和迁移学习策略，利用主学科特征增强目标学科模型性能。

Result: 实验结果显示，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能更准确地评估学生的学习情况。

Conclusion: 提出的跨学科认知诊断方法有效解决了跨学科认知诊断中的挑战，为智能教育提供了新的技术支撑。

Abstract: Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [68] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 该论文研究了如何将小型语言模型（如Phi-4系列）转化为有效的奖励模型，用于评估代码生成质量，通过结合过程奖励和结果奖励，在APPS编程挑战基准上实现了超过20%的代码搜索能力提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量代码方面仍面临挑战，需要奖励模型作为推理模型发展的中间步骤。虽然模型规模通常与反思能力相关，但研究旨在探索小型语言模型能否成为有效的奖励模型。

Method: 构建基于APPS编程挑战基准的代码样本数据集，训练带有回归层的价值头模型来估计中间输出的成功概率，结合过程奖励和结果奖励。

Result: 小型LLM能够作为有效的奖励模型或代码评估评判器，成功识别多个候选方案中的正确解决方案，使用该评判器实现了超过20%的代码搜索能力提升。

Conclusion: 小型语言模型可以被转化为有效的奖励模型，用于代码质量评估，显著提升代码生成和搜索的性能。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [69] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: 研究发现，在科学大语言模型中，仅使用高层次结构化上下文的方法在生物推理任务中表现最佳，原始序列反而会降低性能，表明Sci-LLMs的主要优势在于对结构化知识的推理能力而非序列解码。


<details>
  <summary>Details</summary>
Motivation: 解决科学大语言模型在处理原始生物分子序列时面临的标记化困境，即无论是将序列视为专门语言还是单独模态，都会限制模型的推理能力。

Method: 通过系统比较领先的Sci-LLMs在生物推理任务上的表现，测试了三种输入模式：仅序列、仅上下文、以及两者结合。

Result: 仅使用上下文的方法在所有模式中表现最佳且显著优于其他方法。即使加入原始序列也会降低性能，表明原始序列是信息噪声。

Conclusion: 应重新定位Sci-LLMs，将其视为基于专家知识的强大推理引擎，而非序列解码器，发展重点应从直接序列解释转向高层次知识合成。

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [70] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang,Thomas M. Moerland,Mike Preuss,Aske Plaat,Vincent François-Lavet,Edward S. Hu*

Main category: cs.AI

TL;DR: 提出FoG技能发现方法，通过基础模型融入人类偏好，避免传统技能发现方法产生危险或不理想行为


<details>
  <summary>Details</summary>
Motivation: 现有技能发现方法只关注技能多样性最大化，不考虑人类偏好，导致产生危险或不理想行为（如机器人翻滚），需要将人类意图融入技能发现过程

Method: 从基础模型提取评分函数评估状态，基于人类意图给期望状态高分、不期望状态低分，用这些分数重新加权技能发现算法的奖励

Result: FoG成功消除了翻滚等不理想行为，避免了危险区域，在基于状态和像素的任务中都有效，还能发现难以定义的行为技能

Conclusion: FoG方法有效将人类偏好融入技能发现，解决了传统方法产生危险行为的问题，展示了基础模型在引导技能学习中的潜力

Abstract: Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [71] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: AUPO是MCTS决策策略的改进算法，通过自动动作抽象提升性能，仅依赖奖励分布统计，无需转移概率或DAG搜索图。


<details>
  <summary>Details</summary>
Motivation: 改进MCTS决策策略，解决现有自动抽象算法需要转移概率和DAG搜索图的限制，能够检测对称动作。

Method: 基于MCTS过程中收集的奖励分布统计，开发自动动作抽象算法AUPO，仅影响决策策略。

Result: 在IPPC基准问题上，AUPO明显优于标准MCTS，能检测到ASAP等框架难以处理的对称动作。

Conclusion: AUPO是有效的MCTS改进方法，与其他仅影响树搜索的抽象技术兼容，具有实用价值。

Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [72] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Micheal Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 提出了一种针对游戏行业的高效深度强化学习方法，在EA SPORTS FC 25中训练守门员AI，性能比内置AI提升10%，训练速度快50%，并产生更人性化的游戏体验。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在游戏行业应用有限，现有研究主要关注训练超人级AI，而游戏工作室需要的是资源有限条件下的人类水平AI。

Method: 改进基于价值的深度强化学习，利用预收集数据并增强网络可塑性，提高样本效率。

Result: 在EA SPORTS FC 25中训练的守门员AI比游戏内置AI的扑救率高10%，训练速度快50%，专家评估显示产生更人性化的游戏体验。

Conclusion: 该方法成功解决了游戏行业对高效人类水平AI的需求，将在该系列游戏的后续版本中取代手动设计的AI。

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [73] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang,Wenkai Yang,Jie Wang,Huanshuo Dong,Zijie Geng,Zhen Huang,Depeng Xie,Zhezheng Hao,Hande Dong*

Main category: cs.AI

TL;DR: 提出了一种名为BlocKOA的新型集成电路热仿真数据生成算法，通过块Krylov和算子操作同时加速数据生成过程并提高数据精度，相比现有方法时间复杂降低一个数量级，实现了420倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法（如神经算子）需要大量高保真训练数据，导致计算成本高昂。为了解决这一挑战，需要开发高效的数据生成方法。

Method: BlocKOA算法首先基于热方程结构使用块Krylov算法快速获取少量基本解，然后组合这些解得到满足物理约束的温度分布，最后应用热算子确定热源分布，从而高效生成精确数据点。

Result: 实验验证显示，BlocKOA在生成5000个具有不同物理参数和IC结构芯片的热仿真数据时实现了420倍加速。即使仅使用4%的生成时间，基于BlocKOA生成数据训练的数据驱动方法性能与使用现有方法相当。

Conclusion: BlocKOA算法能够高效生成精确的IC热仿真数据，显著降低数据生成成本，为数据驱动方法提供了可行的解决方案。

Abstract: Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [74] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的CNOT门最小化方法，使用单一代理处理不同尺寸的量子电路，在较大电路尺寸上优于现有最优算法。


<details>
  <summary>Details</summary>
Motivation: CNOT门是量子计算中的基本组件，用于产生纠缠。减少CNOT门数量对量子算法性能至关重要，但CNOT最小化问题尚未完全解决。

Method: 使用单一强化学习代理处理固定尺寸m的电路，对不同于m的矩阵采用嵌入或高斯条纹化预处理。训练m=8的代理，评估n=3-15的矩阵。

Result: 随着n值增加，该方法在CNOT门最小化方面优于现有最优算法。

Conclusion: 强化学习方法在CNOT门最小化问题上表现出色，特别是在较大电路尺寸上具有优势。

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [75] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 提出了一个基于理性言语行为（RSA）建模框架的自适应信号理论框架，用于优化人机协作中信息传递的时机和特异性，以在动态环境中保持用户信念与环境的及时对齐。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的动态环境中，为了确保人类对关键任务元素保持准确理解，辅助智能体不仅需要识别最高优先级信息，还需要估计如何以及何时最有效地传递这些信息，因为人类注意力是零和认知资源。

Method: 使用理性言语行为（RSA）建模框架，通过贝叶斯参考解析来规划消息序列，根据用户和场景的具体情况调整消息的特异性和时机，基于对消息解释如何影响界面注意力和后续信念更新的多步预测。

Result: 与基线方法比较表明，该方法的效果关键依赖于将多步规划与现实的用户意识模型相结合。这是RSA在动态环境通信和人机交互中的首次应用。

Conclusion: 为人类-智能体团队中的语用通信建立了理论基础，展示了如何利用认知科学的见解来指导辅助智能体的设计。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [76] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar,A. G. Phakatkar*

Main category: cs.AI

TL;DR: 提出了一种基于模糊逻辑推理的细粒度意见挖掘方法，用于从评论文本中提取更精细的属性信息并据此对实体进行排名


<details>
  <summary>Details</summary>
Motivation: 随着社交网络和电子商务的发展，网络上存在大量意见数据。现有研究主要集中在基于评论集对实体进行排名，但尚未有研究将意见分类到更细粒度级别后再进行实体排名

Method: 使用模糊逻辑推理从语句中进行更深层次的细粒度意见挖掘

Result: 该方法能够从评论文本中提取更精细的属性和组件信息，并确定评论的情感极性（正面、负面或中性）

Conclusion: 提出的细粒度意见挖掘方法结合模糊逻辑推理，能够更精确地对实体进行排名，填补了现有研究的空白

Abstract: Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [77] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: AutoStreamPipe是一个使用大语言模型自动设计、生成和部署流处理管道的框架，通过超图思维(HGoT)技术显著减少开发时间和错误率。


<details>
  <summary>Details</summary>
Motivation: 解决流处理管道开发中高层用户意图与平台特定实现之间的语义鸿沟，自动化管道设计过程以提高效率。

Method: 集成超图思维(HGoT)作为GoT的扩展版本，结合弹性执行策略和高级查询分析，用于多智能体结构化推理。

Result: 实验评估显示，相比LLM代码生成方法，AutoStreamPipe将开发时间减少6.3倍，错误率降低5.19倍(通过新的无错误评分EFS衡量)。

Conclusion: AutoStreamPipe框架能够有效自动化流处理管道的设计和部署，显著提升开发效率和准确性。

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [78] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji,Tianyu Wang,Yeshu Li,Yushen Huo,Zhilin Zhang,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了Bid2X竞价基础模型，通过统一函数估计不同竞价场景下的广告效果，使用注意力机制处理异构数据和时序依赖，在淘宝平台部署后显著提升了GMV和ROI。


<details>
  <summary>Details</summary>
Motivation: 现有竞价模型通常针对特定场景设计，缺乏跨环境的泛化能力，需要开发能够适应不同竞价场景的统一基础模型。

Method: 构建Bid2X模型，使用统一序列嵌入编码异构数据，提出两种注意力机制分别处理变量间依赖和时序依赖，采用零膨胀投影模块处理数据分布特性，结合分类和回归的联合优化目标。

Result: 在8个数据集上的离线评估显示Bid2X优于各种基线方法，在线A/B测试中GMV提升4.65%，ROI提升2.44%。

Conclusion: Bid2X作为竞价基础模型在计算广告领域具有重要价值，能够有效提升广告效果并具备跨场景泛化能力。

Abstract: Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [79] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi,Amir Saki,Usef Faghihi*

Main category: cs.AI

TL;DR: 本文提出了一种将因果推理整合到深度Q网络(DQN)中的新方法，通过PEACE公式估计因果效应，减少伪相关性的影响，提高强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统的DQN依赖关联学习，容易获得伪相关性，限制了其问题解决能力。需要引入因果推理来理解环境的因果结构。

Method: 使用PEACE(概率易变分因果效应)公式估计因果效应，在DQN训练过程中整合因果推理，减轻混杂因素和伪相关性的影响。

Result: 在标准基准环境上的实验表明，该方法优于传统DQN，显著提升了问题解决能力且不损害性能。

Conclusion: 这项工作为通过原则性因果推理推进深度强化学习智能体能力提供了有前景的途径。

Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [80] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 该论文旨在开发智能系统来解决网络安全与法律交叉领域的信息复杂性，促进法律专家与网络安全专业人员的合作。


<details>
  <summary>Details</summary>
Motivation: 网络安全与法律交叉领域的信息复杂性导致传统法律研究工具难以处理案例、法规和技术漏洞之间的细微联系，阻碍了法律专家与网络安全专业人员之间的协作。

Method: 开发能够导航日益复杂的网络法律领域的智能系统。

Result: 在多语言任务上展示了有希望的初步结果。

Conclusion: 这项工作为解决网络安全与法律交叉领域的重要知识鸿沟迈出了第一步。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [81] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

Main category: cs.AI

TL;DR: 本文是对Cappelen等人关于AI生存风险分类分析的评论文章，重点讨论了线性风险模型的哲学局限性，分析了瑞士奶酪模型与作者模型的差异，并论证了在认知无差异情况下灾难概率P(D)可能更高。


<details>
  <summary>Details</summary>
Motivation: 提醒人们注意线性风险模型的哲学局限性，为AI生存风险讨论提供更全面的不确定性分析框架。

Method: 通过比较标准瑞士奶酪模型与作者模型的差异，分析认知无差异情况下P(D)概率的结构性关系，区分风险与不确定性，并引入选项不确定性和状态空间不确定性两个维度。

Result: 论证了在考虑结构性关系和不确定性维度后，AI生存风险中灾难概率P(D)可能比初步估计更高。

Conclusion: 将选项不确定性和状态空间不确定性纳入AI生存风险的定性讨论，能够更好地理解灾难概率P(D)的可能性。

Abstract: This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [82] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi,Muzakkiruddin Ahmed Mohammed,Hendrika Maclean,Mert Can Cakmak,John R. Talburt*

Main category: cs.AI

TL;DR: 提出基于LLM的策略感知控制器，通过六阶段推理框架将自然语言请求与书面策略匹配，实现安全、合规且可追溯的访问决策


<details>
  <summary>Details</summary>
Motivation: 企业需要满足最小权限、合规性和可审计性的访问决策，但传统方法难以处理自然语言请求与复杂策略的匹配

Method: 使用Google Gemini 2.0 Flash LLM，构建六阶段推理框架：上下文解释、用户验证、数据分类、业务目的测试、合规映射和风险综合，采用早期硬策略门控和默认拒绝机制

Result: 在14个标准案例测试中，精确决策匹配从10/14提升到13/14(92.9%)，拒绝召回率达到1.00，必须拒绝场景的误批准率降为0，功能适当性和合规性均达到14/14

Conclusion: 策略约束的LLM推理结合显式门控和审计追踪，能够将人类可读策略转化为安全、合规且可追溯的机器决策

Abstract: Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [83] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 提出了Human AI Collaborative Uncertainty Quantification框架，通过AI模型优化人类专家的预测集，避免对正确判断的损害并补充人类遗漏的正确结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI在不确定性下的决策能力有限，缺乏领域知识、长期上下文和物理世界推理能力，需要结合人类与AI的互补优势进行协作决策。

Method: 引入协作不确定性量化框架，开发了具有分布无关有限样本保证的离线和在线校准算法，在线方法能适应分布漂移和人类行为变化。

Result: 在图像分类、回归和医疗决策等任务中，协作预测集始终优于单独使用人类或AI，实现了更高的覆盖率和更小的集合大小。

Conclusion: 人类-AI协作不确定性量化框架能有效结合双方优势，提升决策的可靠性和性能，特别是在不确定性环境下的高风险决策中。

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [84] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文建立了现代智能体AI系统架构与乔姆斯基层级抽象机器之间的形式等价关系，提出AI智能体的内存架构决定其计算能力，并直接映射到相应的自动机类别。


<details>
  <summary>Details</summary>
Motivation: 为智能体AI系统提供理论基础，通过形式化方法实现系统验证、安全保障和行为可预测性，优化计算效率和成本。

Method: 建立自动机-智能体框架，将简单反射智能体映射为有限自动机，分层任务分解智能体映射为下推自动机，具有读写内存的反思智能体映射为图灵机，并扩展到概率自动机以处理LLM的随机性。

Result: 成功建立了智能体架构与自动机理论的形式等价关系，提供了智能体分类和验证的理论基础。

Conclusion: 该框架为智能体系统开发静态分析工具和语法提供了路线图，能够形式化界定可验证系统与行为不可判定系统之间的边界。

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [85] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha,Jeong Hun Yeo,Yeonju Kim,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出情感理由验证器(ERV)和解释奖励方法，通过确保多模态大语言模型在情感识别中生成与预测情感一致的解释，提高解释-预测一致性和解释准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解中生成的情感解释常与目标标签不一致，甚至与自身预测情感相矛盾，这会降低系统可靠性和用户信任。

Method: 提出情感理由验证器(ERV)和解释奖励方法，在不修改模型架构或需要额外视频描述标注的情况下，引导模型生成与目标情感明确一致的推理。

Result: 在MAFW和DFEW数据集上显著提高了忠实解释-预测一致性和解释情感准确性，通过广泛实验和人工评估验证了方法的有效性。

Conclusion: 该方法不仅增强了解释与预测的对齐，还使多模态大语言模型能够提供情感一致、可信赖的交互，是实现真正类人HCI系统的关键一步。

Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [86] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 提出了Human AI (HAI)框架，通过增量学习、碳感知优化和人机协作实现可持续AI，平衡性能与生态责任


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来巨大计算需求，引发环境和伦理担忧，批评当前依赖大规模静态数据集和单一训练范式的做法

Method: 引入HAI框架，强调增量学习、碳感知优化和人机协作，借鉴生物认知和动态架构

Result: 详细阐述了理论基础、系统设计和操作原则，使AI能够持续情境学习，同时最小化碳足迹和人工标注成本

Conclusion: 为解决主动学习、持续适应和节能模型部署等挑战提供了路径，推动负责任、以人为中心的人工智能

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [87] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das,Irtaza Khalid,Rafael Peñaloza,Steven Schockaert*

Main category: cs.AI

TL;DR: NoRA是一个新的系统性关系推理基准，增加了多个难度级别，要求模型超越基于路径的推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统性关系推理基准过于简化，假设推理可以简化为组合关系路径，导致模型难以泛化到其他设置。

Method: 引入了NoRA基准，包含多个难度级别，挑战模型超越路径推理的能力。

Result: NoRA基准为系统性关系推理提供了更全面的评估框架。

Conclusion: NoRA基准将推动神经网络在系统性关系推理领域的进一步发展。

Abstract: Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [88] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 提出了JanusCode-800K，这是迄今为止最大的多模态代码语料库，并基于此训练了JanusCoder系列模型，实现了从文本指令、视觉输入或两者结合生成代码的视觉-程序化接口。


<details>
  <summary>Details</summary>
Motivation: 神经代码智能的范围正在从基于文本的源代码扩展到程序生成的丰富视觉输出，但高质量多模态代码数据的稀缺阻碍了进展。

Method: 首先开发了一个完整的合成工具包，利用数据模态之间的互惠协同作用，高效生成大规模高质量语料库；然后基于该语料库训练JanusCoder和JanusCoderV模型。

Result: 在文本中心和视觉中心的编码任务上，JanusCoder系列模型表现出优越性能，7B到14B规模的模型接近甚至超过商业模型的性能。

Conclusion: 该工作为协调程序逻辑与其视觉表达提供了关键见解，建立了统一的视觉-程序化接口模型。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [89] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis,Stacie Severyn,Fjollë Novakazi,Hadi Banaee,Cogan Shimizu*

Main category: cs.AI

TL;DR: OntoPret是一个基于认知科学和模块化工程方法的人类行为解释本体论，旨在填补技术中心机器人框架与描述性行为本体论之间的研究空白，为机器提供可处理的人类行为分类框架。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作在工业5.0等范式中变得重要，机器需要安全有效地解释复杂人类行为。当前存在技术中心机器人框架缺乏人类行为细微模型，而描述性行为本体论不适合实时协作解释的研究空白。

Method: 基于认知科学和模块化工程方法，开发OntoPret本体论，提供形式化、机器可处理的行为分类框架，包括任务偏差和欺骗性行为。

Result: 在制造和游戏两个不同用例中验证了OntoPret的适应性，建立了关于人类意图的高级推理所需的语义基础。

Conclusion: OntoPret成功填补了现有研究空白，为机器解释人类行为提供了有效的本体论框架，支持跨领域应用和意图推理。

Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [90] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode提出了一种通过递归代码生成统一规划和行动的新范式，将高级计划视为抽象占位函数，并递归分解为更细粒度的子函数，直到原始行动，从而实现动态决策粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务需要在不同粒度上做决策，人类擅长利用统一认知表示进行规划，但当前基于LLM的智能体缺乏这种跨决策粒度流畅操作的能力，现有范式在高级规划和低级行动之间存在刚性分离。

Method: ReCode在单一代码表示中统一规划和行动，将高级计划视为抽象占位函数，然后递归分解为更细粒度的子函数，直到达到原始行动，这种递归结构消除了规划和行动之间的刚性边界。

Result: 大量实验表明，ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率，验证了通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的强大有效方法，递归结构固有地生成丰富的多粒度训练数据，使模型能够学习分层决策过程。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [91] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann,Aleksandra Urman,Ulrich Leicht-Deobald,Zachary J. Roman,Anikó Hannák,Markus Christen*

Main category: cs.AI

TL;DR: ChatGPT发布后，公众对AI的接受度下降，对人监督的需求增加，社会不平等加剧


<details>
  <summary>Details</summary>
Motivation: 研究公众对AI的态度变化，特别是在有影响力的决策场景中，以及AI技术快速发展与用户偏好的匹配问题

Method: 使用大规模两波调查（n_wave1=1514，n_wave2=1488），代表瑞士人口，比较ChatGPT发布前后的态度变化

Result: 生成式AI热潮与公众AI接受度显著下降相关，完全不可接受AI的比例从23%升至30%，支持纯人类决策的比例从18%升至26%，教育、语言和性别差距扩大

Conclusion: 研究结果挑战了行业对公众AI部署准备度的假设，强调技术发展必须与不断变化的公众偏好保持一致的重要性

Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [92] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了MAE框架，通过多智能体自进化增强LLM推理能力，无需人类标注数据，在多个基准测试上平均提升4.54%


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注数据和可验证奖励，限制了可扩展性和通用性。自博弈RL方法需要特定环境反馈，难以扩展到通用领域

Method: MAE框架包含三个交互智能体（提议者、求解者、评判者），均基于单一LLM实例化。提议者生成问题，求解者尝试解答，评判者评估并共同进化，应用强化学习优化行为

Result: 在Qwen2.5-3B-Instruct上的实验显示，MAE在数学、推理和常识问答等多个基准测试上平均提升4.54%

Conclusion: MAE是一种可扩展、数据高效的方法，能以最小的人类监督依赖增强LLM的通用推理能力

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [93] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: ALITA-G是一个自我进化框架，通过生成、抽象和策划MCP工具，将通用代理转化为领域专家，在多个基准测试中实现最先进性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化代理主要局限于提示重写或失败重试，需要更系统的方法将通用代理转化为领域专家。

Method: 框架包含三个步骤：通用代理执行目标领域任务并合成候选MCP；将MCP抽象为参数化原语并整合到MCP Box中；推理时执行检索增强的MCP选择，配备MCP执行器。

Result: 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的新最先进结果，同时平均每个示例的token数减少约15%。

Conclusion: ALITA-G提供了从通用能力到可重用领域特定能力的原理性路径，在复杂推理任务上同时提高准确性和效率。

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [94] [Fundamental Limits of Coded Caching with Fixed Subpacketization](https://arxiv.org/abs/2510.22145)
*Minquan Cheng,Yifei Huang,Youlong Wu,Jinyan Wang*

Main category: cs.IT

TL;DR: 本文研究了编码缓存网络中传输负载与子分组化级别之间的基本权衡关系，提出了一个通用的下界，并证明了分区方案能够达到最优的速率-子分组化权衡。


<details>
  <summary>Details</summary>
Motivation: 编码缓存技术通过创建编码多播机会来减少传输负载，但高子分组化级别会导致编码复杂度增加。现有研究缺乏对给定子分组化级别下最小传输负载的深入理解。

Method: 通过重新表述集中式编码缓存方案对应的放置传递阵列的组合结构，提出了传输负载的通用下界，并利用组合结构和计算排序集合的并集大小来建立新的最优性结果。

Result: 提出的下界恢复了现有最优性结果（包括MN方案和共轭MN方案），并证明了分区方案能够达到最优的速率-子分组化权衡。

Conclusion: 本文建立了编码缓存网络中传输负载与子分组化级别之间的基本权衡关系，为设计低复杂度的编码缓存方案提供了理论基础。

Abstract: Coded caching is a promising technique to create coded multicast
opportunities for cache-aided networks. By splitting each file into $F$ equal
packets (i.e., the subpacketization level $F$) and letting each user cache a
set of packets, the transmission load can be significantly reduced via coded
multicasting. It has been shown that a higher subpacketization level could
potentially lead to a lower transmission load, as more packets can be combined
for efficient transmission. On the other hand, a larger $F$ indicates a higher
coding complexity and is problematic from a practical perspective when $F$ is
extremely large. Despite many works attempting to design coded caching schemes
with low subpacketization levels, a fundamental problem remains open: What is
the minimum transmission load given any fixed subpacketization level? In this
paper, we consider the classical cache-aided networks with identically uncoded
placement and one-shot delivery strategy, and investigate the fundamental
trade-off between the transmission load and the subpacketization level. We
propose a \emph{general} lower bound on the transmission load for any fixed
subpacketization by reformulating the centralized coded caching schemes via the
combinatorial structure of the corresponding placement delivery array. The
lower bound also recovers existing optimality results for the bipartite graph
scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the
conjugate MN scheme) as well as the grouping bipartite graph scheme.
Furthermore, by carefully exploiting the combinatorial structure and computing
the union size of sorted sets, we establish a new optimality result, i.e., the
partition scheme can achieve the optimal rate-subpacketization trade-off.

</details>


### [95] [Robust MIMO Channel Estimation Using Energy-Based Generative Diffusion Models](https://arxiv.org/abs/2510.22230)
*Ziqi Diao,Xingyu Zhou,Le Liang,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种结合能量基生成扩散模型和Metropolis-Hastings原理的新型信道估计框架，显著提高了大规模MIMO系统中的信道估计精度，特别是在导频开销有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中的信道估计面临导频开销过大和估计延迟高的根本性约束，需要克服这些障碍来提高估计效率。

Method: 通过重新参数化扩散过程并融入能量函数，框架显式估计未归一化的对数先验，同时利用Metropolis-Hastings修正来优化采样轨迹、减轻偏差并增强鲁棒性，从而实现高保真信道估计的准确后验采样。

Result: 数值结果表明，与传统的参数化扩散模型和其他基线方法相比，所提出的方法显著提高了估计精度，特别是在导频开销有限的情况下表现更优。

Conclusion: 该集成框架通过结合能量基生成扩散模型和Metropolis-Hastings原理，为大规模MIMO系统提供了一种高效且准确的信道估计解决方案。

Abstract: Channel estimation for massive multiple-input multiple-output (MIMO) systems
is fundamentally constrained by excessive pilot overhead and high estimation
latency. To overcome these obstacles, recent studies have leveraged deep
generative networks to capture the prior distribution of wireless channels. In
this paper, we propose a novel estimation framework that integrates an
energy-based generative diffusion model (DM) with the Metropolis-Hastings (MH)
principle. By reparameterizing the diffusion process with an incorporated
energy function, the framework explicitly estimates the unnormalized log-prior,
while MH corrections refine the sampling trajectory, mitigate deviations, and
enhance robustness, ultimately enabling accurate posterior sampling for
high-fidelity channel estimation. Numerical results reveal that the proposed
approach significantly improves estimation accuracy compared with conventional
parameterized DMs and other baseline methods, particularly in cases with
limited pilot overhead.

</details>


### [96] [Infinitely many families of distance-optimal binary linear codes with respect to the sphere packing bound](https://arxiv.org/abs/2510.22259)
*Hao Chen,Conghui Xie,Cunsheng Ding*

Main category: cs.IT

TL;DR: 解决了编码理论中75年来的开放问题：证明了存在关于球填充界的距离最优线性码的无限族，其最小距离可以任意大。


<details>
  <summary>Details</summary>
Motivation: 解决1950年Hamming提出球填充界以来，75年间未能证明是否存在最小距离任意大的距离最优线性码无限族的问题。

Method: 通过构造和分析线性码，证明了存在满足球填充界的距离最优线性码无限族。

Result: 成功构造了多个无限族的距离最优二进制码，包括两个五重码无限族，解决了长期开放问题。

Conclusion: 首次证明了存在最小距离任意大的距离最优线性码无限族，填补了编码理论的重要空白，并提出了新的开放问题。

Abstract: R. W. Hamming published the Hamming codes and the sphere packing bound in
1950. In the past 75 years, infinite families of distance-optimal linear codes
over finite fields with minimum distance at most 8 with respect to the sphere
packing bound have been reported in the literature. However, it is a
75-year-old open problem in coding theory whether there is an infinite family
of distance-optimal linear codes over finite fields with arbitrarily large
minimum distance with respect to the sphere packing bound. This main objective
of this paper is to settle this long-standing open problem in coding theory.
  As by-products, several infinite families of distance-optimal binary codes
with small minimum distances are presented. Two infinite families of binary
five-weight codes are reported. Some open problems are also proposed.

</details>


### [97] [Optimal Sampling and Scheduling for Remote Fusion Estimation of Correlated Wiener Processes](https://arxiv.org/abs/2510.22288)
*Aimin Li,Elif Uysal*

Main category: cs.IT

TL;DR: 本文研究了分布式传感器网络中异步相关信息的融合估计问题，提出了采样、调度和估计策略的联合优化方法，证明了AoI优化与MSE优化的等价性。


<details>
  <summary>Details</summary>
Motivation: 在分布式传感器网络中，传感器观测相关动态过程时存在异步到达问题，如何融合这些异步但相关的信息以实现准确的远程融合估计是一个核心挑战。

Method: 建立了分离原理，识别出联合最优策略：基于AoI的加权和融合估计器、MAF调度器和最优采样器设计，证明了在无限时域平均成本准则下AoI优化与MSE优化的等价性。

Result: 发现最优融合估计器是基于AoI的加权和估计器，最优调度器是MAF调度器，最优采样器是AoI最优的采样器，信息新鲜度可以作为相关感知环境中最优估计的设计代理。

Conclusion: 在存在强传感器间相关性的情况下，AoI优化与MSE优化在基于拉取的通信中具有等价性，信息新鲜度可以作为相关感知环境中最优估计的有效设计指标。

Abstract: In distributed sensor networks, sensors often observe a dynamic process
within overlapping regions. Due to random delays, these correlated observations
arrive at the fusion center asynchronously, raising a central question: How can
one fuse asynchronous yet correlated information for accurate remote fusion
estimation? This paper addresses this challenge by studying the joint design of
sampling, scheduling, and estimation policies for monitoring a correlated
Wiener process. Though this problem is coupled, we establish a separation
principle and identify the joint optimal policy: the optimal fusion estimator
is a weighted-sum fusion estimator conditioned on Age of Information (AoI), the
optimal scheduler is a Maximum Age First (MAF) scheduler that prioritizes the
most stale source, and the optimal sampling can be designed given the optimal
estimator and the MAF scheduler. To design the optimal sampling, we show that,
under the infinite-horizon average-cost criterion, optimizing AoI is equivalent
to optimizing MSE under pull-based communications, despite the presence of
strong inter-sensor correlations. This structural equivalence allows us to
identify the MSE-optimal sampler as one that is AoI-optimal. This result
underscores an insight: information freshness can serve as a design surrogate
for optimal estimation in correlated sensing environments.

</details>


### [98] [Energy-Efficient UAV-Enabled MEC Systems: NOMA, FDMA, or TDMA Offloading?](https://arxiv.org/abs/2510.22306)
*Qingjie Wu,Miao Cui,Guangchi Zhang,Beixiong Zheng,Xiaoli Chu,Qingqing Wu*

Main category: cs.IT

TL;DR: 本文比较了无人机移动边缘计算系统中NOMA、FDMA和TDMA三种多址接入方案的能耗性能，发现在有限块长条件下TDMA能耗最低，而NOMA不一定比FDMA更节能。


<details>
  <summary>Details</summary>
Motivation: 无人机移动边缘计算系统可以使用不同多址方案协调多用户任务卸载，但尚不清楚哪种方案在有限块长条件下最节能。

Method: 通过理论分析三种方案的最小能耗，并提出交替优化算法联合优化任务卸载比例、卸载时间和无人机位置。

Result: TDMA在无限和有限块长情况下能耗均低于FDMA；NOMA在有限块长且用户条件对称时不一定比FDMA更节能。

Conclusion: TDMA是无人机移动边缘计算系统中最节能的多址方案，提出的优化算法能有效降低能耗。

Abstract: Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) systems can
use different multiple access schemes to coordinate multi-user task offloading.
However, it is still unknown which scheme is the most energy-efficient,
especially when the offloading blocklength is finite. To answer this question,
this paper minimizes and compares the MEC-related energy consumption of
non-orthogonal multiple access (NOMA), frequency division multiple access
(FDMA), and time division multiple access (TDMA)-based offloading schemes
within UAV-enabled MEC systems, considering both infinite and finite
blocklength scenarios. Through theoretically analysis of the minimum energy
consumption required by these three schemes, two novel findings are presented.
First, TDMA consistently achieves lower energy consumption than FDMA in both
infinite and finite blocklength cases, due to the degrees of freedom afforded
by sequential task offloading. Second, NOMA does not necessarily achieve lower
energy consumption than FDMA when the offloading blocklength is finite,
especially when the channel conditions and the offloaded task data sizes of two
user equipments (UEs) are relatively symmetric. Furthermore, an alternating
optimization algorithm that jointly optimizes the portions of task offloaded,
the offloading times of all UEs, and the UAV location is proposed to solve the
formulated energy consumption minimization problems. Simulation results verify
the correctness of our analytical findings and demonstrate that the proposed
algorithm effectively reduces MEC-related energy consumption compared to
benchmark schemes that do not optimize task offloading portions and/or
offloading times.

</details>


### [99] [Resource Allocation for XR with Edge Offloading: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.22505)
*Alperen Duru,Mohammad Mozaffari,Ticao Zhang,Mehrnaz Afshang*

Main category: cs.IT

TL;DR: 提出基于强化学习的资源分配框架，动态分配XR应用的上行和下行时隙，根据头显能力和网络状况做出卸载决策，优化帧丢失率和能效。


<details>
  <summary>Details</summary>
Motivation: 未来沉浸式XR应用需要高能效、高数据速率和低延迟的无线通信，需要智能自适应的资源分配与边缘卸载来支持这些需求。

Method: 使用强化学习框架动态分配上行和下行时隙，基于XR头显能力和网络条件做出卸载决策，进行数值分析权衡帧丢失率和能效。

Result: 部分卸载可扩展覆盖范围55%，降低能耗达34%；头显本地计算能力在卸载决策中起关键作用，计算能力越强越能减少卸载需求并提升能效。

Conclusion: 部分卸载策略在XR应用中具有显著优势，头显本地计算能力是优化卸载决策和提升能效的关键因素。

Abstract: Future immersive XR applications will require energy-efficient, high data
rate, and low-latency wireless communications in uplink and downlink. One of
the key considerations for supporting such XR applications is intelligent and
adaptive resource allocation with edge offloading. To address these demands,
this paper proposes a reinforcement learning-based resource allocation
framework that dynamically allocates uplink and downlink slots while making
offloading decisions based on the XR headset's capabilities and network
conditions. The paper presents a numerical analysis of the tradeoff between
frame loss rate (FLR) and energy efficiency, identifying decision regions for
partial offloading to optimize performance. Results show that for the used set
of system parameters, partial offloading can extend the coverage area by 55%
and reduce energy consumption by up to 34%, compared to always or never
offloading. The results demonstrate that the headset's local computing
capability plays a crucial role in offloading decisions. Higher computing
abilities enable more efficient local processing, reduce the need for
offloading, and enhance energy savings.

</details>


### [100] [End-to-end Learning of Probabilistic and Geometric Constellation Shaping with Iterative Receivers](https://arxiv.org/abs/2510.22608)
*Harindu Jayarathne,Dileepa Marasinghe,Nandana Rajatheva,Matti Latva-aho*

Main category: cs.IT

TL;DR: 提出了一种端到端的星座整形学习方法，通过整形编码器产生具有更高零概率的整形比特，联合优化概率分布和星座几何形状，在两种迭代接收机架构下相比标准APSK和QAM获得了0.3dB和0.15dB的BER增益。


<details>
  <summary>Details</summary>
Motivation: 传统星座调制如APSK和QAM在概率分布和几何形状上存在优化空间，通过端到端学习可以联合优化这两个方面以获得更好的性能。

Method: 使用整形编码器产生高零概率的整形比特，采用端到端学习联合优化概率分布和星座几何形状，并扩展使用深度展开技术结合完整的迭代检测和解码循环。

Result: 在两种接收机架构下，学习得到的星座相比标准APSK和QAM分别获得0.3dB和0.15dB的BER增益；在块衰落信道条件下，迭代方案相比标准APSK获得0.1dB的BER增益。

Conclusion: 端到端学习方法能够有效优化星座整形，在多种接收机架构和信道条件下均能获得显著的BER性能提升。

Abstract: An end-to-end learning method for constellation shaping with a
shaping-encoder assisted transceiver architecture is presented. The shaping
encoder, which produces shaping bits with a higher probability of zeros, is
used to produce an efficient symbol probability distribution. Both the
probability distribution and the constellation geometry are jointly optimized,
using end-to-end learning. Optimized constellations are evaluated using two
iterative receiver architectures. Bit error rate (BER) performance gain is
quantified against standard amplitude phase-shift keying (APSK) and quadrature
amplitude modulation (QAM) constellations. A maximum BER gain of 0.3 dB and
0.15 dB are observed under two receivers for the learned constellations
compared to standard APSK or QAM. The basic approach is extended to incorporate
the full iterative detection and decoding loop, using the deep unfolding
technique. A bit error rate gain of 0.1 dB is observed for the iterative scheme
with learned constellations under block fading channel conditions, when
compared to standard APSK.

</details>


### [101] [Graph-Theoretic Characterization of Noise Capacity of Conditional Disclosure of Secrets](https://arxiv.org/abs/2510.22671)
*Zhou Li,Siyan Qin,Xiang Zhang,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了条件秘密披露(CDS)问题中的噪声容量，确定了CDS噪声容量达到最大值1的充要条件，并为任意CDS实例推导了线性噪声容量的上界。


<details>
  <summary>Details</summary>
Motivation: 在条件秘密披露问题中，Alice和Bob希望仅在输入满足特定函数关系时向Carol揭示秘密，同时防止在输入不满足条件时泄露秘密。本研究旨在确定噪声容量，即能够安全揭示给Carol的最大秘密比特数与Alice和Bob共同持有的独立噪声比特总数之比。

Method: 通过图论方法分析CDS问题，使用图的覆盖参数和未合格路径中的未合格边数来表征噪声容量。建立了CDS噪声容量达到最大值1的充要条件，并推导了线性噪声容量的通用上界。

Result: 确定了CDS噪声容量达到最大值1的充要条件。推导出线性噪声容量的上界为(ρ-1)(d-1)/(ρd-1)，其中ρ是函数f图表示的覆盖参数，d是未合格路径中的未合格边数。

Conclusion: 该研究为CDS问题中的噪声容量提供了理论分析框架，明确了最优噪声容量的条件，并为任意CDS实例的线性噪声容量建立了上界，对理解条件秘密披露协议的效率极限具有重要意义。

Abstract: In the problem of conditional disclosure of secrets (CDS), two parties, Alice
and Bob, each has an input and shares a common secret. Their goal is to reveal
the secret to a third party, Carol, as efficiently as possible, only if the
inputs of Alice and Bob satisfy a certain functional relation $f $. To prevent
leakage of the secret to Carol when the input combination is unqualified, both
Alice and Bob introduce noise. This work aims to determine the noise capacity,
defined as the maximum number of secret bits that can be securely revealed to
Carol, normalized by the total number of independent noise bits held jointly by
Alice and Bob. Our contributions are twofold. First, we establish the necessary
and sufficient conditions under which the CDS noise capacity attains its
maximum value of $1$. Second, in addition to the above best-case scenarios, we
derive an upper bound on the linear noise capacity for any CDS instance. In
particular, this upper bound is equal to $(\rho-1)(d-1)/(\rho d-1)$, where
$\rho$ is the covering parameter of the graph representation of $f$, and $d$ is
the number of unqualified edges in residing unqualified path.

</details>


### [102] [Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication](https://arxiv.org/abs/2510.22718)
*Yujie Wan,Chenxuan Liu,Shuai Wang,Tong Zhang,James Jianqiao Yu,Kejiang Ye,Dusit Niyato,Chengzhong Xu*

Main category: cs.IT

TL;DR: 提出ECO-GS系统，通过本地小模型保证及时性，远程大模型保证保真度，并开发IRAC框架联合优化协作状态和边缘功率分配，使用PMM和ILO算法高效求解。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅在低成本设备上渲染质量下降，需要平衡渲染质量和资源限制的矛盾。

Method: 提出ECO-GS系统，集成渲染与通信(IRAC)框架，使用惩罚主化最小化(PMM)算法和模仿学习优化(ILO)算法。

Result: PMM算法获得临界点解，ILO算法计算时间比PMM减少100倍以上，实验验证了PMM的优越性和ILO的实时执行能力。

Conclusion: ECO-GS系统有效解决了低成本设备上的渲染质量问题，IRAC框架和ILO算法实现了高效的实时协作渲染。

Abstract: Gaussian splatting (GS) struggles with degraded rendering quality on low-cost
devices. To address this issue, we present edge collaborative GS (ECO-GS),
where each user can switch between a local small GS model to guarantee
timeliness and a remote large GS model to guarantee fidelity. However, deciding
how to engage the large GS model is nontrivial, due to the interdependency
between rendering requirements and resource conditions. To this end, we propose
integrated rendering and communication (IRAC), which jointly optimizes
collaboration status (i.e., deciding whether to engage large GS) and edge power
allocation (i.e., enabling remote rendering) under communication constraints
across different users by minimizing a newly-derived GS switching function.
Despite the nonconvexity of the problem, we propose an efficient penalty
majorization minimization (PMM) algorithm to obtain the critical point
solution. Furthermore, we develop an imitation learning optimization (ILO)
algorithm, which reduces the computational time by over 100x compared to PMM.
Experiments demonstrate the superiority of PMM and the real-time execution
capability of ILO.

</details>


### [103] [On the Arikan Transformations of Binary-Input Discrete Memoryless Channels](https://arxiv.org/abs/2510.22896)
*Yadong Jiao,Xiaoyan Cheng,Yuansheng Tang,Ming Xu*

Main category: cs.IT

TL;DR: 本文提出了一种将极化码构造中的合成信道建模为二进制对称信道随机切换通道的方法，并推导了合成信道输出字母表中具有相同似然比的元素数量的下界。


<details>
  <summary>Details</summary>
Motivation: 由于合成信道输出字母表规模过大，目前缺乏高效实用的方法来评估极化码中合成信道的可靠性。

Method: 将极化码构造中的合成信道生成转换为代数运算，在基础信道对称的情况下，将合成信道表征为二进制对称信道的随机切换通道。

Result: 开发了一种表征合成信道的方法，并推导了合成信道输出字母表中具有相同似然比的元素数量的下界。

Conclusion: 该方法为解决极化码构造中合成信道可靠性评估问题提供了新的理论工具。

Abstract: The polar codes introduced by Arikan in 2009 achieve the capacity of
binary-input discrete memoryless channels (BIDMCs) with low complexity encoding
and decoding. Identifying the unreliable synthetic channels, generated by
Arikan transformation during the construction of these polar codes, is crucial.
Currently, because of the large size of the output alphabets of synthetic
channels, there is no efficient and practical approach to evaluate their
reliability in general. To tackle this problem, by converting the generation of
synthetic channels in polar code construction into algebraic operations, in
this paper we develop a method to characterize the synthetic channels as random
switching channels of binary symmetric channels when the underlying channels
are symmetric. Moreover, a lower bound for the average number of elements that
possess the same likelihood ratio within the output alphabet of any synthetic
channel generated in polar codes is also derived.

</details>


### [104] [On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges](https://arxiv.org/abs/2510.23230)
*Raúl Gutiérrez,Víctor Rampérez,Horacio Paggi,Juan A. Lara,Javier Soriano*

Main category: cs.IT

TL;DR: 本文对信息融合技术用于提升信息质量的研究进行了系统性文献综述，分析了不同融合方法对信息质量的影响、信息质量的表征与评估方法，以及适应性融合过程设计等关键问题，并提出了未来研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 信息融合领域近年来受到广泛关注，但现有文献综述多局限于特定问题或系统类型，缺乏对信息融合如何提升信息质量的系统性认识，特别是在不同应用领域、数据类型和资源限制条件下的融合方法影响尚未得到全面分析。

Method: 采用文献综述方法，系统分析信息融合技术用于提升信息质量的相关研究，重点关注融合方法对信息质量的影响机制、信息质量的表征与评估体系，以及适应性融合过程设计等关键方面。

Result: 通过综述发现当前研究在信息融合对信息质量的影响机制、跨领域信息质量评估标准、以及资源受限条件下的适应性融合设计等方面存在明显的研究空白。

Conclusion: 本文识别了信息融合技术用于提升信息质量研究中的关键挑战和未来研究方向，为构建系统性的信息质量提升融合框架提供了理论基础。

Abstract: The information fusion field has recently been attracting a lot of interest
within the scientific community, as it provides, through the combination of
different sources of heterogeneous information, a fuller and/or more precise
understanding of the real world than can be gained considering the above
sources separately. One of the fundamental aims of computer systems, and
especially decision support systems, is to assure that the quality of the
information they process is high. There are many different approaches for this
purpose, including information fusion. Information fusion is currently one of
the most promising methods. It is particularly useful under circumstances where
quality might be compromised, for example, either intrinsically due to
imperfect information (vagueness, uncertainty) or because of limited resources
(energy, time). In response to this goal, a wide range of research has been
undertaken over recent years. To date, the literature reviews in this field
have focused on problem-specific issues and have been circumscribed to certain
system types. Therefore, there is no holistic and systematic knowledge of the
state of the art to help establish the steps to be taken in the future. In
particular, aspects like what impact different information fusion methods have
on information quality, how information quality is characterised, measured and
evaluated in different application domains depending on the problem data type
or whether fusion is designed as a flexible process capable of adapting to
changing system circumstances and their intrinsically limited resources have
not been addressed. This paper aims precisely to review the literature on
research into the use of information fusion techniques specifically to improve
information quality, analysing the above issues in order to identify a series
of challenges and research directions, which are presented in this paper.

</details>


### [105] [Pinching-antenna-enabled Federated Learning: Tail Latency, Participation, and Convergence Analysis](https://arxiv.org/abs/2510.23315)
*Yushen Lin,Zihan Chen,Zhiguo Ding*

Main category: cs.IT

TL;DR: 提出了一种捏合天线系统（PASS），通过动态调整天线长度来缩短最差链路距离，从而解决联邦学习在无线网络中的掉队者延迟问题。


<details>
  <summary>Details</summary>
Motivation: 无线网络中的联邦学习受到不可预测信道条件导致的掉队者延迟限制，需要从物理层根源解决这一问题。

Method: 采用PASS系统动态捏合电介质波导上的辐射器，结合物理层感知采样和误差反馈压缩技术。

Result: PASS在同步联邦学习中缩短了最差链路距离，在异步联邦学习中提高了按时完成概率，显著减少了延迟尾部和加速了训练过程。

Conclusion: 通过从物理层根源解决掉队者问题，PASS补充了高层调度策略，能够加速无线联邦学习在同步和异步模式下的训练。

Abstract: Federated learning (FL) in wireless networks is limited by straggler delays
from unpredictable channel conditions. In this paper, we investigate the
pinching-antenna system (PASS), which dynamically 'pinches' the radiator along
a dielectric waveguide to shorten the worst links. In synchronous FL (SFL), we
prove that PASS shortens the worst-link distance, and it increases the on-time
completion probability in asynchronous FL (AFL). Accordingly, SFL exhibits
stochastic dominance on round time, while AFL yields explicit latency and
participation gains. We then pair physical-layer (PHY)-aware sampling with
error-feedback compression and prove that pinching raises the minimum inclusion
probability, thus shrinking both the sampling variability and
compression-induced floors in a Lyapunov analysis. Simulations demonstrate
consistent wall clock speedups and markedly shorter latency tails. By
addressing stragglers at their PHY root, PASS complements higher-layer
scheduling and accelerates wireless FL in both SFL and AFL.

</details>


### [106] [Efficient Repair of (k+2, k) Degraded Read Friendly MDS Array Codes With Sub-packetization 2](https://arxiv.org/abs/2510.23316)
*Jie Li,Xiaohu Tang*

Main category: cs.IT

TL;DR: 提出了两种具有两个校验节点和子分组级别为2的退化读取友好MDS阵列码构造，适用于任意码长。第一种构造在相同参数下具有最小修复带宽，第二种构造支持两种修复机制以优化修复带宽或重建访问。


<details>
  <summary>Details</summary>
Motivation: 设计在小型有限域上具有最优修复性能的MDS阵列码，解决存储系统中节点修复的带宽和访问效率问题。

Method: 第一种构造采用优化的修复带宽设计，第二种构造提供两种修复机制：允许或不允许辅助节点计算，分别优化修复带宽和重建访问。

Result: 第一种构造在所有现有相同参数构造中实现了最小修复带宽，且相对于Zhang等人的下界是渐近最优的。第二种构造实现了修复带宽或重建访问的优化。

Conclusion: 提出的两种DRF MDS阵列码构造在小有限域上实现了优异的修复性能，为存储系统提供了灵活高效的节点修复解决方案。

Abstract: In this paper, we present two constructions of degraded read friendly (DRF)
MDS array codes with two parity nodes and a sub-packetization level of 2 over
small finite fields, applicable for any arbitrary code length. The first
construction achieves the smallest repair bandwidth among all existing
constructions with the same parameters, and is asymptotically optimal with
respect to the lower bound on the average repair bandwidth characterized by
Zhang et al. The second construction supports two repair mechanisms, depending
on whether computation within the helper nodes is permitted or not during the
node repair process, thereby optimizing either the repair bandwidth or the
rebuilding access.

</details>
