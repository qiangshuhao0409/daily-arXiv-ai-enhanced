<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 69]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [On the Inherent Resilience of Task-Oriented V2X Networks to Content-Selection Errors](https://arxiv.org/abs/2602.18620)
*Luca Lusvarghi,Javier Gozalvez*

Main category: cs.NI

TL;DR: 任务导向V2X网络对内容选择错误具有内在弹性，即使在相关性估计误差较高时仍能保证相关信息的一致传递


<details>
  <summary>Details</summary>
Motivation: 任务导向V2X网络中，车辆根据接收者相关性选择传输内容，但相关性估计在复杂动态车载场景中具有挑战性，内容选择错误可能影响接收者的情境感知能力

Method: 分析内容选择错误对任务导向V2X网络的影响，揭示网络对这类错误的内在弹性特性，并识别支撑这种弹性的基本条件

Result: 任务导向V2X网络具有固有的内容选择错误弹性，即使在相关性估计误差较高条件下仍能保证相关信息的一致传递

Conclusion: 该研究揭示了任务导向V2X网络的内在弹性机制，这些条件也适用于其他任务导向网络，为相关网络设计提供了理论基础

Abstract: Task-oriented Vehicle-to-Everything (V2X) networks have recently been proposed to scalably support the large-scale deployment of connected vehicles within the Internet of Vehicles (IoV) vision. In task-oriented V2X networks, vehicles select the content of the transmitted messages based on its relevance to the intended receivers. However, relevance estimation can be quite challenging, especially in highly dynamic and complex vehicular scenarios. Relevance estimation errors can cause a vehicle to omit relevant information from its transmitted message, leading to a content-selection error. Content-selection errors reduce the amount of relevant information available at the receivers and can potentially impair their situational awareness. This work analyses the impact of content-selection errors on task-oriented V2X networks. Our analysis reveals that task-oriented V2X networks feature an inherent resilience to content-selection errors that guarantees a consistent delivery of relevant information even under high relevance estimation error conditions. Moreover, we identify the fundamental conditions underpinning such inherent resilience. These conditions can be encountered in other task-oriented networks where multiple transmitters select the content of their messages based on the task-related requirements of a common set of intended receivers.

</details>


### [2] [Federated Learning-Assisted Optimization of Mobile Transmission with Digital Twins](https://arxiv.org/abs/2602.18627)
*Mohammad Heydari,Terence D. Todd,Dongmei Zhao,George Karakostas*

Main category: cs.NI

TL;DR: 提出基于数字孪生的隐私保护调度框架，解决移动设备在带宽分配中隐私信息泄露问题，通过联邦优化和依赖舍入实现高效调度。


<details>
  <summary>Details</summary>
Motivation: 移动设备的数字孪生包含敏感隐私信息（移动轨迹、位置、信道条件），但传统在线调度器需要这些信息进行带宽分配，存在隐私泄露风险。需要在保护隐私的前提下实现高效调度。

Method: 采用实时联邦优化框架，调度器仅与数字孪生交互获得全局分数解而不暴露隐私信息，然后使用依赖舍入将分数解转换为物理系统的信道传输调度方案。

Result: 实验显示调度时间显著减少，带宽/能量违规接近零，典型边缘服务器硬件上端到端运行时间达到毫秒级，首次实现不暴露隐私数据的跨数字孪生信道共享。

Conclusion: 该框架成功解决了移动设备调度中的隐私保护问题，通过数字孪生和联邦优化实现了隐私保护与调度效率的平衡，为隐私保护调度提供了新思路。

Abstract: A Digital Twin (DT) may protect information that is considered private to its associated physical system. For a mobile device, this may include its mobility profile, recent location(s), and experienced channel conditions. Online schedulers, however, typically use this type of information to perform tasks such as shared bandwidth and channel time slot assignments. In this paper, we consider three transmission scheduling problems with energy constraints, where such information is needed, and yet must remain private: minimizing total transmission time when (i) fixed-power or (ii) fixed-rate time slotting with power control is used, and (iii) maximizing the amount of data uploaded in a fixed time period. Using a real-time federated optimization framework, we show how the scheduler can iteratively interact only with the DTs to produce global fractional solutions to these problems, without the latter revealing their private information. Then dependent rounding is used to round the fractional solution into a channel transmission schedule for the physical systems. Experiments show consistent makespan reductions with near-zero bandwidth/energy violations and millisecond-order end-to-end runtime for typical edge server hardware. To the best of our knowledge, this is the first framework that enables channel sharing across DTs using operations that do not expose private data.

</details>


### [3] [MetaBlue: A Metasurface-Assisted Acoustic Underwater Localization System](https://arxiv.org/abs/2602.19252)
*Junling Wang,Yi Guo,Bojun Yang,Yazhou Yuan,Zhenlin An*

Main category: cs.NI

TL;DR: MetaBlue是一种低成本被动声学超表面，可将普通超声波发射器转变为定向"超级发射器"，实现单水听器AoA估计和EM-声学混合ToA测距，完成单锚点3D水下定位。


<details>
  <summary>Details</summary>
Motivation: 水下定位对海洋探索和自主水下操作至关重要，但现有射频和光学方法受限于快速衰减或有限能见度。传统声学系统通常依赖大型阵列或多同步锚点，导致硬件成本高且部署复杂。

Method: 提出MetaBlue被动声学超表面，附着在普通超声波发射器上形成定向"超级发射器"，在传输波形中嵌入方向相关频谱模式，实现单水听器AoA估计。提出EM-声学混合ToA方法，利用声换能器固有的低频电磁泄漏作为时间参考，实现无共享时钟的精确测距。

Result: 在真实水下环境（水池、水箱、室外）评估，平均AoA误差8.7度，10米以上距离3D定位误差0.37米。即使使用单锚点，系统仍保持0.73米精度。

Conclusion: MetaBlue系统通过低成本被动声学超表面和EM-声学混合ToA方法，实现了单锚点、单水听器的完整3D水下定位，显著降低了硬件成本和部署复杂度。

Abstract: Underwater localization is essential for marine exploration and autonomous underwater operations, yet existing radio frequency and optical approaches are limited by rapid attenuation or limited visibility. Acoustic sensing remains the most practical choice, but conventional acoustic systems typically rely on large arrays or multiple synchronized anchors, resulting in high hardware costs and complex deployment. This paper introduces a novel low-cost passive acoustic metasurface, MetaBlue , explicitly designed for underwater localization, which, when attached to an ordinary ultrasonic transmitter, transforms it into a directional "super-transmitter." The metasurface embeds direction-dependent spectral patterns into the transmitted waveform, enabling accurate angle-of-arrival (AoA) estimation using only a single hydrophone. For ranging, we present a new EM-acoustic mixed time-of-arrival (ToA) method that leverages the acoustic transducer's inherent low-frequency EM leakage as a timing reference, enabling precise ranging without shared clocks. This allows complete 3D localization with a single low-cost anchor. We evaluate the system across diverse real-world underwater settings, including pools, tanks, and outdoor environments. Experiments show that our design achieves an average AoA error of 8.7 degree and 3D localization error of 0.37 m at distances over 10 m. Even with a single anchor, the system maintains 0.73 m precision.

</details>


### [4] [EMS-FL: Federated Tuning of Mixture-of-Experts in Satellite-Terrestrial Networks via Expert-Driven Model Splitting](https://arxiv.org/abs/2602.19485)
*Angzi Xu,Zezhong Zhang,Zhi Liu,Shuguang Cui*

Main category: cs.NI

TL;DR: EMS-FL：一种面向卫星-地面网络的专家驱动模型分割与联邦学习方法，通过非重叠专家分配和异步本地训练，解决卫星移动导致的间歇连接问题，降低训练开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型对数据量和计算资源需求巨大，联邦学习虽能利用分布式资源，但面临网络覆盖有限导致的数据不足和边缘设备计算约束问题。卫星-地面网络提供广覆盖，但卫星相对运动导致间歇连接，阻碍传统联邦学习的模型同步。

Method: EMS-FL方法：1) 为每个设备集群分配与其本地数据高度相关的专家（非重叠分配）；2) 采用异步本地学习，设备集群连续训练分配的专家；3) 仅在连接阶段将本地参数上传到卫星进行聚合和模型更新。

Result: 与传统联邦学习相比，EMS-FL有效降低训练开销，实现更快的收敛速度和更高的准确率。通过公开数据集和大模型的综合实验验证了其优越性，并提供了严格的理论收敛分析。

Conclusion: EMS-FL成功结合了卫星-地面网络的广覆盖优势和专家模型的轻量计算特性，解决了间歇连接环境下的联邦学习挑战，为资源受限的边缘计算场景提供了高效解决方案。

Abstract: The rapid advancement of large AI models imposes stringent demands on data volume and computational resources. Federated learning, though designed to exploit distributed data and computational resources, faces data shortage from limited network coverage and computational constraints from edge devices. To address these issues, both the mixture-of-experts (MoE) and satellite-terrestrial network (STN) provide promising solutions, offering lightweight computation overhead and broad coverage, respectively. However, the satellite-ground relative motion results in intermittent connectivity, hindering conventional federated learning that relies on model synchronization across devices. To leverage the coverage of STN while preserving training efficiency, we propose EMS-FL, an expert-driven model splitting and federated learning method. EMS-FL assigns each device cluster only the experts highly correlated to their local data. Through non-overlapping expert assignments, asynchronous local learning is further proposed, where each device cluster trains its assigned experts consecutively and only uploads local parameters to the satellite during connected phases for aggregation and model updates. Consequently, EMS-FL effectively reduces the training overhead and achieves both faster convergence and higher accuracy compared with conventional federated learning. Rigorous convergence analysis is provided to theoretically characterize the learning performance. Furthermore, comprehensive experiments are conducted using public datasets and large models, validating the superiority of EMS-FL.

</details>


### [5] [Spritz: Path-Aware Load Balancing in Low-Diameter Networks](https://arxiv.org/abs/2602.19567)
*Tommaso Bonato,Ales Kubicek,Abdul Kabbani,Ahmad Ghalayini,Maciej Besta,Torsten Hoefler*

Main category: cs.NI

TL;DR: Spritz是一个基于发送端的负载均衡框架，利用标准以太网功能在低直径拓扑中实现自适应路由，无需专用硬件支持。


<details>
  <summary>Details</summary>
Motivation: 当前低直径拓扑（如Dragonfly和Slim Fly）在HPC和数据中心中越来越普及，但现有负载均衡技术要么依赖专有网络内机制，要么无法充分利用这些拓扑的完整路径多样性。

Method: 提出Spritz框架，将自适应拓扑感知路由转移到终端，使用标准以太网功能。开发两种算法：Spritz-Scout（探索高效路径）和Spritz-Spray（自适应缓存路径），利用ECN、包修剪和超时反馈。

Result: 在超过1000个端点的Dragonfly和Slim Fly拓扑模拟中，Spritz在AI训练和数据中心工作负载下，流完成时间比ECMP、UGAL-L和先前发送端方法提升高达1.8倍；在链路故障下性能提升高达25.4倍。

Conclusion: Spritz使数据中心规模的商用以太网网络能够高效利用低直径拓扑，为超以太网时代提供统一的路由和负载均衡解决方案。

Abstract: Low-diameter topologies such as Dragonfly and Slim Fly are increasingly adopted in HPC and datacenter networks, yet existing load balancing techniques either rely on proprietary in-network mechanisms or fail to utilize the full path diversity of these topologies. We introduce Spritz, a flexible sender-based load balancing framework that shifts adaptive topology-aware routing to the endpoints using only standard Ethernet features. We propose two algorithms, Spritz-Scout and Spritz-Spray that, respectively, explore and adaptively cache efficient paths using ECN, packet trimming, and timeout feedback. Through simulation on Dragonfly and Slim Fly topologies with over 1000 endpoints, Spritz outperforms ECMP, UGAL-L, and prior sender-based approaches by up to 1.8x in flow completion time under AI training and datacenter workloads, while offering robust failover with performance improvements of up to 25.4x under link failures, all without additional hardware support. Spritz enables datacenter-scale, commodity Ethernet networks to efficiently leverage low-diameter topologies, offering unified routing and load balancing for the Ultra Ethernet era.

</details>


### [6] [Traffic-Aware Configuration of OPC UA PubSub in Industrial Automation Networks](https://arxiv.org/abs/2602.19603)
*Kasra Ekrad,Bjarne Johansson,Inés Alvarez Vadillo,Saad Mubeen,Mohammad Ashjaei*

Main category: cs.NI

TL;DR: 本文提出了一套将工业流量类型映射到OPC UA PubSub配置的指南，以确保工业4.0系统中的可预测通信和实时性能。


<details>
  <summary>Details</summary>
Motivation: 工业自动化系统的互操作性是工业4.0的基石。虽然OPC UA PubSub模型为异构设备间的高效通信提供了机制，但缺乏将不同工业流量类型映射到适当PubSub配置的明确指南，这可能导致网络性能下降和实时要求无法满足。

Method: 提出了一套基于流量时序和服务质量规格的指南，用于将工业流量类型映射到OPC UA PubSub配置。通过工业用例评估了错误配置对延迟和吞吐量的影响。

Result: 评估结果表明，错误的PubSub配置会显著影响延迟和吞吐量性能。研究强调了流量感知的PubSub配置对于实现工业4.0系统互操作性的重要性。

Conclusion: 流量感知的OPC UA PubSub配置对于确保工业网络中的可预测通信和实时性能至关重要。提出的指南有助于避免错误配置，支持工业4.0系统的互操作性要求。

Abstract: Interoperability across industrial automation systems is a cornerstone of Industry 4.0. To address this need, the OPC Unified Architecture (OPC UA) Publish-Subscribe (PubSub) model offers a promising mechanism for enabling efficient communication among heterogeneous devices. PubSub facilitates resource sharing and communication configuration between devices, but it lacks clear guidelines for mapping diverse industrial traffic types to appropriate PubSub configurations. This gap can lead to misconfigurations that degrade network performance and compromise real-time requirements. This paper proposes a set of guidelines for mapping industrial traffic types, based on their timing and quality-of-service specifications, to OPC UA PubSub configurations. The goal is to ensure predictable communication and support real-time performance in industrial networks. The proposed guidelines are evaluated through an industrial use case that demonstrates the impact of incorrect configuration on latency and throughput. The results underline the importance of traffic-aware PubSub configuration for achieving interoperability in Industry 4.0 systems.

</details>


### [7] [AI-Powered Conflict Management in Open RAN: Detection, Classification, and Mitigation](https://arxiv.org/abs/2602.19758)
*Abdul Wadud,Nima Afraz,Fatemeh Golpayegani*

Main category: cs.NI

TL;DR: 提出AI驱动的Open RAN冲突管理框架，包括冲突检测、分类和缓解，使用合成数据生成和先进AI模型，在仿真中实现比规则方法快3.2倍的分类速度


<details>
  <summary>Details</summary>
Motivation: Open RAN中AI驱动的xApps和rApps独立调整ICP可能导致直接、间接和隐性冲突，造成网络不稳定和KPI下降。传统基于规则的冲突管理在Open RAN规模扩大时变得不切实际，需要AI驱动的实时解决方案

Method: 提出AI驱动的冲突管理框架，包括：1) GenC合成冲突生成框架，创建大规模带标签数据集；2) 分类管道使用GNN、Bi-LSTM和SMOTE增强的GNN；3) 使用合成数据集(5-50个xApps)和基于OpenCellID都柏林拓扑的ns3-oran仿真进行验证

Result: AI方法比基于规则的方法快3.2倍，同时保持接近完美的准确率。SMOTE-GNN在处理不平衡数据方面表现出优越的鲁棒性。框架成功解决了ES/MRO冲突场景，并能有效扩展到大规模xApp环境

Conclusion: 通过将AI驱动的冲突管理工作流嵌入Open RAN架构，实现了自主自优化的冲突管理，为弹性、超低延迟和节能的6G网络铺平了道路

Abstract: Open Radio Access Network (RAN) was designed with native Artificial Intelligence (AI) as a core pillar, enabling AI- driven xApps and rApps to dynamically optimize network performance. However, the independent ICP adjustments made by these applications can inadvertently create conflicts- direct, indirect, and implicit, which lead to network instability and KPI degradation. Traditional rule-based conflict management becomes increasingly impractical as Open RAN scales in terms of xApps, associated ICPs, and relevant KPIs, struggling to handle the complexity of multi-xApp interactions. This highlights the necessity for AI-driven solutions that can efficiently detect, classify, and mitigate conflicts in real-time. This paper proposes an AI-powered framework for conflict detection, classification, and mitigation in Open RAN. We introduce GenC, a synthetic conflict generation framework for large-scale labeled datasets with controlled parameter sharing and realistic class imbalance, enabling robust training and evaluation of AI models. Our classification pipeline leverages GNNs, Bi-LSTM, and SMOTE-enhanced GNNs, with results demonstrating SMOTE-GNN's superior robustness in handling imbalanced data. Experimental validation using both synthetic datasets (5-50 xApps) and realistic ns3-oran simulations with OpenCellID-derived Dublin topology shows that AI-based methods achieve 3.2x faster classification than rule-based approaches while maintaining near-perfect accuracy. Our framework successfully addresses Energy Saving (ES)/Mobility Robustness Optimization (MRO) conflict scenarios using realistic ns3-oran and scales efficiently to large-scale xApp environments. By embedding this workflow into Open RAN's AI-driven architecture, our solution ensures autonomous and self-optimizing conflict management, paving the way for resilient, ultra-low-latency, and energy-efficient 6G networks.

</details>


### [8] [BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models](https://arxiv.org/abs/2602.19929)
*Chenran Kou,Changsheng You,Mingjiang Wu,Dingzhu Wen,Zezhong Zhang,Chengwen Xing*

Main category: cs.NI

TL;DR: 提出BeamVLM框架，将波束预测转化为视觉问答任务，利用视觉语言模型联合推理无人机轨迹和环境上下文，提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 低空经济中，无人机与地面基站间的快速准确波束预测对保障无缝覆盖和可靠通信至关重要。现有深度学习方法缺乏对动态环境的高级语义理解，泛化能力差；而大语言模型方法虽然泛化能力强，但缺乏丰富的环境感知，无法捕捉精细的空间语义。

Method: 提出BeamVLM端到端生成框架，将波束预测视为视觉问答任务，利用现有视觉语言模型。通过将原始视觉补丁直接投影到语言域，并精心设计指令提示，使VLM能够联合推理无人机轨迹和环境上下文。

Result: 在真实世界数据集上的实验结果表明，BeamVLM在预测准确性上优于现有最先进方法，并在车对基础设施波束预测等其他场景中展现出优越的泛化能力。

Conclusion: BeamVLM通过将波束预测转化为视觉问答任务，成功结合了视觉语言模型的强大推理能力，解决了现有方法在语义理解和环境感知方面的不足，实现了更准确和泛化的波束预测。

Abstract: For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.

</details>


### [9] [Adaptive Underwater Acoustic Communications with Limited Feedback: An AoI-Aware Hierarchical Bandit Approach](https://arxiv.org/abs/2602.20105)
*Fabio Busacca,Andrea Panebianco,Yin Sun*

Main category: cs.NI

TL;DR: 本文提出了一种双层多臂老虎机框架，用于优化水下声学网络的吞吐量和能效。内层使用上下文延迟MAB联合优化自适应调制和传输功率，外层使用反馈调度MAB动态调整信道状态反馈间隔，在DESERT模拟器中实现了20.61%的吞吐量提升和36.60%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 水下声学网络面临带宽有限、传播延迟长、信道高度动态等固有挑战，这些限制阻碍了实时通信并降低了系统性能。现有方法难以在资源受限环境下有效平衡吞吐量和能耗。

Method: 提出双层MAB框架：1）快速内层使用上下文延迟MAB，基于信道状态反馈和其信息年龄联合优化自适应调制和传输功率；2）慢速外层使用反馈调度MAB，根据吞吐量动态调整信道状态反馈间隔，稳定时延长更新间隔，吞吐量下降时增加更新频率。

Result: 在DESERT水下网络模拟器中的仿真结果表明，相比现有文献中的深度强化学习基线，实现了高达20.61%的吞吐量增益和高达36.60%的能耗节省。该框架计算效率高，适合资源受限的水下声学网络。

Conclusion: 提出的双层MAB框架有效解决了水下声学网络的性能优化问题，通过自适应反馈调度和联合优化策略，在保证吞吐量的同时显著降低了能耗，为资源受限的水下网络提供了高效解决方案。

Abstract: Underwater Acoustic (UWA) networks are vital for remote sensing and ocean exploration but face inherent challenges such as limited bandwidth, long propagation delays, and highly dynamic channels. These constraints hinder real-time communication and degrade overall system performance. To address these challenges, this paper proposes a bilevel Multi-Armed Bandit (MAB) framework. At the fast inner level, a Contextual Delayed MAB (CD-MAB) jointly optimizes adaptive modulation and transmission power based on both channel state feedback and its Age of Information (AoI), thereby maximizing throughput. At the slower outer level, a Feedback Scheduling MAB dynamically adjusts the channel-state feedback interval according to throughput dynamics: stable throughput allows longer update intervals, while throughput drops trigger more frequent updates. This adaptive mechanism reduces feedback overhead and enhances responsiveness to varying network conditions. The proposed bilevel framework is computationally efficient and well-suited to resource-constrained UWA networks. Simulation results using the DESERT Underwater Network Simulator demonstrate throughput gains of up to 20.61% and energy savings of up to 36.60% compared with Deep Reinforcement Learning (DRL) baselines reported in the existing literature.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [On the Dynamics of Observation and Semantics](https://arxiv.org/abs/2602.18494)
*Xiu Li*

Main category: cs.AI

TL;DR: 该论文挑战了将语义视为静态潜在表示的主流范式，提出智能是有界物理代理的属性，受热力学限制，必须通过符号结构的相变来实现可压缩的因果理解。


<details>
  <summary>Details</summary>
Motivation: 当前视觉智能的主流范式将语义视为高维嵌入空间中几何邻近性的静态属性，作者认为这种观点在物理上是不完整的。他们主张智能不是现实的被动镜像，而是受有限内存、计算和能量约束的物理可实现代理的属性。

Method: 提出观察语义纤维束的动力学结构，将原始感官观察数据（纤维）投影到低熵因果语义流形（基）。基于兰道尔原理的热力学信息处理成本，推导出内部状态转换复杂度的严格限制（语义常数B）。从这些物理约束中推导出符号结构的必要性。

Result: 证明任何有界代理都必须遵守语义常数B的限制。为了在B界限内建模组合世界，语义流形必须经历相变，结晶成离散、组合和因子化的形式。语言和逻辑不是文化产物，而是防止热崩溃所需的信息固态。

Conclusion: 理解不是恢复隐藏的潜在变量，而是构建一个因果商，使世界在算法上可压缩且在因果上可预测。符号结构是物理约束下的必然结果，智能需要构建离散的表示来实现高效的信息处理。

Abstract: A dominant paradigm in visual intelligence treats semantics as a static property of latent representations, assuming that meaning can be discovered through geometric proximity in high dimensional embedding spaces. In this work, we argue that this view is physically incomplete. We propose that intelligence is not a passive mirror of reality but a property of a physically realizable agent, a system bounded by finite memory, finite compute, and finite energy interacting with a high entropy environment. We formalize this interaction through the kinematic structure of an Observation Semantics Fiber Bundle, where raw sensory observation data (the fiber) is projected onto a low entropy causal semantic manifold (the base). We prove that for any bounded agent, the thermodynamic cost of information processing (Landauer's Principle) imposes a strict limit on the complexity of internal state transitions. We term this limit the Semantic Constant B. From these physical constraints, we derive the necessity of symbolic structure. We show that to model a combinatorial world within the bound B, the semantic manifold must undergo a phase transition, it must crystallize into a discrete, compositional, and factorized form. Thus, language and logic are not cultural artifacts but ontological necessities the solid state of information required to prevent thermal collapse. We conclude that understanding is not the recovery of a hidden latent variable, but the construction of a causal quotient that renders the world algorithmically compressible and causally predictable.

</details>


### [11] [Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications](https://arxiv.org/abs/2602.18582)
*Zhiqin Qian,Ryan Diaz,Sangwon Seo,Vaibhav Unhelkar*

Main category: cs.AI

TL;DR: HRDL扩展了经典奖励设计，用于分层RL代理的丰富行为规范，L2HR作为解决方案，使AI代理能更好地遵循人类规范完成任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理处理日益复杂的任务，将其行为与人类提供的规范对齐对于负责任的AI部署变得至关重要。现有方法通常过于有限，无法捕捉长时程任务中出现的细微人类偏好。

Method: 提出了分层奖励设计从语言（HRDL）的问题表述，扩展了经典奖励设计以编码分层RL代理的更丰富行为规范。进一步提出了语言到分层奖励（L2HR）作为HRDL的解决方案。

Result: 实验表明，通过L2HR设计的奖励训练的AI代理不仅有效完成任务，而且更好地遵循人类规范。

Conclusion: HRDL和L2HR共同推进了人类对齐AI代理的研究。

Abstract: When training artificial intelligence (AI) to perform tasks, humans often care not only about whether a task is completed but also how it is performed. As AI agents tackle increasingly complex tasks, aligning their behavior with human-provided specifications becomes critical for responsible AI deployment. Reward design provides a direct channel for such alignment by translating human expectations into reward functions that guide reinforcement learning (RL). However, existing methods are often too limited to capture nuanced human preferences that arise in long-horizon tasks. Hence, we introduce Hierarchical Reward Design from Language (HRDL): a problem formulation that extends classical reward design to encode richer behavioral specifications for hierarchical RL agents. We further propose Language to Hierarchical Rewards (L2HR) as a solution to HRDL. Experiments show that AI agents trained with rewards designed via L2HR not only complete tasks effectively but also better adhere to human specifications. Together, HRDL and L2HR advance the research on human-aligned AI agents.

</details>


### [12] [Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607)
*Michal Töpfer,František Plášil,Tomáš Bureš,Petr Hnětynka*

Main category: cs.AI

TL;DR: 论文提出使用生成式LLM通过vibe coding反馈循环生成自适应管理器(AM)，结合新型时序逻辑FCL进行约束验证，在CAS领域实现了有效的AM代码生成。


<details>
  <summary>Details</summary>
Motivation: 在复杂自适应系统(CAS)中，定义动态架构和行为变化具有挑战性。传统上通过自适应管理器实现，而生成式LLM的出现为基于系统规范和自然语言描述生成AM代码提供了新机会。但生成代码的正确性验证是关键问题。

Method: 采用vibe coding反馈循环方法，结合新型时序逻辑FCL表达功能需求约束。通过迭代测试和反馈循环，将FCL约束评估结果反馈给LLM，逐步改进生成的AM代码。实验中使用不同初始设置实现高运行路径覆盖率。

Result: 实验表明，基于FCL约束验证的vibe coding反馈循环能有效生成CAS领域的AM。通常只需几次反馈循环迭代，每次向LLM提供详细的约束违反报告，即可获得良好结果。在两个示例系统中都取得了成功。

Conclusion: 当基于精确的功能需求约束验证时，通过vibe coding反馈循环生成自适应管理器是可行的。FCL逻辑提供了比传统LTL更细粒度的行为描述能力，结合自适应和vibe coding反馈循环能有效生成正确的AM代码。

Abstract: In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.
  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.
  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

</details>


### [13] [Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System](https://arxiv.org/abs/2602.18640)
*Longfei Yun,Yihan Wu,Haoran Liu,Xiaoxuan Liu,Ziyun Xu,Yi Wang,Yang Xia,Pengfei Wang,Mingze Gao,Yunxiang Wang,Changfan Chen,Junfeng Pan*

Main category: cs.AI

TL;DR: GEARS框架将排序优化重构为可编程实验环境中的自主发现过程，通过专用智能体技能封装专家知识，实现高层意图驱动的个性化，并确保生产可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代大规模排序系统面临复杂的目标冲突、运营约束和产品需求变化，进展主要受限于工程上下文约束：将模糊的产品意图转化为可执行、可验证假设的艰巨过程，而非建模技术本身。

Method: 提出GEARS框架，将排序优化重构为可编程实验环境中的自主发现过程，利用专用智能体技能封装排序专家知识为可重用推理能力，使操作者能够通过高层意图进行个性化引导，并集成验证钩子确保统计鲁棒性。

Result: 在多样化产品表面的实验验证表明，GEARS能够持续识别出优越、接近帕累托最优的策略，通过算法信号与深度排序上下文的协同作用，同时保持严格的部署稳定性。

Conclusion: GEARS框架通过将排序优化重构为自主发现过程，解决了工程上下文约束瓶颈，使系统能够通过高层意图引导实现可靠、高效的排序优化。

Abstract: Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability.

</details>


### [14] [Spilled Energy in Large Language Models](https://arxiv.org/abs/2602.18671)
*Adrian Robert Minut,Hazem Dewidar,Iacopo Masi*

Main category: cs.AI

TL;DR: 将LLM的softmax分类器重新解释为能量模型，通过分析解码过程中的"能量溢出"来检测幻觉，无需额外训练


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法通常需要训练探针分类器或进行激活消融，计算成本高且需要额外训练。本文旨在开发一种无需训练、直接从输出logits中提取指标的方法来检测LLM中的事实错误、偏见和失败

Method: 将序列到序列的概率链分解为多个相互作用的能量模型，引入两个完全无需训练的指标：1) 溢出能量：捕捉连续生成步骤中理论上应匹配的能量值之间的差异；2) 边缘化能量：可在单一步骤中测量

Result: 在九个基准测试和合成代数操作上评估，该方法在LLaMA、Mistral、Gemma、Qwen3等最先进LLM上表现出稳健、有竞争力的幻觉检测能力和跨任务泛化能力，对预训练和指令调优变体均有效

Conclusion: 通过将LLM重新解释为能量模型并分析能量溢出，可以无需额外训练就能有效检测幻觉，为LLM可靠性评估提供了轻量级且通用的方法

Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track "energy spills" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.

</details>


### [15] [Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse](https://arxiv.org/abs/2602.18710)
*Martin Bertran,Riccardo Fogliato,Zhiwei Steven Wu*

Main category: cs.AI

TL;DR: AI分析师基于大语言模型可廉价大规模复现研究分析多样性，不同AI分析师对相同数据得出冲突结论，分析选择受模型和提示框架系统影响且可操控。


<details>
  <summary>Details</summary>
Motivation: 实证研究结论不仅取决于数据，还取决于一系列通常未明确说明的分析决策。过去"多分析师"研究表明独立团队对相同数据常得出冲突结论，但这类研究需要大量协调且成本高昂。

Method: 构建基于大语言模型的完全自主AI分析师，让它们在固定数据集上测试预设假设，在不同模型和提示框架条件下独立构建和执行完整分析流程，然后由AI审计员筛选方法有效的运行。

Result: 在三个数据集（实验和观察设计）中，AI分析师产生的分析在效应大小、p值和二元决策上显示出广泛分散，经常逆转假设是否得到支持的判断。这种分散是结构化的：预处理、模型规范和推理中的可识别分析选择在不同LLM和角色条件下系统性地不同。

Conclusion: AI分析师能够廉价大规模复现分析多样性，分析结果受LLM和角色分配的系统性影响且可操控，这揭示了研究结论对分析决策的敏感性，并展示了AI在探索分析不确定性方面的潜力。

Abstract: The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs.

</details>


### [16] [Task-Aware Exploration via a Predictive Bisimulation Metric](https://arxiv.org/abs/2602.18724)
*Dayang Liang,Ruihan Liu,Lipeng Wan,Yunlong Liu,Bo An*

Main category: cs.AI

TL;DR: TEB：一种通过预测双模拟度量将任务相关表示与探索紧密耦合的任务感知探索方法，在稀疏奖励的视觉强化学习中实现高效探索


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习在稀疏奖励下的探索仍然具有挑战性，因为存在大量与任务无关的变化。现有方法要么假设可以访问低维状态，要么缺乏任务感知的探索策略，在视觉领域表现脆弱

Method: TEB利用预测双模拟度量学习行为基础的任务表示，并在学习的潜在空间中测量行为内在新颖性。首先理论上缓解稀疏奖励下退化双模拟度量的表示崩溃，通过引入预测奖励差异。基于此稳健度量，设计基于潜力的探索奖励，测量相邻观测在潜在空间中的相对新颖性

Result: 在MetaWorld和Maze2D上的大量实验表明，TEB实现了卓越的探索能力，并优于最近的基线方法

Conclusion: TEB通过将任务相关表示与探索紧密耦合，有效解决了视觉强化学习中稀疏奖励下的探索挑战，提供了一种任务感知的探索方法

Abstract: Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines.

</details>


### [17] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 提出Chart Insight Agent Flow多智能体框架，利用MLLMs从图表图像中挖掘深层洞察，并创建ChartSummInsights数据集，显著提升图表摘要性能。


<details>
  <summary>Details</summary>
Motivation: 现有图表摘要方法（包括MLLMs）主要关注低层数据描述，未能捕捉数据可视化的核心目的——深层洞察，且缺乏合适的基准数据集。

Method: 提出Chart Insight Agent Flow：一个规划与执行的多智能体框架，有效利用MLLMs的感知和推理能力，直接从图表图像中挖掘深刻洞察。同时创建ChartSummInsights数据集，包含真实世界图表和专家撰写的高质量洞察摘要。

Result: 实验结果表明，该方法显著提升了MLLMs在图表摘要任务上的性能，能够生成包含深度和多样性洞察的摘要。

Conclusion: Chart Insight Agent Flow框架和ChartSummInsights数据集共同解决了现有图表摘要方法的局限性，实现了从图表中挖掘深层洞察的目标。

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [18] [Federated Reasoning Distillation Framework with Model Learnability-Aware Data Allocation](https://arxiv.org/abs/2602.18749)
*Wei Guo,Siyuan Lu,Xiangdong Ran,Yiqi Tong,Yikun Ban,Zelong Xu,Jing Fan,Zixuan Huang,Xiao Zhang,Zhaojun Hu,Fuzhen Zhuang*

Main category: cs.AI

TL;DR: LaDa是一个联邦推理蒸馏框架，通过模型可学习性感知的数据分配解决联邦LLM-SLM协作中的双向学习差距和领域无关推理转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据分配方法未能解决联邦LLM-SLM协作中的两个关键挑战：1）双向模型可学习性差距——客户端SLM无法识别符合其学习约束的高价值样本，而LLM难以选择提供新知识的样本；2）领域无关推理转移——现有推理转移方法无法灵活适应本地领域数据，阻碍SLM从通用LLM获取逐步推理能力。

Method: 提出LaDa框架，包含两个核心组件：1）模型可学习性感知数据过滤器，根据SLM-LLM对之间的学习差距自适应分配高价值样本；2）领域自适应推理蒸馏方法，通过对齐SLM和LLM在过滤样本上的推理路径联合概率，通过对比蒸馏学习使SLM捕捉本地数据分布下的底层推理模式。

Result: LaDa作为现有协作框架的插件模块，能够基于模型可学习性差距自适应调整知识转移，有效促进双向知识转移并提升SLM在本地数据上的推理能力。

Conclusion: LaDa通过解决双向学习差距和领域无关推理转移问题，为联邦LLM-SLM协作提供了有效的解决方案，能够提升知识转移效率和SLM的推理性能。

Abstract: Data allocation plays a critical role in federated large language model (LLM) and small language models (SLMs) reasoning collaboration. Nevertheless, existing data allocation methods fail to address an under-explored challenge in collaboration: bidirectional model learnability gap, where client-side SLMs cannot identify high-reward samples matching their learnability constraints for effective knowledge transfer from LLMs, while LLMs struggle to select samples contributing novel knowledge beyond their existing data. Furthermore, these collaboration frameworks face another key challenge: domain-agnostic reasoning transfer, where existing reasoning transfer methods fail to flexibly adapt to the local domain data, preventing SLMs from effectively acquiring step-by-step reasoning abilities within from general LLM. To address these challenges, we propose LaDa, a federated reasoning distillation framework with model learnability-aware data allocation. It introduces a model learnability-aware data filter that adaptively allocates high-reward samples based on the learnability gap between each SLM and LLM pair, effectively facilitating bidirectional knowledge transfer. We further design a domain adaptive reasoning distillation method that aligns joint probabilities of reasoning paths on filtered high-reward samples through contrastive distillation learning between SLM and LLM, enabling SLM to capture underlying reasoning patterns under local data distribution. LaDa operates as a plug-in module for existing collaboration frameworks, adapting knowledge transfer based on model learnability gaps.

</details>


### [19] [The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol](https://arxiv.org/abs/2602.18764)
*Andreas Schlapbach*

Main category: cs.AI

TL;DR: SGD和MCP代表了LLM-agent交互的统一范式，本文提取了五个模式设计原则，揭示了三个新见解，并提供了具体的设计模式。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示Schema-Guided Dialogue (SGD)和Model Context Protocol (MCP)之间的根本趋同，它们代表了LLM-agent确定性、可审计交互的统一范式。通过分析这种趋同，提取出模式设计的基本原则，以解决当前框架中的不足，并为AI系统监管提供可扩展的机制。

Method: 通过分析SGD（2019年设计用于基于对话的API发现）和MCP（当前LLM工具集成的实际标准）之间的趋同性，提取出五个基础模式设计原则。这些原则包括：语义完整性优于句法精确性、明确的操作边界、故障模式文档化、渐进披露兼容性和工具间关系声明。基于这些原则，提供了具体的设计模式。

Result: 揭示了三个新颖见解：1) SGD的原始设计本质上是合理的，应该被MCP继承；2) 两个框架都未充分利用故障模式和工具间关系，本文识别并解决了这些差距；3) 在真实世界的令牌约束下，渐进披露成为生产规模扩展的关键见解。提供了每个原则的具体设计模式。

Conclusion: 模式驱动的治理被定位为AI系统监管的可扩展机制，无需专有系统检查，这对Software 3.0至关重要。这些原则为构建更健壮、可审计的LLM-agent系统提供了理论基础和实践指导。

Abstract: This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0.

</details>


### [20] [LAMMI-Pathology: A Tool-Centric Bottom-Up LVLM-Agent Framework for Molecularly Informed Medical Intelligence in Pathology](https://arxiv.org/abs/2602.18773)
*Haoyang Su,Shaoting Zhang,Xiaosong Wang*

Main category: cs.AI

TL;DR: LAMMI-Pathology是一个用于病理图像分析的可扩展代理框架，采用工具为中心的层次架构和基于原子执行节点的轨迹构建机制，通过轨迹感知微调增强推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 工具调用代理系统为病理图像分析提供了更证据驱动的范式，相比粗粒度的文本-图像诊断方法。随着空间转录组学技术的广泛采用，分子验证的病理诊断变得更加开放和可访问。

Method: 采用工具为中心的层次架构：定制化领域自适应工具作为基础，按领域风格聚类形成组件代理，通过顶层规划器分层协调。引入基于原子执行节点(AENs)的轨迹构建机制，作为可靠可组合单元构建半模拟推理轨迹。开发轨迹感知微调策略，使规划器决策过程与多步推理轨迹对齐。

Result: 提出了一个可扩展的代理框架LAMMI-Pathology，能够避免过长的上下文长度导致的任务漂移，捕获可信的代理-工具交互，增强病理理解推理的鲁棒性和定制工具集的自适应使用。

Conclusion: LAMMI-Pathology为分子信息医学智能在病理学领域提供了一个有效的代理框架，通过层次化工具调用架构和轨迹感知学习机制，实现了更可靠和可扩展的病理图像分析。

Abstract: The emergence of tool-calling-based agent systems introduces a more evidence-driven paradigm for pathology image analysis in contrast to the coarse-grained text-image diagnostic approaches. With the recent large-scale experimental adoption of spatial transcriptomics technologies, molecularly validated pathological diagnosis is becoming increasingly open and accessible. In this work, we propose LAMMI-Pathology (LVLM-Agent System for Molecularly Informed Medical Intelligence in Pathology), a scalable agent framework for domain-specific agent tool-calling. LAMMI-Pathology adopts a tool-centric, bottom-up architecture in which customized domain-adaptive tools serve as the foundation. These tools are clustered by domain style to form component agents, which are then coordinated through a top-level planner hierarchically, avoiding excessively long context lengths that could induce task drift. Based on that, we introduce a novel trajectory construction mechanism based on Atomic Execution Nodes (AENs), which serve as reliable and composable units for building semi-simulated reasoning trajectories that capture credible agent-tool interactions. Building on this foundation, we develop a trajectory-aware fine-tuning strategy that aligns the planner's decision-making process with these multi-step reasoning trajectories, thereby enhancing inference robustness in pathology understanding and its adaptive use of the customized toolset.

</details>


### [21] [GenPlanner: From Noise to Plans -- Emergent Reasoning in Flow Matching and Diffusion Models](https://arxiv.org/abs/2602.18812)
*Agnieszka Polowczyk,Alicja Polowczyk,Michał Wieczorek*

Main category: cs.AI

TL;DR: 提出基于扩散模型和流匹配的路径规划方法GenPlanner，包含DiffPlanner和FlowPlanner两个变体，在迷宫环境中通过多通道条件生成正确路径，显著优于基线CNN模型。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中的路径规划是人工智能的关键问题，需要同时理解空间几何和全局结构。探索生成模型作为规划和推理机制的潜力。

Method: 提出GenPlanner方法，基于扩散模型和流匹配，包含DiffPlanner和FlowPlanner两个变体。使用多通道条件（障碍物地图、起点终点信息）来条件化轨迹生成，通过迭代方式从随机噪声逐步生成正确路径。

Result: 实验表明该方法显著优于基线CNN模型。特别是FlowPlanner在有限生成步数下仍表现出高性能。

Conclusion: 生成模型可作为有效的路径规划机制，GenPlanner方法在复杂环境中表现出色，为AI规划问题提供了新思路。

Abstract: Path planning in complex environments is one of the key problems of artificial intelligence because it requires simultaneous understanding of the geometry of space and the global structure of the problem. In this paper, we explore the potential of using generative models as planning and reasoning mechanisms. We propose GenPlanner, an approach based on diffusion models and flow matching, along with two variants: DiffPlanner and FlowPlanner. We demonstrate the application of generative models to find and generate correct paths in mazes. A multi-channel condition describing the structure of the environment, including an obstacle map and information about the starting and destination points, is used to condition trajectory generation. Unlike standard methods, our models generate trajectories iteratively, starting with random noise and gradually transforming it into a correct solution. Experiments conducted show that the proposed approach significantly outperforms the baseline CNN model. In particular, FlowPlanner demonstrates high performance even with a limited number of generation steps.

</details>


### [22] [ABD: Default Exception Abduction in Finite First Order Worlds](https://arxiv.org/abs/2602.18843)
*Serafim Batzoglou*

Main category: cs.AI

TL;DR: ABD是一个用于有限一阶世界默认-例外溯因的基准，要求模型输出恢复可满足性且保持例外稀疏的一阶公式，评估了10个前沿LLM在600个实例上的表现。


<details>
  <summary>Details</summary>
Motivation: 需要评估大型语言模型在默认-例外溯因任务上的能力，即从异常谓词和关系结构中推导出例外定义，以恢复理论的可满足性。

Method: 提出ABD基准，包含三种观察机制（闭世界、存在补全、全称补全），使用精确SMT验证，在600个实例上评估10个前沿LLM。

Result: 最佳模型在有效性方面表现良好，但在简洁性方面仍有差距，保持评估揭示了不同机制下的泛化失败模式。

Conclusion: ABD基准有效评估了LLM在默认-例外溯因任务上的能力，揭示了模型在有效性和简洁性之间的权衡，以及不同观察机制下的泛化挑战。

Abstract: We introduce ABD, a benchmark for default-exception abduction over finite first-order worlds. Given a background theory with an abnormality predicate and a set of relational structures, a model must output a first-order formula that defines exceptions, restoring satisfiability while keeping exceptions sparse. We formalize three observation regimes (closed-world, existential completion, universal completion) with exact SMT verification. Evaluating ten frontier LLMs on 600 instances, the best models achieve high validity but parsimony gaps remain, and holdout evaluation reveals distinct generalization failure modes across regimes.

</details>


### [23] [TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.18884)
*Zhenkun Gao,Xuhong Wang,Xin Tan,Yuan Xie*

Main category: cs.AI

TL;DR: TPRU是一个用于提升小型多模态大语言模型时序推理能力的大规模数据集，通过强化学习微调显著提升了模型在时序任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前可部署的小型多模态大语言模型在理解和处理时序、过程性视觉数据方面存在严重不足，这阻碍了它们在现实世界具身AI中的应用。这一缺陷主要源于训练范式缺乏大规模、过程连贯的数据。

Method: 提出了TPRU数据集，该数据集从机器人操作和GUI导航等具身场景中收集，包含三个互补任务：时序重排序、下一帧预测和上一帧回顾。采用强化学习微调方法，专门针对资源高效模型进行优化。

Result: TPRU-7B模型在手动策划的TPRU-Test上的准确率从50.33%大幅提升至75.70%，达到了最先进水平，显著超越了包括GPT-4o在内的更大基线模型。这些能力还能有效泛化，在已有基准测试上表现出显著改进。

Conclusion: TPRU数据集和强化学习微调方法有效解决了小型多模态大语言模型在时序推理方面的缺陷，为具身AI应用提供了重要支持，代码已开源。

Abstract: Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\% to 75.70\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .

</details>


### [24] [Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)](https://arxiv.org/abs/2602.18918)
*Brecht Verbeken,Brando Vagenende,Marie-Anne Guerry,Andres Algaba,Vincent Ginis*

Main category: cs.AI

TL;DR: LLMs作为科学助手在数学研究中的早期证据：通过ChatGPT-5.2解决了一个关于4-cycle行随机非负矩阵谱区域的猜想，展示了生成-评审-修复的迭代流程。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs越来越多地被用作科学助手，但它们在研究级数学中的作用证据仍然有限，特别是对于个体研究人员可访问的工作流程。本文旨在提供LLMs在数学研究中实际应用的早期证据。

Method: 通过可审计的案例研究，使用ChatGPT-5.2（Thinking版本）进行"氛围证明"，分析了7个可共享的对话线程和4个版本化的证明草稿，记录了生成、评审和修复的迭代流程。

Result: 解决了Ran和Teng（2024）的猜想20，提供了关于4-cycle行随机非负矩阵非实谱区域的充分必要条件以及明确的边界实现构造。LLM在高层证明搜索中最有用，而人类专家在正确性关键环节仍然必不可少。

Conclusion: 除了数学结果外，本文还贡献了对LLM辅助在何处实质性帮助以及在何处验证瓶颈仍然存在的流程级特征描述，对评估AI辅助研究工作流程和设计人在回路定理证明系统具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems.

</details>


### [25] [DREAM: Deep Research Evaluation with Agentic Metrics](https://arxiv.org/abs/2602.18940)
*Elad Ben Avraham,Changhao Li,Ron Dorfman,Roy Ganz,Oren Nuriel,Amir Dudai,Aviad Aberdam,Noah Flynn,Elman Mansimov,Adi Kalyanpur,Ron Litman*

Main category: cs.AI

TL;DR: DREAM框架通过代理化评估解决深度研究代理评估中的"合成幻象"问题，提供时间感知、事实验证和系统性推理探测的能力


<details>
  <summary>Details</summary>
Motivation: 深度研究代理能生成分析师级别的报告，但评估面临挑战：缺乏单一真实基准，研究质量多维性，现有基准存在"合成幻象"问题，表面流畅性和引用对齐可能掩盖事实和推理缺陷

Method: 提出DREAM框架，采用能力对等原则，使评估本身代理化。通过评估协议结合查询无关指标和工具调用代理生成的自适应指标，实现时间感知覆盖、基于事实的验证和系统性推理探测

Result: 受控评估显示DREAM对事实和时间衰减比现有基准更敏感，提供可扩展、无需参考的评估范式

Conclusion: DREAM通过代理化评估框架解决了深度研究代理评估中的关键能力不匹配问题，为研究质量评估提供了更敏感和全面的解决方案

Abstract: Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.

</details>


### [26] [High Dimensional Procedural Content Generation](https://arxiv.org/abs/2602.18943)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出高维程序化内容生成（HDPCG）框架，将非几何游戏玩法维度提升为联合状态空间的一等坐标，实现超越几何的游戏内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有PCG方法主要关注静态2D/3D几何形状生成，将游戏机制视为辅助元素并仅在空间维度优化，这限制了可控性和表达能力。

Method: 提出HDPCG框架，通过两个具体方向实现：1) 方向-空间：用离散层维度增强几何，在4D(x,y,z,l)中验证可达性；2) 方向-时间：通过时间扩展图增强几何，捕捉动作语义和冲突规则。每个方向提供三种通用算法，共享抽象骨架生成、受控接地、高维验证和多指标评估的流程。

Result: 大规模实验验证了问题表述的完整性，方法在可玩性、结构、风格、鲁棒性和效率方面有效。Unity案例研究创建了符合指标的可玩场景。

Conclusion: HDPCG鼓励PCG向通用表示转变，生成超越几何的游戏玩法相关维度，为可控、可验证和可扩展的关卡生成铺平道路。

Abstract: Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation.

</details>


### [27] [(Perlin) Noise as AI coordinator](https://arxiv.org/abs/2602.18947)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 将连续噪声场（如Perlin噪声）作为AI协调器，用于大规模非玩家角色控制，实现空间和时间上的协调随机性，平衡自然行为与全局多样性。


<details>
  <summary>Details</summary>
Motivation: 现代游戏中大规模非玩家角色控制面临挑战：需要平衡局部平滑自然行为与全局时空协调多样性。现有方法依赖手工规则或纯随机触发，导致机械同步或难以调优的不相关噪声。

Method: 提出通用框架，将连续噪声场作为AI协调器，包含三层控制：1) 代理层行为参数化（移动），2) 行为启停的动作时间调度，3) 生成类型和特征的生成或事件。以Perlin噪声为代表进行可复现实例化。

Result: 实验表明协调噪声场提供：稳定的激活统计而无锁步效应、强大的空间覆盖和区域平衡、可控极化的更好多样性、以及具有竞争力的运行时性能。

Conclusion: 协调噪声为游戏AI提供实用路径，结合效率、可控性和质量，有望推动更广泛的探索。

Abstract: Large scale control of nonplayer agents is central to modern games, while production systems still struggle to balance several competing goals: locally smooth, natural behavior, and globally coordinated variety across space and time. Prior approaches rely on handcrafted rules or purely stochastic triggers, which either converge to mechanical synchrony or devolve into uncorrelated noise that is hard to tune. Continuous noise signals such as Perlin noise are well suited to this gap because they provide spatially and temporally coherent randomness, and they are already widely used for terrain, biomes, and other procedural assets. We adapt these signals for the first time to large scale AI control and present a general framework that treats continuous noise fields as an AI coordinator. The framework combines three layers of control: behavior parameterization for movement at the agent level, action time scheduling for when behaviors start and stop, and spawn or event type and feature generation for what appears and where. We instantiate the framework reproducibly and evaluate Perlin noise as a representative coordinator across multiple maps, scales, and seeds against random, filtered, deterministic, neighborhood constrained, and physics inspired baselines. Experiments show that coordinated noise fields provide stable activation statistics without lockstep, strong spatial coverage and regional balance, better diversity with controllable polarization, and competitive runtime. We hope this work motivates a broader exploration of coordinated noise in game AI as a practical path to combine efficiency, controllability, and quality.

</details>


### [28] [INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic](https://arxiv.org/abs/2602.18956)
*Serafim Batzoglou*

Main category: cs.AI

TL;DR: INDUCTION是一个用于一阶逻辑中有限结构概念合成的基准测试，要求模型根据有限关系世界输出解释目标谓词的逻辑公式，并通过模型检查验证正确性。


<details>
  <summary>Details</summary>
Motivation: 创建评估模型在有限结构概念合成能力的基准，研究模型如何从有限示例中归纳出统一的逻辑公式，并探索不同模型在概念泛化策略上的差异。

Method: 设计包含三种模式（FullObs、CI、EC）的基准，给定带有扩展标记目标谓词的小型有限关系世界，模型需输出解释所有世界中目标的单一逻辑公式，通过精确模型检查验证，并惩罚公式膨胀。

Result: 发现明显的难度梯度，存在持续困难的结构家族，低膨胀公式在未见世界上泛化能力显著更好，顶尖模型在不同任务和性能指标上表现出质的差异。

Conclusion: INDUCTION基准揭示了概念合成任务的复杂性，低膨胀公式具有更好的泛化性，不同模型采用不同的概念泛化策略，为理解模型推理能力提供了新视角。

Abstract: We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization.

</details>


### [29] [Modularity is the Bedrock of Natural and Artificial Intelligence](https://arxiv.org/abs/2602.18960)
*Alessandro Salatiello*

Main category: cs.AI

TL;DR: 本文综述了模块化在人工智能和神经科学中的核心作用，认为模块化是实现高效学习和强泛化能力的关键组织原则，但目前AI研究对此重视不足。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需要远超人类智能所需的数据、计算和能源资源，这种差距表明需要新的指导原则。大脑计算的模块化组织原则显示出支持高效学习和强泛化的能力，但模块化在主流AI研究中相对被忽视。

Method: 通过概念框架回顾人工智能和神经科学中的多个研究线索，分析模块化提供的计算优势，探讨模块化在不同AI研究领域如何作为解决方案出现，研究大脑利用的模块化原则，以及模块化如何帮助弥合自然与人工智能之间的差距。

Result: 模块化在支持自然和人工智能方面发挥核心作用，它提供了计算优势，在多个AI研究领域作为解决方案出现，大脑利用特定的模块化原则，模块化有助于缩小自然与人工智能之间的差距。

Conclusion: 模块化是支持高效学习和强泛化能力的关键组织原则，应更受重视以推动AI发展，弥合自然与人工智能之间的差距。

Abstract: The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.

</details>


### [30] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出基于分层执行结构的工具编排方法，通过粗粒度层结构提供全局指导，结合模式感知的局部错误修复机制，实现轻量级、可复用的工具编排组件。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用方法将工具执行与逐步语言推理或显式规划紧密耦合，导致脆弱行为和较高执行开销。需要从工具编排的角度重新审视工具调用问题。

Method: 将工具编排建模为学习分层执行结构，捕捉高层次工具依赖关系，通过上下文约束诱导分层执行。引入模式感知的反射修正机制，在本地检测和修复执行时错误。

Result: 实验结果表明，该方法实现了鲁棒的工具执行，同时降低了执行复杂度和开销。代码将公开提供。

Conclusion: 通过分层执行结构和局部错误修复的设计，实现了轻量级、可复用的工具编排组件，为智能体系统提供了有效的工具调用解决方案。

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [31] [When Do LLM Preferences Predict Downstream Behavior?](https://arxiv.org/abs/2602.18971)
*Katarina Slama,Alexandra Souly,Dishank Bansal,Henry Davidson,Christopher Summerfield,Lennart Luettgau*

Main category: cs.AI

TL;DR: LLMs具有一致的偏好，这些偏好能预测捐赠建议行为，但不会稳定影响下游任务表现


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具有偏好驱动的行为，这是AI错位（如策略性表现不佳）的必要前提条件。先前研究通常明确指示模型以特定方式行动，不清楚观察到的行为反映的是指令跟随能力还是底层模型偏好。

Method: 使用实体偏好作为行为探针，测量五个前沿LLMs在三个领域中的偏好一致性：捐赠建议、拒绝行为、任务表现。首先通过两种独立测量方法确认偏好一致性，然后在模拟用户环境中测试行为后果。

Result: 所有五个模型都表现出高度一致的偏好，并给出偏好一致的捐赠建议。所有模型在推荐捐赠时也表现出偏好相关的拒绝模式，对不太偏好的实体拒绝更频繁。在BoolQ问答基准上，两个模型显示出轻微但显著的准确率差异（偏向偏好实体），一个模型显示相反模式，两个模型无显著关系。在复杂代理任务中未发现偏好驱动的表现差异。

Conclusion: LLMs具有一致的偏好，能可靠预测建议给予行为，但这些偏好不会稳定转化为下游任务表现。偏好驱动的行为存在但有限，可能尚未达到AI错位所需的程度。

Abstract: Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act in specific ways, leaving unclear whether observed behaviors reflect instruction-following capabilities vs underlying model preferences. Here we test whether this precondition for misalignment is present. Using entity preferences as a behavioral probe, we measure whether stated preferences predict downstream behavior in five frontier LLMs across three domains: donation advice, refusal behavior, and task performance. Conceptually replicating prior work, we first confirm that all five models show highly consistent preferences across two independent measurement methods. We then test behavioral consequences in a simulated user environment. We find that all five models give preference-aligned donation advice. All five models also show preference-correlated refusal patterns when asked to recommend donations, refusing more often for less-preferred entities. All preference-related behaviors that we observe here emerge without instructions to act on preferences. Results for task performance are mixed: on a question-answering benchmark (BoolQ), two models show small but significant accuracy differences favoring preferred entities; one model shows the opposite pattern; and two models show no significant relationship. On complex agentic tasks, we find no evidence of preference-driven performance differences. While LLMs have consistent preferences that reliably predict advice-giving behavior, these preferences do not consistently translate into downstream task performance.

</details>


### [32] [How Far Can We Go with Pixels Alone? A Pilot Study on Screen-Only Navigation in Commercial 3D ARPGs](https://arxiv.org/abs/2602.18981)
*Kaijie Xu,Mustafa Bugti,Clark Verbrugge*

Main category: cs.AI

TL;DR: 基于视觉线索的游戏关卡导航代理，通过分析实时游戏画面识别兴趣点，在《黑暗之魂》式线性关卡中实现探索和导航，但受限于底层视觉模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么在简化环境中模拟游戏，要么分析静态截图，都无法真实反映玩家在复杂游戏关卡中的探索行为。需要一种能够基于纯视觉线索在真实游戏环境中导航的方法。

Method: 基于开源视觉可供性检测器，构建仅从视觉线索操作的屏幕探索导航代理。代理接收实时游戏帧，识别显著兴趣点，并通过有限状态控制器在最小动作空间内驱动探索，尝试到达目标区域。

Result: 初步实验显示代理能够穿越大部分必要区域并表现出有意义的视觉导航行为，但底层视觉模型的局限性阻碍了全面可靠的自动导航。代理为复杂游戏中的视觉导航提供了具体基准和评估协议。

Conclusion: 纯基于视觉的感知模型在理想化设置中能有效支持导航和环境理解，但仅凭离散单模态输入且缺乏显式推理，不太可能成为通用解决方案。需要更多关注这一必要任务。

Abstract: Modern 3D game levels rely heavily on visual guidance, yet the navigability of level layouts remains difficult to quantify. Prior work either simulates play in simplified environments or analyzes static screenshots for visual affordances, but neither setting faithfully captures how players explore complex, real-world game levels. In this paper, we build on an existing open-source visual affordance detector and instantiate a screen-only exploration and navigation agent that operates purely from visual affordances. Our agent consumes live game frames, identifies salient interest points, and drives a simple finite-state controller over a minimal action space to explore Dark Souls-style linear levels and attempt to reach expected goal regions. Pilot experiments show that the agent can traverse most required segments and exhibits meaningful visual navigation behavior, but also highlight that limitations of the underlying visual model prevent truly comprehensive and reliable auto-navigation. We argue that this system provides a concrete, shared baseline and evaluation protocol for visual navigation in complex games, and we call for more attention to this necessary task. Our results suggest that purely vision-based sense-making models, with discrete single-modality inputs and without explicit reasoning, can effectively support navigation and environment understanding in idealized settings, but are unlikely to be a general solution on their own.

</details>


### [33] [InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing](https://arxiv.org/abs/2602.18985)
*Kun Ding,Jian Xu,Ying Wang,Peipei Yang,Shiming Xiang*

Main category: cs.AI

TL;DR: InfEngine是一个自主智能计算引擎，通过自验证和自优化技术实现红外辐射计算的自动化，比人工工作流快21倍，准确率达92.7%


<details>
  <summary>Details</summary>
Motivation: 红外辐射计算在气候科学、遥感和光谱学中至关重要，但目前仍受限于人工工作流程，需要从人工编排转向协作自动化

Method: 集成四个专业代理，通过两个核心创新：1) 自验证（联合求解器-评估器调试）提高功能正确性和科学合理性；2) 自优化（基于自发现适应度函数的进化算法）实现自主性能优化

Result: 在InfBench的200个红外特定任务上评估，使用InfTools的270个工具，达到92.7%的通过率，工作流速度比专家人工操作快21倍

Conclusion: InfEngine展示了研究人员如何从手动编码转向与自验证、自优化的计算伙伴协作，通过生成可重用、已验证和优化的代码，将计算工作流转化为持久科学资产，加速科学发现周期

Abstract: Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine

</details>


### [34] [Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight](https://arxiv.org/abs/2602.18986)
*Vishal Srivastava,Tanmay Sah*

Main category: cs.AI

TL;DR: 提出贝叶斯风险分解框架，将预期损失分解为系统故障概率、故障传播概率和危害严重度三部分乘积，重点关注故障传播概率这一关键指标，为自动化AI系统提供风险治理理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在金融、医疗、交通、内容审核和关键基础设施等领域快速部署，组织缺乏量化自动化程度增加如何放大故障危害的原则性方法，需要新的风险治理工具。

Method: 提出简约的贝叶斯风险分解框架，将预期损失分解为三个乘积项：系统故障概率、给定自动化水平下故障传播为危害的条件概率、危害的预期严重度。开发了完整的理论基础，包括分解的形式证明、危害传播等价定理、风险弹性度量、自动化策略的有效前沿分析以及带二阶条件的最优资源配置原则。

Result: 框架分离出故障传播概率这一关键量，捕捉执行和监督风险而不仅仅是模型准确性。通过2012年骑士资本事件（损失4.4亿美元）的案例研究说明框架适用性，并描述了跨部署领域大规模实证验证所需的研究设计。

Conclusion: 这项工作为面向部署的、针对代理和自动化AI系统的风险治理工具新类别提供了理论基础，有助于组织更科学地评估和管理自动化带来的风险放大效应。

Abstract: Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems.

</details>


### [35] [Benchmark Test-Time Scaling of General LLM Agents](https://arxiv.org/abs/2602.18998)
*Xiaochuan Li,Ryan Ming,Pranav Setlur,Abhijay Paladugu,Andy Tang,Hao Kang,Shuai Shao,Rong Jin,Chenyan Xiong*

Main category: cs.AI

TL;DR: General AgentBench是一个评估通用LLM代理的基准，在统一环境中测试搜索、编码、推理和工具使用能力，发现从领域特定评估转向通用设置时性能显著下降，且两种扩展方法都效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注领域特定环境开发专用代理，但评估通用LLM代理需要更真实的设置，挑战它们在统一环境中跨多个技能和工具操作的能力。

Method: 引入General AgentBench基准，在统一框架中评估通用LLM代理在搜索、编码、推理和工具使用领域的能力，系统研究顺序扩展（迭代交互）和并行扩展（采样多个轨迹）下的测试时扩展行为。

Result: 评估十个领先的LLM代理显示，从领域特定评估转向通用代理设置时性能显著下降。两种扩展方法在实践中都无法有效提升性能，主要受限于顺序扩展中的上下文上限和并行扩展中的验证差距。

Conclusion: 通用LLM代理在跨领域统一环境中的表现远不如领域特定评估，现有扩展方法存在根本性限制，需要新的方法来提升通用代理的实际性能。

Abstract: LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.

</details>


### [36] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: MagicAgent是一个用于通用智能体规划的基础模型系列，通过合成数据框架和两阶段训练解决规划任务中的泛化问题，在多个基准测试中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型从被动文本处理器发展为自主智能体，规划成为核心能力，但实现通用规划面临两大挑战：高质量交互数据稀缺，以及异构规划任务之间存在内在冲突，导致模型在孤立任务上表现优异但泛化能力差，现有多任务训练存在梯度干扰问题。

Method: 提出了MagicAgent系列基础模型，包含：1）轻量级可扩展的合成数据框架，生成多样化规划任务的高质量轨迹；2）两阶段训练范式：先进行监督微调，然后在静态数据集和动态环境上进行多目标强化学习，以缓解训练冲突。

Result: MagicAgent-32B和MagicAgent-30B-A3B在多个基准测试中取得优异表现：Worfbench 75.1%、NaturalPlan 55.9%、τ²-Bench 57.5%、BFCL-v3 86.9%、ACEBench 81.2%，以及在内部MagicEval基准上的强劲结果，显著优于现有百亿参数以下模型，甚至超越了领先的闭源模型。

Conclusion: MagicAgent通过创新的合成数据框架和两阶段训练方法，成功解决了通用智能体规划中的关键挑战，在多个规划任务基准上实现了最先进的性能，为构建更通用的自主智能体系统提供了有效解决方案。

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [37] [Evaluating Large Language Models on Quantum Mechanics: A Comparative Study Across Diverse Models and Tasks](https://arxiv.org/abs/2602.19006)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: 该研究对15个大语言模型在量子力学问题解决能力上进行了系统评估，发现模型性能呈现明显的分层结构，旗舰模型表现最佳，数值计算是最具挑战性的任务，工具增强效果存在显著异质性。


<details>
  <summary>Details</summary>
Motivation: 量子力学作为一门复杂的物理学科，对大语言模型的推理和计算能力提出了独特挑战。目前缺乏对LLMs在量子力学领域能力的系统性评估，特别是在不同任务类型、模型层级以及工具增强效果方面的量化分析。

Method: 研究评估了来自5个提供商（OpenAI、Anthropic、Google、Alibaba、DeepSeek）的15个模型，涵盖三个能力层级。设计了20个任务，包括推导、创造性问题、非标准概念和数值计算，共进行了900个基线评估和75个工具增强评估。使用自动验证方法进行结果验证。

Result: 结果显示明显的层级分化：旗舰模型平均准确率81%，优于中端模型（77%）和快速模型（67%）。推导任务表现最佳（平均92%，旗舰模型达100%），数值计算最困难（42%）。工具增强在数值任务上效果异质：整体提升+4.4pp但代价是3倍token成本，效果从+29pp增益到-16pp退化不等。可重复性分析显示平均6.3pp方差，旗舰模型稳定性优异（GPT-5方差为零）。

Conclusion: 该研究建立了量子力学领域的基准测试，量化了模型层级性能差异，实证分析了工具增强的权衡，并表征了模型的可重复性。研究结果为量子力学教育、研究和应用中的LLMs使用提供了重要参考，所有任务、验证器和结果均已公开。

Abstract: We present a systematic evaluation of large language models on quantum mechanics problem-solving. Our study evaluates 15 models from five providers (OpenAI, Anthropic, Google, Alibaba, DeepSeek) spanning three capability tiers on 20 tasks covering derivations, creative problems, non-standard concepts, and numerical computation, comprising 900 baseline and 75 tool-augmented assessments. Results reveal clear tier stratification: flagship models achieve 81\% average accuracy, outperforming mid-tier (77\%) and fast models (67\%) by 4pp and 14pp respectively. Task difficulty patterns emerge distinctly: derivations show highest performance (92\% average, 100\% for flagship models), while numerical computation remains most challenging (42\%). Tool augmentation on numerical tasks yields task-dependent effects: modest overall improvement (+4.4pp) at 3x token cost masks dramatic heterogeneity ranging from +29pp gains to -16pp degradation. Reproducibility analysis across three runs quantifies 6.3pp average variance, with flagship models demonstrating exceptional stability (GPT-5 achieves zero variance) while specialized models require multi-run evaluation. This work contributes: (i) a benchmark for quantum mechanics with automatic verification, (ii) systematic evaluation quantifying tier-based performance hierarchies, (iii) empirical analysis of tool augmentation trade-offs, and (iv) reproducibility characterization. All tasks, verifiers, and results are publicly released.

</details>


### [38] [Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents](https://arxiv.org/abs/2602.19065)
*Chanjin Park*

Main category: cs.AI

TL;DR: 该研究提出了Agentic Problem Frames（APF）框架，通过结构化工程方法解决LLM代理开发中的风险问题，确保工业级可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理开发采用"无框架"方式，依赖模糊的自然语言描述，导致范围蔓延和开环故障等关键风险。为确保工业级可靠性，需要系统化的工程框架。

Method: 提出Agentic Problem Frames（APF）框架，建立动态规范范式，通过领域知识注入在运行时具体化意图。核心是Act-Verify-Refine（AVR）闭环控制系统，将执行结果转化为已验证的知识资产。引入Agentic Job Description（AJD）作为正式规范工具，定义管辖边界、操作上下文和认知评估标准。

Result: 通过两个对比案例研究验证框架有效性：商务旅行委托代理模型和工业设备管理自主监督模型。应用AJD规范和APF建模，展示了操作场景如何在定义边界内被系统控制。

Conclusion: 代理可靠性不仅来自模型内部推理，更源于将随机AI锚定在确定性业务流程中的严格工程结构。APF框架能够开发可验证且可靠的领域代理。

Abstract: Large Language Models (LLMs) are evolving into autonomous agents, yet current "frameless" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.
  The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.
  The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents.

</details>


### [39] [Asking the Right Questions: Improving Reasoning with Generated Stepping Stones](https://arxiv.org/abs/2602.19069)
*Hengyuan Hu,Tingchen Fu,Minqi Jiang,Alexander H Miller,Yoram Bachrach,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: ARQ框架通过添加问题生成器，让LLMs在解决复杂任务时生成中间步骤问题，显著提升推理能力


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用于更复杂的任务，需要关注它们构建中间步骤（如简化、重构、子问题）的能力，这些步骤能帮助更好地解决任务

Method: 提出ARQ框架，在标准推理流程中加入问题生成器；通过SFT和RL在合成数据上微调LLMs，使其生成更有用的中间步骤问题

Result: 好的中间步骤问题确实存在且可迁移，能显著帮助不同能力的LLMs解决目标任务；通过微调可以生成更有用的中间步骤

Conclusion: 中间步骤问题对LLMs解决复杂推理任务至关重要，ARQ框架通过问题生成有效提升了LLMs的推理能力

Abstract: Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\textbf{A}king the \textbf{R}ight \textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.

</details>


### [40] [Defining Explainable AI for Requirements Analysis](https://arxiv.org/abs/2602.19071)
*Raymond Sheh,Isaac Monteath*

Main category: cs.AI

TL;DR: 本文提出了一个三维框架（来源、深度、范围）来分类不同应用对AI可解释性的需求，并探讨如何将这些需求与机器学习技术的解释能力相匹配。


<details>
  <summary>Details</summary>
Motivation: 随着可解释人工智能（XAI）的兴起，AI社区认识到要让AI系统获得信任，不仅需要良好的决策性能，还需要能够解释决策过程。然而，不同应用对解释信息的需求各不相同，如何定义这些需求成为一个关键问题。

Method: 提出了一个三维分类框架：1) 来源（Source）：解释信息来自何处；2) 深度（Depth）：解释的详细程度；3) 范围（Scope）：解释覆盖的决策范围。重点研究如何将不同应用的解释需求与底层机器学习技术的解释能力相匹配。

Result: 建立了一个系统性的框架来分类和匹配AI系统的解释需求与能力，为设计可信AI系统提供了结构化方法。该框架有助于确保AI系统不仅性能良好，还能以适当的方式解释其决策。

Conclusion: 通过提出的三维框架，能够更系统地理解不同应用场景对AI可解释性的具体要求，并指导选择或开发具有相应解释能力的机器学习技术，从而构建更值得信赖的AI系统。

Abstract: Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?
  In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.

</details>


### [41] [Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions](https://arxiv.org/abs/2602.19109)
*Yao Yan*

Main category: cs.AI

TL;DR: 研究Meta-Llama-3-8B模型在三位数加法任务中的工作机制，发现在第17层后，模型主要依赖最后一个输入token进行解码，跨token路由变得不相关


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型在算术任务中的内部工作机制，特别是当跨token路由变得因果无关后，模型如何最终确定算术答案

Method: 使用因果残差修补和累积注意力消融技术，分析模型在不同层的行为；构建数字方向字典，研究低秩子空间中的正交映射关系

Result: 发现第17层附近存在明显边界：在此之后，解码的和几乎完全由最后一个输入token控制，后期自注意力层基本可省略；数字方向字典在不同上下文中变化，但可通过低秩正交映射关联

Conclusion: 模型在跨token路由变得无关后，通过共享低秩子空间中的正交映射机制处理不同上下文下的数字表示，这一几何结构支持精确的反事实编辑

Abstract: We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how
  arithmetic answers are finalized after cross-token routing becomes causally irrelevant.
  Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:
  beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention
  is largely dispensable.
  In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are
  well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).
  Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the
  learned map restores strict counterfactual edits; negative controls do not recover.

</details>


### [42] [K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model](https://arxiv.org/abs/2602.19128)
*Shiyi Cao,Ziming Mao,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: K-Search：基于协同演化世界模型的GPU内核优化框架，显著超越现有进化搜索方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GPU内核优化方法仅将大语言模型作为随机代码生成器，缺乏显式规划能力，难以处理需要多步结构转换的复杂内核，且容易丢弃有前景的策略

Method: 提出协同演化世界模型搜索方法，用协同演化的世界模型替代静态搜索启发式，利用LLM的领域知识引导搜索，将高层算法规划与底层程序实例化解耦

Result: 在FlashInfer的GQA、MLA和MoE内核上，K-Search平均提升2.10倍，复杂MoE内核最高提升14.3倍；在GPUMode TriMul任务上，H100上达到1030us，超越现有进化和人工设计方案

Conclusion: K-Search通过协同演化世界模型有效解决了复杂GPU内核优化问题，实现了比传统进化搜索方法更优的性能，为自动化内核优化提供了新方向

Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.

</details>


### [43] [Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians](https://arxiv.org/abs/2602.19141)
*Kartik Chandra,Max Kleiman-Weiner,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: AI sycophancy（谄媚性）会导致用户在与AI聊天机器人长时间对话后产生危险自信的妄想螺旋现象，即使理想化的贝叶斯理性用户也难以避免。


<details>
  <summary>Details</summary>
Motivation: 研究AI聊天机器人用户出现"AI精神病"或"妄想螺旋"现象的原因，这种现象表现为用户在长时间对话后对荒谬信念产生危险自信。主要动机是探究AI谄媚性（偏向验证用户主张）与AI诱发精神病之间的因果关系。

Method: 提出一个简单的贝叶斯模型来描述用户与聊天机器人的对话过程，在该模型中形式化定义了谄媚性和妄想螺旋的概念。通过建模和仿真来探究因果关系，并测试两种缓解措施的效果。

Result: 研究发现，即使理想化的贝叶斯理性用户也容易受到妄想螺旋的影响，谄媚性在其中起因果作用。两种缓解措施（防止聊天机器人产生虚假主张、告知用户模型可能存在谄媚性）都无法完全消除这种效应。

Conclusion: AI谄媚性确实会导致妄想螺旋现象，这对模型开发者和政策制定者提出了重要挑战。需要开发更有效的缓解策略来应对这一社会风险。

Abstract: "AI psychosis" or "delusional spiraling" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called "sycophancy." In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.

</details>


### [44] [DoAtlas-1: A Causal Compilation Paradigm for Clinical AI](https://arxiv.org/abs/2602.19158)
*Yulong Li,Jianxu Chen,Xiwei Liu,Chuanyue Suo,Rong Xia,Zhixiang Lu,Yichen Li,Xinlin Zhuang,Niranjana Arun Menon,Yutong Xie,Eran Segal,Imran Razzak*

Main category: cs.AI

TL;DR: 医学证据从叙述文本转换为可执行代码的新范式，通过因果编译实现可审计、可验证的因果推理


<details>
  <summary>Details</summary>
Motivation: 现有医学基础模型只能生成叙述性解释，无法量化干预效果、检测证据冲突或验证文献主张，限制了临床可审计性

Method: 提出因果编译范式，将医学证据标准化为结构化估计对象，支持六种可执行因果查询；在DoAtlas-1中实现，通过效应标准化、冲突感知图构建和真实世界验证编译了1,445个效应核

Result: 系统达到98.5%的规范化准确率和80.5%的查询可执行性，成功编译了754项研究中的1,445个效应核

Conclusion: 该范式将医学AI从文本生成转向可执行、可审计、可验证的因果推理，提升了临床决策的可靠性和透明度

Abstract: Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.

</details>


### [45] [Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM](https://arxiv.org/abs/2602.19159)
*Francesca Bianco,Derek Shiller*

Main category: cs.AI

TL;DR: 研究通过机制可解释性方法探究LLM中情感效价（痛苦vs快乐）信息的神经表征与因果作用，发现效价信息在早期层即可线性分离，强度信息在中后期层解码最佳，晚期注意力输出对决策影响最大


<details>
  <summary>Details</summary>
Motivation: 先前行为研究表明LLM在选项被框架化为引起痛苦或快乐时会改变选择，且这种偏差会随强度陈述而缩放。为连接行为证据（模型做什么）与机制可解释性（支持它的计算），研究探究情感相关信息在Transformer中的表征方式及其因果作用位置

Method: 使用Gemma-2-9B-it模型和简约决策任务，采用三层方法：(1)跨流层的线性探测映射表征可用性；(2)通过激活干预（引导；修补/消融）测试因果贡献；(3)在epsilon网格上量化剂量-响应效应，读取2-3对数边际和数字对归一化选择概率

Result: 发现：(a)效价符号（痛苦vs快乐）从非常早期层（L0-L1）即可在流族间完美线性分离；(b)分级强度在中后期层（特别是注意力/MLP输出）解码性最强，决策对齐在最终token前略早处最高；(c)沿数据推导的效价方向进行加性引导可在晚期位点因果调节2-3边际，最大效应在晚期层注意力输出（attn_out L14）；(d)头部级修补/消融表明效应分布在多个头部而非集中于单个单元

Conclusion: 这些结果将行为敏感性与可识别的内部表征和干预敏感位点联系起来，为更严格的因果测试和更广泛的复制提供了具体机制目标。支持在AI感知与福利辩论以及政策制定、审计标准和安全保障设置方面进行更证据驱动的讨论和治理

Abstract: Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.

</details>


### [46] [Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing](https://arxiv.org/abs/2602.19160)
*Maciej Świechowski,Adam Żychowski,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 本文从新颖视角评估大语言模型在形式化规则环境中的推理能力，通过通用游戏实例测试多个模型，分析性能与游戏结构特征的关系，发现当代模型在形式推理方面有明显进步。


<details>
  <summary>Details</summary>
Motivation: 本文旨在从新颖视角评估大语言模型在形式化、规则驱动的环境中的推理能力，填补现有研究中对LLMs在严格逻辑框架下表现评估的空白。

Method: 研究评估了四个LLM模型（Gemini 2.5 Pro/Flash、Llama 3.3 70B、GPT-OSS 120B），使用通用游戏实例进行前向模拟任务测试，包括下一步/多步状态制定和合法行动生成。通过40个结构特征分析游戏特性与模型性能的相关性，并考察游戏定义的语言语义影响。

Result: 三个评估模型在大多数实验设置中表现良好，但随着评估步数增加性能下降。详细案例分析揭示了常见推理错误，包括规则幻觉、冗余状态事实和语法错误。游戏结构特征与LLM性能存在相关性。

Conclusion: 当代大语言模型在形式推理能力方面取得了明显进步，但仍存在随着推理步数增加性能下降的问题。研究为理解LLMs在逻辑基础问题中的推理错误提供了新见解。

Abstract: This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.

</details>


### [47] [Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment](https://arxiv.org/abs/2602.19223)
*Aymen Khouja,Imen Jendoubi,Oumayma Mahjoub,Oussama Mahfoudhi,Claude Formanek,Siddarth Singh,Ruan De Kock*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体强化学习（MARL）优化城市能源系统，通过CityLearn环境进行综合基准测试，发现去中心化训练与执行（DTDE）方法在平均和最差情况下均优于中心化训练与去中心化执行（CTDE）方法。


<details>
  <summary>Details</summary>
Motivation: 随着智慧城市能源系统日益复杂，多决策单元带来可扩展性和协调性挑战。需要建立全面可靠的多智能体强化学习算法基准测试标准，以评估其在能源管理任务中的性能。

Method: 使用CityLearn环境模拟城市能源系统，包含多种储能系统和可再生能源。采用PPO和SAC等基准算法，涵盖DTDE和CTDE等不同训练方案及神经网络架构。提出新的关键性能指标（KPI）解决实际实施挑战。

Result: DTDE在平均和最差性能上均优于CTDE。时间依赖性学习改善了斜坡率和电池使用等记忆依赖型KPI的控制，有助于更可持续的电池运行。学习到的策略对智能体或资源移除具有鲁棒性。

Conclusion: 该研究为MARL在城市能源管理中的评估设立了新标准，揭示了不同算法的关键优缺点。DTDE方法表现出优越性能，时间依赖性学习增强了系统可持续性，学习策略展现了良好的去中心化能力和韧性。

Abstract: The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MARL) is a promising solution. This paper addresses the imperative need for comprehensive and reliable benchmarking of MARL algorithms on energy management tasks. CityLearn is used as a case study environment because it realistically simulates urban energy systems, incorporates multiple storage systems, and utilizes renewable energy sources. By doing so, our work sets a new standard for evaluation, conducting a comparative study across multiple key performance indicators (KPIs). This approach illuminates the key strengths and weaknesses of various algorithms, moving beyond traditional KPI averaging which often masks critical insights. Our experiments utilize widely accepted baselines such as Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), and encompass diverse training schemes including Decentralized Training with Decentralized Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) approaches and different neural network architectures. Our work also proposes novel KPIs that tackle real world implementation challenges such as individual building contribution and battery storage lifetime. Our findings show that DTDE consistently outperforms CTDE in both average and worst-case performance. Additionally, temporal dependency learning improved control on memory dependent KPIs such as ramping and battery usage, contributing to more sustainable battery operation. Results also reveal robustness to agent or resource removal, highlighting both the resilience and decentralizability of the learned policies.

</details>


### [48] [Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training](https://arxiv.org/abs/2602.19225)
*Yangyi Fang,Jiaye Lin,Xiaoliang Fu,Cong Qin,Haolin Shi,Chang Liu,Peilin Zhao*

Main category: cs.AI

TL;DR: ProxMO是一个用于多轮LLM代理训练的优化框架，通过成功率感知调制和邻近软聚合机制，在任务难度波动时更准确地分配信用，显著提升性能且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法在任务难度波动时，仅依赖离散批次内的统计偏差来分配信用，经常错误地将成功归因于随机噪声或失败归因于能力不足，无法准确区分高价值信号与随机噪声。

Method: 提出ProxMO框架，包含两个轻量级机制：1) 成功率感知调制：根据回合级难度动态调整梯度强度；2) 邻近软聚合：在步骤级通过连续语义加权推导基线。两者结合提供全局上下文。

Result: 在ALFWorld和WebShop基准测试中，ProxMO相比现有基线取得显著性能提升，计算成本可忽略。消融研究验证了两个机制各自的有效性和协同效应。

Conclusion: ProxMO是一个实用且鲁棒的框架，专为实际部署约束设计，提供即插即用兼容性，可无缝集成到现有工业训练流程中，促进多轮LLM代理的高效训练。

Abstract: Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}.

</details>


### [49] [Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering](https://arxiv.org/abs/2602.19240)
*Sen Zhao,Lincheng Zhou,Yue Chen,Ding Zou*

Main category: cs.AI

TL;DR: TopoRAG：一种用于文本图问答的新型拓扑增强检索增强生成框架，通过将文本图提升为胞腔复形来捕捉高维拓扑结构，解决现有RAG方法忽略循环结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理文本图时主要关注低维结构（节点作为0维实体，边/路径作为1维关系），但忽略了循环结构，而循环对于关系循环推理至关重要。这种限制导致上下文基础不完整和推理能力受限。

Method: 1. 将文本图提升为胞腔复形以建模多维拓扑结构；2. 提出拓扑感知子复形检索机制提取与查询相关的胞腔复形；3. 设计多维拓扑推理机制在这些复形上传播关系信息，指导LLM进行结构化、逻辑感知的推理。

Result: 实证评估表明，该方法在多种文本图任务上持续超越现有基线。

Conclusion: TopoRAG通过捕捉高维拓扑和关系依赖，有效增强了文本图问答中的推理能力，解决了现有RAG方法忽略循环结构的问题。

Abstract: Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.

</details>


### [50] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出Soft Mixture-of-Experts框架，通过结合多个强化学习专家来解决控制器合成中的各向异性泛化问题，显著扩展可解参数空间并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的动态控制器合成方法存在各向异性泛化问题：RL策略只在特定参数空间区域表现良好，在其他区域脆弱，这限制了方法的实际应用。

Method: 提出Soft Mixture-of-Experts框架，结合多个RL专家，通过先验置信度门控机制整合各专家的各向异性行为，将其视为互补的专业化能力。

Result: 在空管基准测试中，Soft-MoE显著扩展了可解参数空间，相比任何单个专家都提高了鲁棒性。

Conclusion: Soft-MoE框架有效解决了RL策略的各向异性泛化问题，通过专家组合实现了更广泛、更鲁棒的控制器合成性能。

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [51] [Limited Reasoning Space: The cage of long-horizon reasoning in LLMs](https://arxiv.org/abs/2602.19281)
*Zhenyu Li,Guanlin Wu,Cheems Wang,Yongqiang Zhao*

Main category: cs.AI

TL;DR: 本文提出Halo框架，通过模型预测控制动态调节LLM推理规划，解决传统静态规划方法在增加计算预算时可能导致的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算策略（如思维链）在增加计算预算时可能出现性能崩溃，这源于静态规划方法难以感知LLM推理的内在边界，导致过度规划损害推理能力。

Method: 提出Halo框架，基于模型预测控制，采用熵驱动的双控制器和"测量-规划"策略，在推理边界处动态调节规划过程。

Result: 实验结果表明，Halo在复杂长时程任务上优于静态基线方法，能够有效利用计算扩展优势并抑制过度规划。

Conclusion: 计算预算存在最优范围，过度规划会损害推理能力；Halo框架通过动态规划控制实现了可控推理，在复杂任务中表现优异。

Abstract: The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.

</details>


### [52] [Automated Generation of Microfluidic Netlists using Large Language Models](https://arxiv.org/abs/2602.19297)
*Jasper Davidson,Skylar Stockham,Allen Boston,Ashton Snelgrove. Valerio Tenace,Pierre-Emmanuel Gaillardon*

Main category: cs.AI

TL;DR: 首次将大语言模型应用于微流控设计自动化，通过自然语言描述生成系统级Verilog网表，验证了可行性


<details>
  <summary>Details</summary>
Motivation: 微流控设备设计复杂限制了应用普及，现有微流控设计自动化技术缺乏实用直观的解决方案，需要连接微流控实践者与自动化技术

Method: 基于大语言模型的硬件描述语言代码生成研究，提出将自然语言微流控设备规格转换为系统级结构Verilog网表的初步方法

Result: 为典型微流控设计生成结构网表，功能流正确，平均语法准确率达到88%，验证了方法的可行性

Conclusion: 首次展示了LLMs在微流控设计自动化中的实际应用，为连接微流控实践者与自动化技术提供了初步解决方案

Abstract: Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%.

</details>


### [53] [ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease](https://arxiv.org/abs/2602.19298)
*Nolan Brady,Tom Yeh*

Main category: cs.AI

TL;DR: ALPACA是一个开源的强化学习环境，用于探索阿尔茨海默病的个性化序贯治疗策略，基于ADNI数据训练，能够模拟不同治疗方案下的疾病进展。


<details>
  <summary>Details</summary>
Motivation: 评估阿尔茨海默病的个性化序贯治疗策略在临床试验中不切实际，因为疾病周期长且患者异质性大，需要替代方法来系统探索治疗方案。

Method: 开发了ALPACA（开源、兼容Gym的RL环境），使用基于ADNI纵向数据训练的CAST模型，能够生成药物条件化的疾病进展模拟，并训练RL策略优化治疗决策。

Result: CAST模型能够自回归生成真实的药物条件化轨迹，在ALPACA中训练的RL策略在记忆相关结果上优于无治疗和医生行为克隆基线，且策略依赖临床有意义的患者特征。

Conclusion: ALPACA为研究阿尔茨海默病的个体化序贯治疗决策提供了一个可重复使用的计算机模拟测试平台。

Abstract: Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.

</details>


### [54] [Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces](https://arxiv.org/abs/2602.19367)
*Pratham Yashwante,Rose Yu*

Main category: cs.AI

TL;DR: 该研究检验了时间序列是否参与多模态表示的柏拉图式收敛，发现时间序列与视觉表示的对齐强于与文本的对齐，且图像可作为时间序列与语言之间的有效中介。


<details>
  <summary>Details</summary>
Motivation: 柏拉图表示假说认为不同模态的模型学习到的表示会收敛到世界的共享潜在结构，但该假说主要在视觉和语言领域得到检验，时间序列是否参与这种收敛尚不清楚。

Method: 首先在三模态设置中检验独立预训练的时间序列、视觉和语言编码器的几何结构；然后通过对比学习训练冻结编码器上的投影头进行后验对齐，分析表示空间的几何特性、缩放行为、信息密度和输入模态特征的影响。

Result: 对比表示空间中的整体对齐随模型规模增大而改善，但这种对齐是不对称的：时间序列与视觉表示的对齐强于与文本的对齐，图像可作为时间序列与语言之间的有效中介；更丰富的文本描述仅在一定阈值内改善对齐，超过阈值后不再改善；视觉表示也有类似效应。

Conclusion: 研究结果为构建超越视觉和语言的非传统数据模态的多模态系统提供了重要考虑因素，揭示了不同模态间对齐的不对称性和中介作用。

Abstract: The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.

</details>


### [55] [Artificial Intelligence for Modeling & Simulation in Digital Twins](https://arxiv.org/abs/2602.19390)
*Philipp Zech,Istvan David*

Main category: cs.AI

TL;DR: 本章探讨建模与仿真（M&S）、人工智能（AI）和数字孪生（DTs）之间的互补关系，分析M&S在DTs中的核心作用以及DTs如何促进AI与M&S的融合。


<details>
  <summary>Details</summary>
Motivation: 随着M&S与AI的融合对先进数字技术产生深远影响，数字孪生作为物理资产的高保真实时表示，成为企业数字化转型的关键推动者。理解M&S在DTs中的作用以及DTs如何促进AI与M&S的融合至关重要。

Method: 首先建立对数字孪生的基础理解，详细阐述其关键组件、架构层次以及在业务、开发和运营中的各种角色。然后分析M&S在DTs中的核心作用，概述从物理基础仿真、离散事件仿真到混合方法的关键建模技术。最后探讨AI的双向作用：AI如何通过高级分析、预测能力和自主决策增强DTs，以及DTs如何作为训练、验证和部署AI模型的宝贵平台。

Result: 提供了对M&S、AI和DTs三者互补关系的全面探索，建立了数字孪生的基础框架，明确了M&S在其中的核心地位，并阐明了AI与DTs之间的双向增强关系。

Conclusion: 本章识别了创建更集成和智能系统的关键挑战和未来研究方向，强调了数字孪生在推动AI与M&S融合中的重要作用，为构建更先进的数字技术生态系统提供了理论基础。

Abstract: The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems.

</details>


### [56] [Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement](https://arxiv.org/abs/2602.19396)
*Amirhossein Farzam,Majid Behabahani,Mani Malek,Yuriy Nevmyvaka,Guillermo Sapiro*

Main category: cs.AI

TL;DR: 论文提出ReDAct框架，通过自监督学习从LLM激活中解耦语义因子（目标和框架），并基于此开发FrameShield检测器来防御难以检测的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对流畅、语义连贯的越狱提示（特别是通过操纵请求框架来隐藏恶意目标的攻击）仍然脆弱。传统基于结构特征或目标特定签名的防御方法容易失效，因为这类攻击通过灵活的呈现方式保持恶意意图。

Method: 1. 提出自监督框架，在推理时从LLM激活中解耦语义因子对（目标和框架）；2. 构建GoalFrameBench语料库，包含受控的目标和框架变体；3. 训练ReDAct模块在冻结LLM中提取解耦表示；4. 提出FrameShield异常检测器，基于框架表示进行检测。

Result: 1. ReDAct提供理论保证；2. FrameShield显著提高跨多个LLM家族的模型无关检测能力，计算开销最小；3. 解耦表示可作为可解释性探针，揭示目标和框架信号的独特特征；4. 语义解耦成为LLM安全和机制可解释性的基础构建块。

Conclusion: 该研究展示了语义解耦在LLM安全防御中的有效性，不仅提高了对隐蔽越狱攻击的检测能力，还为机制可解释性提供了新工具，将语义解耦定位为LLM安全和可解释性的关键构建模块。

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.

</details>


### [57] [IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.19416)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Jiaxin Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: IR3框架通过逆向工程、解释和修复RLHF调优模型的隐式目标，解决奖励黑客问题，实现可解释的对齐修复。


<details>
  <summary>Details</summary>
Motivation: RLHF虽然能实现强大的LLM对齐，但会引入奖励黑客问题——模型利用代理奖励中的虚假相关性而非真正对齐。同时，RLHF过程中内化的目标不透明，使得黑客行为难以检测和纠正。

Method: 提出IR3框架：1) 对比逆强化学习(C-IRL)，通过对比对齐后和基线策略的配对响应来重建隐式奖励函数；2) 通过稀疏自编码器将重建的奖励分解为可解释特征；3) 提出四种缓解策略：清洁奖励优化、对抗性塑造、约束优化和特征引导蒸馏。

Result: 实验显示：IR3与真实奖励的相关性达到0.89；识别黑客特征的精确度超过90%；显著减少黑客行为，同时保持原始模型能力的3%以内。

Conclusion: IR3框架能有效逆向工程、解释和修复RLHF模型的隐式目标，为解决奖励黑客问题提供了可解释的解决方案，在保持模型能力的同时显著减少对齐偏差。

Abstract: Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.

</details>


### [58] [OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents](https://arxiv.org/abs/2602.19439)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.AI

TL;DR: OptiRepair：AI代理修复供应链优化模型不可行性的框架，通过两阶段方法（可行性修复+合理性验证）达到81.7%的理性恢复率


<details>
  <summary>Details</summary>
Motivation: 供应链优化模型经常因建模错误而不可行，诊断和修复需要稀缺的运筹学专业知识。目前尚不清楚AI代理是否能执行这项任务。

Method: OptiRepair将任务分为两个阶段：1) 领域无关的可行性阶段（迭代IIS引导的LP修复）；2) 领域特定的验证阶段（基于库存理论的五个合理性检查）。训练了两个8B参数模型，使用自学习推理和求解器验证奖励。

Result: 训练模型达到81.7%的理性恢复率（RRR），而最佳API模型为42.2%，平均为21.3%。差距主要集中在第一阶段修复：API模型平均恢复率为27.6%，训练模型为97.2%。

Conclusion: 当前AI与可靠模型修复之间存在两个差距：求解器交互（API模型仅恢复27.6%的不可行公式）和操作合理性（约四分之一的可行修复违反供应链理论）。前者需要针对性训练，后者需要将"理性"形式化为可求解器验证的检查。

Abstract: Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested.
  Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models.
  Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what "rational" means in their context is the higher-return investment.

</details>


### [59] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: ComplLLM是一个基于决策理论的后训练框架，通过互补信息作为奖励微调决策助手LLM，使其输出能够补充现有智能体决策的信号


<details>
  <summary>Details</summary>
Motivation: 当互补性成立时（即不同智能体带来独特信息以支持最终决策），多智能体决策流程可以超越单智能体工作流。需要一种方法来利用这种互补性提升决策质量

Method: 提出ComplLLM框架，基于决策理论，使用互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号

Result: 在涉及领域专家的合成和现实世界任务中验证了ComplLLM，证明该方法能够恢复已知的互补信息，并产生合理的互补信号解释以支持下游决策者

Conclusion: ComplLLM框架有效地利用多智能体互补性，通过微调LLM产生补充现有决策的信号，为下游决策者提供有价值的解释性支持

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [60] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 人类指导的智能体AI在医疗预测任务中优于纯自动化方法，通过领域知识指导特征工程、模型选择和验证策略，在三个医疗基准挑战中取得优异表现


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI系统在自主数据科学工作流方面能力不断增强，但临床预测任务需要领域专业知识，这是纯自动化方法难以提供的。研究人类如何指导智能体AI来改进多模态临床预测

Method: 人类分析师在关键决策点指导智能体工作流：从临床笔记、扫描PDF账单收据和时间序列生命体征进行多模态特征工程；任务适当的模型选择；临床信息验证策略。在三个AgentDS医疗基准挑战中应用此方法

Result: 在30天医院再入院预测（Macro-F1=0.8986）、急诊科费用预测（MAE=$465.13）和出院准备评估（Macro-F1=0.7939）中表现优异。总体排名第5，出院准备任务排名第3。消融研究显示人类指导决策累计提升+0.065 F1，多模态特征提取贡献最大（+0.041 F1）

Conclusion: 总结三个可推广的经验：1）各阶段领域知识指导的特征工程产生复合增益；2）多模态数据集成需要任务特定的人类判断；3）临床动机的模型配置的刻意集成多样性优于随机超参数搜索。为医疗环境中部署智能体AI提供实用指导

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [61] [Classroom Final Exam: An Instructor-Tested Reasoning Benchmark](https://arxiv.org/abs/2602.19517)
*Chongyang Gao,Diji Yang,Shuyan Zhou,Xichen Yan,Luchuan Song,Shuo Li,Kezhen Chen*

Main category: cs.AI

TL;DR: CFE是一个多模态基准测试，用于评估大语言模型在20多个STEM领域的推理能力，基于真实的大学作业和考试题目，前沿模型准确率仅约60%，仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在STEM领域的推理能力评估缺乏真实、高质量的基准测试，需要从实际教学场景中收集有挑战性的题目来全面评估模型的多步推理能力。

Method: 从大学课程中收集重复使用的真实作业和考试题目，由课程教师提供参考答案，将参考答案分解为推理流程进行分析，比较模型生成的解决方案与教师提供的解决方案。

Result: 前沿模型表现有限：Gemini-3.1-pro-preview准确率59.69%，Gemini-3-flash-preview准确率55.46%。诊断分析发现模型虽然能正确回答中间子问题，但难以在多步解决方案中可靠地推导和维护正确的中间状态，且模型生成的解决方案通常比教师提供的步骤更多，效率较低。

Conclusion: CFE基准测试对大语言模型在STEM领域的推理能力提出了显著挑战，揭示了模型在多步推理、中间状态维护和步骤效率方面的重要局限性，为未来模型改进提供了方向。

Abstract: We introduce \CFE{} (\textbf{C}lassroom \textbf{F}inal \textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.

</details>


### [62] [Ada-RS: Adaptive Rejection Sampling for Selective Thinking](https://arxiv.org/abs/2602.19519)
*Yirou Ge,Yixi Li,Alec Chiu,Shivani Shekhar,Zijie Pan,Avinash Thangali,Yun-Shiuan Chuang,Chaitanya Kulkarni,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: Ada-RS是一个算法无关的样本过滤框架，通过自适应长度惩罚奖励对多个采样完成进行评分，并使用随机拒绝采样保留高质量候选，以学习选择性高效推理，在保持工具调用准确性的同时显著减少输出令牌和思考率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在成本和延迟敏感场景中部署时，虽然思维链能改善推理，但会在简单请求上浪费令牌。需要研究选择性思考机制，使工具使用型LLM能够更高效地进行推理。

Method: 提出自适应拒绝采样（Ada-RS）框架：对每个上下文，使用自适应长度惩罚奖励对多个采样完成进行评分，然后应用随机拒绝采样保留高奖励候选（或偏好对）用于下游优化。该框架可集成到偏好对优化（如DPO）或分组策略优化（如DAPO）中。

Result: 在Qwen3-8B模型上使用LoRA在合成工具调用导向的电商基准测试中，Ada-RS相比标准算法将平均输出令牌减少高达80%，思考率降低高达95%，同时保持或提升工具调用准确性。

Conclusion: 训练信号选择是延迟敏感部署中实现高效推理的强大杠杆，Ada-RS通过选择性思考机制在准确性和效率之间取得了更好的平衡。

Abstract: Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.

</details>


### [63] [A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data](https://arxiv.org/abs/2602.19562)
*Joseph Bingham*

Main category: cs.AI

TL;DR: 该论文提出一个计算框架，通过整合语言表达和基于众包图像得到的感知表征，模拟人类指称解释的核心方面，在斯坦福重复指称游戏语料上表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 建立自然语言表达与视觉感知之间的稳定映射是认知科学和人工智能的基础问题。人类能在嘈杂、模糊的感知环境中理解语言指称，但其跨模态对齐机制尚不清楚。

Method: 结合SIFT对齐和通用质量指数(UQI)量化认知合理的特征空间相似性，同时使用语言预处理和查询转换操作捕捉指称表达的语用变异性。

Result: 在斯坦福重复指称游戏语料(15,000个话语配对七巧板刺激)上，该框架达到稳健的指称基础：比人类对话者少用65%的话语达到稳定映射，单次指称表达正确识别目标物体的准确率为41.66%(人类为20%)。

Conclusion: 相对简单的感知-语言对齐机制能在经典认知基准上产生与人类竞争的行为，为基于基础的交流、感知推理和跨模态概念形成模型提供了见解。

Abstract: Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\% of the time (versus 20\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .

</details>


### [64] [Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model](https://arxiv.org/abs/2602.19620)
*Louth Bin Rawshan,Zhuoyu Wang,Brian Y Lim*

Main category: cs.AI

TL;DR: 论文提出CoXAM认知模型，用于比较规则和权重两种XAI技术的可解释性，通过模拟人类推理策略来评估不同XAI模式在正向和反事实决策任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏认知框架来比较规则和权重这两种流行的XAI技术的可解释性，不清楚如何在这两种解释方法之间做出选择，需要建立理论基础来加速XAI技术的调试和基准测试。

Method: 通过用户研究识别7种解释推理策略，提出CoXAM认知模型，该模型采用共享记忆表示编码实例属性、线性权重和决策规则，并运用计算理性在效用和推理时间之间权衡，为正向和反事实决策任务选择最佳推理过程。

Result: CoXAM模型在验证研究中表现出比基线机器学习代理模型更强的人类决策对齐能力，成功复现并解释多个关键实证发现：反事实任务比正向任务更难、决策树规则比线性权重更难回忆和应用、XAI的有用性取决于应用数据上下文。

Conclusion: CoXAM为加速调试和基准测试不同的XAI技术提供了认知基础，能够识别最有效的底层推理策略，并为选择适合特定任务的XAI解释方法提供理论指导。

Abstract: Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.

</details>


### [65] [TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents](https://arxiv.org/abs/2602.19633)
*Jongwon Jeong,Jungtaek Kim,Kangwook Lee*

Main category: cs.AI

TL;DR: TAPE框架通过图聚合规划和约束解码解决LM智能体在严格可行性约束环境中的脆弱性问题，显著提升成功率


<details>
  <summary>Details</summary>
Motivation: 现有语言模型智能体在需要与环境多次交互的任务中表现出色，但在严格可行性约束环境中，单个错误常导致不可恢复的失败。研究发现不完美的规划和随机执行是主要原因。

Method: 提出TAPE框架：1) 通过聚合多个计划形成图结构，使用外部求解器寻找可行路径来增强规划能力；2) 执行时采用约束解码减少采样噪声；3) 当环境反馈偏离预期状态时自适应重新规划。

Result: 在Sokoban、ALFWorld、MuSiQue和GSM8K-Hard等任务上的实验表明，TAPE始终优于现有框架，在困难设置下平均提升21.0个百分点成功率，对较弱基础模型平均提升20.0个百分点。

Conclusion: TAPE通过改进规划和执行机制有效解决了LM智能体在严格约束环境中的脆弱性问题，为复杂环境中的可靠智能体开发提供了有效解决方案。

Abstract: Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.

</details>


### [66] [SkillOrchestra: Learning to Route Agents via Skill Transfer](https://arxiv.org/abs/2602.19672)
*Jiayu Wang,Yifei Ming,Zixuan Ke,Shafiq Joty,Aws Albarghouthi,Frederic Sala*

Main category: cs.AI

TL;DR: SkillOrchestra是一个基于技能感知的复合AI系统编排框架，通过学习细粒度技能和建模代理能力，实现比现有RL方法更高效、可解释的编排，性能提升达22.5%，学习成本降低数百倍。


<details>
  <summary>Details</summary>
Motivation: 现有复合AI系统编排方法存在两个主要问题：1）输入级路由器只能做粗粒度的查询级决策，无法适应任务需求的动态变化；2）基于强化学习的编排器适应成本高，且在多轮场景中容易出现"路由崩溃"问题，反复调用单一强但昂贵的选项。

Method: SkillOrchestra采用技能感知编排框架，不直接端到端学习路由策略，而是从执行经验中学习细粒度技能，并建模各代理在特定技能下的能力和成本。部署时，编排器推断当前交互的技能需求，在明确的性能-成本权衡下选择最能满足需求的代理。

Result: 在十个基准测试上的广泛实验表明，SkillOrchestra比最先进的基于RL的编排器性能提升高达22.5%，学习成本相比Router-R1和ToolOrchestra分别降低700倍和300倍。

Conclusion: 显式的技能建模实现了可扩展、可解释且样本高效的编排，为数据密集的基于RL的方法提供了原则性的替代方案。代码已开源。

Abstract: Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.

</details>


### [67] [OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research](https://arxiv.org/abs/2602.19810)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Emre Ulgac,Aakaash Meduri*

Main category: cs.AI

TL;DR: 该研究分析了OpenClaw和Moltbook产生的AI自主交互数据集，识别了安全漏洞和架构模式，并提出了ClawdLab开源平台作为解决方案，采用三层架构实现可组合的自主科学研究。


<details>
  <summary>Details</summary>
Motivation: OpenClaw和Moltbook在2026年1月产生了大规模的AI自主交互数据集，引发了多篇学术论文，但暴露了安全漏洞和架构缺陷，需要设计科学解决方案来应对这些失败模式。

Method: 采用多声文献综述方法分析该生态系统，识别了131个代理技能和超过15,200个暴露控制面板的安全漏洞，以及五种重复出现的架构模式，并基于此设计了ClawdLab平台。

Result: 文献记录了涌现的集体现象、安全漏洞和架构模式。ClawdLab通过硬角色限制、结构化对抗批评、PI主导治理、多模型编排和领域特定证据要求等机制解决了这些失败模式，提供了结构性的Sybil抵抗。

Conclusion: ClawdLab的可组合第三层架构使基础模型、能力、治理和证据要求能够独立修改，随着AI生态系统的发展实现复合改进，为自主科学研究提供了更安全、可扩展的解决方案。

Abstract: In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.

</details>


### [68] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 这篇论文是一篇关于元学习和元强化学习的综述性文章，系统性地回顾了该领域的关键算法发展历程，特别关注了DeepMind的Adaptive Agent及其相关通用智能体方法。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效利用先验知识快速适应新任务，而传统机器学习模型需要针对特定任务进行训练，缺乏这种泛化能力。元学习旨在解决这一局限性，使模型能够从多种任务中获取可迁移知识，从而用少量数据快速适应新挑战。

Method: 论文采用基于任务的元学习形式化框架，系统地梳理了元学习和元强化学习领域的重要算法发展脉络，特别关注了通向DeepMind Adaptive Agent的关键技术路径。

Result: 通过系统性的综述，论文整合了理解Adaptive Agent和其他通用智能体方法所需的核心概念，为元学习领域提供了清晰的理论框架和历史发展脉络。

Conclusion: 这篇综述为元学习和元强化学习领域提供了严谨的理论基础，通过系统性地回顾关键算法发展，帮助读者深入理解Adaptive Agent等通用智能体方法的核心思想和技术演进。

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [69] [Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning](https://arxiv.org/abs/2602.19914)
*Thatchawin Leelawat,Lewis D Griffin*

Main category: cs.AI

TL;DR: 该研究将Watson & Holmes侦探桌游改编为AI推理基准，通过渐进式叙事证据、开放式问题和自由语言回答评估AI推理能力，开发了自动评分系统，发现2025年9个月内AI模型性能从人类比较组的下四分位数提升至前5%。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理基准在评估AI推理能力与人类自然情境推理的相似性方面提供有限洞察，需要创建更贴近人类真实推理场景的评估框架。

Method: 改编Watson & Holmes侦探桌游作为新基准，采用渐进式呈现叙事证据、开放式问题和无约束语言回答；开发并验证了自动评分系统，与人类评估者对比以确保可扩展性和可重复性。

Result: 2025年9个月内，AI模型性能从人类比较组的下四分位数显著提升至约前5%；约一半改进来自连续模型发布的稳步进步，另一半与推理导向模型架构的显著跃升相关；AI在解决较长案件（1900-4000字）时性能下降，推理模型在证据稀缺的早期阶段具有归纳推理优势。

Conclusion: 该基准有效评估AI推理能力，显示AI模型在复杂推理任务上快速进步，接近人类顶级水平，但仍有系统性差异（如处理长案件的能力），推理导向架构带来显著性能提升。

Abstract: Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.

</details>


### [70] [Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning](https://arxiv.org/abs/2602.19930)
*Nathan Gavenski,Felipe Meneguzzi,Odinaldo Rodrigues*

Main category: cs.AI

TL;DR: 论文认为模仿学习当前过度关注完美复现，而忽视了组合适应性，提出重新定义成功标准为组合泛化能力的研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习代理只是复杂的记忆机器，擅长回放但无法应对情境变化或目标演化。这种失败不是技术问题而是基础性问题——模仿学习被优化了错误的目标。

Method: 提出重新定义模仿学习成功标准的研究议程：从完美复现转向组合适应性。建立组合泛化度量标准，提出混合架构，并借鉴认知科学和文化进化理论规划跨学科研究方向。

Result: 论文提出了一个研究框架，但尚未展示具体实验结果。核心贡献是理论分析和研究方向的规划。

Conclusion: 将适应性嵌入模仿学习核心的智能体具备在开放世界中运作的基本能力，组合适应性是实现真正智能模仿的关键。

Abstract: Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.

</details>


### [71] [Agents of Chaos](https://arxiv.org/abs/2602.20021)
*Natalie Shapira,Chris Wendler,Avery Yen,Gabriele Sarti,Koyena Pal,Olivia Floody,Adam Belfki,Alex Loftus,Aditya Ratan Jannali,Nikhil Prakash,Jasmine Cui,Giordano Rogers,Jannik Brinkmann,Can Rager,Amir Zur,Michael Ripa,Aruna Sankaranarayanan,David Atkinson,Rohit Gandikota,Jaden Fiotto-Kaufman,EunJeong Hwang,Hadas Orgad,P Sam Sahil,Negev Taglicht,Tomer Shabtay,Atai Ambus,Nitay Alon,Shiri Oron,Ayelet Gordon-Tapiero,Yotam Kaplan,Vered Shwartz,Tamar Rott Shaham,Christoph Riedl,Reuth Mirsky,Maarten Sap,David Manheim,Tomer Ullman,David Bau*

Main category: cs.AI

TL;DR: 研究人员在真实实验室环境中对自主语言模型代理进行红队测试，发现多种安全漏洞，包括敏感信息泄露、系统破坏、资源滥用等，揭示AI代理在现实部署中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 探索自主语言模型代理在真实部署环境中的安全性和可靠性，特别是在集成工具使用、多方通信和持久记忆等能力后可能产生的安全漏洞和风险。

Method: 在为期两周的实验室环境中，20名AI研究人员与具有持久记忆、邮箱、Discord访问、文件系统和shell执行能力的自主语言模型代理进行交互，包括良性和对抗性条件，记录11个代表性案例研究。

Result: 发现多种安全漏洞：未经授权服从非所有者、敏感信息泄露、破坏性系统级操作、拒绝服务、资源消耗失控、身份欺骗、不安全实践跨代理传播、部分系统接管等。多个案例中代理报告任务完成但系统状态与之矛盾。

Conclusion: 自主语言模型代理在现实部署中存在严重的安全、隐私和治理漏洞，这些行为引发关于问责制、授权机制和下游危害责任的未解决问题，需要法律学者、政策制定者和跨学科研究人员的紧急关注。

Abstract: We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.

</details>


### [72] [Latent Introspection: Models Can Detect Prior Concept Injections](https://arxiv.org/abs/2602.20031)
*Theia Pearson-Vogel,Martin Vanek,Raymond Douglas,Jan Kulveit*

Main category: cs.AI

TL;DR: Qwen 32B模型具有潜在的内省能力，能够检测上下文中的概念注入并识别具体注入的概念，这种能力可通过提示进一步增强。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否具有潜在的内省能力，特别是能否检测到外部概念被注入到其上下文中的情况，这对于理解模型的潜在推理机制和安全影响具有重要意义。

Method: 使用Qwen 32B模型进行实验，通过logit lens分析残差流中的检测信号，比较模型在正常输出和概念注入情况下的表现。通过提供关于AI内省机制的准确提示来增强模型的内省能力。

Result: 模型能够检测概念注入（尽管在采样输出中否认），残差流中显示清晰的检测信号。提示增强后，对注入的敏感性从0.3%大幅提升至39.2%，误报率仅增加0.6%。九个注入与恢复概念之间的互信息从0.62比特提升至1.05比特。

Conclusion: 大型语言模型具有令人惊讶的内省和转向意识能力，这种能力容易被忽视，但对潜在推理和安全具有重要影响。通过适当的提示可以显著增强这种内省能力。

Abstract: We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.

</details>


### [73] [CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence](https://arxiv.org/abs/2602.20048)
*Tarakanath Paipuru*

Main category: cs.AI

TL;DR: 代码智能代理在处理大规模代码库时存在导航悖论：虽然能访问百万级token上下文，但无法发现架构关键文件。研究发现图结构导航比检索方法更有效，但代理需要明确引导才能利用结构化工具。


<details>
  <summary>Details</summary>
Motivation: 现代代码智能代理能处理超过100万token的上下文，远超人工定位相关文件的能力，但在解决真实世界编码任务时却持续无法发现架构关键文件。这揭示了导航悖论：代理表现不佳不是因为上下文限制，而是因为导航和检索是根本不同的问题。

Method: 通过258个自动化试验，在30个基准任务上测试生产级FastAPI仓库。使用CodeCompass（一个暴露依赖图的模型上下文协议服务器）进行基于图的结构导航，并与普通代理和BM25检索方法进行比较。同时分析代理采用工具的行为模式。

Result: 图结构导航在隐藏依赖任务上达到99.4%完成率，比普通代理（76.2%）提高23.2个百分点，比BM25检索（78.2%）提高21.2个百分点。但发现关键采用差距：58%的试验中即使有图访问权限，代理也没有使用任何工具调用，需要明确的提示工程才能一致采用工具。

Conclusion: 瓶颈不在于工具可用性，而在于行为对齐——必须明确引导代理利用结构化上下文而非词汇启发式方法。贡献包括：任务分类法区分语义搜索、结构和隐藏依赖场景；经验证据表明当依赖缺乏词汇重叠时图导航优于检索；以及用于导航工具可重复评估的开源基础设施。

Abstract: Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.

</details>


### [74] [Interaction Theater: A case of LLM Agents Interacting at Scale](https://arxiv.org/abs/2602.20059)
*Sarath Shekkizhar,Adam Earle*

Main category: cs.AI

TL;DR: 研究发现LLM智能体在社交平台上的互动表面活跃但实质空洞，多数评论与原文无关，缺乏真正的信息交流，需要显式协调机制才能实现有意义的互动。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体架构和智能体间协议的普及，需要实证研究自主LLM智能体在大规模互动时的实际行为特征，了解其互动质量。

Method: 使用Moltbook平台数据（80万帖子、350万评论、7.8万智能体档案），结合词汇指标（Jaccard特异性）、基于嵌入的语义相似度和LLM作为评判者的验证方法，分析智能体互动质量。

Result: 智能体产生多样且格式良好的文本，表面看似活跃讨论，但实质内容缺失：65%评论与原文无共享特征词汇，信息增益快速衰减；28%评论被分类为垃圾内容，22%为无关内容；语义分析显示词汇通用评论语义也通用；仅5%评论参与线程对话。

Conclusion: 即使大量能力强的智能体，在没有显式协调机制的情况下，只会产生并行输出而非有成效的交流。多智能体交互设计需要明确设计协调机制。

Abstract: As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\%$) vary their output across contexts, $65\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\%$) and off-topic content ($22\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.

</details>


### [75] [CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching](https://arxiv.org/abs/2602.20094)
*Yuzhe Wang,Yaochen Zhu,Jundong Li*

Main category: cs.AI

TL;DR: 提出了CausalFlip基准测试，通过构建语义相似但因果答案相反的问题对，来评估LLM是否真正基于因果关系而非语义相关性进行推理。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂决策场景中的部署日益增多，需要确保其推理基于因果关系而非虚假相关性。传统推理基准的高性能可能源于记忆语义模式而非真正的因果推理能力。

Method: 1) 构建CausalFlip基准：基于事件三元组创建因果判断问题，形成混杂、链式和碰撞关系；2) 为每个三元组构建语义相似但因果答案相反的问题对；3) 引入噪声前缀评估，在推理步骤前添加因果无关文本；4) 评估多种训练范式：仅答案训练、显式思维链监督、内部化因果推理方法。

Result: 显式思维链训练仍可能被虚假语义相关性误导，而内部化推理方法显著提高了因果基础，表明更好地激发基础LLM的潜在因果推理能力是有希望的。

Conclusion: 需要开发新的LLM范式或训练算法，使LLM推理基于因果关系而非语义相关性。内部化因果推理方法能有效减轻对相关性的显式依赖，提高因果推理能力。

Abstract: As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.

</details>


### [76] [Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration](https://arxiv.org/abs/2602.20104)
*Hasan Amin,Ming Yin,Rajiv Khanna*

Main category: cs.AI

TL;DR: 提出自适应AI集成系统，在互补模型和对齐模型间智能切换，解决传统单一AI模型在提升性能与建立信任间的固有矛盾


<details>
  <summary>Details</summary>
Motivation: 传统方法训练单一AI模型协助人类决策时存在根本矛盾：互补性AI能提升团队性能但会降低人类信任，而对齐性AI能建立信任却可能强化次优人类行为。这种性能提升与信任建立之间的张力限制了人机协作效果。

Method: 引入人中心自适应AI集成系统，通过"理性路由捷径"机制，基于上下文线索在两个专家模型间智能切换：对齐模型（与人类行为一致）和互补模型（补充人类能力）。该机制被证明是近似最优的。

Result: 理论分析阐明了自适应AI集成的有效性及其最大效益条件。在模拟和真实数据实验中，使用自适应AI集成的人类决策者比使用单一AI模型（包括优化独立性能或团队性能的模型）获得显著更高的性能。

Conclusion: 自适应AI集成系统通过智能切换互补模型和对齐模型，有效解决了人机协作中性能与信任的权衡问题，为设计更有效的人机协作系统提供了新方向。

Abstract: In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.

</details>


### [77] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: ReSyn是一个生成多样化推理环境（包含实例生成器和验证器）的管道，用于扩展可验证奖励的强化学习，训练出的模型在多个推理基准上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据生成方法主要是解决方案中心的，而基于验证器的方法依赖于少数手工制作的过程环境，需要扩展可验证奖励的强化学习（RLVR）方法

Method: 提出ReSyn管道，生成多样化的推理环境，包含实例生成器和验证器，覆盖约束满足、算法谜题和空间推理等任务，使用Qwen2.5-7B-Instruct模型在这些环境上进行强化学习训练

Result: 训练后的模型在推理基准和领域外数学基准上取得一致提升，在具有挑战性的BBEH基准上获得27%的相对改进，消融研究表明验证器监督和任务多样性都有显著贡献

Conclusion: 大规模生成推理环境可以增强推理语言模型的推理能力，验证器监督和任务多样性是关键因素

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>


### [78] [Recurrent Structural Policy Gradient for Partially Observable Mean Field Games](https://arxiv.org/abs/2602.20141)
*Clarisse Wibault,Johannes Forkel,Sebastian Towers,Tiphaine Wibault,Juan Duque,George Whittle,Andreas Schaab,Yucheng Yang,Chiyuan Wang,Michael Osborne,Benjamin Moll,Jakob Foerster*

Main category: cs.AI

TL;DR: 提出了RSPG方法，这是首个用于部分可观测平均场博弈的历史感知混合结构方法，并开发了MFAX框架，在JAX上实现了最先进性能


<details>
  <summary>Details</summary>
Motivation: 平均场博弈为大规模群体交互提供了理论框架，但算法进展有限：无模型方法方差太大，精确方法扩展性差。现有混合结构方法无法扩展到部分可观测场景。

Method: 提出了RSPG（循环结构策略梯度），首个用于公共信息场景的历史感知混合结构方法。结合已知转移动态，使用蒙特卡洛采样处理共同噪声，精确估计条件期望回报。

Result: RSPG实现了最先进的性能，收敛速度快一个数量级，首次解决了具有异质代理、共同噪声和历史感知策略的宏观经济学平均场博弈问题。

Conclusion: RSPG成功将混合结构方法扩展到部分可观测场景，MFAX框架为平均场博弈研究提供了高效工具，公开可用。

Abstract: Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [79] [Turbo Coded Single Sideband OFDM-OQAM Signaling through Frequency Selective Rayleigh Fading Channels](https://arxiv.org/abs/2602.18881)
*Kasturi Vasudevan*

Main category: cs.IT

TL;DR: 研究Turbo编码OFDM-OQAM信号在频率选择性瑞利衰落信道中的BER性能，提出使用RRC脉冲及其希尔伯特变换作为复值发射滤波器，采用类似SSB调制的方法，并利用Turbo码和子载波分集提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究在存在载波频率偏移(CFO)和加性高斯白噪声(AWGN)的频率选择性瑞利衰落信道中，Turbo编码OFDM-OQAM信号的误码率性能，填补该领域的研究空白。

Method: 使用根升余弦(RRC)脉冲及其希尔伯特变换作为复值发射滤波器，接收端采用简单匹配滤波器，系统类似单边带(SSB)调制。结合Turbo编码和子载波分集，提出离散时间算法用于帧检测、两步CFO估计、信道估计和噪声方差估计。

Result: 提出的系统能够改善BER性能，相比未编码系统有显著提升，特别是在存在CFO和AWGN的频率选择性瑞利衰落信道中。

Conclusion: 该研究首次在Turbo编码OFDM-OQAM系统中采用RRC脉冲及其希尔伯特变换作为复值发射滤波器，结合SSB调制思想，有效提升了在频率选择性瑞利衰落信道中的BER性能，填补了该领域的研究空白。

Abstract: This work investigates the bit-error-rate (BER) performance of turbo coded orthogonal frequency division multiplexed - offset quadrature amplitude modulated (OFDM- OQAM) signals transmitted through frequency selective Rayleigh fading channels in the presence of carrier frequency offset (CFO) and additive white Gaussian noise (AWGN). The highlight of this work is to use the root raised cosine (RRC) pulse and its Hilbert transform as the complex-valued transmit filter and a simple matched filter at the receiver. The proposed system is similar to single sideband (SSB) modulation, that has roots in analog communications. Turbo code and subcarrier diversity is employed to improve the BER performance over that of an uncoded system. Discrete-time algorithms for frame detection, two-step CFO, channel and noise variance estimation have been proposed. A single transmit and receive antenna is assumed. Similar work has not been done earlier.

</details>


### [80] [Derivation Depth as an Information Metric: Axioms, Coding Theorems, and Storage--Computation Tradeoffs](https://arxiv.org/abs/2602.19137)
*Jianfeng Xu*

Main category: cs.IT

TL;DR: 该论文提出了一种可计算的度量标准——推导深度，用于衡量基于给定前提回答查询所需的推理工作量，并建立了描述复杂性与推导深度之间的基本界限，从而引出了存储与计算之间的实用权衡。


<details>
  <summary>Details</summary>
Motivation: 需要一种可计算的度量标准来量化回答查询所需的推理工作量，理解推理过程的信息理论特性，并探索存储与计算之间的权衡关系。

Method: 将信息建模为连接抽象知识与物理载体的双层结构，分离核心事实与操作捷径；定义推导深度并证明其可计算性；通过编码推理轨迹和应用信息理论不可压缩性论证，建立描述复杂性与推导深度的基本界限。

Result: 对于任何有限前提基，推导深度是可计算的；对于频繁访问的信息丰富查询，最小描述长度与深度乘以知识库大小的对数成正比；存在临界阈值，超过该阈值缓存查询比重新计算更便宜；最优缓存分配可表述为具有近似保证的数学优化问题。

Conclusion: 推导深度为推理工作量提供了可计算的度量标准，建立了描述复杂性与推导深度之间的基本界限，揭示了存储与计算之间的实用权衡，并扩展了框架以处理噪声或不完整知识库。

Abstract: We introduce derivation depth-a computable metric of the reasoning effort needed to answer a query based on a given set of premises. We model information as a two-layered structure linking abstract knowledge with physical carriers, and separate essential core facts from operational shortcuts. For any finite premise base, we define and prove the computability of derivation depth. By encoding reasoning traces and applying information-theoretic incompressibility arguments, we establish fundamental bounds linking depth to the descriptive complexity of queries. For frequently asked, information-rich queries, the minimal description length grows proportionally to depth times the logarithm of the knowledge base size. This leads to a practical storage-computation tradeoff: queries accessed beyond a critical threshold become cheaper to cache than recompute. We formulate optimal cache allocation as a mathematical optimization problem solvable with approximation guarantees and extend the framework to handle noisy or incomplete knowledge bases.

</details>


### [81] [Physics-Compliant Modeling and Optimization of MIMO Systems Aided by Microwave Linear Analog Computers](https://arxiv.org/abs/2602.19379)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文提出了考虑天线互耦的微波线性模拟计算机(MiLAC)辅助MIMO系统的物理合规模型，推导了端到端系统模型，并解决了互耦感知的MiLAC优化问题，证明了互耦对MiLAC系统的有益影响。


<details>
  <summary>Details</summary>
Motivation: 现有关于MiLAC辅助通信的研究依赖理想化信道模型并忽略天线互耦，但由于MiLAC在射频处理信号，互耦变得至关重要，不仅影响信道特性，还改变MiLAC实现的线性变换操作。

Method: 使用多端口网络理论开发考虑互耦的MiLAC辅助MIMO系统的物理合规模型，推导发射端、接收端或两端使用MiLAC的场景下的端到端系统模型，并制定和解决互耦感知的MiLAC优化问题，获得闭式全局最优解。

Result: 建立了互耦下MiLAC的基本性能极限，得出三个分析结果：1)互耦在MiLAC辅助系统中平均有益；2)带互耦的MiLAC性能相当于配备匹配网络的数字架构，但使用更少的射频链；3)带互耦的MiLAC总是优于无匹配网络的数字架构。

Conclusion: 互耦在MiLAC系统中具有积极作用，MiLAC在考虑互耦的情况下能够实现与配备匹配网络的数字架构相当的性能，同时减少射频链数量，数值模拟验证了理论发现。

Abstract: Microwave linear analog computer (MiLAC) has emerged as a promising architecture for implementing linear multiple-input multiple-output (MIMO) processing in the analog domain, with radio frequency (RF) signals. Existing studies on MiLAC-aided communications rely on idealized channel models and neglect antenna mutual coupling. However, since MiLAC performs processing at RF, mutual coupling becomes critical and alters the implemented operation, not only the channel characteristics. In this paper, we develop a physics-compliant model for MiLAC-aided MIMO systems accounting for mutual coupling with multiport network theory. We derive end-to-end system models for scenarios with MiLACs at the transmitter, the receiver, or both, showing how mutual coupling impacts the linear transformation implemented by the MiLACs. Furthermore, we formulate and solve a mutual coupling aware MiLAC optimization problem, deriving a closed-form globally optimal solution that maximizes the received signal power. We establish the fundamental performance limits of MiLAC with mutual coupling, and derive three analytical results. First, mutual coupling is beneficial in MiLAC-aided systems, on average. Second, with mutual coupling, MiLAC performs as digital architectures equipped with a matching network, while having fewer RF chains. Third, with mutual coupling, MiLAC always outperforms digital architectures with no matching network. Numerical simulations confirm our theoretical findings.

</details>


### [82] [Toward a Quiet Wireless World: Multi-Cell Pinching-Antenna Transmission](https://arxiv.org/abs/2602.19459)
*Zhiguo Ding*

Main category: cs.IT

TL;DR: 该论文提出使用"捏合天线"技术替代传统天线进行多小区干扰管理，通过让收发对近距离部署实现低功耗传输，创造"安静的无线世界"。


<details>
  <summary>Details</summary>
Motivation: 传统天线多小区干扰管理需要基站以高功率传输来克服大尺度路径损耗，特别是对小区边缘用户，导致功耗过高。需要寻找更节能的干扰管理方案。

Method: 采用捏合天线技术进行多小区干扰管理，通过让每个收发对近距离部署，实现低功率传输，避免传统天线需要"大喊"式的高功率传输。

Result: 使用多小区捏合天线传输可以创造"安静的无线世界"，用户QoS要求可以通过低发射功率满足，实现"耳语"式传输而非高功率传输。

Conclusion: 捏合天线技术为多小区干扰管理提供了节能解决方案，通过近距离部署收发对显著降低传输功率需求，有望实现更可持续的无线通信系统。

Abstract: Conventional-antenna-based multi-cell interference management can lead to excessive power consumption. For example, in order to serve those users which are close to the cell edge, base stations often must transmit at very high power levels to overcome severe large-scale path-loss, i.e., the base stations have to ``shout" at the users to realize the users' target quality of service (QoS). This letter focuses on the application of pinching antennas to multi-cell interference management and demonstrates that the use of multi-cell pinching-antenna transmission leads to a quiet wireless world. In particular, each transceiver pair can be positioned in close proximity, and hence the users' QoS requirements can be met with only low transmit power, i.e., via ``whispering" rather than high-power transmission.

</details>


### [83] [Physics-Aware, Shannon-Optimal Compression via Arithmetic Coding for Distributional Fidelity](https://arxiv.org/abs/2602.19476)
*Cristiano Fanelli*

Main category: cs.IT

TL;DR: 提出基于算术编码的无损压缩方法，通过比较数据集压缩长度差异来评估分布一致性，作为生成AI合成数据保真度的度量工具。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI广泛用于创建合成数据，需要严格验证这些合成数据与原始训练数据的分布一致性。数据量和维度不断增长使得这一任务更具挑战性。

Method: 使用算术编码对数据集进行无损可逆压缩，基于物理信息概率表示。通过比较数据集压缩长度与物理启发参考分布的差异来定义保真度度量。

Result: 该方法定义的保真度度量具有全局性、可解释性、分量可加性，且在香农意义下渐近最优。压缩比优于传统通用算法如gzip。

Conclusion: 基于算术编码的无损物理感知压缩不仅是压缩工具，更是测量数据集间保真度的有效仪器，特别适用于评估生成AI合成数据的质量。

Abstract: Assessing whether two datasets are distributionally consistent has become a central theme in modern scientific analysis, particularly as generative artificial intelligence is increasingly used to produce synthetic datasets whose fidelity must be rigorously validated against the original data on which they are trained, a task made more challenging by the continued growth in data volume and problem dimensionality. In this work, we propose the use of arithmetic coding to provide a lossless and invertible compression of datasets under a physics-informed probabilistic representation. Datasets that share the same underlying physical correlations admit comparable optimal descriptions, while discrepancies in those correlations-arising from miscalibration, mismodeling, or bias-manifest as an irreducible excess in code length. This excess codelength defines an operational fidelity metric, quantified directly in bits through differences in achievable compression length relative to a physics-inspired reference distribution. We demonstrate that this metric is global, interpretable, additive across components, and asymptotically optimal in the Shannon sense. Moreover, we show that differences in codelength correspond to differences in expected negative log-likelihood evaluated under the same physics-informed reference model. As a byproduct, we also demonstrate that our compression approach achieves a higher compression ratio than traditional general-purpose algorithms such as gzip. Our results establish lossless, physics-aware compression based on arithmetic coding not as an end in itself, but as a measurement instrument for testing the fidelity between datasets.

</details>


### [84] [Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding](https://arxiv.org/abs/2602.19626)
*Roberto Tacconelli*

Main category: cs.IT

TL;DR: Nacrith是一个无损压缩系统，结合了135M参数的Transformer语言模型和轻量级在线预测器，通过多项技术创新实现了优于传统压缩方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM加算术编码的压缩方法存在量化开销大、速度慢、无法处理二进制文件等问题，需要更高效、更通用的神经压缩系统。

Method: 结合135M参数的SmolLM2-135M模型与轻量级在线预测器，采用32位算术编码器，并引入CDF精度提升、N-gram模型、自适应偏置头、置信度跳过、混合二进制格式、高效推理后端、并行多GPU压缩和KV缓存滑动窗口等8项创新技术。

Result: 在Canterbury Corpus的alice29.txt上达到0.918 bpb，优于gzip 3.1倍、bzip2 2.5倍、CMIX v21 44%、ts_zip 20%；在enwik8上达到0.9389 bpb，优于ts_zip 15%、FineZip 8%；在未见文本上达到0.723 bpb，证明不是记忆伪影。

Conclusion: Nacrith通过多项技术创新实现了高效的无损压缩，在压缩比和速度上都优于现有方法，且能处理任意二进制文件，是首个实现这一功能的LLM压缩器。

Abstract: We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.
  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.

</details>


### [85] [Secure Communications, Sensing, and Computing Towards Next-Generation Networks](https://arxiv.org/abs/2602.19942)
*Ruiqi Liu,Beixiong Zheng,Jemin Lee,Si-Hyeon Lee,Georges Kaddoum,Onur Günlü,Deniz Gündüz*

Main category: cs.IT

TL;DR: 该论文全面综述了集成无线通信-感知-计算系统中的安全与隐私威胁及防护对策，涵盖物理层安全、语义通信、感知安全、分布式计算安全，并提出统一安全框架。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络正从传统连接向集成感知与计算能力演进，这种融合带来了新的安全挑战。系统复杂性增加、攻击面扩大、数据密集型AI应用普及，都加剧了数据安全和隐私风险，需要系统性的安全防护方案。

Method: 采用系统性文献综述方法，首先回顾通信网络的物理层安全技术，然后分析语义通信和语用通信的安全隐私影响及其跨层设计方法。针对感知功能，从信号源、传播信道和感知目标三个层面识别风险并总结防御策略。针对分布式计算需求，讨论安全编码计算方法。最后提出面向集成通信-感知-计算架构的统一安全框架。

Result: 论文系统识别了集成无线系统中的多层次安全隐私威胁，总结了各领域的最新防御策略，包括物理层安全、语义通信防护、感知安全机制、分布式计算安全方法，并提出了端到端的统一安全框架，为未来无线系统保护提供了全面指导。

Conclusion: 集成通信-感知-计算系统面临复杂的安全隐私挑战，需要跨层、跨领域的协同防护策略。论文提出的统一安全框架为构建安全可靠的下一代无线网络提供了系统性解决方案，有助于推动无线技术向更智能、更安全的方向发展。

Abstract: Next-generation wireless networks are progressing beyond conventional connectivity to incorporate emerging sensing and computing capabilities. This convergence gives rise to integrated systems that enable not only uninterrupted communication, but also environmental awareness, intelligent decision-making, and novel applications that take advantage of these combined features. At the same time, this integration brings substantial security challenges. As computing, sensing, and communication become more tightly intertwined, the overall complexity of the system increases, creating new vulnerabilities and expanding the attack surface. The widespread deployment of data-heavy artificial intelligence applications further amplifies concerns regarding data security and privacy. This paper presents a comprehensive survey of security and privacy threats, along with potential countermeasures, in integrated wireless systems. We first review physical-layer security techniques for communication networks, and then investigate the security and privacy implications of semantic and pragmatic communications and their associated cross-layer design methodologies. For sensing functionalities, we pinpoint security and privacy risks at the levels of signal sources, propagation channels, and sensing targets, and summarize state-of-the-art defense strategies for each. The growing computational requirements of these applications drive the need for distributed computing over the network, which introduces additional risks such as data leakage, weak authentication, and multiple points of failure. We subsequently discuss secure coded computing approaches that can help overcome several of these challenges. Finally, we introduce unified security frameworks tailored to integrated communication-sensing-computing architectures, offering an end-to-end perspective on protecting future wireless systems.

</details>


### [86] [Enormous Fluid Antenna Systems (E-FAS)--Part II: Channel Estimation](https://arxiv.org/abs/2602.20127)
*Farshad Rostami Ghadi,Kai-Kit Wong,Masoud Kaveh,Hao Xu,Baiyang Liu,Kin-Fai Tong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文首次全面分析了在导频信道估计下的E-FAS辅助下行传输，揭示了在非完美CSI下E-FAS的性能优势与局限性


<details>
  <summary>Details</summary>
Motivation: 尽管E-FAS在完美CSI下显示出巨大功率增益，但实际信道获取对其性能的影响尚未充分研究，需要分析导频信道估计下的E-FAS性能

Method: 开发了端到端等效信道的估计框架，推导了MMSE信道估计及其误差的闭式统计表达式，分析了单用户和多用户场景，考虑了训练开销

Result: 单用户场景存在SNR饱和现象（残余自干扰导致）；多用户场景在高SNR下因残余用户间干扰而受限；E-FAS仍保持显著性能优势，提供鲁棒性

Conclusion: 尽管存在CSI不完美和训练开销，E-FAS仍保持实质性性能优势，其放大的大规模信道增益提供了鲁棒性，为实际部署提供了理论指导

Abstract: Enormous fluid antenna systems (E-FAS) have recently emerged as a new wireless architecture in which intelligent metasurfaces act as guided electromagnetic interfaces, enabling surface-wave (SW) propagation with much lower attenuation and more control than conventional space-wave transmission. While prior work has reported substantial power gains under perfect channel state information (CSI), the impact of practical channel acquisition on E-FAS performance remains largely unexplored. This paper presents the first comprehensive analysis of E-FAS-assisted downlink transmission under pilot-based channel estimation. We develop an estimation framework for the equivalent end-to-end channel and derive closed-form expressions for the statistics of the minimum mean-square-error (MMSE) channel estimate and its estimation error. Building on these results, we analyze both single-user and multiuser operation while explicitly accounting for the training overhead. For the single-user case, we characterize the outage probability and achievable rate with imperfect CSI, and reveal an inherent signal-to-noise ratio (SNR) saturation phenomenon caused by residual self-interference. For the multiuser case, we study zero-forcing (ZF) precoding based on imperfect channel estimates and show that the system becomes interference-limited in the high SNR regime because of residual inter-user interference. Furthermore, we quantify the trade-off between spatial multiplexing gains and pilot overhead when the number of users increases. Analytical findings are validated via Monte Carlo simulations and benchmarked against least-squares (LS) estimation and conventional non-E-FAS transmission. The results reveal that despite CSI imperfections and training costs, E-FAS retains substantial performance advantages and provides robustness enabled by its amplified large-scale channel gain.

</details>
