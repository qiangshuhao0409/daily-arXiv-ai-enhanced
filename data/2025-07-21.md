<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica通过生成具有真实性和可控性的训练数据集，解决了机器学习模型在网络中的领域适应问题，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在网络领域中的领域适应问题，即模型在不同生产环境中表现不佳的挑战。

Method: NetReplica将网络建模为具有特定属性的瓶颈链路集合，利用生产网络痕迹实现真实性，并通过精细控制链路属性实现可控性。

Result: 评估显示，NetReplica生成的样本不仅匹配现有数据特征，还能补充Puffer数据中不足或缺失的样本。模型在NetReplica增强数据集上训练后，传输时间预测误差降低了47%。

Conclusion: NetReplica为解决领域适应问题提供了有效方法，显著提升了基于机器学习的网络系统的性能。

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [2] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS是一种自适应5G上行链路感知方案，通过融合DMRS和SRS信号提升信道状态信息（CSI）的更新频率和感知机会，支持更多用户。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，基站将DMRS和SRS的CSI测量视为独立信息流，限制了CSI的准确性和更新频率。CARTS旨在融合这两种信号，提升ISAC服务的性能。

Method: CARTS提出了一种信道拼接和补偿方法，整合异步的DMRS和SRS CSI估计，并设计了实时SRS触发算法，确保所有用户获得足够的感知机会。

Result: 实验表明，CARTS显著提升了可扩展性，信道估计误差（NMSE）为0.167，用户跟踪精度达85厘米，支持的用户数量是基线方法的两倍。

Conclusion: CARTS通过融合DMRS和SRS，无需额外无线资源即可提升CSI可用性，为ISAC提供了实用的标准兼容解决方案。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [3] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: 论文提出了一种无求解器的框架ATRO，通过交替优化拓扑和路由来解决可重构数据中心网络中的扩展性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 可重构数据中心网络的规模和复杂性增加，需要更高效和可扩展的算法来优化逻辑拓扑和路由。

Method: ATRO框架通过交替进行拓扑优化（TO）和路由优化（RO），利用单调性结构和加速二分搜索方法（ABSM）实现高效求解。

Result: ATRO在一跳场景中达到全局最优，在多跳场景中显著优于基线方法，具有可扩展性和鲁棒性。

Conclusion: ATRO是一种高效且可扩展的解决方案，适用于可重构数据中心网络的拓扑和路由优化。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [4] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: 研究将主动可重构智能表面（RIS）中继与高空平台站（HAPS）结合，以提升下一代无线系统中非地面网络（NTN）性能。主动RIS因其信号放大能力更适合长距离HAPS链路。


<details>
  <summary>Details</summary>
Motivation: 长距离HAPS链路存在严重的路径损耗和双重衰落，被动RIS架构效果有限，主动RIS因其信号放大能力更具优势。

Method: 提出一个和速率最大化问题，联合优化功率分配和RIS单元分配，并探索多种子连接主动RIS架构以降低功耗和硬件复杂度。

Result: 仿真显示主动RIS配置在服务质量（QoS）上显著优于被动RIS，全连接架构吞吐量最高，但子连接架构在实用功率限制下能效更优。

Conclusion: 主动RIS支持的HAPS系统有望满足未来超蜂窝覆盖和绿色网络的需求。

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [5] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 提出了一种高可靠、可扩展的管道方法，用于检测由大型语言模型（LLM）主导的网站内容，解决了现有检测器在复杂网页内容上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的内容可能存在抄袭和幻觉问题，且网站通常不披露此类内容，人类读者难以区分，因此需要开发可靠的检测方法。

Method: 通过分类多个散文式页面的LLM文本检测器输出，而非直接提取页面文本，训练并评估检测器，使用两个总计120个网站的真实数据集。

Result: 在两个数据集中实现了100%的准确率，并在实际网络环境中检测到大量LLM主导的网站。

Conclusion: LLM主导的网站在搜索引擎结果中占比高且增长迅速，对用户和网络生态系统可能产生重大影响。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种基于图的架构，通过并行执行和动态资源分配优化LLM驱动的交通管理任务，显著降低令牌消耗和响应延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有链式系统（如TrafficGPT）在顺序任务执行、高令牌使用和扩展性差方面的不足，以应对复杂的现实交通场景。

Method: 采用图结构表示任务及其依赖关系，引入Brain Agent分解查询并协调专业代理，支持并行多查询处理和上下文感知令牌管理。

Result: 实验显示，GraphTrafficGPT令牌消耗减少50.2%，响应延迟降低19.0%，多查询执行效率提升23.0%。

Conclusion: GraphTrafficGPT通过图结构和并行处理显著提升了交通管理任务的效率和扩展性。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [7] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette通过分解偏好到属性维度并基于社区价值观预测偏好，优于GPT-4o 46.6%，同时提供可解释的社区特定洞察。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，缺乏对偏好背后原因的理解。PrefPalette旨在通过多属性决策原则解决这一问题。

Method: PrefPalette采用两步方法：(1) 生成合成数据以隔离属性效应，(2) 基于注意力的偏好建模，学习不同社区如何动态加权属性。

Result: 在Reddit的45个社区中，PrefPalette平均预测准确率比GPT-4o高46.6%，并揭示了社区特定的偏好模式。

Conclusion: PrefPalette不仅提升了偏好预测性能，还提供了透明、可解释的洞察，为更可信、价值感知的个性化应用奠定了基础。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [8] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型（LLM）和符号系统的新方法，以可控透明的方式开发专家系统，解决LLM的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识系统中存在幻觉和不可验证事实的问题，需开发可控透明的解决方案。

Method: 通过限制领域和结构化提示提取知识，生成可验证的Prolog符号表示，结合人类专家验证。

Result: 实验显示生成的知识库具有高事实一致性和语义连贯性，结合了LLM的召回能力和符号系统的精确性。

Conclusion: 该方法为敏感领域提供了可靠、可解释且可扩展的AI应用基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [9] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应关注实体及其关系建模，而非仅关注像素和文字，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，应直接建模这些实体。关系学习虽重要，却未广泛应用。

Method: 通过分析关系学习在企业和数据中的实际应用，探讨其局限性和潜力。

Result: 关系学习仅在少数受限场景中成功，需进一步改进以提升其影响力。

Conclusion: 关系学习应成为AI核心方向，需更多研究和实践以实现其潜力。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [10] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG是一种双图RAG系统，通过实体网络图和文档导航图建模语言关系和文档结构，显著提升多跳问题回答性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG系统在处理复杂法规文本和多跳查询时的不足。

Method: 引入双图架构（实体网络图和文档导航图），结合图遍历和向量语义搜索。

Result: 在多跳问题数据集上达到92.8%精确率、85.5%召回率和87.3% F1分数，显著优于基线。

Conclusion: BifrostRAG为复杂技术文档处理提供了可迁移的解决方案。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [11] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 论文探讨了通过最终答案自动诊断学生解题错误的潜力，以减少组合爆炸问题，并在二次方程求解任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统中，学生可能将多步任务合并为一步，导致错误诊断因组合爆炸问题变得困难。通过最终答案诊断可以缓解这一问题。

Method: 设计了一种基于最终答案的错误诊断服务，用于识别学生合并步骤时的错误规则，并在二次方程求解数据集上进行了验证。

Result: 结果显示，该方法能诊断29.4%的未诊断步骤，且与教师诊断结果在97%的情况下一致。

Conclusion: 该方法为未来进一步探索基于最终答案的错误诊断提供了基础。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [12] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出了一种结合模型追踪和基于约束建模的方法，用于诊断学生在多步任务中的输入，并在解决二次方程的实际数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 结合两种诊断方法的优势，以更灵活地处理学生输入中的多步合并或偏离策略的情况。

Method: 通过定义约束作为学生输入与策略步骤的共同属性，设计了一个多步策略诊断系统，并在二次方程数据集上进行了验证。

Result: 系统诊断与教师编码在所有140个学生步骤中完全一致。

Conclusion: 该方法能够有效诊断学生的多步输入，且与人工诊断结果高度一致。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [13] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM是一种基于轻量级LLM的框架，通过整合位置、运动、环境和生理四个维度的上下文信息，显著提升了活动日志生成的准确性、效率和语义丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性方面存在不足，需要一种更高效且全面的解决方案。

Method: DailyLLM采用轻量级LLM框架，结合结构化提示和高效特征提取，实现高级活动理解。

Result: 实验表明，DailyLLM在BERTScore精度上比70B参数的SOTA基线提升17%，推理速度快10倍，且可在个人电脑和树莓派上高效部署。

Conclusion: DailyLLM为活动日志生成提供了一种高效、轻量且全面的解决方案，显著优于现有方法。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [14] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个新型的本体可视化工具，旨在通过直观的图形界面展示本体结构和推理知识，解决现有工具在可视化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法有效展示本体结构和依赖关系，限制了用户对大型本体框架的理解。

Method: OntView基于DL推理器，采用“所见即所意”范式，支持可视化通用概念包含（GCI），并提供简化视图功能（如本体摘要、TBox元素聚焦和动态分支隐藏）。

Result: OntView成功实现了直观的本体可视化，并提供了避免信息过载的多种简化视图方式。

Conclusion: OntView通过开源发布，为社区提供了一个功能强大且用户友好的本体可视化工具。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [15] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力，通过语义交互建模和修辞框架融合冲突启发式。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而本研究旨在通过语义相互依赖的过程融合冲突启发式，生成连贯且上下文敏感的叙述。

Method: 结合启发式提取、语义激活和组合合成，利用量子认知研究的语义相互依赖性，通过案例研究（Meta vs. FTC）进行验证。

Result: 初步验证通过语义指标显示框架有效性，能够生成连贯的叙述。

Conclusion: 讨论了动态干扰调优等扩展方向，展示了框架的潜力与局限性。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [16] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，用于动态图中的时间链接预测，通过结合短期时间新近性和长期全局结构模式，显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有T-GNN在动态图中因计算开销大而导致的扩展性和效率问题。

Method: EAGLE包含时间感知模块和结构感知模块，通过自适应权重机制平衡两者，避免复杂的多跳消息传递。

Result: 在七个真实世界时间图上，EAGLE在性能和效率上均优于现有T-GNN，速度提升超过50倍。

Conclusion: EAGLE通过轻量级设计，在动态图时间链接预测任务中实现了高效且有效的解决方案。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [17] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 本文提出了一种因果知识转移框架，帮助多智能体强化学习（MARL）在非静态环境中高效共享知识，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法在非静态环境中难以泛化，智能体需要昂贵的重新训练来适应变化。本文旨在解决这一问题。

Method: 通过建模碰撞为因果干预，生成恢复动作宏（macro），并在智能体间在线转移，实现零样本适应。

Result: 实验表明，该方法能缩小随机探索与完全重新训练策略之间约一半的差距，且效果受环境复杂性和智能体目标异质性影响。

Conclusion: 因果知识转移框架为MARL在动态环境中的适应性提供了有效解决方案。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [18] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控、可扩展的创意生成。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在生成新颖且相关内容时的局限性，避免依赖领域特定启发式或结构化提示。

Method: 提出潜在空间创意框架，无需手工规则，适应不同领域、输入格式和创意任务。

Result: 初步结果显示该框架作为通用人机协作创意工具的潜力。

Conclusion: 该框架为可控创意生成提供了灵活且可扩展的解决方案。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [19] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出了一种名为ADPC的新型视觉-语言因果干预框架，用于辅助阿尔茨海默病的诊断，通过消除混杂因素提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别和干预轻度认知障碍（MCI）对延缓阿尔茨海默病（AD）进展至关重要，但诊断中存在数据选择偏差和变量复杂关系的挑战。

Method: 结合MRI、fMRI图像和LLM生成的文本数据，通过因果干预消除混杂因素，分类认知正常（CN）、MCI和AD。

Result: 实验表明ADPC在CN/MCI/AD分类中表现优异，达到SOTA指标。

Conclusion: 研究展示了因果推理与多模态学习结合在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [20] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 提出了一种新颖的时态和约束扩展逻辑框架，用于在ASP中处理高分辨率动态系统。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑方法（如ASP）在细粒度时态和数值推理中的挑战。

Method: 结合线性时态逻辑和约束逻辑，扩展Here-and-There逻辑及其非单调均衡。

Result: 实现了首个专为ASP设计的非单调时态约束推理系统。

Conclusion: 为ASP范式下处理复杂动态系统奠定了逻辑基础。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [21] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个新的本体匹配框架，利用大型语言模型（LLMs）和检索增强生成（RAG）技术动态丰富语义上下文，通过优化技术显著提升匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或专用模型，适应性有限，KROMA旨在通过LLMs和RAG技术提升语义互操作性和匹配效率。

Method: KROMA结合了双相似度概念匹配和轻量级本体优化步骤，减少LLMs调用开销，并通过知识检索和上下文增强提升匹配效果。

Result: 实验表明，KROMA在多个基准数据集上优于传统系统和前沿LLM方法，同时保持通信开销可控。

Conclusion: KROMA证明了知识检索、提示增强和本体优化技术在大规模本体匹配中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [22] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在促进透明、可重复和稳健的AI算法开发，并提供了血糖预测的基准分析。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病管理中AI算法开发因缺乏高质量数据集而受限的问题。

Method: 收集并公开10个糖尿病数据集（Glucose-ML），进行数据比较分析和血糖预测案例研究。

Result: 同一算法在不同数据集上的预测结果差异显著，研究结果为开发稳健AI解决方案提供了建议。

Conclusion: Glucose-ML为糖尿病AI研究提供了宝贵资源，并强调了数据集选择对算法性能的重要性。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [23] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: 该研究提出了一种基于生成式AI的人体运动模拟方法（G-AI-HMS），通过结合文本到文本和文本到运动模型，提升了工业任务中运动模拟的逼真度。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法在运动逼真度上表现不佳，影响了工人行为、安全和生产效率的评估。

Method: G-AI-HMS利用大型语言模型将任务描述转化为运动感知语言，并通过计算机视觉验证AI生成的运动与真实人类动作的相似性。

Result: 在八项任务的案例研究中，AI生成的运动在空间准确性、姿态对齐和时间相似性上表现优于人工描述，显著降低了关节误差和时间偏差。

Conclusion: G-AI-HMS显著提升了运动模拟的逼真度，为工业任务中的行为评估提供了更可靠的工具。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [24] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型（LLMs）在解释无损评估（NDE）轮廓图中的应用，提出了一种将LLMs整合到桥梁检测工作流程中的框架，以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全对交通管理部门至关重要，但NDE数据的解释耗时且需要专业知识，可能延迟决策。LLMs的进步为自动化分析提供了新途径。

Method: 研究评估了多个LLMs在解释五种NDE轮廓图时的表现，包括生成详细描述、识别缺陷、提供建议和准确性。部分模型的表现更优。

Result: 九种模型中有四种能提供更好的图像描述，其中ChatGPT-4和Claude 3.5 Sonnet生成的摘要更有效。LLMs显著提升了效率和准确性。

Conclusion: LLMs在桥梁维护中具有潜力，能通过并行图像标注和摘要加速决策，提升基础设施管理和安全评估。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [25] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升GPU计算性能，并展示出跨架构的优异移植性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，GPU计算资源需求激增，亟需自动化CUDA优化策略。现有模型在CUDA优化上成功率低，因此提出CUDA-L1。

Method: 采用强化学习框架，通过速度提升奖励信号训练模型，无需人工干预或领域知识。

Result: 在NVIDIA A100上平均加速17.7倍，峰值达449倍，且在其他GPU架构上表现优异。模型还能发现优化技巧和性能瓶颈。

Conclusion: CUDA-L1证明强化学习可有效提升CUDA优化性能，为自动化GPU优化开辟新途径。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [26] [Round-Preserving Asymptotic Compression of Prior-Free Interactive Protocols](https://arxiv.org/abs/2507.13464)
*Gurleen Padda,Dave Touchette*

Main category: cs.IT

TL;DR: 本文研究了无先验交互通信协议中通信复杂性与信息复杂性之间的关系，改进了Braverman的结果，实现了轮次保留和有限共享随机性。


<details>
  <summary>Details</summary>
Motivation: 探索通信复杂性与信息复杂性之间的关系，并提供更自然的证明方法。

Method: 通过估计输入联合类型（经验分布），并将其用于无先验反向Shannon定理协议中。

Result: 实现了轮次保留和有限共享随机性，改进了Braverman的结果。

Conclusion: 研究为无先验交互通信协议提供了更高效的模拟方法。

Abstract: There is a close relationship between the communication complexity and
information complexity of communication problems, as demonstrated by results
such as Shannon's noiseless source coding theorem, and the Slepian-Wolf
theorem. Here, we study this relationship in the prior-free and interactive
setting, where we provide an alternate proof for the result of Braverman [SIAM
Review, vol. 59, no. 4, 2017], that the amortized communication complexity of
simulating a prior-free interactive communication protocol, is equal to its
prior-free information cost. While this is a known result, our approach
addresses the need for a more natural proof of it. We also improve on the
result by achieving round preservation, and using a bounded quantity of shared
randomness. We do this by showing that the communicating parties can produce a
reliable estimate of the joint type, or empirical distribution, of their
inputs. This estimate is then used in our protocol for the prior-free reverse
Shannon theorem with side information at the receiver. These results are then
generalized to the interactive setting to obtain our main result.

</details>


### [27] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: 该论文提出了一个框架，用于对Kolmogorov结构函数进行对偶化，并引入统计力学中的概念（如配分函数和自由能）建立信息论与统计力学的数学类比。通过Legendre-Fenchel对偶性证明了结构函数与自由能的关系，并通过实验验证了模型复杂度与泛化能力之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过信息论与统计力学的类比，为计算复杂性提供新的理论工具，并揭示模型复杂度与泛化能力之间的关系。

Method: 方法包括对Kolmogorov结构函数的对偶化，引入统计力学中的配分函数和自由能，并通过Legendre-Fenchel对偶性证明其关系。实验部分使用线性和树基回归模型验证理论。

Result: 结果表明，模型复杂度的方差在损失-复杂度权衡处达到峰值，类似于相变现象。实验验证了理论预测，展示了模型复杂度与泛化能力的相互作用。

Conclusion: 结论是提出的框架成功地将信息论与统计力学联系起来，为理解模型复杂度和泛化能力提供了新的视角，并通过实验验证了其有效性。

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


### [28] [Efficient Decoding of Double-circulant and Wozencraft Codes from Square-root Errors](https://arxiv.org/abs/2507.13548)
*Oren Dubin,Noam Oz,Noga Ron-Zewi*

Main category: cs.IT

TL;DR: 本文提出了针对两种双循环码家族的高效解码算法，并探讨了将其转化为Wozencraft码的可行性及其解码效率。


<details>
  <summary>Details</summary>
Motivation: 研究双循环码的解码效率，并探索将其转化为其他类型码的可能性，以提升解码性能。

Method: 基于Sidon集和循环码的双循环码解码算法，并通过隐式转换将其转化为Wozencraft码。

Result: 成功实现了基于Sidon集的双循环码到Wozencraft码的高效转换，并验证了解码效率的保持。

Conclusion: 本文为双循环码的解码提供了高效方法，并展示了其在Wozencraft码中的应用潜力，但基于循环码的转换存在局限性。

Abstract: We present efficient decoding algorithms from square-root errors for two
known families of double-circulant codes: A construction based on Sidon sets
(Bhargava, Taveres, and Shiva, \emph{IEEE IT 74}; Calderbank, \emph{IEEE IT
83}; Guruswami and Li, \emph{IEEE IT 2025}), and a construction based on cyclic
codes (Chen, Peterson, and Weldon, \emph{Information and Control 1969}). We
further observe that the work of Guruswami and Li implicitly gives a
transformation from double-circulant codes of certain block lengths to
Wozencraft codes which preserves that distance of the codes, and we show that
this transformation also preserves efficiency of decoding. By instantiating
this transformation with the first family of double-circulant codes based on
Sidon sets, we obtain an explicit construction of a Wozencraft code that is
efficiently decodable from square-root errors. We also discuss limitations on
instantiating this transformation with the second family of double-circulant
codes based on cyclic codes.

</details>


### [29] [Density Evolution Analysis of Sparse-Block IDMA](https://arxiv.org/abs/2507.13689)
*Jean-Francois Chamberland,Gianluigi Liva,Krishna Narayanan*

Main category: cs.IT

TL;DR: SB-IDMA是一种新型无源多址协议，旨在提升5G新空口标准的性能，本文通过密度演化分析其接收器性能。


<details>
  <summary>Details</summary>
Motivation: 改进3GPP 5G新空口标准中的无授权两步随机接入传输协议性能。

Method: 采用密度演化分析SB-IDMA的连续干扰消除接收器。

Result: 提供了SB-IDMA性能的理论表征。

Conclusion: 密度演化分析为SB-IDMA的性能提供了理论支持。

Abstract: Sparse block interleaver division multiple access (SB-IDMA) is a recently
introduced unsourced multiple access protocol that aims to improve the
performance of the grant-free two-step random access transmission protocol of
the 3GPP 5G New Radio standard. We introduced a density evolution analysis of
the successive interference cancellation receiver of SB-IDMA, providing a
theoretical characterization of its performance.

</details>


### [30] [Asymptotically Optimal Codes Correcting One Substring Edit](https://arxiv.org/abs/2507.13808)
*Yuting Li,Yuanyuan Tang,Hao Lou,Ryan Gabrys,Farzad Farnoud*

Main category: cs.IT

TL;DR: 论文研究了纠正子串编辑错误的编码问题，提出了冗余度为log n + O(log log n)的渐近最优编码构造方法。


<details>
  <summary>Details</summary>
Motivation: 子串编辑错误（替换子串）在实际应用中常见，现有编码冗余度较高，需优化。

Method: 构造了一种编码方法，能够纠正一个子串编辑错误，冗余度显著降低。

Result: 提出的编码冗余度为log n + O(log log n)，达到渐近最优。

Conclusion: 该方法在纠正子串编辑错误方面具有高效性和最优性。

Abstract: The substring edit error is the operation of replacing a substring $u$ of $x$
with another string $v$, where the lengths of $u$ and $v$ are bounded by a
given constant $k$. It encompasses localized insertions, deletions, and
substitutions within a window. Codes correcting one substring edit have
redundancy at least $\log n+k$. In this paper, we construct codes correcting
one substring edit with redundancy $\log n+O(\log \log n)$, which is
asymptotically optimal.

</details>


### [31] [Secretive Hotplug Coded Caching](https://arxiv.org/abs/2507.13961)
*Mallikharjuna Chinnapadamala,Charul Rajput,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 本文研究了热插拔编码缓存模型中的保密性问题，提出了两种针对已知HpPDA类的保密方案，并在某些内存区域中优于基线方案。


<details>
  <summary>Details</summary>
Motivation: 解决热插拔编码缓存系统中用户离线时的保密性问题，确保用户无法通过缓存内容或服务器传输获取未请求文件的信息。

Method: 提出了两种针对两类HpPDA的保密方案，并与基于经典编码缓存设置的基线方案进行比较。

Result: 数值结果表明，所提方案在某些内存区域中优于基线方案。

Conclusion: 本文提出的保密方案在热插拔编码缓存系统中有效提升了保密性能，尤其在特定内存区域表现更优。

Abstract: In this work, we consider a coded caching model called \textit{hotplug coded
caching}, in which some users are offline during the delivery phase. The
concept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching
systems has been introduced in the literature, and two classes of HpPDAs are
known. In this paper, we consider a secrecy constraint in hotplug coded caching
setup, where users should not learn anything about any file from their cache
content, and active users should not gain any information about files other
than their demanded file from either their cache content or the server
transmissions. We propose two secretive schemes for the two classes of HpPDAs
and compare them with a baseline scheme, which is a secretive scheme using PDAs
for the classical coded caching setup and can be trivially adapted for the
hotplug coded caching setup. We numerically show that our schemes outperform
the baseline scheme in certain memory regions.

</details>


### [32] [Bounds and Constructions of High-Memory Spatially-Coupled Codes](https://arxiv.org/abs/2507.14064)
*Lei Huang*

Main category: cs.IT

TL;DR: 本文利用Clique Lovász局部引理，为空间耦合（SC）码中影响解码性能的有害组合结构提供了内存和提升度的充分条件，并首次提出基于Moser-Tardos算法的构造性方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在消除SC码中的有害组合结构，以提升解码性能。

Method: 应用Clique Lovász局部引理和Moser-Tardos算法，分析LLL分布和M-T分布的特性。

Result: 建立了有害结构间的依赖关系，并给出了消除部分结构后剩余结构概率变化的上界。

Conclusion: 消除4-cycles会增加6-cycles活跃概率，最多为e^{8/3}倍。

Abstract: In this paper, we apply the Clique Lov\'asz Local Lemma to provide sufficient
conditions on memory and lifting degree for removing certain harmful
combinatorial structures in spatially-coupled (SC) codes that negatively impact
decoding performance. Additionally, we present, for the first time, a
constructive algorithm based on the Moser-Tardos algorithm that ensures
predictable performance. Furthermore, leveraging the properties of
LLL-distribution and M-T-distribution, we establish the dependencies among the
harmful structures during the construction process. We provide upper bounds on
the probability change of remaining harmful structures after eliminating some
of them. In particular, the elimination of 4-cycles increases the probability
of 6-cycles becoming active by at most a factor of $e^{8/3}$.

</details>


### [33] [Error Correcting Codes for Segmented Burst-Deletion Channels](https://arxiv.org/abs/2507.14070)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 研究了分段突发删除信道，提出了一种针对任意字母表的纠错编码方法，冗余度与段长度对数成正比。


<details>
  <summary>Details</summary>
Motivation: 现实中的同步错误通常以突发形式出现，因此需要研究分段突发删除信道。

Method: 利用现有的一突发删除码对每段输入子序列进行编码，并添加约束以帮助解码器识别段边界。

Result: 编码的冗余度与段长度b的对数成正比（O(log b)）。

Conclusion: 提出的编码方法有效解决了分段突发删除信道中的纠错问题。

Abstract: We study segmented burst-deletion channels motivated by the observation that
synchronization errors commonly occur in a bursty manner in real-world
settings. In this channel model, transmitted sequences are implicitly divided
into non-overlapping segments, each of which may experience at most one burst
of deletions. In this paper, we develop error correction codes for segmented
burst-deletion channels over arbitrary alphabets under the assumption that each
segment may contain only one burst of t-deletions. The main idea is to encode
the input subsequence corresponding to each segment using existing one-burst
deletion codes, with additional constraints that enable the decoder to identify
segment boundaries during the decoding process from the received sequence. The
resulting codes achieve redundancy that scales as O(log b), where b is the
length of each segment.

</details>
