{"id": "2509.13441", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13441", "abs": "https://arxiv.org/abs/2509.13441", "authors": ["MohammadHossien Alishahi", "Ming Zeng", "Paul Fortier", "Omer Waqar", "Muhammad Hanif", "Dinh Thai Hoang", "Diep N. Nguyen", "Quoc-Viet Pham"], "title": "Efficient STAR-RIS Mode for Energy Minimization in WPT-FL Networks with NOMA", "comment": "published in IEEE TCOM", "summary": "With the massive deployment of IoT devices in 6G networks, several critical\nchallenges have emerged, such as large communication overhead, coverage\nlimitations, and limited battery lifespan. FL, WPT, multi-antenna AP, and RIS\ncan mitigate these challenges by reducing the need for large data\ntransmissions, enabling sustainable energy harvesting, and optimizing the\npropagation environment. Compared to conventional RIS, STAR-RIS not only\nextends coverage from half-space to full-space but also improves energy saving\nthrough appropriate mode selection. Motivated by the need for sustainable,\nlow-latency, and energy-efficient communication in large-scale IoT networks,\nthis paper investigates the efficient STAR-RIS mode in the uplink and downlink\nphases of a WPT-FL multi-antenna AP network with non-orthogonal multiple access\nto minimize energy consumption, a joint optimization that remains largely\nunexplored in existing works on RIS or STAR-RIS. We formulate a non-convex\nenergy minimization problem for different STAR-RIS modes, i.e., energy\nsplitting (ES) and time switching (TS), in both uplink and downlink\ntransmission phases, where STAR-RIS phase shift vectors, beamforming matrices,\ntime and power for harvesting, uplink transmission, and downlink transmission,\nlocal processing time, and computation frequency for each user are jointly\noptimized. To tackle the non-convexity, the problem is decoupled into two\nsubproblems: the first subproblem optimizes STAR-RIS phase shift vectors and\nbeamforming matrices across all WPT-FL phases using block coordinate descent\nover either semi-definite programming or Rayleigh quotient problems, while the\nsecond one allocates time, power, and computation frequency via the\none-dimensional search algorithms or the bisection algorithm."}
{"id": "2509.13661", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13661", "abs": "https://arxiv.org/abs/2509.13661", "authors": ["Kareem M. Attiah", "Wei Yu"], "title": "Uplink-Downlink Duality for Beamforming in Integrated Sensing and Communications", "comment": "18 pages, 5 figures, submitted to an IEEE journal for possible\n  publication", "summary": "This paper considers the beamforming and power optimization problem for a\nclass of integrated sensing and communications (ISAC) problems that utilize the\ncommunication signals simultaneously for sensing. We formulate the problem of\nminimizing the Bayesian Cram\\'er-Rao bound (BCRB) on the mean-squared error of\nestimating a vector of parameters, while satisfying downlink\nsignal-to-interference-and-noise-ratio constraints for a set of communication\nusers at the same time. The proposed optimization framework comprises two key\nnew ingredients. First, we show that the BCRB minimization problem corresponds\nto maximizing beamforming power along certain sensing directions of interest.\nSecond, the classical uplink-downlink duality for multiple-input\nmultiple-output communications can be extended to the ISAC setting, but unlike\nthe classical communication problem, the dual uplink problem for ISAC may\nentail negative noise power and needs to include an extra condition on the\nuplink beamformers. This new duality theory opens doors for an efficient\niterative algorithm for optimizing power and beamformers for ISAC."}
{"id": "2509.13701", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13701", "abs": "https://arxiv.org/abs/2509.13701", "authors": ["Tam Ninh Thi-Thanh", "Nguyen Minh Quan", "Do Son Tung", "Trinh Van Chien", "Hung Tran"], "title": "Clustering Strategies in Satellite-Aided Communications", "comment": "6 pages and 4 figures. Accepted by IEEE ATC", "summary": "With the rapid advancement of next-generation satellite networks, addressing\nclustering tasks, user grouping, and efficient link management has become\nincreasingly critical to optimize network performance and reduce interference.\nIn this paper, we provide a comprehensive overview of modern clustering\napproaches based on machine learning and heuristic algorithms. The experimental\nresults indicate that improved machine learning techniques and graph\ntheory-based methods deliver significantly better performance and scalability\nthan conventional clustering methods, such as the pure clustering algorithm\nexamined in previous research. These advantages are especially evident in\nlarge-scale satellite network scenarios. Furthermore, the paper outlines\npotential research directions and discusses integrated, multi-dimensional\nsolutions to enhance adaptability and efficiency in future satellite\ncommunication."}
{"id": "2509.13955", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13955", "abs": "https://arxiv.org/abs/2509.13955", "authors": ["Zheyu Wu", "Junjie Ma", "Ya-Feng Liu", "Bruno Clerckx"], "title": "Asymptotic Analysis of Nonlinear One-Bit Precoding in Massive MIMO Systems via Approximate Message Passing", "comment": "39 pages, 6 figures, submitted for possible publication", "summary": "Massive multiple-input multiple-output (MIMO) systems employing one-bit\ndigital-to-analog converters offer a hardware-efficient solution for wireless\ncommunications. However, the one-bit constraint poses significant challenges\nfor precoding design, as it transforms the problem into a discrete and\nnonconvex optimization task. In this paper, we investigate a widely adopted\n``convex-relaxation-then-quantization\" approach for nonlinear symbol-level\none-bit precoding. Specifically, we first solve a convex relaxation of the\ndiscrete minimum mean square error precoding problem, and then quantize the\nsolution to satisfy the one-bit constraint. To analyze the high-dimensional\nasymptotic performance of this scheme, we develop a novel analytical framework\nbased on approximate message passing (AMP). This framework enables us to derive\na closed-form expression for the symbol error probability (SEP) at the receiver\nside in the large-system limit, which provides a quantitative characterization\nof how model and system parameters affect the SEP performance. Our empirical\nresults suggest that the $\\ell_\\infty^2$ regularizer, when paired with an\noptimally chosen regularization parameter, achieves optimal SEP performance\nwithin a broad class of convex regularization functions. As a first step\ntowards a theoretical justification, we prove the optimality of the\n$\\ell_\\infty^2$ regularizer within the mixed $\\ell_\\infty^2$-$\\ell_2^2$\nregularization functions."}
{"id": "2509.13490", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13490", "abs": "https://arxiv.org/abs/2509.13490", "authors": ["Paul Bergeron", "Sandhya Aneja"], "title": "GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic", "comment": null, "summary": "This paper presents the identification of congestion control protocols TCP\nReno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an\naccuracy of 97.04% using a GRU-based learning model. We used a faster neural\nnetwork architecture on a more complex and competitive network in comparison to\nexisting work and achieved comparably high accuracy."}
{"id": "2509.13332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness."}
{"id": "2509.13981", "categories": ["cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.13981", "abs": "https://arxiv.org/abs/2509.13981", "authors": ["Thomas Jacob Maranzatto", "Sennur Ulukus"], "title": "Deriving Moments in the Age of Gossip Process from Percolation", "comment": "3 figures, 5 pages", "summary": "This paper concerns fundamental identities in the study of age of information\n(AoI) in gossip networks. We recover known recursive identities for arbitrary\nkth moments of the age process based on the recent connection between AoI and\nfirst passage percolation. Apart from the connection to percolation, our proofs\nare more concise and can be followed using only elementary facts from\nprobability. Our argument generalizes some techniques known in the statistical\nphysics community, and we remark on connections to the Eden model."}
{"id": "2509.13511", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13511", "abs": "https://arxiv.org/abs/2509.13511", "authors": ["Duo Cheng", "Ramanujan K Sheshadri", "Ahan Kak", "Nakjung Choi", "Xingyu Zhou", "Bo Ji"], "title": "Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning", "comment": "accepted for publication at ACM MobiHoc 2025", "summary": "Network slicing plays a crucial role in realizing 5G/6G advances, enabling\ndiverse Service Level Agreement (SLA) requirements related to latency,\nthroughput, and reliability. Since network slices are deployed end-to-end\n(E2E), across multiple domains including access, transport, and core networks,\nit is essential to efficiently decompose an E2E SLA into domain-level targets,\nso that each domain can provision adequate resources for the slice. However,\ndecomposing SLAs is highly challenging due to the heterogeneity of domains,\ndynamic network conditions, and the fact that the SLA orchestrator is oblivious\nto the domain's resource optimization. In this work, we propose Odin, a\nBayesian Optimization-based solution that leverages each domain's online\nfeedback for provably-efficient SLA decomposition. Through theoretical analyses\nand rigorous evaluations, we demonstrate that Odin's E2E orchestrator can\nachieve up to 45% performance improvement in SLA satisfaction when compared\nwith baseline solutions whilst reducing overall resource costs even in the\npresence of noisy feedback from the individual domains."}
{"id": "2509.13333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md."}
{"id": "2509.13714", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13714", "abs": "https://arxiv.org/abs/2509.13714", "authors": ["Benoit Pit-Claudel", "Muriel Médard", "Manya Ghobadi"], "title": "LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones", "comment": null, "summary": "The emergence of ultra-low latency applications, such as financial\ntransactions, has driven the development of hybrid backbone networks that rely\non fiber, satellite, and microwave links. Despite providing low latencies,\nthese hybrid networks suffer from occasional environmental packet loss caused\nby poor weather, construction, and line of sight blockage. Paradoxically,\ntoday's hybrid backbones rely on conventional transport protocols that take\npacket loss to signal network congestion, as opposed to transient environmental\nobstacles. A common approach to address this challenge is to use network coding\n(NC) between the end hosts to recover from these occasional packet loss events.\nHowever, current NC proposals assume full access to the end-hosts' stack to\nperform end-to-end encoding/decoding operations. In this paper, we introduce\nLINC, a novel system that provides in-network NC capabilities to mitigate\nenvironmental packet loss events without requiring cooperation from the end\nhosts. LINC uses a systematic block coding approach on a link-by-link basis,\nencoding and decoding packets inside the network. We model the tradeoff in\ngoodput between end-to-end retransmissions and redundant packets introduced by\nLINC, and propose an optimization formulation to determine the optimal choice\nof coding parameters. Our simulations on real-world backbone topologies\ndemonstrate that LINC reduces the end-to-end latency by up to 18% by\neliminating unnecessary retransmissions."}
{"id": "2509.13604", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13604", "abs": "https://arxiv.org/abs/2509.13604", "authors": ["Yoseph Berhanu Alebachew", "Mulugeta Libsie"], "title": "A Framework for Multi-source Prefetching Through Adaptive Weight", "comment": null, "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."}
{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}."}
{"id": "2509.13714", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13714", "abs": "https://arxiv.org/abs/2509.13714", "authors": ["Benoit Pit-Claudel", "Muriel Médard", "Manya Ghobadi"], "title": "LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones", "comment": null, "summary": "The emergence of ultra-low latency applications, such as financial\ntransactions, has driven the development of hybrid backbone networks that rely\non fiber, satellite, and microwave links. Despite providing low latencies,\nthese hybrid networks suffer from occasional environmental packet loss caused\nby poor weather, construction, and line of sight blockage. Paradoxically,\ntoday's hybrid backbones rely on conventional transport protocols that take\npacket loss to signal network congestion, as opposed to transient environmental\nobstacles. A common approach to address this challenge is to use network coding\n(NC) between the end hosts to recover from these occasional packet loss events.\nHowever, current NC proposals assume full access to the end-hosts' stack to\nperform end-to-end encoding/decoding operations. In this paper, we introduce\nLINC, a novel system that provides in-network NC capabilities to mitigate\nenvironmental packet loss events without requiring cooperation from the end\nhosts. LINC uses a systematic block coding approach on a link-by-link basis,\nencoding and decoding packets inside the network. We model the tradeoff in\ngoodput between end-to-end retransmissions and redundant packets introduced by\nLINC, and propose an optimization formulation to determine the optimal choice\nof coding parameters. Our simulations on real-world backbone topologies\ndemonstrate that LINC reduces the end-to-end latency by up to 18% by\neliminating unnecessary retransmissions."}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community."}
{"id": "2509.13724", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13724", "abs": "https://arxiv.org/abs/2509.13724", "authors": ["Jan Janak", "Kahlil Dozier", "Lauren Berny", "Liang Hu", "Dan Rubenstein", "Charles Jennings", "Henning Schulzrinne"], "title": "Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing", "comment": null, "summary": "Mission-critical voice (MCV) communications systems have been a critical tool\nfor the public safety community for over eight decades. Public safety users\nexpect MCV systems to operate reliably and consistently, particularly in\nchallenging conditions. Because of these expectations, the Public Safety\nCommunications Research (PSCR) Division of the National Institute of Standards\nand Technology (NIST) has been interested in correlating impairments in MCV\ncommunication systems and public safety user quality of experience (QoE).\nPrevious research has studied MCV voice quality and intelligibility in a\ncontrolled environment. However, such research has been limited by the\nchallenges inherent in emulating real-world environmental conditions.\nAdditionally, there is the question of the best metric to use to reflect QoE\naccurately.\n  This paper describes our efforts to develop the methodology and tools for\nhuman-subject experiments with MCV. We illustrate their use in human-subject\nexperiments in emulated real-world environments. The tools include a testbed\nfor emulating real-world MCV systems and an automated speech recognition (ASR)\nrobot approximating human subjects in transcription tasks. We evaluate QoE\nthrough a Levenshtein Distance-based metric, arguing it is a suitable proxy for\nmeasuring comprehension and the QoE. We conducted human-subject studies with\nAmazon MTurk volunteers to understand the influence of selected system\nparameters and impairments on human subject performance and end-user QoE. We\nalso compare the performance of several ASR system configurations with\nhuman-subject performance. We find that humans generally perform better than\nASR in accuracy-related MCV tasks and that the codec significantly influences\nthe end-user QoE and ASR performance."}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. Güzel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rocktäschel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents."}
{"id": "2509.13901", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13901", "abs": "https://arxiv.org/abs/2509.13901", "authors": ["Saptarshi Ghosh", "Ioannis Mavromatis", "Konstantinos Antonakoglou", "Konstantinos Katsaros"], "title": "Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach", "comment": "Accepted for publication at IEEE CSCN 2025", "summary": "GitOps has emerged as a foundational paradigm for managing cloud-native\ninfrastructures by enabling declarative configuration, version-controlled\nstate, and automated reconciliation between intents and runtime deployments.\nDespite its widespread adoption, the performance and scalability of GitOps\ntools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.\nThis paper presents a reproducible, metric-driven benchmarking, assessing the\nlatency and resource overheads of three widely used GitOps operators: Argo CD,\nFlux CD, and ConfigSync. We conduct controlled experiments under both single-\nand multi-intent scenarios, capturing key performance indicators such as\nlatency and resource consumption. Our results highlight trade-offs between the\ntools in terms of determinism, resource efficiency, and responsiveness. We\nfurther investigate a realistic orchestration scenario, using Nephio as our\norchestrator, to quantify the processing latency and overhead in declarative\nend-to-end deployment pipelines. Our findings can offer valuable insights for\ntool selection and optimisation in future autonomous network orchestration\nsystems."}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA"}
{"id": "2509.13954", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13954", "abs": "https://arxiv.org/abs/2509.13954", "authors": ["Surya Agustian", "Sandra Permana", "Salman Teguh Pratista", "Syarifu Adam", "Iswandi"], "title": "Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University", "comment": "6 pages of paper submitting to conference", "summary": "To some organizations, building campus network is sometimes considered to be\nvery expensive; and this has made the project uneasy to perform. Moreover, if\nthe organization without sufficient IT knowledge does not have capable IT\nengineers, leaving this project to third parties without supervision would lead\nto unexpected larger expenses. For this reason, in the year of 2003, YARSI\nUniversity formed CMIS (Center for Management Infor-mation System) to perform\ntasks in designing, operations and maintenance of campus network and its\nservices. By combining Open Source operating system run on a local assembled\npersonal computer as gateway and router, and switching technology from Cisco,\nwe designed a low-cost UTP-based campus network which covering rooms and\nbuildings in YARSI environment. Meanwhile the internet access through several\nbroadband connections and dedicated wireless was shared to more than 100\nsimultaneous users by a captive portal system. With this strategy, we can\nsignificantly reduce cost for purchasing, maintenance and operations of network\ninfrastructure and internet access. Our model in designing low-cost campus\nnetwork and internet connections could be adopted by rural community or\norganizations that have limited budget to have internet access."}
{"id": "2509.13351", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems."}
{"id": "2509.13993", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13993", "abs": "https://arxiv.org/abs/2509.13993", "authors": ["Vincent Mutolo", "Rhea Parekh", "Dan Rubenstein"], "title": "Path-Oblivious Entanglement Swapping for the Quantum Internet", "comment": "To appear, ACM HotNets 2025, November 2025", "summary": "Proposed Bell pair swapping protocols, an essential component of the Quantum\nInternet, are planned-path: specific, structured, routing paths are reserved\nprior to the execution of the swapping process. This makes sense when one\nassumes the state used in the swapping process is expensive, fragile, and\nunstable. However, lessons from classical networking have shown that while\nreservations seem promising in concept, flexible, reservation-light or free\napproaches often outperform their more restrictive counterparts in\nwell-provisioned networks. In this paper, we propose that a path-oblivious\napproach is more amenable to supporting swapping as quantum state evolves into\na cheaper, more robust form. We formulate the swapping process as a linear\nprogram and present and evaluate a fairly naive baseline swapping protocol that\ntries to balance Bell pairs throughout the network. Preliminary results show\nthat while naive balancing leaves room for improvement, investigating\npath-oblivious swapping is a promising direction."}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration."}
{"id": "2509.14002", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14002", "abs": "https://arxiv.org/abs/2509.14002", "authors": ["Rongyu Zhang", "Xize Duan", "Jiaming Liu", "Li Du", "Yuan Du", "Dan Wang", "Shanghang Zhang", "Fangxin Wang"], "title": "RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery", "comment": null, "summary": "Recently, content-aware methods have been employed to reduce bandwidth and\nenhance the quality of Internet video delivery. These methods involve training\ndistinct content-aware super-resolution (SR) models for each video chunk on the\nserver, subsequently streaming the low-resolution (LR) video chunks with the SR\nmodels to the client. Prior research has incorporated additional partial\nparameters to customize the models for individual video chunks. However, this\nleads to parameter accumulation and can fail to adapt appropriately as video\nlengths increase, resulting in increased delivery costs and reduced\nperformance. In this paper, we introduce RepCaM++, an innovative framework\nbased on a novel Re-parameterization Content-aware Modulation (RepCaM) module\nthat uniformly modulates video chunks. The RepCaM framework integrates extra\nparallel-cascade parameters during training to accommodate multiple chunks,\nsubsequently eliminating these additional parameters through\nre-parameterization during inference. Furthermore, to enhance RepCaM's\nperformance, we propose the Transparent Visual Prompt (TVP), which includes a\nminimal set of zero-initialized image-level parameters (e.g., less than 0.1%)\nto capture fine details within video chunks. We conduct extensive experiments\non the VSD4K dataset, encompassing six different video scenes, and achieve\nstate-of-the-art results in video restoration quality and delivery bandwidth\ncompression."}
{"id": "2509.13357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation."}
{"id": "2509.13364", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation"}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems."}
{"id": "2509.13379", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems."}
{"id": "2509.13389", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos Núñez-Molina", "Vicenç Gómez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported."}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git."}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts."}
{"id": "2509.13570", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction."}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner."}
{"id": "2509.13615", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR."}
{"id": "2509.13704", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation."}
{"id": "2509.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR."}
{"id": "2509.13773", "categories": ["cs.AI", "cs.IR", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13773", "abs": "https://arxiv.org/abs/2509.13773", "authors": ["Zhipeng Bian", "Jieming Zhu", "Xuyang Xie", "Quanyu Dai", "Zhou Zhao", "Zhenhua Dong"], "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103", "summary": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience."}
{"id": "2509.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13880", "abs": "https://arxiv.org/abs/2509.13880", "authors": ["Mingwei Zhang", "Zhenhao Gu", "Liangda Fang", "Cunjing Ge", "Ziliang Chen", "Zhao-Rong Lai", "Quanlong Guan"], "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques", "comment": null, "summary": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances."}
{"id": "2509.13968", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13968", "abs": "https://arxiv.org/abs/2509.13968", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance."}
{"id": "2509.14030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent."}
{"id": "2509.14195", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14195", "abs": "https://arxiv.org/abs/2509.14195", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning", "comment": "8 pages, 3 figures", "summary": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning."}
