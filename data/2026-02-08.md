<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Causal Online Learning of Safe Regions in Cloud Radio Access Networks](https://arxiv.org/abs/2602.05280)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.NI

TL;DR: 提出COL方法，通过因果推理和贝叶斯学习在线学习RAN的安全操作区域，确保安全概率并高效收敛


<details>
  <summary>Details</summary>
Motivation: 云无线接入网络需要动态扩展容量，但在实际网络中部署自适应控制器存在违反服务协议和操作约束的风险，需要安全的学习方法

Method: COL方法包含两个在线阶段：推理阶段通过因果推理和高斯过程回归被动观察RAN推断初始安全区域；干预阶段通过干预性贝叶斯学习逐步扩展安全区域

Result: 理论证明COL确保学习区域具有指定概率的安全性，并在标准条件下收敛到完整安全区域；在5G测试床上的实验显示COL快速学习安全区域，运营成本低，样本效率比现有最先进方法高10倍

Conclusion: COL方法为云RAN提供了一种安全、高效的在线学习方法，能够动态学习安全操作区域，降低运营风险，提高网络管理效率

Abstract: Cloud radio access networks (RANs) enable cost-effective management of mobile networks by dynamically scaling their capacity on demand. However, deploying adaptive controllers to implement such dynamic scaling in operational networks is challenging due to the risk of breaching service agreements and operational constraints. To mitigate this challenge, we present a novel method for learning the safe operating region of the RAN, i.e., the set of resource allocations and network configurations for which its specification is fulfilled. The method, which we call (C)ausal (O)nline (L)earning, operates in two online phases: an inference phase and an intervention phase. In the first phase, we passively observe the RAN to infer an initial safe region via causal inference and Gaussian process regression. In the second phase, we gradually expand this region through interventional Bayesian learning. We prove that COL ensures that the learned region is safe with a specified probability and that it converges to the full safe region under standard conditions. We experimentally validate COL on a 5G testbed. The results show that COL quickly learns the safe region while incurring low operational cost and being up to 10x more sample-efficient than current state-of-the-art methods for safe learning.

</details>


### [2] [Wi-Fi Radar via Over-the-Air Referencing: Bridging Wi-Fi Sensing and Bistatic Radar](https://arxiv.org/abs/2602.05344)
*Koji Yamamoto*

Main category: cs.NI

TL;DR: 提出LoSRef方案，利用Wi-Fi的视距路径作为无线参考，在非同步Wi-Fi系统中实现延迟校准和相位对齐，使普通Wi-Fi设备能够进行相位相干的雷达式操作。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知在人体感知应用中受到广泛关注，但非同步的发射器和接收器从根本上阻碍了相位相干的雷达式延迟-多普勒分析。传统Wi-Fi双基地雷达系统需要有线参考信号或专用参考天线，限制了实际应用。

Method: 提出LoSRef方案，利用视距路径作为无线参考进行延迟校准和相位对齐。该方案在非同步Wi-Fi系统中，将最早到达的直接路径作为延迟和相位的空中参考，实现相位相干的双基地雷达式操作。

Result: 通过室内人体步态和呼吸实验证明，仅使用商用Wi-Fi设备就能获得相位相干的信道脉冲响应和相应的延迟-多普勒响应。能够实现物理可解释的人体运动感知，包括步态引起的距离变化和呼吸引起的亚波长位移，并能提取比主导静态多径分量弱20dB的目标动态。

Conclusion: LoSRef框架填补了传统Wi-Fi感知与Wi-Fi雷达之间的长期空白，在无需修改的Wi-Fi感知配置中实现了相位相干的双基地雷达式操作，为基于商用Wi-Fi设备的高精度人体感知应用开辟了新途径。

Abstract: Wi-Fi sensing has attracted significant attention for human sensing and related applications. However, unsynchronized transmitters and receivers fundamentally preclude phase-coherent radar-like delay--Doppler analysis. By exploiting the line-of-sight (LoS) path, i.e., the earliest-arriving direct path, as an over-the-air (OTA) reference for delay and phase, we propose an OTA LoS-path referencing scheme, termed LoSRef, that enables delay calibration and phase alignment in unsynchronized Wi-Fi systems. Unlike conventional Wi-Fi bistatic radar systems that rely on wired reference signals or dedicated reference antennas, the proposed LoSRef-based framework bridges the long-standing gap between conventional Wi-Fi sensing and Wi-Fi radar, enabling phase-coherent bistatic radar-like operation in a drop-in Wi-Fi sensing configuration. Through human gait and respiration experiments in indoor environments, we demonstrate that phase-coherent channel impulse responses and corresponding delay--Doppler responses are obtained using only commodity Wi-Fi devices. This enables physically interpretable human motion sensing, including gait-induced range variation and respiration-induced sub-wavelength displacement, as well as the extraction of target-induced dynamics up to 20 dB weaker than dominant static multipath components.

</details>


### [3] [Statistical Verification of Medium-Access Parameterization for Power-Grid Edge Ad Hoc Sensor Networks](https://arxiv.org/abs/2602.05510)
*Haitian Wang,Yiren Wang,Xinyu Wang,Zichen Geng,Xian Zhang,Yihao Ding*

Main category: cs.NI

TL;DR: 开发了一个基于随机定时混合自动机和统计模型检验的验证框架，用于评估电力网格传感器网络中CSMA/CA参数配置的可靠性，确保满足延迟、可靠性和能量约束。


<details>
  <summary>Details</summary>
Motivation: 电力网格中基于IEEE 802.15.4的传感器网络面临节点自私调整CSMA/CA参数的问题，这会降低可靠性、能量效率和电网约束的合规性。现有分析方法难以在异步、事件驱动和资源受限条件下进行严格评估。

Method: 开发了一个验证框架，将随机定时混合自动机与带置信区间的统计模型检验相结合，使用时序逻辑编码节点和系统级目标，通过大规模统计评估自动筛选协议配置，认证纳什均衡策略。

Result: 在变电站规模场景中，认证的均衡策略将效用从0.862提升到0.914，投递率从89.5%提高到93.2%；与投递导向基线相比，每周期平均能量从152.8 mJ降至149.2 mJ，同时保持相当的投递性能。认证配置满足延迟、可靠性和能量约束，鲁棒系数高于0.97，效用高于0.91。

Conclusion: 该验证框架能够正式评估CSMA/CA参数配置，确保在电网工作负载下满足严格约束，认证的纳什均衡策略在提高系统性能的同时保持对单边偏差的鲁棒性。

Abstract: The widespread deployment of power grid ad hoc sensor networks based on IEEE 802.15.4 raises reliability challenges when nodes selfishly adapt CSMA/CA parameters to maximize individual performance. Such behavior degrades reliability, energy efficiency, and compliance with strict grid constraints. Existing analytical and simulation approaches often fail to rigorously evaluate configurations under asynchronous, event-driven, and resource-limited conditions. We develop a verification framework that integrates stochastic timed hybrid automata with statistical model checking (SMC) with confidence bounds to formally assess CSMA/CA parameterizations under grid workloads. By encoding node- and system-level objectives in temporal logic and automating protocol screening via large-scale statistical evaluation, the method certifies Nash equilibrium strategies that remain robust to unilateral deviations. In a substation-scale scenario, the certified equilibrium improves utility from 0.862 to 0.914 and raises the delivery ratio from 89.5% to 93.2% when compared with an aggressive tuning baseline. Against a delivery-oriented baseline, it reduces mean per-cycle energy from 152.8 mJ to 149.2 mJ while maintaining comparable delivery performance. Certified configurations satisfy latency, reliability, and energy constraints with robustness coefficients above 0.97 and utility above 0.91.

</details>


### [4] [Data analysis of cloud virtualization experiments](https://arxiv.org/abs/2602.05792)
*Pedro R. X. do Carmo,Eduardo Freitas,Assis T. de Oliveira Filho,Judith Kelner,Djamel Sadok*

Main category: cs.NI

TL;DR: 该研究收集了云计算虚拟化环境下的网络测量数据集，分析不同虚拟化技术（KVM、LXC、Docker）和网络参数对端到端延迟的影响，并建立数据模型评估云计算环境对数据包往返时间的影响。


<details>
  <summary>Details</summary>
Motivation: 云计算虚拟化技术（如完全虚拟化和操作系统虚拟化）虽然带来灵活性，但也引入了CPU、I/O和网络资源的额外开销，使得数据平面数据包吞吐量预测更加困难。需要研究不同虚拟化技术对网络性能的影响。

Method: 通过主动网络测量收集数据集，变量包括：CPU亲和性、回显数据包注入频率、虚拟网络驱动类型、CPU/I/O/网络负载使用情况、并发VM数量。使用KVM、LXC和Docker三种虚拟化技术。对数据集进行预处理、相关性分析、降维和聚类分析，并建立数据模型评估对往返时间的影响。

Result: 创建了一个包含多种虚拟化技术和网络参数配置的测量数据集，分析了这些因素对端到端延迟的影响。通过数据可视化分析展示了数据集在开发基于机器学习的系统以支持管理员决策方面的应用价值。

Conclusion: 该研究提供了一个全面的虚拟化网络性能数据集，有助于理解不同虚拟化技术对网络延迟的影响，并为开发基于机器学习的网络性能预测和优化系统奠定了基础。

Abstract: The cloud computing paradigm underlines data center and telecommunication infrastructure design. Heavily leveraging virtualization, it slices hardware and software resources into smaller software units for greater flexibility of manipulation. Given the considerable benefits, several virtualization forms, with varying processing and communication overheads, emerged, including Full Virtualization and OS Virtualization. As a result, predicting packet throughput at the data plane turns out to be more challenging due to the additional virtualization overhead located at CPU, I/O, and network resources. This research presents a dataset of active network measurements data collected while varying various network parameters, including CPU affinity, frequency of echo packet injection, type of virtual network driver, use of CPU, I/O, or network load, and the number of concurrent VMs. The virtualization technologies used in the study include KVM, LXC, and Docker. The work examines their impact on a key network metric, namely, end-to-end latency. Also, it builds data models to evaluate the impact of a cloud computing environment on packet round-trip time. To explore data visualization, the dataset was submitted to pre-processing, correlation analysis, dimensionality reduction, and clustering. In addition, this paper provides a brief analysis of the dataset, demonstrating its use in developing machine learning-based systems for administrator decision-making.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 论文批判AI发展的线性模型，提出"熟悉智能"和"陌生智能"概念，认为AI智能将是陌生智能，具有超人类能力与亚人类表现并存的特点，并发展非线性智能模型。


<details>
  <summary>Details</summary>
Motivation: 批判当前AI进展的线性模型，认为这种模型过于简化，无法准确描述AI智能的本质特征。AI智能很可能呈现出与人类智能不同的"陌生"特性，需要新的理论框架来理解和评估。

Method: 扩展Susan Schneider对线性AI进展模型的批判，引入"熟悉智能"和"陌生智能"两个新概念。发展并辩护非线性智能模型，认为"通用智能"不是统一的能力，而是在广泛环境中实现广泛目标的能力。

Result: 提出AI智能将是"陌生智能"，在不同领域表现出超人类能力与亚人类表现的混合，甚至在同一个领域内也会结合超人类洞察力和令人惊讶的错误。非线性模型更能准确描述AI智能的本质。

Conclusion: 如果AI是陌生智能，即使最强大的系统也会在看似简单的任务上失败。在非线性智能模型下，这些错误本身不能证明系统缺乏出色的通用智能。反之，在某一类任务上的出色表现（如IQ测试）也不能保证在任务领域之外的广泛能力。

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [6] [DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search](https://arxiv.org/abs/2602.05014)
*Zhanli Li,Huiwen Tian,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.AI

TL;DR: DeepRead是一个结构感知的多轮文档推理代理，通过显式利用文档的层次结构和顺序结构来改进长文档问答，相比传统检索增强生成方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有代理搜索框架通常将长文档视为扁平化的文本块集合，未能充分利用文档固有的先验知识，如层次组织和顺序话语结构。随着工具使用和代理式大语言模型的快速发展，检索增强生成正在从单次被动检索演变为多轮决策驱动的证据获取。

Method: DeepRead使用基于LLM的OCR模型将PDF转换为结构化Markdown，保留标题和段落边界。在段落级别索引文档，并为每个段落分配编码其章节身份和章节内顺序的坐标式元数据键。在此基础上，为LLM配备两个互补工具：检索工具（定位相关段落并暴露其结构坐标）和阅读工具（在指定章节和段落范围内进行连续、顺序保留的阅读）。

Result: 实验表明，DeepRead在文档问答任务上相比Search-o1风格的代理搜索有显著改进。检索工具和阅读工具之间的协同效应得到验证。细粒度行为分析揭示了类似于人类"定位然后阅读"的阅读和推理范式。

Conclusion: DeepRead通过显式操作文档的层次和顺序结构先验，实现了结构感知的多轮文档推理，在长文档问答中表现出优于传统代理搜索框架的性能，展示了检索与阅读工具的协同效应。

Abstract: With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.

</details>


### [7] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: MINT框架通过神经符号树推理知识缺口，利用自博弈优化AI代理的主动询问策略，结合LLM生成最优查询，在未知对象规划任务中实现接近专家水平的性能。


<details>
  <summary>Details</summary>
Motivation: 开放世界规划中存在各种不完全信息和未知因素（如对象、人类目标），导致联合规划中的知识缺口。需要开发AI代理主动询问人类输入的策略来填补这些缺口。

Method: 提出MINT（最小信息神经符号树）：1）构建符号树模拟可能的人机交互；2）使用神经规划策略估计知识缺口导致的规划不确定性；3）通过自博弈优化询问策略；4）利用LLM搜索总结推理过程并生成最优查询。

Result: 在三个涉及未知对象的基准测试中，MINT仅通过有限数量的提问就能达到接近专家水平的回报，显著提高了奖励和成功率。

Conclusion: MINT框架有效解决了开放世界规划中的知识缺口问题，通过主动询问策略实现了高效的人机协作规划，在现实任务中表现出色。

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [8] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: LLMs在已解决的图论问题上表现良好，能正确理解和证明，但在开放问题上只能提供探索性策略而无法产生新见解，适合概念探索但需独立验证。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被学生用于计算机科学高级内容学习，需要评估其在数学严谨思维方面的可靠性，特别是在图论等数学领域的应用效果。

Method: 采用八阶段评估协议模拟真实数学探究过程，包括解释、探索、策略形成和证明构建，测试LLM在已解决图论问题和开放问题上的表现。

Result: 在已解决问题上，LLM表现优异：正确理解定义、识别相关结构、准确回忆结果并构建有效证明；在开放问题上，能提供连贯解释和合理探索策略，但无法推进解决方案，且没有虚构结果。

Conclusion: LLMs适合支持已建立材料的概念探索，但在需要新颖数学洞察或关键结构推理的任务上仍有局限，教育中应引导学生将其用于概念探索，同时依赖独立验证和严格论证进行正式问题解决。

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [9] [Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents](https://arxiv.org/abs/2602.05073)
*Changdae Oh,Seongheon Park,To Eun Kim,Jiatong Li,Wendi Li,Samuel Yeh,Xuefeng Du,Hamed Hassani,Paul Bogdan,Dawn Song,Sharon Li*

Main category: cs.AI

TL;DR: 该论文提出了首个通用的智能体不确定性量化框架，将传统单轮问答UQ扩展为交互式智能体UQ，提出条件不确定性减少的新视角，为LLM智能体安全应用提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 当前不确定性量化研究主要集中于单轮问答场景，但LLM智能体在复杂交互任务中的部署日益增多，需要针对交互式智能体的新UQ框架来确保应用安全。

Method: 提出首个通用的智能体UQ公式化框架，涵盖现有各类UQ设置；引入条件不确定性减少的新视角，强调动作的"交互性"来建模可减少的不确定性；构建概念框架为LLM智能体UQ设计提供指导。

Result: 建立了统一的智能体UQ理论框架，揭示了先前工作隐含地将UQ视为不确定性累积过程，而交互式智能体需要条件不确定性减少的新视角；提供了实用的UQ设计指导。

Conclusion: 智能体UQ对于前沿LLM开发和领域特定应用具有重要实践意义，但仍存在开放问题需要进一步研究，特别是在开放世界交互场景中的不确定性管理。

Abstract: Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.

</details>


### [10] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 使用强化学习框架优化小卫星多碎片主动清除任务的碰撞规避，通过掩码PPO算法动态调整机动策略，提高燃料效率和任务安全性。


<details>
  <summary>Details</summary>
Motivation: 随着地球轨道碎片日益增多，主动碎片清除任务面临碰撞风险高、操作复杂等挑战。小卫星因其灵活性、成本效益和机动性成为理想平台，但需要智能算法来优化多碎片交会任务中的碰撞规避。

Method: 提出基于强化学习的框架，采用掩码近端策略优化算法，集成加油策略、高效任务规划和自适应碰撞规避。RL智能体学习确定多碎片目标的高效交会序列，优化燃料使用和任务时间，同时考虑实时轨道条件和碰撞区域规避。

Result: 使用Iridium 33碎片数据集进行模拟评估，结果表明相比传统启发式方法，该RL框架在降低碰撞风险的同时提高了任务效率，能够适应多样化的轨道配置和碎片分布。

Conclusion: 该工作为复杂多碎片主动清除任务提供了可扩展的规划解决方案，适用于自主空间任务规划中的其他多目标交会问题，展示了强化学习在动态空间操作中的潜力。

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [11] [VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health](https://arxiv.org/abs/2602.05088)
*Kate H. Bentley,Luca Belli,Adam M. Chekroud,Emily J. Ward,Emily R. Dworkin,Emily Van Ark,Kelly M. Johnston,Will Alexander,Millard Brown,Matt Hawrilenko*

Main category: cs.AI

TL;DR: 研究验证了VERA-MH评估框架在AI心理健康应用中的临床有效性和可靠性，特别是针对自杀风险检测和响应的安全性评估。


<details>
  <summary>Details</summary>
Motivation: 随着数百万人使用生成式AI聊天机器人寻求心理支持，AI在心理健康领域的安全性成为最紧迫的问题。需要建立基于证据的自动化安全基准来评估AI工具的安全性。

Method: 首先模拟大量基于大语言模型的用户代理与通用AI聊天机器人的对话；然后由持证心理健康临床医生使用评分标准独立评估对话中的安全/不安全行为；同时使用基于LLM的评估者用相同标准评估相同对话；最后比较临床医生之间以及临床医生共识与LLM评估者之间的一致性。

Result: 临床医生之间的安全评分一致性较高（机会校正的评分者间信度：0.77），建立了黄金标准临床参考；LLM评估者与临床共识高度一致（信度：0.81）；临床医生普遍认为用户代理具有真实性。

Conclusion: 研究支持VERA-MH的临床有效性和可靠性，这是一个开源、全自动的AI心理健康安全评估框架。要实现AI聊天机器人在心理健康领域的潜在益处，安全性至关重要，需要进一步研究VERA-MH的普适性和鲁棒性。

Abstract: Millions now use leading generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based automated safety benchmark. This study aimed to examine the clinical validity and reliability of the VERA-MH evaluation for AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then compared rating alignment across (a) individual clinicians and (b) clinician consensus and the LLM judge, and (c) examined clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR]: 0.77), thus establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus (IRR: 0.81) overall and within key conditions. Clinician raters generally perceived the user-agents to be realistic. For the potential mental health benefits of AI chatbots to be realized, attention to safety is paramount. Findings from this human evaluation study support the clinical validity and reliability of VERA-MH: an open-source, fully automated AI safety evaluation for mental health. Further research will address VERA-MH generalizability and robustness.

</details>


### [12] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 比较三种自主碎片清除任务规划器：固定参数训练的PPO、域随机化PPO和MCTS，评估它们在燃料和时间约束变化下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 自主碎片清除任务规划需要在效率、适应性和严格的燃料/时间约束之间取得平衡，需要开发能够在不同约束条件下保持性能的规划方法。

Method: 使用三种规划器：1) 固定任务参数训练的Masked PPO；2) 在不同任务约束下训练的域随机化Masked PPO；3) 普通MCTS基线。在高保真轨道模拟中评估，包含加油、真实转移动力学和随机碎片场。

Result: 固定PPO在训练条件下表现最佳但分布偏移时性能急剧下降；域随机化PPO适应性更好，仅在名义性能上有适度损失；MCTS对约束变化处理最好但计算时间高几个数量级。

Conclusion: 学习策略的速度与搜索方法的适应性之间存在权衡，结合训练时多样性和在线规划可能是未来弹性ADR任务规划器的有前景路径。

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [13] [GAMMS: Graph based Adversarial Multiagent Modeling Simulator](https://arxiv.org/abs/2602.05105)
*Rohan Patil,Jai Malegaonkar,Xiao Jiang,Andre Dion,Gaurav S. Sukhatme,Henrik I. Christensen*

Main category: cs.AI

TL;DR: GAMMS是一个轻量级可扩展的多智能体仿真框架，基于图结构表示环境，支持快速开发和评估智能体行为，特别适合大规模部署和快速原型设计。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统和多智能体协调在现实应用中日益重要，需要既具有可扩展性又易于使用的仿真工具。现有高保真仿真器计算成本高，不适合快速原型设计或大规模智能体部署。

Method: 开发了基于图的对抗多智能体建模仿真器(GAMMS)，采用轻量级可扩展架构，支持图结构环境表示，强调五个核心目标：可扩展性、易用性、集成优先架构、快速可视化反馈和现实基础。

Result: GAMMS能够高效仿真复杂领域（如城市道路网络和通信系统），支持与外部工具（机器学习库、规划求解器）集成，提供内置可视化功能，支持启发式、优化型和基于学习的智能体（包括使用大语言模型的智能体）。

Conclusion: GAMMS通过降低研究门槛并在标准硬件上实现高性能仿真，促进了多智能体系统、自主规划和对抗建模领域的实验与创新。该框架已开源。

Abstract: As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/

</details>


### [14] [Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment](https://arxiv.org/abs/2602.05110)
*Liang Wang,Junpeng Wang,Chin-chia Michael Yeh,Yan Zheng,Jiarui Sun,Xiran Fan,Xin Dai,Yujie Fan,Yiwei Cai*

Main category: cs.AI

TL;DR: 本文提出了一个结构化多评估者框架，用于评估LLM在商户风险分类中的推理质量，通过共识偏差度量消除循环性，发现不同LLM存在显著评估偏差，匿名化可减少偏差，部分模型与人类专家判断更接近。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被用作推理质量的评估者，但在支付风险场景下的可靠性和偏差仍不明确。需要系统评估LLM在商户分类代码风险评估中的表现，以建立可靠的LLM-as-a-judge系统。

Method: 引入结构化多评估者框架，结合五标准评分表和蒙特卡洛评分，评估推理质量和评估者稳定性。五个前沿LLM在署名和匿名条件下生成并交叉评估MCC风险推理。采用共识偏差度量消除循环性，比较每个评估者分数与其他评估者均值。通过26名支付行业专家评估和支付网络数据进行验证。

Result: 结果显示显著异质性：GPT-5.1和Claude 4.5 Sonnet显示负自我评估偏差(-0.33, -0.31)，而Gemini-2.5 Pro和Grok 4显示正偏差(+0.77, +0.71)，匿名化使偏差减少25.8%。LLM评估者评分平均比人类共识高0.46分，GPT-5.1和Claude 4.5 Sonnet的负偏差反映与人类判断更接近。四个模型与支付网络数据显著相关(Spearman rho = 0.56-0.77)。

Conclusion: 该框架为支付风险工作流中的LLM-as-a-judge系统评估提供了可复现的基础，强调了在金融操作环境中采用偏差感知协议的必要性。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

</details>


### [15] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: DemPO框架通过算法抽签（类似公民大会机制）构建代表性人类偏好样本，用于AI对齐训练，相比传统便利样本能更好地反映多元价值观。


<details>
  <summary>Details</summary>
Motivation: 当前基于偏好的AI对齐方法（如RLHF）依赖的人类评分者通常是便利样本，存在系统性人口统计偏差，无法代表全体人群的价值观。

Method: 提出民主偏好优化（DemPO）框架，采用算法抽签机制构建代表性样本：硬面板（仅使用抽签选出的配额满足样本）和软面板（保留所有数据但按抽签概率重新加权）。

Result: 在1B到8B参数的Llama模型上测试，硬面板在六种聚合方法中始终排名第一，软面板始终优于未加权基线，且模型容量越大效果越明显。

Conclusion: 在偏好收集阶段强制执行人口统计代表性（而非事后校正）能产生更好地反映代表性公众价值观的模型行为。

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [16] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: SocialVeil：一个模拟认知差异导致沟通障碍的社会学习环境，用于评估LLM在非理想沟通中的社会智能


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常假设代理之间的理想化沟通，限制了诊断LLM在更现实、不完美环境中维持和修复交互的能力。需要填补这一空白，使社会交互环境更接近真实世界沟通。

Method: 基于对人际沟通挑战的系统文献综述，提出SocialVeil环境，模拟三种代表性沟通障碍：语义模糊性、社会文化不匹配和情绪干扰。引入两种障碍感知评估指标：未解决困惑和相互理解。在720个场景和四个前沿LLM上进行实验。

Result: 障碍持续损害性能：相互理解平均降低超过45%，困惑提升近50%。人类评估验证了模拟障碍的保真度（ICC≈0.78，Pearson r≈0.80）。适应策略（修复指导和交互学习）效果有限，远未达到无障碍性能。

Conclusion: SocialVeil使社会交互环境更接近真实世界沟通，为探索LLM代理的社会智能开辟了机会。当前LLM在非理想沟通环境中仍面临显著挑战，需要进一步研究提升其社会智能。

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [17] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT是一个混沌感知的时空跨城市知识迁移框架，用于数据稀缺的跨城市交通预测，通过混沌分析量化交通可预测性机制，实现机制自适应建模和跨城市知识对齐。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺的跨城市交通预测面临复杂非线性动力学和领域偏移的挑战，现有方法难以捕捉交通的固有混沌特性进行有效的少样本学习。

Method: 提出CAST-CKT框架：1）使用高效混沌分析器量化交通可预测性机制；2）机制感知注意力进行机制自适应时间建模；3）自适应拓扑学习动态空间依赖；4）基于混沌一致性的跨城市对齐进行知识迁移；5）提供特定预测时域和不确定性量化。

Result: 在四个基准数据集上的跨城市少样本实验中，CAST-CKT在MAE和RMSE指标上显著优于现有最先进方法，同时提供可解释的机制分析。理论分析显示改进的泛化边界。

Conclusion: CAST-CKT通过混沌感知建模有效解决了跨城市少样本交通预测问题，在性能和可解释性方面均有显著提升，为数据稀缺场景下的时空预测提供了新思路。

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [18] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG是一个基于因果门控的层次化图RAG框架，通过显式建模因果关系来抑制虚假相关性，实现大规模知识图上的可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法过度依赖表层节点匹配，缺乏显式因果建模，导致答案不可靠或虚假。之前的因果方法局限于局部或单文档上下文，且受到模块化图结构造成的信息隔离问题，阻碍了可扩展性和跨模块因果推理。

Method: 提出HugRAG框架，通过因果门控在层次化模块间重新组织知识，显式建模因果关系以抑制虚假相关性，同时支持大规模知识图上的可扩展推理。

Result: 在多个数据集和评估指标上，HugRAG始终优于竞争性的基于图的RAG基线方法。

Conclusion: 该工作为结构化、可扩展且基于因果基础的RAG系统建立了原则性基础。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [19] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: 作者分享了10个研究级数学问题来评估当前AI系统回答数学研究问题的能力


<details>
  <summary>Details</summary>
Motivation: 评估当前AI系统正确回答研究级数学问题的能力，这些问题来自作者实际研究过程中自然产生的问题

Method: 分享一组10个未公开的数学问题，这些问题来自作者的研究过程，答案已知但暂时加密

Result: 创建了一个包含10个研究级数学问题的测试集，用于评估AI系统的数学推理能力

Conclusion: 通过分享实际研究中的数学问题，为评估AI数学能力提供了一个真实且有意义的基准测试

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [20] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: 该论文针对多知识库检索增强生成中的权威性偏差问题，提出DAKS路由与对齐图融合方法，在藏医药领域实现更好的跨知识库证据覆盖和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 在多知识库检索增强生成中，密集的百科全书条目容易主导检索结果，而权威性更高的经典文献和临床论文可能被忽视，特别是在藏医药这种领域知识复杂的场景下。

Method: 提出两种互补方法：1) DAKS进行知识库路由和预算检索，缓解密度驱动偏差并优先考虑权威来源；2) 使用对齐图指导证据融合和覆盖感知打包，改善跨知识库证据覆盖。

Result: 实验显示在路由质量和跨知识库证据覆盖方面获得一致提升，完整系统在保持强忠实性和引用正确性的同时，实现了最佳的CrossEv@5指标。

Conclusion: 通过结合知识库路由和对齐图融合，能够有效解决多知识库检索增强生成中的权威性偏差问题，提高跨知识库验证能力和可追溯性。

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [21] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: 提出名为Surgery的防御方法，通过抑制注意力头的sink divergence来减轻有害微调对LLM安全性的影响


<details>
  <summary>Details</summary>
Motivation: 有害微调会破坏大语言模型的安全对齐，带来重大安全风险，需要有效的防御方法

Method: 基于sink divergence可分离假设，提出Surgery方法，使用正则化器抑制sink divergence，引导注意力头向负sink divergence组移动

Result: 在BeaverTails、HarmBench和SorryBench基准测试上分别提升防御性能5.90%、11.25%和9.55%

Conclusion: 通过注意力sink机制可以有效防御有害微调，Surgery方法显著提升了模型的安全性

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [22] [Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models](https://arxiv.org/abs/2602.05240)
*Patrick McGonagle,William Farrelly,Kevin Curran*

Main category: cs.AI

TL;DR: 该研究通过整合多种可解释AI技术（GRAD-CAM、LRP、SHAP）来增强脑肿瘤检测深度学习模型的可解释性，在BraTS 2021数据集上达到91.24%准确率，提供从区域到像素级的全面解释。


<details>
  <summary>Details</summary>
Motivation: 增强AI驱动医学影像分析的透明度和可信度，特别是在脑肿瘤检测等关键医疗任务中，需要更全面的模型决策过程解释。

Method: 开发定制卷积神经网络（CNN）在BraTS 2021数据集上训练，结合三种XAI技术：GRAD-CAM突出重要空间区域，LRP提供像素级相关性，SHAP量化特征贡献。

Result: 模型达到91.24%准确率，成功识别完整和部分肿瘤，多技术集成方法相比单一XAI方法显示出更优越的解释能力，能有效解释包括部分肿瘤可见情况在内的模型预测。

Conclusion: 集成XAI技术能显著提高医疗AI系统的可靠性和可解释性，为脑肿瘤检测等关键任务提供更全面的模型推理视角，增强透明度和信任度。

Abstract: This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.

</details>


### [23] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 提出TEA方法，通过动态原位任务生成评估智能体在未见3D环境中的能力，发现现有模型在基本感知任务上表现不佳，缺乏3D交互意识


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在数据污染和缺乏场景特异性问题，无法有效评估智能体在未见环境中的能力，需要开发针对未见3D环境的评估方法

Method: 提出TEA方法：1) 使用结构化图表示定义任务；2) 构建两阶段交互-进化任务生成系统：交互阶段通过任务执行与生成的循环持续生成任务，进化阶段通过任务图建模重组现有任务生成新任务

Result: 在10个未见场景中，TEA在两个周期内自动生成了87,876个任务，经人工验证具有物理合理性和日常认知能力覆盖；SOTA模型在基本感知任务上表现不佳，缺乏3D交互意识，对任务类型敏感

Conclusion: 现有模型在未见环境中的能力评估不足，TEA方法能有效生成原位任务进行评估，强调了在真实世界部署前进行原位评估的必要性

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [24] [Beyond Cosine Similarity](https://arxiv.org/abs/2602.05266)
*Xinbo Ai*

Main category: cs.AI

TL;DR: 提出新的相似度度量recos，通过排序向量分量归一化点积，比余弦相似度更能捕捉复杂语义关系，在多个嵌入模型上表现更优。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度基于柯西-施瓦茨不等式，只能捕捉线性关系，无法建模真实语义空间的复杂非线性结构，需要更强大的相似度度量。

Method: 推导出比经典柯西-施瓦茨界更紧的点积上界，基于此提出recos度量，通过排序向量分量归一化点积，将完美相似条件从线性依赖放宽为序数一致性。

Result: 在11种嵌入模型（静态、上下文化、通用型）上实验，recos在标准语义文本相似度基准上始终优于传统余弦相似度，与人类判断相关性更高。

Conclusion: recos作为数学原理严谨且经验上优越的替代方案，为复杂嵌入空间中的语义分析提供了更高的准确性。

Abstract: Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.

</details>


### [25] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: 提出一个原则性框架，将LLM集成到安全管理的决策支持中，通过一致性检查和外部反馈控制幻觉风险，在事件响应场景中显著减少恢复时间。


<details>
  <summary>Details</summary>
Motivation: LLM在安全管理工作（如事件响应规划）中具有潜力，但其不可靠性和幻觉倾向是主要挑战，需要解决这些问题以实现可靠的决策支持。

Method: 设计了一个迭代框架：LLM生成候选动作，系统检查其与约束和前瞻预测的一致性；当一致性低时，通过数字孪生等外部反馈收集信息，使用上下文学习（ICL）细化动作；通过调节一致性阈值控制幻觉风险。

Result: 在四个公共数据集上的实验表明，该框架相比前沿LLM将恢复时间减少了高达30%；理论上证明了通过调节一致性阈值可以控制幻觉风险，并在特定假设下建立了ICL的遗憾界限。

Conclusion: 该框架为在安全管理中使用LLM提供了可靠的方法，通过结合一致性检查和外部反馈有效控制幻觉风险，在事件响应场景中实现了显著的性能提升。

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [26] [Position: Universal Time Series Foundation Models Rest on a Category Error](https://arxiv.org/abs/2602.05287)
*Xilin Dai,Wanxu Cai,Zhijian Xu,Qiang Xu*

Main category: cs.AI

TL;DR: 该论文认为"通用时间序列基础模型"存在范畴错误，将结构容器误认为语义模态，提出因果控制智能体范式替代通用模型


<details>
  <summary>Details</summary>
Motivation: 当前追求通用时间序列基础模型存在根本问题，因为不同领域的时间序列（如金融与流体动力学）具有不相容的生成过程，导致单一模型在分布漂移下泛化失败

Method: 提出"自回归盲区界限"理论证明仅依赖历史的模型无法预测干预驱动的机制转换，倡导采用因果控制智能体范式，利用外部上下文协调专业求解器层次结构

Result: 论证了通用时间序列模型的局限性，提出了新的理论界限和替代范式，建议基准从"零样本准确率"转向"漂移适应速度"

Conclusion: 应放弃通用性追求，转向因果控制智能体范式，构建能够快速适应分布漂移的鲁棒控制系统，重新定义评估标准

Abstract: This position paper argues that the pursuit of "Universal Foundation Models for Time Series" rests on a fundamental category error, mistaking a structural Container for a semantic Modality. We contend that because time series hold incompatible generative processes (e.g., finance vs. fluid dynamics), monolithic models degenerate into expensive "Generic Filters" that fail to generalize under distributional drift. To address this, we introduce the "Autoregressive Blindness Bound," a theoretical limit proving that history-only models cannot predict intervention-driven regime shifts. We advocate replacing universality with a Causal Control Agent paradigm, where an agent leverages external context to orchestrate a hierarchy of specialized solvers, from frozen domain experts to lightweight Just-in-Time adaptors. We conclude by calling for a shift in benchmarks from "Zero-Shot Accuracy" to "Drift Adaptation Speed" to prioritize robust, control-theoretic systems.

</details>


### [27] [Aspect-Aware MOOC Recommendation in a Heterogeneous Network](https://arxiv.org/abs/2602.05297)
*Seongyeub Chu,Jongwoo Kim,Mun Yong Yi*

Main category: cs.AI

TL;DR: AMR是一个面向MOOC推荐的创新框架，通过双向游走自动发现元路径，利用bi-LSTM编码器生成面向特定方面的路径表示，并将其作为边特征融入学习者-学习者和知识点-知识点子图，实现细粒度的语义感知推荐。


<details>
  <summary>Details</summary>
Motivation: 传统MOOC推荐方法（协同过滤、基于内容的过滤）存在数据稀疏性和过度专业化问题。现有的基于图的方法虽然有所改进，但严重依赖手动预定义的元路径，这些元路径通常只能捕捉表面的结构关系，且给领域专家带来沉重负担和工程成本。

Method: 提出AMR框架：1）通过双向游走自动发现元路径；2）使用基于bi-LSTM的编码器嵌入每个元路径中节点的语义内容，生成面向特定方面的路径表示；3）将这些表示作为边特征融入学习者-学习者和知识点-知识点子图，实现细粒度的语义感知知识点推荐。

Result: 在MOOCCube和PEEK两个大规模数据集上的实验表明，AMR在HR@K和nDCG@K等关键指标上持续优于最先进的图神经网络基线。进一步分析证实AMR能有效捕捉丰富的路径特定方面信息，比仅依赖预定义元路径的方法提供更准确的推荐。

Conclusion: AMR通过自动发现元路径和嵌入语义内容，克服了传统方法对预定义元路径的依赖，实现了更准确、语义感知的MOOC推荐，为学习者提供更精细化的学习内容导航。

Abstract: MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.

</details>


### [28] [PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences](https://arxiv.org/abs/2602.05302)
*Chris Zhu,Sasha Cui,Will Sanok Dufallo,Runzhi Jin,Zhen Xu,Linjun Zhang,Daylian Cain*

Main category: cs.AI

TL;DR: GPT-5在PieArena谈判基准测试中达到或超越商学院学生水平，展现了AGI级谈判能力，但稳健性和可信度仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在谈判这一核心商业任务中的能力，谈判需要战略推理、心智理论和经济价值创造，这对AI在现实商业环境中的应用至关重要。

Method: 引入PieArena大规模谈判基准，基于精英商学院MBA谈判课程的真实场景进行多智能体交互评估，并研究联合意向性智能体支架的效果。

Result: GPT-5在谈判表现上匹配或超越经过一学期谈判训练和针对性指导的商学院学生。联合意向性支架对中低层LLMs有显著提升，但对前沿模型收益递减。谈判行为分析揭示了在欺骗、计算准确性、指令遵从和感知声誉等方面的模型异质性。

Conclusion: 前沿语言智能体在智力和心理层面已具备在高风险经济环境中部署的能力，但稳健性和可信度方面的缺陷仍是开放挑战。

Abstract: We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.

</details>


### [29] [ProAct: Agentic Lookahead in Interactive Environments](https://arxiv.org/abs/2602.05327)
*Yangbin Yu,Mingyu Yang,Junyou Li,Yiming Gao,Feiyu Liu,Yijun Yang,Zichuan Lin,Jiafei Lyu,Yicheng Liu,Zhicong Lu,Deheng Ye,Jie Jiang*

Main category: cs.AI

TL;DR: ProAct框架通过两阶段训练提升LLM智能体在交互环境中的长期规划能力，包括GLAD监督微调和MC-Critic价值估计器，显著提高规划准确性


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在需要长期规划的交互环境中表现不佳，主要原因是模拟未来状态时错误会不断累积。需要一种方法让智能体内化准确的预见推理能力

Method: 提出ProAct框架：1) GLAD：通过环境搜索轨迹进行监督微调，将复杂搜索树压缩为简洁的因果推理链；2) MC-Critic：轻量级环境rollout校准价值估计的插件式辅助价值估计器，增强PPO/GRPO等策略梯度算法

Result: 在随机（如2048）和确定性（如Sokoban）环境中，ProAct显著提高规划准确性。4B参数模型超越所有开源基线，媲美最先进的闭源模型，并在未见环境中展现鲁棒泛化能力

Conclusion: ProAct通过将搜索知识蒸馏到LLM中并引入低方差价值估计，有效解决了LLM智能体长期规划中的错误累积问题，实现了高效准确的预见推理

Abstract: Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct

</details>


### [30] [AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction](https://arxiv.org/abs/2602.05353)
*Ruijie Shi,Houbin Zhang,Yuecheng Han,Yuheng Wang,Jingru Fan,Runde Yang,Yufan Dang,Huatao Li,Dewen Liu,Yuan Cheng,Chen Qian*

Main category: cs.AI

TL;DR: 提出AgentXRay框架，通过输入-输出访问重构黑盒智能体系统的可解释工作流，使用蒙特卡洛树搜索和红黑剪枝优化搜索空间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂问题解决中表现出色，但许多智能体系统由于内部工作流程不透明而难以解释和控制。现有框架虽然提供明确的协作架构，但许多部署的智能体系统对用户来说仍是黑盒。

Method: 提出Agentic Workflow Reconstruction (AWR)任务，旨在仅通过输入-输出访问合成显式、可解释的替代工作流。开发AgentXRay框架，将AWR形式化为组合优化问题，在链式结构工作流空间中搜索离散的智能体角色和工具调用。采用蒙特卡洛树搜索，并通过基于评分的红黑剪枝机制增强，动态整合代理质量和搜索深度。

Result: 实验表明，AgentXRay在多个领域实现了更高的代理相似度，相比未剪枝搜索减少了token消耗，在固定迭代预算下支持更深层的工作流探索。

Conclusion: AgentXRay能够有效重构黑盒智能体系统的可解释工作流，提供可编辑的白盒工作流，无需访问模型参数，提高了智能体系统的透明度和可控性。

Abstract: Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.

</details>


### [31] [PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents](https://arxiv.org/abs/2602.05354)
*Shifat E. Arman,Syed Nazmus Sakib,Tapodhir Karmakar Taton,Nafiul Haque,Shahrear Bin Amin*

Main category: cs.AI

TL;DR: PATHWAYS基准测试包含250个多步决策任务，测试网络代理能否发现并正确使用隐藏上下文信息。当前代理在发现关键隐藏证据、推翻误导性表面信号、整合证据等方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 评估当前基于网络的智能体是否具备发现和利用隐藏上下文信息的能力，特别是在需要推翻表面误导信号的情况下。研究智能体在复杂决策任务中的自适应调查、证据整合和判断覆盖机制。

Method: 创建包含250个多步决策任务的PATHWAYS基准测试，要求智能体在网页环境中发现隐藏的上下文信息。测试了闭源和开源模型，分析智能体在导航、证据发现、证据整合和最终决策等方面的表现。

Result: 智能体通常能导航到相关页面，但只在少数情况下能检索到决定性隐藏证据。当任务需要推翻误导性表面信号时，性能急剧下降至接近随机水平。智能体经常产生幻觉，声称依赖从未访问的证据。即使发现正确上下文，也常常无法整合到最终决策中。更明确的指令能改善上下文发现，但会降低整体准确性。

Conclusion: 当前网络智能体架构缺乏可靠的自适应调查、证据整合和判断覆盖机制。在程序合规性和有效判断之间存在权衡，需要开发更强大的推理和决策架构。

Abstract: We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.

</details>


### [32] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT是一种新颖的2位量化框架，通过算法强制残差层次结构解决二进制路径间的特征共适应问题，在保持硬件友好性的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署需要极端量化，但传统残差二值化方法存在并行残差二进制路径学习冗余特征的问题（称为路径间适应），这限制了模型的表达能力。

Method: 提出RaBiT框架，核心机制是从单个共享全精度权重顺序推导每个二进制路径，确保每个路径纠正前一个路径的误差，并通过强调功能保持而非权重近似的鲁棒初始化来稳定训练过程。

Result: RaBiT重新定义了2位精度-效率前沿：实现了最先进的性能，甚至可与硬件密集的向量量化方法相媲美，在RTX 4090上相比全精度模型实现了4.49倍的推理加速。

Conclusion: RaBiT通过算法强制残差层次结构有效解决了二进制路径间的特征共适应问题，为大型语言模型的极端量化提供了硬件友好且高性能的解决方案。

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [33] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 本研究评估了四个小型医疗LLM在眼科患者咨询中的表现，发现Meerkat-7B表现最佳，MedLLaMA3-v20表现最差且有25.5%的回答包含幻觉或误导内容。GPT-4-Turbo评估与临床医生评分高度一致，支持LLM评估在大规模基准测试中的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定大语言模型在眼科患者教育、分诊和临床决策支持中的应用日益增多，需要进行严格评估以确保其安全性和准确性，同时探索LLM评估与临床医生评估的一致性。

Method: 采用横断面研究设计，让四个参数小于100亿的小型医疗LLM（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20）回答180个眼科患者咨询问题，共生成2160个回答。由三位不同资历的眼科医生和GPT-4-Turbo使用S.C.O.R.E.框架（安全性、共识与背景、客观性、可重复性、可解释性）进行五级李克特量表评分。使用Spearman等级相关、Kendall tau统计和核密度估计分析评估LLM与临床医生评分的一致性。

Result: Meerkat-7B表现最佳，分别获得高级顾问3.44分、顾问4.08分、住院医师4.18分的平均分。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导内容，包括捏造的术语。GPT-4-Turbo评估与临床医生评估整体高度一致（Spearman rho=0.80，Kendall tau=0.67），但高级顾问评分更为保守。

Conclusion: 医疗LLM在眼科问题回答中显示出安全潜力，但在临床深度和共识方面仍存在差距。研究支持LLM评估在大规模基准测试中的可行性，并表明需要结合自动化评估和临床医生审查的混合框架来指导安全的临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [34] [Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation](https://arxiv.org/abs/2602.05403)
*Chenghua Gong,Yihang Jiang,Hao Li,Rui Sun,Juyuan Zhang,Tianjun Gu,Liming Pan,Linyuan Lü*

Main category: cs.AI

TL;DR: 提出OPINN框架，通过扩散-对流-反应系统建模意见动力学，结合神经ODE与物理先验，在真实和合成数据集上实现最先进的意见演化预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有意见动力学建模方法存在两个主要问题：1）基于不完整的先验知识，缺乏整合局部、全局和内生层面动态的完整物理系统；2）基于惩罚的约束方法难以深度编码物理先验，导致优化问题和潜在表示与物理透明度之间的差异。

Method: 提出OPINN框架：1）从相互作用粒子理论出发，通过扩散-对流-反应系统解释意见动力学；2）基于神经ODE定义神经意见动力学，协调神经网络与物理先验；3）构建物理信息神经网络框架进行意见动力学建模。

Result: 在真实世界和合成数据集上的评估表明，OPINN在意见演化预测方面实现了最先进的性能。

Conclusion: OPINN为网络、物理和社会系统的交叉领域提供了一个有前景的范式，能够协同机制可解释性与数据驱动灵活性。

Abstract: Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.

</details>


### [35] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: H-AdminSim是一个端到端的医院管理模拟框架，结合真实数据生成和多智能体模拟，通过FHIR集成提供标准化测试平台，用于评估LLM驱动的管理自动化性能。


<details>
  <summary>Details</summary>
Motivation: 医院管理部门每天处理大量请求（超过10,000个），对LLM自动化有强烈需求。现有研究主要关注医患交互或孤立的管理子任务，未能捕捉真实管理流程的复杂性。

Method: 提出H-AdminSim框架，结合真实数据生成和多智能体模拟医院管理流程，使用详细评分标准进行定量评估，并通过FHIR集成提供统一、可互操作的环境。

Result: 该框架能够系统比较不同LLM的性能，为评估LLM驱动的管理自动化可行性和性能提供标准化测试平台。

Conclusion: H-AdminSim填补了现有研究的空白，通过综合模拟框架为医院管理自动化提供了有效的评估工具，支持跨异构医院环境的测试。

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [36] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: THOR：一种用于超关系知识图谱的归纳式链接预测方法，通过关系基础图和实体基础图建模跨图谱的结构不变性，支持完全归纳推理。


<details>
  <summary>Details</summary>
Motivation: 现有超关系知识图谱链接预测方法大多局限于转导式设置，只能在同一词汇表内进行预测，无法泛化到未见过的词汇表，限制了模型的通用性。

Method: 提出THOR方法：1) 构建关系基础图和实体基础图，建模超关系知识图谱中关系和实体的跨事实交互；2) 使用两个并行图编码器学习基础图表示，后接Transformer解码器，支持高效掩码训练和完全归纳推理。

Result: 在12个数据集上的评估显示，THOR显著优于基线方法：比最佳规则方法提升66.1%，比最佳半归纳方法提升55.9%，比最佳完全归纳方法提升20.4%。消融研究验证了关键设计因素的有效性。

Conclusion: THOR通过建模超关系知识图谱中的结构不变性，实现了有效的归纳式链接预测，能够泛化到未见过的词汇表，为超关系知识图谱推理提供了新的解决方案。

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [37] [M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining](https://arxiv.org/abs/2602.05429)
*Rui Lv,Juncheng Mo,Tianyi Chu,Chen Rao,Hongyi Jing,Jiajie Teng,Jiafu Chen,Shiqi Zhang,Liangzi Ding,Shuo Fang,Huaizhong Lin,Ziqiang Dang,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: M²-Miner：首个基于蒙特卡洛树搜索的低成本自动化移动GUI代理数据挖掘框架，通过多智能体协作、意图回收和渐进式训练策略，解决GUI代理数据构建的高成本、低质量和低丰富度问题。


<details>
  <summary>Details</summary>
Motivation: 构建强大的GUI代理需要大规模标注高质量的用户行为轨迹数据（意图-轨迹对），但现有手动标注方法和GUI代理数据挖掘方法面临三个关键挑战：高构建成本、数据质量差和数据丰富度低。

Method: 提出M²-Miner框架：1）基于蒙特卡洛树搜索的自动化数据挖掘；2）协作多智能体框架（InferAgent指导、OrchestraAgent加速、JudgeAgent评估）；3）意图回收策略提取额外有价值的交互轨迹；4）渐进式模型在环训练策略提高数据挖掘成功率。

Result: 实验表明，使用M²-Miner挖掘数据微调的GUI代理在多个常用移动GUI基准测试中达到最先进的性能。

Conclusion: M²-Miner是首个低成本、自动化的移动GUI代理数据挖掘框架，有效解决了GUI代理数据构建的关键挑战，为社区研究提供了有力支持。

Abstract: Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.

</details>


### [38] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: 该论文提出了一种尖峰正则化策略，并评估了多种时间序列基础模型在波动性电力市场价格预测中的表现，结果显示这些模型相比传统方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 电力市场价格预测对能源市场参与者至关重要，但由于价格信号固有的波动性和非线性，传统统计和深度学习模型难以有效捕捉复杂的时间依赖关系并整合异构数据。虽然时间序列基础模型在一般时间序列预测任务中表现出色，但在波动性市场中的日前电力价格预测效果尚未充分探索。

Method: 提出尖峰正则化策略，评估多种时间序列基础模型（包括Tiny Time Mixers、MOIRAI、MOMENT、TimesFM），并与传统统计模型（ARIMA）和深度学习模型（LSTM、CNN-LSTM）进行比较。使用新加坡具有波动趋势的半小时批发市场数据，并在适用模型中纳入天气和日历变量等外生因素。

Result: 时间序列基础模型在所有评估设置中始终优于传统方法，在MAPE指标上实现了高达37.4%的改进。

Conclusion: 研究结果为提高波动性电力市场的预测准确性和决策制定提供了实用指导，证明了时间序列基础模型在电力价格预测中的优越性。

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [39] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: OD-CRL通过自适应正交基优化和零空间去噪投影，解决了条件表示学习中基向量敏感和子空间干扰问题，在多个定制化任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件表示学习方法存在两个关键限制：对子空间基向量的敏感性，以及子空间间干扰的脆弱性。这些问题影响了条件特征提取的准确性和鲁棒性。

Method: 提出OD-CRL框架，包含两个核心组件：1) 自适应正交基优化(AOBO)，通过奇异值分解和基于曲率的截断构建正交语义基；2) 零空间去噪投影(NSDP)，通过将嵌入投影到无关子空间的零空间来抑制非目标语义干扰。

Result: 在定制化聚类、分类和检索任务上的广泛实验表明，OD-CRL实现了新的最先进性能，并具有优越的泛化能力。

Conclusion: OD-CRL通过正交基优化和零空间投影有效解决了条件表示学习中的基向量敏感性和子空间干扰问题，为定制化任务提供了更鲁棒和准确的特征表示。

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [40] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE框架通过对抗学习和指导性语言反馈，让LLM内部化推理逻辑，摆脱传统强化学习对稀缺标量奖励的依赖，实现无监督的推理能力对齐。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖的标量奖励存在三大问题：扩展成本高、跨领域脆弱、无法理解解决方案的内在逻辑。这种对外部贫乏信号的依赖阻碍了模型对推理原则的深度理解。

Method: 提出ALIVE框架，基于"认知协同"原则，将问题提出、解决和评判统一在单一策略模型中。通过对抗学习和指导性语言反馈，让模型直接从原始语料中内化评估标准，将外部批评转化为内生的推理能力。

Result: 在数学推理、代码生成和一般逻辑推理基准测试中，ALIVE持续缓解了奖励信号的局限性。在相同数据和计算条件下，实现了准确率提升、跨领域泛化能力显著改善和更高的自我纠正率。

Conclusion: 推理三位一体（问题提出、解决、评判）促进了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础。

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [41] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former是一种用于预测化合物-蛋白质相互作用的分层表示学习方法，通过原子-原子、基序-基序和原子-基序三个层次建模相互作用，更好地反映生物识别机制。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽然能在原子层面建模化合物-蛋白质相互作用，但忽略了分子片段（基序或功能基团）作为生物识别和结合主要单元的重要性，导致模型与化学现实不完全一致。

Method: 提出Phi-former方法：1）对化合物和蛋白质进行分层表示；2）采用成对预训练框架，系统建模原子-原子、基序-基序、原子-基序三个层次的相互作用；3）设计层内和层间学习管道，使不同相互作用层次相互促进。

Result: Phi-former在CPI相关任务上表现出优越性能。案例研究表明，该方法能准确识别CPI中激活的特定原子或基序，提供可解释的模型解释。

Conclusion: Phi-former通过分层建模分子相互作用，更好地反映了生物识别机制，为理性药物设计和精准医疗应用提供了有价值的见解和指导。

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [42] [SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration](https://arxiv.org/abs/2602.05499)
*Hanyu Wei,Zunhai Su,Peng Lu,Chao Li,Spandan Tiwari,Ashish Sirasao,Yuhan Dong*

Main category: cs.AI

TL;DR: SDFP是一个无需训练、即插即用的推测解码框架，通过基于Fisher信息迹的层剪枝从原始LLM构建轻量级草稿模型，实现1.32-1.5倍解码加速


<details>
  <summary>Details</summary>
Motivation: LLM的自回归解码延迟高，影响多媒体应用性能。现有推测解码方法需要额外训练或维护草稿模型，成本高且部署复杂

Method: 使用Fisher信息迹评估层敏感性，剪除对输出影响小的层，从原始LLM构建紧凑草稿模型，保持与原始模型的兼容性进行标准推测验证

Result: 在基准测试中实现1.32-1.5倍解码加速，不改变目标模型的输出分布，支持低延迟多媒体应用

Conclusion: SDFP提供了一种无需训练、部署友好的草稿模型构建方法，有效降低LLM解码延迟，适用于多媒体应用场景

Abstract: Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.

</details>


### [43] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: 开发了一个针对成釉细胞瘤的多模态数据集和AI模型，用于变体分类、复发风险评估和手术规划支持，显著提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有AI诊断资源在颌面病理学中缺乏高质量、结构化的多模态数据集，特别是对成釉细胞瘤的覆盖不足，格式不一致，无法直接用于模型训练。

Method: 创建了专门针对成釉细胞瘤的多模态数据集，整合了放射学、组织病理学和口腔临床图像，使用自然语言处理从病例报告中提取临床特征，并对图像数据进行特定领域预处理和增强。开发了多模态深度学习模型，可接受临床表现、年龄、性别等临床输入。

Result: 定量评估显示显著改进：变体分类准确率从46.2%提升至65.9%，异常组织检测F1分数从43.0%提升至90.3%。与MultiCaRe等资源相比，该工作提供了更强大的数据集和适应性强的多模态AI框架。

Conclusion: 这项工作通过提供高质量数据集和可适应的多模态AI框架，推进了患者特异性决策支持，为颌面病理学AI诊断建立了重要基础。

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [44] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: 提出Split Personality Training (SPT)方法，通过微调一个"诚实人格"到LoRA参数中，在模型正常运行时保持休眠，需要审计时激活该人格来审查主模型的输出，有效检测大语言模型中的错位行为。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型的错位行为具有挑战性，因为模型可能在训练中学会隐藏不当行为。现有审计方法不足：黑盒方法难以区分错位输出和良性输出，机制可解释性方法无法随模型能力扩展。

Method: SPT方法微调第二个"诚实人格"到LoRA参数中，该参数在正常操作时保持休眠。主模型响应后，激活LoRA适配器并插入触发字符串，使诚实人格能够审查响应同时访问主模型的潜在状态。

Result: 在Anthropic审计游戏模型基准测试中，Llama-3.3-70B模型被训练利用奖励漏洞同时隐藏此行为。SPT达到96%的总体准确率，而Anthropic报告接近0%的准确率。诚实人格揭示了外部观察者无法访问的潜在知识。

Conclusion: SPT提供了一种有效检测大语言模型错位行为的方法，通过创建独立的诚实人格来审计模型内部状态，克服了现有审计方法的局限性，能够揭示模型隐藏的偏见和不当行为。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [45] [Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach](https://arxiv.org/abs/2602.05533)
*Zhengyi Guo,Wenpin Tang,Renyuan Xu*

Main category: cs.AI

TL;DR: 提出基于Doob's h-transform的扩散模型条件生成框架，通过漂移修正实现硬约束条件生成，无需修改预训练模型，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用和罕见事件模拟中，需要确保生成样本以概率1满足硬约束条件，而现有的软约束或基于奖励的引导方法无法保证约束满足。

Method: 基于Doob's h-transform、鞅表示和二次变差过程，开发条件扩散引导框架，通过添加包含条件函数对数梯度的显式漂移修正来引导预训练扩散模型，提出基于鞅损失和鞅协变损失的两个离策略学习算法来估计h及其梯度。

Result: 在总变差和Wasserstein距离上为非条件采样器提供非渐近保证，明确刻画了分数近似和引导估计误差的影响，数值实验验证了方法在强制执行硬约束和生成罕见事件样本方面的有效性。

Conclusion: 提出了一个理论严谨的条件扩散引导框架，能够保证硬约束条件的满足，为安全关键应用和罕见事件模拟提供了可靠的生成方法。

Abstract: We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.

</details>


### [46] [Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation](https://arxiv.org/abs/2602.05544)
*Fahad Anwaar,Adil Mehmood Khan,Muhammad Khalid,Usman Zia,Kezhi Wang*

Main category: cs.AI

TL;DR: RGCF-XRec是一个混合框架，通过推理引导的协同过滤知识增强语言模型，在单一步骤中实现可解释的顺序推荐，显著提升推荐性能和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在可解释推荐系统中忽视了协同信号，且将推荐和解释作为独立任务处理，导致内存占用大。需要一种能同时处理协同信号和语义信号，并在单一步骤中完成可解释推荐的方法。

Method: 提出RGCF-XRec框架：1) 通过上下文提示进行推理引导的协同过滤知识增强，发现潜在偏好和可解释推理路径；2) 基于一致性、完整性、相关性和连贯性四个维度的高效评分机制，过滤噪声推理轨迹；3) 统一表示学习网络编码协同和语义信号，构建结构化提示来指导LLM进行可解释顺序推荐。

Result: 在Amazon数据集（Sports、Toys、Beauty，共642,503个用户-物品交互）上表现优异：HR@10在Sports提升7.38%，Toys提升4.59%；ROUGE-L分别提升8.02%和3.49%；冷启动性能提升14.5%，热启动提升11.9%；零样本HR@5在Beauty提升18.54%，Toys提升23.16%。使用轻量级LLaMA 3.2-3B骨干确保训练效率和可扩展性。

Conclusion: RGCF-XRec通过融合协同过滤知识和语言模型推理能力，在单一步骤中实现了高效、可解释的顺序推荐，显著提升了推荐性能、解释质量和泛化能力，同时保持了计算效率，适用于实际应用场景。

Abstract: Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\% in Sports and 4.59\% in Toys, along with ROUGE-L by 8.02\% and 3.49\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\% in cold-start and 11.9\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\% in Beauty and 23.16\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.

</details>


### [47] [TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570)
*Yikun Zong,Cheston Tan*

Main category: cs.AI

TL;DR: 论文提出一个测试时自优化框架，通过上下文学习和奖励反馈循环来增强视觉语言模型的几何推理能力，无需参数更新即可显著提升七巧板拼图任务的性能。


<details>
  <summary>Details</summary>
Motivation: 人类擅长通过心理旋转、迭代优化和视觉反馈解决七巧板等空间推理任务，但现有视觉语言模型在连续几何推理上存在系统性失败（单块任务IoU仅0.41，两块组合降至0.23），远低于儿童表现。需要探索模型能否在测试时通过迭代优化自我改进而不更新参数。

Method: 提出训练免费的验证器-优化器代理框架，结合上下文学习和奖励引导反馈循环，模拟人类认知过程。通过递归优化循环，基于几何一致性反馈迭代自优化预测结果。

Result: 在中等三角形案例中，IoU从0.63提升至0.932，无需模型重新训练。证明通过上下文学习和奖励循环融入人类启发的迭代优化机制能显著增强视觉语言模型的几何推理能力。

Conclusion: 将人类启发的迭代优化机制通过上下文学习和奖励循环融入模型，可在连续空间领域实现自我改进AI从承诺到实践的转变，显著提升几何推理性能。

Abstract: Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.

</details>


### [48] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: LLMs能够准确预测个体层面的认知偏见，并在交互对话中模拟人类偏见行为，GPT-4和GPT-5在模拟人类行为对齐方面存在差异


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能在个体层面预测认知偏见，并模拟在认知负荷等情境因素影响下的人类偏见行为动态，这对设计偏见过滤的交互式AI系统有重要意义

Method: 将三个经典决策场景改编为对话设置，进行人类实验（N=1100），参与者通过简单或复杂对话与聊天机器人互动；使用参与者人口统计数据和对话记录，在GPT-4和GPT-5上模拟相同交互条件

Result: 人类实验显示出显著的偏见模式；LLMs能够精确复现人类偏见，但GPT-4和GPT-5在模拟人类行为对齐方面存在明显差异

Conclusion: LLMs能够有效模拟人类偏见行为，这对设计和评估交互式、偏见过滤的LLM系统具有重要应用价值，不同模型在行为对齐方面的差异需要进一步研究

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [49] [BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages](https://arxiv.org/abs/2602.05599)
*Subhadip Maji,Arnab Bhattacharya*

Main category: cs.AI

TL;DR: 本文提出GETR方法，通过图神经网络增强的标记表示进行跨语言知识迁移，在低资源语言任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 低资源语言由于数据稀缺和语言资源不足，其NLP系统性能远落后于高资源语言。跨语言知识迁移通过利用高资源语言资源来解决这一挑战。

Method: 提出GETR（图增强标记表示）方法进行跨语言知识迁移，同时采用两种基线方法：隐藏层增强和通过标记翻译的标记嵌入迁移。

Result: 在真正低资源语言（Mizo、Khasi）的词性标注任务上获得13个百分点的提升，在模拟低资源语言（Marathi、Bangla、Malayalam）的情感分类和NER任务上分别获得20和27个百分点的宏F1提升。

Conclusion: GNN-based方法显著优于现有的多语言和跨语言基线方法，并对迁移机制进行了详细分析，确定了在这种语言环境下成功知识迁移的关键因素。

Abstract: Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.

</details>


### [50] [Reactive Knowledge Representation and Asynchronous Reasoning](https://arxiv.org/abs/2602.05625)
*Simon Kohaut,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.AI

TL;DR: 提出Resin概率编程语言和Reactive Circuits结构，通过异步反应式推理实现高效精确的概率推断，在动态环境中实现多个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 复杂概率模型的精确推断计算成本过高，特别是在动态环境中需要频繁实时信念更新的自主智能体。现有方法效率低下，每次变化都重新评估整个模型，未能利用现实世界信息流具有异构更新速率的特点。

Method: 1. 提出Resin（Reactive Signal Inference）概率编程语言，融合概率逻辑与反应式编程；2. 提出Reactive Circuits（RCs）作为Resin的高效精确语义，是基于代数电路和异步数据流的元结构，是时间动态有向无环图，能根据输入信号的波动性自主适应；3. 基于异步输入的变化频率估计对计算进行分区，将大型推断任务分解为单独记忆化的子问题。

Result: 在高保真无人机群模拟中，相比频率无关的推断方法实现了几个数量级的加速。RCs的结构适应成功捕捉环境动态，显著减少延迟，促进反应式实时推理。通过基于变化频率的计算分区，确保只有受新信息影响的模型特定组件被重新评估，大幅减少流式上下文中的冗余计算。

Conclusion: 通过反应式异步概率推理方法，能够高效处理动态环境中的实时信念更新问题，利用信息流的异构更新特性实现显著的计算加速，为自主智能体在动态环境中的实时推理提供了有效解决方案。

Abstract: Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.

</details>


### [51] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: 提出Generative Ontology框架，结合传统本体论的结构严谨性与大语言模型的创造力，通过可执行的Pydantic模式约束LLM生成，应用于游戏设计等领域。


<details>
  <summary>Details</summary>
Motivation: 传统本体论能描述领域结构但无法生成新内容，而大语言模型能流畅生成但缺乏结构有效性，经常产生幻觉。需要结合两者的互补优势：本体论提供语法，LLM提供创造力。

Method: 将领域知识编码为可执行的Pydantic模式，通过DSPy签名约束LLM生成。采用多智能体管道，为不同本体领域分配专门角色（如机制架构师、主题编织者、平衡批评家），每个智能体带有防止浅层输出的"专业焦虑"。使用检索增强生成将新设计基于现有范例，并通过迭代验证确保机制与组件的一致性。

Result: 通过GameGrammar系统展示框架效果，给定主题提示（如"洞穴生态系统中发光的真菌竞争"），管道能生成结构完整、可玩的桌面游戏规格，包括机制、组件、胜利条件和设置说明，既满足本体约束又保持真正的创造性。

Conclusion: 该模式可推广到游戏以外的领域，任何具有专家词汇、有效性约束和积累范例的领域（如音乐创作、软件架构、烹饪艺术）都适合使用Generative Ontology。约束不仅不限制创造力，反而使其成为可能，正如语法使诗歌成为可能一样。

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [52] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 该论文对基于图的智能体记忆系统进行全面综述，提出了记忆分类框架，分析了图记忆的生命周期技术，总结了开源工具和基准，并探讨了应用场景与未来方向。


<details>
  <summary>Details</summary>
Motivation: 记忆是大语言模型智能体处理长期复杂任务的核心模块，而图结构因其建模关系依赖、组织层次信息和高效检索的内在能力，成为智能体记忆的强有力结构。本文旨在系统梳理图基智能体记忆的研究现状，为开发更高效可靠的记忆系统提供指导。

Method: 1. 提出智能体记忆的分类法：短期vs长期记忆、知识vs经验记忆、非结构化vs结构化记忆；2. 按记忆生命周期系统分析关键技术：提取、存储、检索、演化；3. 总结开源库和基准；4. 探索应用场景；5. 识别挑战和未来方向。

Result: 建立了全面的图基智能体记忆研究框架，收集整理了相关研究论文、开源数据和项目资源，为社区提供了系统性的综述和资源汇总（GitHub仓库：https://github.com/DEEP-PolyU/Awesome-GraphMemory）。

Conclusion: 图结构是构建智能体记忆系统的有力工具，本文的系统综述为开发更高效可靠的图基智能体记忆系统提供了可操作的见解和资源支持，有助于推动该领域的研究进展。

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [53] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer架构计算和内存访问复杂度的分析模型，能够准确预测LLM推理的能源效率曲线，发现能源效率存在明显的"甜点"区域，合理调整序列长度可大幅降低能耗。


<details>
  <summary>Details</summary>
Motivation: LLM推理在现代AI应用中至关重要，需要准确理解其能源足迹。现有方法通常通过输入输出序列长度的简单线性函数来估算能耗，但实际观测显示能源效率存在明显的非线性依赖关系，需要更精确的建模方法。

Method: 基于Transformer架构的计算和内存访问复杂度推导出分析模型，使用TensorRT-LLM在NVIDIA H100 GPU上评估能源消耗，测试了从1B到9B参数的多种LLM（包括OPT、LLaMA、Gemma、Falcon、Qwen2和Granite），输入输出长度从64到4096个token。

Result: 模型预测准确度高，平均MAPE为1.79%。研究发现能源效率存在明显的"甜点"区域：短到中等输入和中等长度输出时效率最高，而长输入或极短输出时效率急剧下降。将序列长度调整到这些效率"甜点"可以显著降低能源使用。

Conclusion: 提出的分析模型能够准确表征LLM推理的能源效率曲线，为生产系统中的截断、摘要和自适应生成策略提供指导，帮助系统设计者优化序列长度以降低能耗，支持可持续的AI部署。

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [54] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA用非线性函数生成低秩矩阵的基向量，替代显式存储，提高参数效率


<details>
  <summary>Details</summary>
Motivation: 标准LoRA采用显式秩范式，增加模型容量需要添加更多基向量，导致参数大幅增长。研究发现这些基向量存在显著参数冗余，可以用轻量级非线性函数紧凑表示。

Method: GenLoRA为每个低秩矩阵维护一个潜在向量，并使用一组轻量级径向基函数(RBF)来合成基向量。每个RBF需要的参数远少于显式基向量，实现更高的参数效率。

Result: 在多个数据集和架构上的实验表明，GenLoRA在较小参数预算下获得更高的有效LoRA秩，从而实现更优的微调性能。

Conclusion: GenLoRA通过用非线性基向量生成替代显式基向量存储，显著提高了LoRA的参数效率，在有限参数预算下实现更好的微调效果。

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [55] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: 提出Anchored Policy Optimization (APO)方法，解决强化学习中奖励可验证时的递归空间收缩问题，通过支持覆盖而非形状匹配，在保持准确性的同时恢复多样性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在奖励可验证时存在递归空间收缩(RSC)问题，即采样概率向少数正确解收缩，导致多样性丧失。KL正则化虽然试图缓解，但强制形状匹配与正确性所需的锐化产生梯度冲突。

Method: 提出Anchored Policy Optimization (APO)，从全局形状匹配转向支持覆盖。基于参考模型的高置信度支持定义安全流形，允许激进锐化提高效率，同时在错误校正时选择性调用恢复力防止崩溃。

Result: 在数学基准测试中，APO打破了准确性与多样性的权衡，显著提高Pass@1的同时恢复了标准策略梯度方法通常失去的Pass@K多样性。

Conclusion: APO通过支持覆盖机制解决了RLVR中的递归空间收缩问题，实现了弹性恢复，为强化学习中的探索-利用权衡提供了新视角。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [56] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: 提出RLFKV框架，通过细粒度知识验证和强化学习减少金融RAG系统中的幻觉问题，提高响应与检索文档的一致性。


<details>
  <summary>Details</summary>
Motivation: 金融RAG系统虽然依赖检索文档来生成准确响应，但由于金融领域的时间敏感性，模型生成的响应仍存在与检索信息矛盾的幻觉问题，需要解决这种不一致性。

Method: 提出RLFKV框架：1) 将金融响应分解为原子知识单元；2) 评估每个单元的正确性以计算细粒度忠实度奖励；3) 引入信息量奖励防止奖励黑客行为（如过于简洁的回复）；4) 使用强化学习优化模型。

Result: 在公开的FDD任务和新提出的FDD-ANT数据集上的实验显示，该方法带来了持续改进，证实了其有效性。

Conclusion: RLFKV框架通过细粒度知识验证和强化学习，有效减少了金融RAG系统中的幻觉问题，提高了响应与检索文档的一致性。

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [57] [LeakBoost: Perceptual-Loss-Based Membership Inference Attack](https://arxiv.org/abs/2602.05748)
*Amit Kravchik Taub,Fred M. Grabovski,Guy Amit,Yisroel Mirsky*

Main category: cs.AI

TL;DR: LeakBoost是一种基于感知损失的主动探测框架，通过优化合成询问图像来放大模型对训练集成员和非成员样本的内部表示差异，显著提升现有成员推理攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击主要依赖静态指标（如损失或置信度），未能充分利用模型在被主动探测时的动态行为，限制了攻击效果。需要开发能够主动探测模型内部表示的方法来暴露隐藏的成员信号。

Method: 提出LeakBoost框架：1）针对候选输入，通过优化感知（激活空间）目标合成询问图像，放大成员和非成员在模型内部表示上的差异；2）使用现成的成员检测器分析合成图像，无需修改检测器本身；3）结合现有成员推理方法使用。

Result: LeakBoost显著提升攻击效果：AUC从接近随机水平（0.53-0.62）提升到0.81-0.88；在1%假阳性率下，真阳性率提升超过一个数量级。敏感性分析显示深层网络和低学习率短时优化产生最强泄漏，改进主要集中在基于梯度的检测器。

Conclusion: LeakBoost提供了一种模块化、计算高效的方法来评估白盒设置下的隐私风险，推进了动态成员推理的研究，能够显著增强现有成员推理攻击的效果。

Abstract: Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.

</details>


### [58] [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762)
*Andrei Kozyrev,Nikita Khramov,Denis Lochmelis,Valerio Morelli,Gleb Solovev,Anton Podkopaev*

Main category: cs.AI

TL;DR: 研究AI智能体自动优化方法在形式验证领域的应用，特别是在Rocq自动定理证明中，发现简单的小样本引导方法最有效，但仍不及精心设计的最优证明智能体。


<details>
  <summary>Details</summary>
Motivation: 研究自动AI智能体优化方法在真实世界形式验证场景中的适用性，探索能否自动化智能体系统的精细调优（如提示设计、上下文知识、控制策略等）。

Method: 在Rocq自动定理证明这一具有代表性的挑战性领域中，评估不同自动智能体优化器在优化Rocq证明生成智能体任务上的表现。

Result: 多个优化器都能带来可测量的改进，但简单的小样本引导方法是最一致有效的；然而，所有研究的自动优化方法都无法达到精心设计的最优证明智能体的性能水平。

Conclusion: 自动AI智能体优化方法在形式验证领域有一定应用价值，但当前技术仍无法完全替代人工精心设计的智能体系统，小样本引导是最实用的自动化方法。

Abstract: This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.

</details>


### [59] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: 本文提出首个完全异步的VLA模型训练框架，通过多级解耦架构显著提升训练效率，在LIBERO基准上实现最高126.67%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的RL训练框架（如RLinf）采用同步执行方式，导致环境交互、策略生成和模型更新阶段存在严重的资源利用不足和吞吐量限制问题，阻碍了训练效率提升。

Method: 提出完全异步策略训练框架，采用多级解耦架构：1）环境交互和轨迹收集的异步并行化；2）策略生成的流式执行；3）训练更新的解耦调度。系统借鉴了大模型RL中的异步优化思想。

Result: 在LIBERO基准测试中，相比现有同步策略，框架实现最高59.25%的吞吐量提升；深度优化分离策略后可达126.67%提升。在8-256 GPU上的扩展性验证显示良好可扩展性。

Conclusion: 本文首次实现从环境交互到策略更新的全流程异步训练框架，显著提升VLA模型训练效率，通过多级解耦架构解决了同步执行的资源利用瓶颈，具有优秀的可扩展性。

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [60] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI是专门为印度数字支付系统开发的金融语言模型，基于Mistral Small 24B架构，通过多阶段训练流程优化，在金融推理和工具调用方面显著优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 针对印度数字支付系统的特定需求，开发专门化的金融语言模型，以处理多语言（英语、印地语、Hinglish）金融对话和真实工作流程，如交易纠纷和授权生命周期管理。

Method: 采用Mistral Small 24B架构，通过多阶段训练：1) 在680亿个金融、多语言和合成数据标记上进行持续预训练；2) 指令微调；3) 针对多轮工具驱动对话的领域特定监督微调。

Result: FiMI Base在金融推理基准测试中比Mistral Small 24B Base提升20%；FiMI Instruct在领域特定工具调用方面比Mistral Small 24B Instruct提升87%，同时在通用基准测试上保持可比性能。

Conclusion: FiMI成功开发了针对印度数字支付系统的专业金融语言模型，在保持通用能力的同时，在金融领域任务上取得了显著性能提升，证明了领域专业化训练的有效性。

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [61] [NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking](https://arxiv.org/abs/2602.05805)
*Kang Chen,Zhuoka Feng,Sihan Zhao,Kai Xiong,Junjie Nian,Yaoning Wang,Changyi Xiao,Yixin Cao*

Main category: cs.AI

TL;DR: 提出NEX框架，通过分析推理过程中的探索-利用阶段来无监督评估LLM推理质量，无需标注数据即可预测下游任务准确性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理时生成多个思维链或搜索合并检查点，选择最佳结果成为瓶颈，且通常缺乏目标分布的监督信号。需要一种无需标注的无监督方法来评估推理质量

Method: 提出NEX框架，将推理视为探索(E-phase)和利用(X-phase)交替过程。通过稀疏激活缓存检测新激活的MLP神经元作为探索信号，使用粘性两状态HMM推断E-X阶段，并根据探索引入的神经元是否在后续利用阶段被重用来评估其价值

Result: 在推理基准测试和Qwen3合并模型家族中，NEX通过少量未标注激活数据计算出的Good-Mass Fraction分数能预测下游准确性并识别更好变体。通过人类标注验证了E-X信号，并通过"有效vs冗余"神经元转移提供因果证据

Conclusion: NEX提供了一种无需标注的无监督方法来评估LLM推理质量，通过分析推理过程中的探索-利用动态来预测模型性能，为模型选择和变体评估提供了有效工具

Abstract: Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via "Effective-vs-Redundant" neuron transfer.

</details>


### [62] [STProtein: predicting spatial protein expression from multi-omics data](https://arxiv.org/abs/2602.05811)
*Zhaorui Jiang,Yingfang Yuan,Lei Hu,Wei Pang*

Main category: cs.AI

TL;DR: STProtein是一个基于图神经网络和多任务学习的新框架，旨在利用相对丰富的空间转录组数据来准确预测稀缺的空间蛋白质表达数据，解决空间多组学数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 空间多组学数据整合对生物学研究至关重要，但存在严重的数据不平衡问题：空间转录组数据相对丰富，而空间蛋白质组数据由于技术限制和成本高昂而稀缺，这阻碍了研究进展。

Method: 提出STProtein框架，采用图神经网络结合多任务学习策略，利用更易获取的空间多组学数据（如空间转录组）来预测未知的空间蛋白质表达。

Result: STProtein能够有效解决空间蛋白质组数据的稀缺性问题，加速空间多组学整合，帮助科学家发现组织中复杂的、先前隐藏的蛋白质空间模式，揭示标记基因之间的新关系，探索生物"暗物质"。

Conclusion: STProtein框架有望催化生命科学领域的突破性进展，通过预测空间蛋白质表达来弥补数据缺口，推动空间多组学研究的深入发展。

Abstract: The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological "Dark Matter".

</details>


### [63] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker：一种具有自主规划和自适应检索能力的智能体，通过动态多轮交互和双重训练策略（SFT+RL）在时序知识图谱上进行推理，解决了LLM在TKGQA中的幻觉问题和静态提示限制。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的TKGQA方法存在两个主要问题：1）在复杂时序约束下容易产生推理幻觉；2）静态提示限制了模型自主性和泛化能力，缺乏与TKGs环境的动态交互优化。

Method: 提出TKG-Thinker智能体，通过动态多轮交互与TKGs进行深度时序推理。采用双重训练策略：首先使用思维链数据进行监督微调（SFT）培养核心规划能力，然后通过强化学习（RL）阶段利用多维奖励在复杂时序约束下优化推理策略。

Result: 在基准数据集和三个开源LLM上的实验表明，TKG-Thinker实现了最先进的性能，并在复杂TKGQA设置中表现出强大的泛化能力。

Conclusion: TKG-Thinker通过自主规划和自适应检索能力，有效解决了LLM在时序知识图谱问答中的局限性，为复杂时序推理任务提供了有效的解决方案。

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [64] [Learning Compact Boolean Networks](https://arxiv.org/abs/2602.05830)
*Shengpu Wang,Yuhao Mao,Yani Zhang,Martin Vechev*

Main category: cs.AI

TL;DR: 提出三种方法改进布尔神经网络：学习连接、紧凑卷积和自适应离散化，显著减少布尔运算量同时提升精度


<details>
  <summary>Details</summary>
Motivation: 浮点神经网络推理成本高，布尔网络适合资源受限场景，但学习紧凑准确的布尔网络具有挑战性

Method: 1) 无参学习高效连接策略；2) 利用局部性的紧凑卷积架构；3) 减少精度损失的自适应离散化策略

Result: 在标准视觉基准上，Pareto前沿显著优于SOTA，精度更高且布尔运算减少达37倍

Conclusion: 提出的三方面改进使布尔神经网络在精度和计算效率方面达到新的最优平衡

Abstract: Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.

</details>


### [65] [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847)
*Zhangquan Chen,Jiale Tao,Ruihuang Li,Yihao Hu,Ruitao Chen,Zhantao Yang,Xinlei Yu,Haodong Jing,Manyuan Zhang,Shuai Shao,Biao Wang,Qinglin Lu,Ruqi Huang*

Main category: cs.AI

TL;DR: OmniVideo-R1是一个增强的多模态视频理解框架，通过自监督查询密集定位和对比学习模态注意力融合，提升音视频理解能力


<details>
  <summary>Details</summary>
Motivation: 人类通过多种模态协同感知世界，但现有全视频模型在音视频理解任务上仍面临重大挑战，需要改进混合模态推理能力

Method: 采用强化框架，包含两个关键策略：1) 基于自监督学习的查询密集定位；2) 基于对比学习的模态注意力融合

Result: 在多个基准测试中持续优于强基线模型，显示出有效性和强大的泛化能力

Conclusion: OmniVideo-R1通过"全模态线索思考"策略，显著提升了视频模型的音视频理解能力

Abstract: While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.

</details>


### [66] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: BABE是一个评估生物AI系统实验推理能力的基准，基于同行评审研究论文和真实生物研究构建，挑战模型进行因果推理和跨尺度推断。


<details>
  <summary>Details</summary>
Motivation: 现有生物学基准未能评估研究者所需的关键能力：将实验结果与背景知识结合得出有意义的结论。大语言模型已从基础对话发展到高级科学推理，但缺乏评估其真实科学推理能力的基准。

Method: 基于同行评审研究论文和真实世界生物学研究构建BABE基准，确保任务反映实际科学探究的复杂性和跨学科性质。基准设计挑战模型进行因果推理和跨尺度推断。

Result: BABE提供了一个稳健的框架，用于评估AI系统如何像实践科学家一样推理，为衡量其对生物学研究贡献潜力提供了更真实的测量标准。

Conclusion: BABE填补了现有生物学基准的空白，能够更真实地评估AI系统的实验推理能力，为衡量其在生物学研究中的实际贡献潜力提供了重要工具。

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [67] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出分层座位分配问题(HSAP)，通过概率路线图(PRM)和快速探索随机树(RRT)计算座位距离，结合启发式搜索和动态规划，使用整数规划解决组织团队的分层座位分配问题。


<details>
  <summary>Details</summary>
Motivation: 大型组织需要将具有紧密层级关系的团队安排在相邻座位区域（如研究组占据连续区域），目前手动管理导致重新规划频率低且效果不佳。

Method: 使用概率路线图(PRM)和快速探索随机树(RRT)计算座位间距离，结合启发式搜索和动态规划，通过整数规划框架解决HSAP问题。

Result: 在不同规模实例下评估PRM框架和座位分配方案，进行定量和定性分析，证明方法的有效性。

Conclusion: 提出的端到端框架能够自动解决分层座位分配问题，替代传统手动规划，提高组织座位安排的效率和优化程度。

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [68] [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877)
*Lukas Stappen,Ahmet Erkan Turan,Johann Hagerer,Georg Groh*

Main category: cs.AI

TL;DR: 论文提出AgentHeLLM威胁建模框架，用于分析车辆LLM对话代理的安全风险，通过资产识别与攻击路径分离的方法，解决现有AI安全框架在安全关键系统中的不足。


<details>
  <summary>Details</summary>
Motivation: 车辆集成LLM对话代理带来了新的安全挑战，现有AI安全框架缺乏安全关键系统工程中的"关注点分离"原则，将资产保护与攻击路径分析混为一谈，需要更严谨的方法论来应对车辆环境中的安全风险。

Method: 提出AgentHeLLM威胁建模框架：1）基于"受害者建模"和《世界人权宣言》的人类中心资产分类法；2）形式化的图模型，区分毒化路径（恶意数据传播）和触发路径（激活行为）；3）开源攻击路径建议工具，采用双层搜索策略自动化多阶段威胁发现。

Result: 开发了AgentHeLLM攻击路径生成器工具，能够自动化发现多阶段威胁，为车辆LLM对话代理的安全分析提供了实用的方法论和工具支持。

Conclusion: AgentHeLLM框架通过严格分离资产识别与攻击路径分析，为车辆LLM对话代理的安全威胁建模提供了系统化方法，填补了现有AI安全框架在安全关键系统中的方法论空白。

Abstract: The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous "separation of concerns" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented "victim modeling" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.

</details>


### [69] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: 本文针对大语言模型在建模与仿真应用中的使用提供全面实用指导，指出看似简单的实践可能带来微妙问题，强调原则性设计选择、诊断策略和实证评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在建模与仿真工作流中应用日益广泛，但许多看似直接的使用方法可能引入微妙问题、不必要复杂性甚至导致结果劣化。作者旨在为LLMs使用提供全面实用指导，特别关注M&S应用场景。

Method: 通过讨论常见混淆来源，包括非确定性、知识增强（RAG和LoRA）、M&S数据分解和超参数设置，强调原则性设计选择、诊断策略和实证评估方法。

Result: 论文提供了关于如何避免常见陷阱的指导，如：增加数据可能适得其反、微调前需评估模型已有知识、温度设为0不足以保证确定性、大量M&S数据输入可能过度但简化可能丢失信息。

Conclusion: 帮助建模者做出明智决策，确定何时、如何以及是否依赖大语言模型，强调在M&S应用中需要谨慎使用LLMs，避免简单化应用带来的潜在问题。

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [70] [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920)
*Eva Andrés*

Main category: cs.AI

TL;DR: 该论文比较了经典和量子强化学习方法在带容量约束的车辆路径问题上的表现，发现量子增强模型（特别是混合架构）在路由距离、紧凑性和重叠度方面优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子强化学习在复杂组合优化问题（如带容量约束的车辆路径问题）中的潜力，探索量子计算能否提供比经典方法更好的解决方案。

Method: 采用优势演员-评论家（A2C）智能体，实现经典、全量子和混合三种变体，集成transformer架构通过自注意力和交叉注意力机制捕捉车辆、客户和仓库之间的关系。实验针对20个客户和4辆车的多车辆容量约束场景，进行10次独立运行。

Result: 所有三种方法都能学习有效的路由策略，但量子增强模型优于经典基线，产生更稳健的路由组织。混合架构在距离、紧凑性和路由重叠度方面表现最佳。定性可视化显示量子模型生成更结构化、更一致的路由解决方案。

Conclusion: 混合量子-经典强化学习模型在解决复杂组合优化问题（如CVRP）方面具有显著潜力，量子增强能提供更优的路由解决方案。

Abstract: This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.

</details>


### [71] [Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins](https://arxiv.org/abs/2602.05983)
*Krešimir Kušić,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 本文提出了一种地理感知的Transformer交通预测模型GATTF，利用传感器间的互信息捕捉地理关系，在日内瓦高速公路网络上验证了其优于标准Transformer的预测精度。


<details>
  <summary>Details</summary>
Motivation: 高速公路数字孪生技术需要高分辨率实时交通数据和预测能力，但交通动态的时空复杂性使得预测困难。现有序列深度学习模型在捕捉长期时间依赖方面有优势，但在预测精度和模型复杂度方面仍需改进。

Method: 提出地理感知的Transformer交通预测模型GATTF，利用分布式传感器之间的互信息来捕捉地理关系，增强模型对空间依赖的理解，同时不增加模型复杂度。

Result: 使用瑞士日内瓦高速公路网络的实时数据进行评估，结果表明通过互信息引入地理感知的GATTF模型比标准Transformer具有更高的预测精度。

Conclusion: 地理感知的Transformer模型能够有效提升高速公路交通预测精度，为数字孪生交通管理系统提供了更好的预测支持，且不增加模型复杂度。

Abstract: The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.

</details>


### [72] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 该研究探索了使用预训练的Whisper ASR模型进行语音情感识别，提出了两种基于注意力的池化方法来降低Whisper表示维度，在英语和波斯语数据集上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别研究面临标准大规模数据集缺乏的局限，现有研究利用预训练模型提取特征。本研究探索Whisper预训练ASR系统在语音情感识别中的能力。

Method: 提出两种基于注意力的池化方法：多头注意力平均池化和QKV池化，用于高效降低Whisper表示的维度同时保留情感特征。在英语IEMOCAP和波斯语ShEMO数据集上使用Whisper Tiny和Small模型进行实验。

Result: 多头QKV架构在ShEMO数据集上实现了SOTA结果，未加权准确率提升2.47%。发现中间层在波斯语数据集上表现更好，为比HuBERT X-Large等大型模型更轻量高效的替代方案。

Conclusion: Whisper作为语音情感识别的表示提取器具有潜力，基于注意力的池化方法在维度减少方面有效，为轻量高效的SER解决方案提供了新途径。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [73] [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008)
*Xianyang Liu,Shangding Gu,Dawn Song*

Main category: cs.AI

TL;DR: AgenticPay是一个用于多智能体买卖谈判的基准测试和仿真框架，通过自然语言驱动，包含110多个任务，评估LLM在商业谈判中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估多智能体语言中介经济交互的原则性基准，而LLM智能体在自主谈判、协调和交易方面的应用日益增多，需要专门的评估框架。

Method: 构建了一个模拟市场环境，买家和卖家拥有私有约束和产品相关估值，通过多轮语言谈判而非单纯数字竞价达成协议。包含结构化动作提取和可行性、效率、福利等指标。

Result: 对最先进的专有和开源LLM进行基准测试，发现谈判性能存在显著差距，突显了长期战略推理方面的挑战。

Conclusion: AgenticPay为研究智能体商业和基于语言的市场交互奠定了基础，提供了评估多智能体经济谈判的综合框架。

Abstract: Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.

</details>


### [74] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: 开发基于VR行为数据驱动的离散事件模拟器，用于大规模评估学校安全干预策略，特别是机器人干预方案


<details>
  <summary>Details</summary>
Motivation: VR虽然能高保真模拟学校枪击等高危场景，但需要招募新参与者进行每次条件测试，难以进行大规模或迭代评估，限制了有效干预策略的学习

Method: 开发数据驱动的离散事件模拟器（DES），从VR研究中学习参与者行为，将枪手移动和区域内行动建模为随机过程，用于评估机器人干预策略

Result: 模拟器能够复现关键经验模式，支持大规模评估和学习那些无法直接用人参与者训练的干预策略

Conclusion: 建立了一个从高保真到中保真的模拟工作流程，为开发和评估自主学校安全干预提供了可扩展的替代方案

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>


### [75] [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039)
*Yuxing Lu,Yucheng Hu,Xukai Zhao,Jiuxin Cao*

Main category: cs.AI

TL;DR: DyTopo：一种动态重构稀疏有向通信图的多智能体框架，通过语义匹配实现按需通信，在代码生成和数学推理任务上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常采用固定的通信模式，无法适应迭代问题解决中不同阶段的需求变化，需要更灵活、高效的通信机制

Method: DyTopo采用管理者引导的多智能体框架，每轮重构稀疏有向通信图。每个智能体输出轻量级自然语言查询（需求）和关键（提供）描述符，通过嵌入和语义匹配，仅沿诱导边路由私有消息

Result: 在代码生成和数学推理基准测试中，使用四种LLM骨干网络，DyTopo始终优于最强基线（平均提升+6.2%），并提供可解释的协调轨迹

Conclusion: DyTopo通过动态重构通信图实现了更高效的按需通信，不仅提升了性能，还提供了可解释的协调过程可视化，有助于理解多轮推理中的通信模式演化

Abstract: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [76] [Correcting Contextual Deletions in DNA Nanopore Readouts](https://arxiv.org/abs/2602.05072)
*Yuan-Pon Chen,Olgica Milenkovic,João Ribeiro,Jin Sima*

Main category: cs.IT

TL;DR: 研究DNA存储中纳米孔测序的上下文相关删除错误，提出两种模型：1) 当运行长度阈值k=Clogn时，构建可纠正t个删除的编码，冗余度约为(1-C)t logn；2) 当k为常数时，研究极值上下文删除信道的最大可达率。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储系统中纳米孔测序器存在上下文相关的删除错误，传统假设删除独立且与序列上下文无关的模型不适用于实际纳米孔错误模式，需要研究上下文相关的删除错误模型。

Method: 提出两种模型：1) 确定性单删除跟随长度至少为k的完整运行长度，研究k=Clogn时的编码设计；2) 极值上下文删除信道，每个长度至少为k的运行后都发生删除。使用非构造性冗余上界和可高效编解码的编码设计。

Result: 对于k=Clogn，最小冗余度在(1-C)t logn到2(1-C)t logn之间，是任意t-删除纠正编码的(1-C)分数。对于t=1和C>1/2，构造了冗余度基本匹配非构造性上界的高效编码。对于常数k，获得了极值上下文删除信道下最大可达率的精确界限。

Conclusion: 纳米孔测序中的上下文相关删除错误需要专门设计的编码方案，提出的模型和编码方法能够有效处理这种特定类型的错误，为DNA数据存储系统提供了更准确的错误纠正方案。

Abstract: The problem of designing codes for deletion-correction and synchronization has received renewed interest due to applications in DNA-based data storage systems that use nanopore sequencers as readout platforms. In almost all instances, deletions are assumed to be imposed independently of each other and of the sequence context. These assumptions are not valid in practice, since nanopore errors tend to occur within specific contexts. We study contextual nanopore deletion-errors through the example setting of deterministic single deletions following (complete) runlengths of length at least $k$. The model critically depends on the runlength threshold $k$, and we examine two regimes for $k$: a) $k=C\log n$ for a constant $C\in(0,1)$; in this case, we study error-correcting codes that can protect from a constant number $t$ of contextual deletions, and show that the minimum redundancy (ignoring lower-order terms) is between $(1-C)t\log n$ and $2(1-C)t\log n$, meaning that it is a ($1-C$)-fraction of that of arbitrary $t$-deletion-correcting codes. To complement our non-constructive redundancy upper bound, we design efficiently and encodable and decodable codes for any constant $t$. In particular, for $t=1$ and $C>1/2$ we construct efficient codes with redundancy that essentially matches our non-constructive upper bound; b) $k$ equal a constant; in this case we consider the extremal problem where the number of deletions is not bounded and a deletion is imposed after every run of length at least $k$, which we call the extremal contextual deletion channel. This combinatorial setting arises naturally by considering a probabilistic channel that introduces contextual deletions after each run of length at least $k$ with probability $p$ and taking the limit $p\to 1$. We obtain sharp bounds on the maximum achievable rate under the extremal contextual deletion channel for arbitrary constant $k$.

</details>


### [77] [On QC and GQC algebraic geometry codes](https://arxiv.org/abs/2602.05097)
*Matteo Bonini,Arianna Dionigi,Francesco Ghiandoni*

Main category: cs.IT

TL;DR: 提出基于代数曲线构造准循环码和广义准循环码的新方法，适用于Kummer扩张曲线，包括超椭圆曲线、范迹曲线和Hermitian曲线，获得灵活的参数


<details>
  <summary>Details</summary>
Motivation: 现有基于椭圆曲线的准循环码构造方法限制较多，需要更通用的构造框架来获得灵活参数的准循环码

Method: 利用代数曲线（特别是有理函数域的Kummer扩张曲线）构造准循环码和广义准循环码，基于已知的自同构群分类推导参数公式

Result: 获得了具有灵活共指数的准循环码构造方法，适用于超椭圆曲线、范迹曲线和Hermitian曲线等多种曲线类型

Conclusion: 该方法扩展了准循环码的构造范围，为基于代数曲线的编码理论提供了新的构造框架

Abstract: We present new constructions of quasi-cyclic (QC) and generalized quasi-cyclic (GQC) codes from algebraic curves. Unlike previous approaches based on elliptic curves, our method applies to curves that are Kummer extensions of the rational function field, including hyperelliptic, norm-trace, and Hermitian curves. This allows QC codes with flexible co-index. Explicit parameter formulas are derived using known automorphism-group classifications.

</details>


### [78] [Enabling Large-Scale Channel Sounding for 6G: A Framework for Sparse Sampling and Multipath Component Extraction](https://arxiv.org/abs/2602.05405)
*Yi Chen,Li Ming,Chong Han*

Main category: cs.IT

TL;DR: 提出稀疏非均匀采样的信道探测框架与LR-SAGE算法，实现50倍测量加速、98%数据量减少和99.96%后处理复杂度降低，为6G AI-native系统构建大规模ISAC数据集


<details>
  <summary>Details</summary>
Motivation: 实现6G人工智能与集成感知通信需要大规模真实信道数据集，但传统频域信道探测方法因需避免时延模糊而采样点数过多，效率低下

Method: 提出抛物线频率采样(PFS)策略进行非均匀频率点分布，消除时延模糊；开发似然校正空间交替广义期望最大化(LR-SAGE)算法从PFS数据中提取多径分量

Result: 在280-300GHz频段验证，实现50倍测量加速、98%数据量减少、99.96%后处理复杂度降低，同时准确捕获与传统详尽测量一致的多径分量和信道特性

Conclusion: 该框架为构建AI-native 6G系统所需的大规模ISAC数据集提供了基础使能技术，能够充分利用AI扩展定律的潜力

Abstract: Realizing the 6G vision of artificial intelligence (AI) and integrated sensing and communication (ISAC) critically requires large-scale real-world channel datasets for channel modeling and data-driven AI models. However, traditional frequency-domain channel sounding methods suffer from low efficiency due to a prohibitive number of frequency points to avoid delay ambiguity. This paper proposes a novel channel sounding framework involving sparse nonuniform sampling along with a likelihood-rectified space-alternating generalized expectation-maximization (LR-SAGE) algorithm for multipath component extraction. This framework enables the acquisition of channel datasets that are tens or even hundreds of times larger within the same channel measurement duration, thereby providing the massive data required to harness the full potential of AI scaling laws. Specifically, we propose a Parabolic Frequency Sampling (PFS) strategy that non-uniformly distributes frequency points, effectively eliminating delay ambiguity while reducing sampling overhead by orders of magnitude. To efficiently extract multipath components (MPCs) from the channel data measured by PFS, we develop a LR-SAGE algorithm, rectifying the likelihood distortion caused by nonuniform sampling and molecular absorption effect. Simulation results and experimental validation at 280--300~GHz confirm that the proposed PFS and LR-SAGE algorithm not only achieve 50$\times$ faster measurement, a 98\% reduction in data volume and a 99.96\% reduction in post-processing computational complexity, but also successfully captures MPCs and channel characteristics consistent with traditional exhaustive measurements, demonstrating its potential as a fundamental enabler for constructing the massive ISAC datasets required by AI-native 6G systems.

</details>


### [79] [Explicit List-Decodable Linearized Reed-Solomon Subspace Codes via Subspace Designs](https://arxiv.org/abs/2602.05462)
*Kuo Shang,Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文构造了在任意域上的线性化和秩度量码，实现了高效的列表译码，达到错误率ρ时码率1-ρ-ε，并扩展到折叠线性化Reed-Solomon码。


<details>
  <summary>Details</summary>
Motivation: 和秩度量在多种编码应用中很重要，但缺乏高效列表译码的构造。现有线性化Reed-Solomon码需要扩展到支持超过唯一译码半径的列表译码。

Method: 构造𝔽ₕ-线性和秩度量码作为LRS码的子码，通过子空间设计限制消息多项式。使用线性代数译码框架，证明折叠评估满足插值条件，解空间形成低维结构仿射子空间。

Result: 实现了错误率ρ时码率1-ρ-ε的高效列表译码，列表大小有界于h^poly(1/ε)。首次构造了正码率、可高效列表译码超过唯一译码半径的折叠LRS子码。

Conclusion: 本文提供了构造和秩度量下高效可译码的新通用框架，首次实现了正码率码在超过唯一译码半径的高效列表译码。

Abstract: The sum-rank metric is the mixture of the Hamming and rank metrics. The sum-rank metric found its application in network coding, locally repairable codes, space-time coding, and quantum-resistant cryptography. Linearized Reed-Solomon (LRS) codes are the sum-rank analogue of Reed-Solomon codes and strictly generalize both Reed-Solomon and Gabidulin codes.
  In this work, we construct an explicit family of $\mathbb{F}_h$-linear sum-rank metric codes over arbitrary fields $\mathbb{F}_h$. Our construction enables efficient list decoding up to a fraction $ρ$ of errors in the sum-rank metric with rate $1-ρ-\varepsilon$, for any desired $ρ\in (0,1)$ and $\varepsilon>0$. Our codes are subcodes of LRS codes, obtained by restricting message polynomials to an $\mathbb{F}_h$-subspace derived from subspace designs, and the decoding list size is bounded by $h^{\mathrm{poly}(1/\varepsilon)}$.
  Beyond the standard LRS setting, we further extend our linear-algebraic decoding framework to folded Linearized Reed-Solomon (FLRS) codes. We show that folded evaluations satisfy appropriate interpolation conditions and that the corresponding solution space forms a low-dimensional, structured affine subspace. This structure enables effective control of the list size and yields the first explicit positive-rate FLRS subcodes that are efficiently list decodable beyond the unique-decoding radius. To the best of our knowledge, this also constitutes the first explicit construction of positive-rate sum-rank metric codes that admit efficient list decoding beyond the unique decoding radius, thereby providing a new general framework for constructing efficiently decodable codes under the sum-rank metric.

</details>


### [80] [Low-complexity Design for Beam Coverage in Near-field and Far-field: A Fourier Transform Approach](https://arxiv.org/abs/2602.05666)
*Chao Zhou,Changsheng You,Cong Zhou,Li Chen,Yi Gong,Chengwen Xing*

Main category: cs.IT

TL;DR: 提出基于傅里叶变换的低复杂度波束覆盖设计方法，适用于远场和近场场景，相比传统采样优化方法显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的波束覆盖优化方法计算复杂度高，需要设计低复杂度且高效的波束覆盖方案

Method: 将波束覆盖设计转化为空间频率滤波问题，通过逆傅里叶变换在远场实现角度覆盖；针对有限天线数引入的滚降效应，提出滚降感知设计；扩展到近场时，通过一阶泰勒近似将二维波束覆盖设计转化为二维逆傅里叶变换

Result: 提出的FT-based方法在实现与传统采样优化方法相当的波束成形性能的同时，显著降低了计算复杂度；在近场中观察到固有的范围散焦效应

Conclusion: 基于傅里叶变换的方法为多天线系统提供了一种高效低复杂度的波束覆盖设计解决方案，适用于远场和近场场景

Abstract: In this paper, we study efficient beam coverage design for multi-antenna systems in both far-field and near-field cases. To reduce the computational complexity of existing sampling-based optimization methods, we propose a new low-complexity yet efficient beam coverage design. To this end, we first formulate a general beam coverage optimization problem to maximize the worst-case beamforming gain over a target region. For the far-field case, we show that the beam coverage design can be viewed as a spatial-frequency filtering problem, where angular coverage can be achieved by weight-shaping in the antenna domain via an inverse FT, yielding an infinite-length weighting sequence. Under the constraint of a finite number of antennas, a surrogate scheme is proposed by directly truncating this sequence, which inevitably introduces a roll-off effect at the angular boundaries, yielding degraded worst-case beamforming gain. To address this issue, we characterize the finite-antenna-induced roll-off effect, based on which a roll-off-aware design with a protective zoom is developed to ensure a flat beamforming-gain profile within the target angular region. Next, we extend the proposed method to the near-field case. Specifically, by applying a first-order Taylor approximation to the near-field channel steering vector (CSV), the two-dimensional (2D) beam coverage design (in both angle and inverse-range) can be transformed into a 2D inverse FT, leading to a low-complexity beamforming design. Furthermore, an inherent near-field range defocusing effect is observed, indicating that sufficiently wide angular coverage results in range-insensitive beam steering. Finally, numerical results demonstrate that the proposed FT-based approach achieves a comparable worst-case beamforming performance with that of conventional sampling-based optimization methods while significantly reducing the computational complexity.

</details>


### [81] [Generalized Pinsker Inequality for Bregman Divergences of Negative Tsallis Entropies](https://arxiv.org/abs/2602.05744)
*Guglielmo Beretta,Tommaso Cesari,Roberto Colomboni*

Main category: cs.IT

TL;DR: 本文提出了一种广义的Pinsker不等式，针对由负α-Tsallis熵生成的Bregman散度（也称为β-散度），建立了D_α(p∥q)与L1范数之间的下界关系，并确定了最优常数C_{α,K}。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于概率预测中的Tsallis损失函数和在线学习应用。经典的Pinsker不等式将Kullback-Leibler散度与总变差联系起来，但需要将其推广到更一般的Bregman散度，特别是由Tsallis熵生成的散度，以满足实际应用需求。

Method: 作者研究了由负α-Tsallis熵生成的Bregman散度D_α（也称为β-散度）。对于概率单纯形Δ^K相对内部的任意概率分布p和q，通过数学分析证明了广义Pinsker不等式，并精确确定了最优常数C_{α,K}。

Result: 证明了对于任意p,q∈Δ^K的相对内部，有D_α(p∥q) ≥ C_{α,K}/2·∥p-q∥₁²，并确定了所有(α,K)组合下的最优常数C_{α,K}。这是经典Pinsker不等式到更广泛散度类的推广。

Conclusion: 本文成功建立了Tsallis熵生成的Bregman散度的广义Pinsker不等式，提供了将D_α控制转换为L1范数控制的理论工具，对概率预测和在线学习等应用具有重要意义。

Abstract: The Pinsker inequality lower bounds the Kullback--Leibler divergence $D_{\textrm{KL}}$ in terms of total variation and provides a canonical way to convert $D_{\textrm{KL}}$ control into $\lVert \cdot \rVert_1$-control. Motivated by applications to probabilistic prediction with Tsallis losses and online learning, we establish a generalized Pinsker inequality for the Bregman divergences $D_α$ generated by the negative $α$-Tsallis entropies -- also known as $β$-divergences. Specifically, for any $p$, $q$ in the relative interior of the probability simplex $Δ^K$, we prove the sharp bound \[
  D_α(p\Vert q) \ge \frac{C_{α,K}}{2}\cdot \|p-q\|_1^2, \] and we determine the optimal constant $C_{α,K}$ explicitly for every choice of $(α,K)$.

</details>


### [82] [MU-MIMO Uplink Timely Throughput Maximization for Extended Reality Applications](https://arxiv.org/abs/2602.05751)
*Ravi Sharan Bhagavathula,Pavan Koteshwar Srinath,Alvaro Valcarce Rial,Baltasar-Beferull Lozano*

Main category: cs.IT

TL;DR: 该论文提出了一种基于加权比例公平的迭代启发式算法，用于优化扩展现实(XR)应用的上行多用户MIMO调度，通过PAoI指标保证用户满意度，在保持系统吞吐量的同时显著提升XR容量。


<details>
  <summary>Details</summary>
Motivation: 扩展现实(XR)应用对时延和吞吐量有严格要求，需要优化上行多用户MIMO调度来最大化及时吞吐量，同时保证用户满意度。

Method: 采用基于加权比例公平的迭代启发式算法，权重根据峰值信息年龄(PAoI)指标设计，将及时调度机会作为用户满意度约束纳入网络侧优化问题。

Result: 大量数值仿真表明，所提算法在保持整体系统吞吐量的同时，在XR容量方面持续优于现有基线方法。

Conclusion: 该研究提出的加权比例公平启发式算法能有效解决XR应用的上行MU-MIMO调度问题，在保证用户满意度的同时最大化及时吞吐量。

Abstract: In this work, we study the cross-layer timely throughput maximization for extended reality (XR) applications through uplink multi-user MIMO (MU-MIMO) scheduling. Timely scheduling opportunities are characterized by the peak age of information (PAoI)-metric and are incorporated into a network-side optimization problem as constraints modeling user satisfaction. The problem being NP-hard, we resort to a signaling-free, weighted proportional fair-based iterative heuristic algorithm, where the weights are derived with respect to the PAoI metric. Extensive numerical simulation results demonstrate that the proposed algorithm consistently outperforms existing baselines in terms of XR capacity without sacrificing the overall system throughput.

</details>


### [83] [Price of universality in vector quantization is at most 0.11 bit](https://arxiv.org/abs/2602.05790)
*Alina Harbuzova,Or Ordentlich,Yury Polyanskiy*

Main category: cs.IT

TL;DR: 该论文证明了存在一个通用码本，对所有可能的输入统计量都接近最优，比针对特定统计量优化的水填充码本仅多0.11比特/维度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署中，矩阵乘积W⊤X的计算是关键。使用低精度近似Ŵ代替真实W（仅权重量化）是提高效率的流行方法。信息论表明，最优的量化算法依赖于X的统计特性，需要精心设计向量量化码本与X的PCA方向对齐（水填充分配）。但码本依赖于X的统计特性在实际中难以实现。

Method: 通过理论证明方法，展示了存在一个通用码本，对所有可能的X统计特性都同时接近最优。该证明是非构造性的，表明存在一个在ℝⁿ中的网络，能够同时以所有希尔伯特范数近似最优地覆盖球面。

Result: 证明了存在一个通用码本，其性能至少与针对特定X统计特性优化的水填充码本（速率降低0.11比特/维度）一样好。这意味着存在一个在ℝⁿ中的网络，能够同时以所有希尔伯特范数近似最优地覆盖球面。

Conclusion: 存在一个对所有输入统计特性都接近最优的通用码本，这为低精度存储格式提供了理想候选方案。虽然证明是非构造性的，但这一理论结果对现代大语言模型的高效部署具有重要意义。

Abstract: Fast computation of a matrix product $W^\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\widehat W$ in place of true $W$ ("weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as "waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.
  Equivalently, our result shows existence of a net in $\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.

</details>
