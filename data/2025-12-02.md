<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 20]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IT](#cs.IT) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Trust or Bust: A Survey of Threats in Decentralized Wireless Networks](https://arxiv.org/abs/2512.00023)
*Hetvi Shastri,Akanksha Atrey,Andre Beck,Nirupama Ravi*

Main category: cs.NI

TL;DR: 论文分析了去中心化无线网络中热点运营商可能实施的对抗性行为模式，发现这些攻击可使运营商收益增加三倍以上，强调需要开发检测和缓解技术。


<details>
  <summary>Details</summary>
Motivation: 去中心化无线网络允许个体实体提供无订阅连接服务以获取报酬，但这些实体缺乏传统运营商通过数十年建立的信任基础，可能为自身利益而利用服务，例如热点运营商可能故意违反连接协议来增加利润。

Method: 论文研究并开发了去中心化无线网络中对抗性行为模式的分类法，通过案例研究分析了提供商驱动的攻击模式。

Result: 研究发现提供商驱动的攻击可能使提供商收益增加三倍以上，突显了去中心化无线网络中对抗性行为的严重经济影响。

Conclusion: 论文强调迫切需要开发新技术来检测和缓解去中心化无线网络中的对抗性行为，以确保网络的安全性和可靠性。

Abstract: The recent emergence of decentralized wireless networks empowers individual entities to own, operate, and offer subscriptionless connectivity services in exchange for monetary compensation. While traditional connectivity providers have built trust over decades through widespread adoption, established practices, and regulation, entities in a decentralized wireless network, lacking this foundation, may be incentivized to exploit the service for their own advantage. For example, a dishonest hotspot operator can intentionally violate the agreed upon connection terms in an attempt to increase their profits. In this paper, we examine and develop a taxonomy of adversarial behavior patterns in decentralized wireless networks. Our case study finds that provider-driven attacks can potentially more than triple provider earnings. We conclude the paper with a discussion on the critical need to develop novel techniques to detect and mitigate adversarial behavior in decentralized wireless networks.

</details>


### [2] [Multi-Server FL with Overlapping Clients: A Latency-Aware Relay Framework](https://arxiv.org/abs/2512.00025)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: 提出了一种基于重叠客户端的无云多服务器联邦学习框架，利用重叠区域客户端作为中继进行服务器间模型交换，无需额外通信链路，通过优化路由策略最大化模型传播范围。


<details>
  <summary>Details</summary>
Motivation: 多服务器联邦学习中，不同边缘服务器覆盖区域可能存在重叠，位于重叠区域的客户端可以访问多个边缘服务器的模型。传统方法存在通信瓶颈，需要利用这一特性来改善服务器间模型交换效率。

Method: 提出无云多服务器联邦学习框架，利用重叠客户端作为中继进行服务器间模型交换；推导非凸目标下非IID数据的收敛上界；基于冲突图的局部搜索算法优化路由策略和传输调度。

Result: 实验结果表明，相比现有方法，该方案在性能上有显著提升，能够在不引入新通信链路的情况下，通过多跳中继实现最广泛的模型传播覆盖。

Conclusion: 提出的基于重叠客户端的无云多服务器联邦学习框架有效解决了服务器间模型交换问题，通过理论分析和优化算法实现了高效的模型传播，显著提升了系统性能。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. In a typical multi-server FL architecture, the regions covered by different edge servers (ESs) may overlap. Under this architecture, clients located in the overlapping areas can access edge models from multiple ESs. Building on this observation, we propose a cloud-free multi-server FL framework that leverages Overlapping Clients (OCs) as relays for inter-server model exchange while uploading the local updated model to ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs without introducing new communication links. We derive a new convergence upper bound for non-convex objectives under non-IID data and an arbitrary number of cells, which explicitly quantifies the impact of inter-server propagation depth on convergence error. Guided by this theoretical result, we formulate an optimization problem that aims to maximize dissemination range of each ES model among all ESs within a limited latency. To solve this problem, we develop a conflict-graph-based local search algorithm optimizing the routing strategy and scheduling the transmission times of individual ESs to its neighboring ESs. This enables ES models to be relayed across multiple hops through neighboring ESs by OCs, achieving the widest possible transmission coverage for each model without introducing new communication links. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.

</details>


### [3] [An optimization framework for task allocation in the edge/hub/cloud paradigm](https://arxiv.org/abs/2512.00029)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: 提出基于BILP的优化框架，解决边缘/枢纽/云架构中的任务分配问题，最小化延迟或能耗


<details>
  <summary>Details</summary>
Motivation: 物联网边缘/枢纽/云架构中的任务分配面临计算、通信和能量限制的挑战，需要针对这种简化架构的优化框架

Method: 采用二进制整数线性规划（BILP）方法，在设计时提供边缘/枢纽/云环境中的最优任务分配方案

Result: 通过真实用例和合成基准测试验证，该方法能产生最优且可扩展的结果，支持不同应用和计算设备的设计空间探索

Conclusion: 提出的BILP框架能有效解决边缘/枢纽/云架构中的任务分配问题，在延迟和能耗优化方面表现优异

Abstract: With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices.

</details>


### [4] [Refined Bayesian Optimization for Efficient Beam Alignment in Intelligent Indoor Wireless Environments](https://arxiv.org/abs/2512.00036)
*Parth Ashokbhai Shiroya,Amod Ashtekar,Swarnagowri Shashidhar,Mohammed E. Eltayeb*

Main category: cs.NI

TL;DR: 提出R-BO框架，利用毫米波收发器模式的结构特性，通过贝叶斯优化和局部细化实现室内环境下的高效波束对准，减少88%的探测开销。


<details>
  <summary>Details</summary>
Motivation: 室内智能无线环境需要快速可靠的波束对准来维持高吞吐量连接。传统穷举波束训练成本过高，而室内密集散射体和硬件缺陷导致多径和旁瓣泄漏，降低了室外算法的效果。

Method: 提出精炼贝叶斯优化(R-BO)框架，利用高斯过程代理模型配合Matern核函数和期望改进采集函数，在线重新优化超参数以适应不规则变化，并在预测最优值周围进行局部细化。

Result: 在室内实验室43个接收位置上的实验显示：10度内波束对准准确率达97.7%，平均损耗小于0.3dB，相比穷举搜索减少88%的探测开销。

Conclusion: R-BO为实时室内智能无线环境提供了一种高效、自适应的波束对准解决方案，能够有效应对室内多径和硬件缺陷带来的挑战。

Abstract: Future intelligent indoor wireless environments re- quire fast and reliable beam alignment to sustain high-throughput links under mobility and blockage. Exhaustive beam training achieves optimal performance but is prohibitively costly. In indoor settings, dense scatterers and transceiver hardware imperfections introduce multipath and sidelobe leakage, producing measurable power across multiple angles and reducing the effectiveness of outdoor-oriented alignment algorithms. This paper presents a Refined Bayesian Optimization (R-BO) framework that exploits the inherent structure of mmWave transceiver patterns, where received power gradually increases as the transmit and receive beams converge toward the optimum. R-BO integrates a Gaussian Process (GP) surrogate with a Matern kernel and an Expected Improvement (EI) acquisition function, followed by a localized refinement around the predicted optimum. The GP hyperparam- eters are re-optimized online to adapt to irregular variations in the measured angular power field caused by reflections and sidelobe leakage. Experiments across 43 receiver positions in an indoor laboratory demonstrate 97.7% beam-alignment accuracy within 10 degrees, less than 0.3 dB average loss, and an 88% reduction in probing overhead compared to exhaustive search. These results establish R-BO as an efficient and adaptive beam-alignment solution for real-time intelligent indoor wireless environments.

</details>


### [5] [LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation](https://arxiv.org/abs/2512.00039)
*Tasnim Ahmed,Siana Rizwan,Naveed Ejaz,Salimur Choudhury*

Main category: cs.NI

TL;DR: 论文提出NL4RA数据集和LM4Opt RA框架，用于评估LLM在资源分配优化问题中的数学建模能力，并引入LAME自动评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法处理动态环境、相互依赖变量和异构约束的复杂资源分配优化问题，需要专门的数据集和评估方法来衡量LLM在这类数学推理任务上的表现。

Method: 1) 创建NL4RA数据集（50个LP/ILP/MILP资源分配问题）；2) 提出LM4Opt RA多候选框架，结合直接提示、少样本提示和思维链等策略；3) 引入LAME自动评估指标替代传统文本相似度指标。

Result: Llama-3.1-70B在LM4Opt-RA框架下获得0.8007的LAME分数，显著优于其他模型，Llama-3.1-8B紧随其后。提出的方法在LAME和其他指标上均超越基线LLM。

Conclusion: 虽然基线LLM显示出潜力，但仍落后于人类专家；提出的LM4Opt RA框架和LAME评估指标为资源分配优化问题的自动数学建模提供了有效解决方案。

Abstract: Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics.

</details>


### [6] [LPWAN based IoT Architecture for Distributed Energy Monitoring in Deep Indoor Environments](https://arxiv.org/abs/2512.00998)
*Christof Röhrig,Benz Cramer*

Main category: cs.NI

TL;DR: 实验比较多种LPWAN技术在建筑穿透和覆盖性能，并提出结合多种LPWAN技术的分布式混合物联网架构


<details>
  <summary>Details</summary>
Motivation: 建筑能耗监测需要在地下空间等无线信号难以覆盖的区域部署传感器，需要评估不同LPWAN技术的穿透能力和覆盖范围

Method: 实验比较LoRaWAN、NB-IoT、Sigfox 0G和Wi-SUN四种LPWAN技术；提出基于抽象层的分布式混合物联网架构，使用MQTT协议实现发布-订阅消息模式

Result: 通过实验评估了不同LPWAN技术的建筑穿透和无线覆盖性能；实现了将专有二进制数据解码并转换为标准化JSON格式的抽象层

Conclusion: 混合LPWAN架构可以优化成本和覆盖范围，为建筑能耗监测提供可靠解决方案

Abstract: Continuous energy monitoring is essential for identifying potential savings and predicting the energy requirements of buildings. Energy meters are often located in underground spaces that are difficult to reach with wireless technology. This paper presents an experimental study comparing different Low Power Wide Area Networks (LPWAN) technologies in terms of building penetration and radio coverage. The technologies Low Power Long Range Wide Area Networks (LoRaWAN), Narrow Band Internet of Things (NB-IoT), Sigfox 0G and Wireless Smart Ubiquitous Networks (Wi-SUN) are evaluated experimentally. It also proposes a distributed hybrid IoT architecture that combines multiple LPWAN technologies using an abstraction layer to optimize cost and coverage. Communication is message-based using the publish-subscribe messaging pattern. It is implemented using the MQTT protocol. The abstraction layer decodes the proprietary binary data and converts it to a normalized JSON format.

</details>


### [7] [Constrained Network Slice Assignment via Large Language Models](https://arxiv.org/abs/2512.00040)
*Sagar Sudhakara,Pankaj Rajak*

Main category: cs.NI

TL;DR: LLM用于5G网络切片资源分配，通过零-shot直接分配和提供语义洞察辅助整数规划，性能接近传统方法但需结合精确求解器满足约束


<details>
  <summary>Details</summary>
Motivation: 网络切片需要根据服务需求（如高带宽、低延迟）将用户分配到虚拟切片，传统优化算法复杂，探索LLM能否简化这一过程

Method: 两种方法：1) LLM零-shot直接分配用户服务请求到切片；2) LLM提供语义洞察估计请求相似度，辅助整数规划模型

Result: LLM零-shot能产生合理的切片分配初稿，但可能违反容量或延迟约束；LLM引导的分组结合优化求解器后，性能与传统使用详细数值数据的方法相当

Conclusion: LLM单独使用不能完美满足所有约束，但能显著减少搜索空间，与精确求解器结合为5G网络切片资源分配提供了有前景的方法

Abstract: Modern networks support network slicing, which partitions physical infrastructure into virtual slices tailored to different service requirements (for example, high bandwidth or low latency). Optimally allocating users to slices is a constrained optimization problem that traditionally requires complex algorithms. In this paper, we explore the use of Large Language Models (LLMs) to tackle radio resource allocation for network slicing. We focus on two approaches: (1) using an LLM in a zero-shot setting to directly assign user service requests to slices, and (2) formulating an integer programming model where the LLM provides semantic insight by estimating similarity between requests. Our experiments show that an LLM, even with zero-shot prompting, can produce a reasonable first draft of slice assignments, although it may violate some capacity or latency constraints. We then incorporate the LLM's understanding of service requirements into an optimization solver to generate an improved allocation. The results demonstrate that LLM-guided grouping of requests, based on minimal textual input, achieves performance comparable to traditional methods that use detailed numerical data, in terms of resource utilization and slice isolation. While the LLM alone does not perfectly satisfy all constraints, it significantly reduces the search space and, when combined with exact solvers, provides a promising approach for efficient 5G network slicing resource allocation.

</details>


### [8] [Mesh Augmentation of LoRaWAN-based IoT Networks](https://arxiv.org/abs/2512.00161)
*Ram Ramanathan,Dmitrii Dugaev,Liang Tan,Warren Ramanathan*

Main category: cs.NI

TL;DR: LIMA协议通过多跳网状网络扩展LoRaWAN覆盖范围并降低终端设备能耗，无需修改现有设备和标准。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN的单跳架构在大型、偏远和射频挑战环境中存在覆盖范围有限和终端设备能耗过高的问题，现有解决方案要么不兼容LoRaWAN，要么仅限于单跳中继。

Method: LIMA通过LIMA路由器构建网状网络，使用反向路径转发建立路由，通过LIMA隧道传输LoRaWAN消息，透明扩展自适应数据速率(ADR)，并在设备可直接从网关访问时抑制重复转发。

Result: NS-3模拟显示LIMA将投递率、可扩展性、终端设备能耗分别提升高达5倍、8倍和12.6倍，延迟降低2.3倍。原型测试证实LIMA可在现有LoRaWAN系统中成功部署。

Conclusion: LIMA协议能够显著提升LoRaWAN的覆盖范围和能效，同时保持与现有设备和标准的完全兼容性，为物联网通信提供了有效的增强方案。

Abstract: LoRaWAN is a leading standard and technology for low-power, long-range Internet-of-Things (IoT) communications. However, its single-hop architecture results in limited effective range and excessive power consumption for end devices, especially when deployed in large, remote and RF-challenged environments. Existing solutions are either incompatible with LoRaWAN, or limit relaying to a single hop. We present LIMA, a protocol for augmenting an existing or new LoRaWAN deployment with a mesh network of LIMA Routers. LIMA increases the effective coverage range well beyond the maximum LoRa range via multi-hopping, and significantly reduces the energy consumed by end-devices. LIMA requires no changes to the end-device, the servers or the LoRaWAN standard. LIMA builds routes using reverse path forwarding, tunnels LoRaWAN messages over LIMA, provides transparent extension of the existing Adaptive Data Rate (ADR), and suppresses duplicate forwarding if the device is directly reachable from the Gateway. Simulations using Network Simulator 3 (ns-3) show that LIMA increases the delivery rate, scalability, ED energy consumption by up to 5x, 8x and 12.6x respectively, and reduces latency by up to 2.3x. Table-top and outdoor testing with a prototype constructed using a commercial gateway as a starting point confirm that LIMA can be successfully deployed within an existing LoRaWAN system, and can provide range and energy gains transparently.

</details>


### [9] [On the Prediction of Wi-Fi Performance through Deep Learning](https://arxiv.org/abs/2512.00211)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 该研究使用CNN和LSTM深度学习模型预测Wi-Fi网络中的帧传输成功率(FDR)，CNN在推理延迟方面表现更优，精度损失较小。


<details>
  <summary>Details</summary>
Motivation: 现代工业Wi-Fi系统需要可靠、可预测的通信，特别是在要求连续运行和低延迟的场景中。预测无线信道质量变化能够实现自适应策略并显著提高系统鲁棒性。

Method: 使用卷积神经网络(CNN)和长短期记忆网络(LSTM)两种深度学习模型，基于真实场景收集的二进制时间序列（成功/失败）来预测帧传输成功率(FDR)。比较两种模型在预测精度和计算复杂度方面的表现。

Result: 两种模型都能以良好精度预测FDR演变，即使仅使用最小信息（单个二进制序列）。CNN显示出显著更低的推理延迟，与LSTM相比精度损失较小。

Conclusion: CNN模型在资源受限系统中更具应用潜力，因为它在保持良好预测精度的同时具有更低的推理延迟，更适合工业Wi-Fi网络的实时预测需求。

Abstract: Ensuring reliable and predictable communications is one of the main goals in modern industrial systems that rely on Wi-Fi networks, especially in scenarios where continuity of operation and low latency are required. In these contexts, the ability to predict changes in wireless channel quality can enable adaptive strategies and significantly improve system robustness. This contribution focuses on the prediction of the Frame Delivery Ratio (FDR), a key metric that represents the percentage of successful transmissions, starting from time sequences of binary outcomes (success/failure) collected in a real scenario. The analysis focuses on two models of deep learning: a Convolutional Neural Network (CNN) and a Long Short-Term Memory network (LSTM), both selected for their ability to predict the outcome of time sequences. Models are compared in terms of prediction accuracy and computational complexity, with the aim of evaluating their applicability to systems with limited resources. Preliminary results show that both models are able to predict the evolution of the FDR with good accuracy, even from minimal information (a single binary sequence). In particular, CNN shows a significantly lower inference latency, with a marginal loss in accuracy compared to LSTM.

</details>


### [10] [Analysis of the operation of a TSN switch and other devices using executable QR codes](https://arxiv.org/abs/2512.00221)
*Stefano Scanzio,Pietro Chiavassa,Gianluca Cena*

Main category: cs.NI

TL;DR: 本文介绍了一种可执行QR码（sQRy）技术，通过具体案例展示了如何在工业网络环境中利用该技术评估TSN交换机的运行状态，即使在没有网络连接的情况下也能分析其状态LED。


<details>
  <summary>Details</summary>
Motivation: 在工业网络环境中，当缺乏互联网连接时，技术人员需要一种便捷的方法来评估TSN交换机的运行状态。传统方法可能受限，因此需要开发一种不依赖网络连接的技术解决方案。

Method: 提出了可执行QR码（sQRy）技术，详细描述了从生成可执行QR码的完整生成链，到在移动设备上执行该代码的完整执行链。通过具体示例展示了整个技术流程。

Result: 成功演示了sQRy技术在工业网络环境中的应用，能够通过分析TSN交换机的状态LED来评估其运行状态，即使在无网络连接的情况下也能正常工作。

Conclusion: 可执行QR码技术为工业网络环境中的设备状态评估提供了一种创新的解决方案，特别适用于缺乏网络连接的场景，具有实际应用价值。

Abstract: Executable QR codes, also known as sQRy, are a technology aimed at inserting executable programs in a QR code. Through a concrete example, in this paper, we demonstrate their usage in the context of industrial networks in order to assess the operation of a TSN switch by analyzing its status LEDs even in the absence of an internet connection. The entire generation chain that is used to create the sQRy, as well as the corresponding execution chain that, starting from the sQRy, runs it on a mobile device, has been detailed through examples.

</details>


### [11] [Design and Evaluation of a Multi-Agent Perception System for Autonomous Flying Networks](https://arxiv.org/abs/2512.00259)
*Diogo Ferreira,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: MAPS系统利用多模态大语言模型和AI代理，通过无人机收集的视觉和音频数据感知用户需求，自动生成服务等级规范，为自主飞行网络提供感知层支持。


<details>
  <summary>Details</summary>
Motivation: 当前自主飞行网络方法主要关注无人机部署、路由和资源管理，但缺乏对用户及其服务需求的自主感知能力，这是实现零接触网络操作的关键能力。

Method: 提出多智能体感知系统(MAPS)，采用模块化可扩展架构，利用多模态大语言模型和AI代理技术，解析无人机收集的视觉和音频数据，自动生成描述用户数量、空间分布和流量需求的服务等级规范。

Result: 在合成多模态应急数据集上评估，用户检测准确率超过70%，90%情况下SLS生成时间低于130秒。结果表明结合音频和视觉模态能提升用户检测效果。

Conclusion: MAPS为自主零接触飞行网络提供了必要的感知层，证明了多模态AI在自主网络感知中的有效性。

Abstract: Autonomous Flying Networks (FNs) are emerging as a key enabler of on-demand connectivity in dynamic and infrastructure-limited environments. However, current approaches mainly focus on UAV placement, routing, and resource management, neglecting the autonomous perception of users and their service demands - a critical capability for zero-touch network operation.
  This paper presents the Multi-Agent Perception System (MAPS), a modular and scalable system that leverages multi-modal large language models (MM-LLMs) and agentic Artificial Intelligence (AI) to interpret visual and audio data collected by UAVs and generate Service Level Specifications (SLSs) describing user count, spatial distribution, and traffic demand. MAPS is evaluated using a synthetic multimodal emergency dataset, achieving user detection accuracies above 70% and SLS generation under 130 seconds in 90% of cases. Results demonstrate that combining audio and visual modalities enhances user detection and show that MAPS provides the perception layer required for autonomous, zero-touch FNs.

</details>


### [12] [Smart-TCP: An Agentic AI-based Autonomous and Adaptive TCP Protocol](https://arxiv.org/abs/2512.00491)
*Yule Han,Kezhi Wang,Kun Yang*

Main category: cs.NI

TL;DR: Smart-TCP：基于LLM的智能TCP协议，将传统TCP状态机控制逻辑重构为具备上下文感知、自主推理和工具使用能力的智能体架构


<details>
  <summary>Details</summary>
Motivation: 传统TCP协议依赖硬编码状态机和确定性算法，难以满足智能自主网络架构的需求。需要更灵活、智能的协议控制机制来适应复杂网络环境

Method: 1. 采用基于LLM的智能体范式；2. 提出Smart-TCP架构，将TCP核心控制逻辑重构为自主智能体；3. 使用上下文聚合机制合成协议上下文；4. 利用LLM进行自主逻辑推理；5. 调用算术逻辑单元(ALU)作为计算工具；6. 建立基于该架构的双智能体交互框架

Result: Smart-TCP智能体在静态预测和错误检测方面表现出色，端到端会话成功率达到93.33%，验证了基于智能体AI的TCP协议技术可行性

Conclusion: 基于LLM的智能体范式能够有效重构TCP协议控制逻辑，为智能自主网络架构提供了可行的技术路径，展示了AI驱动的网络协议设计的潜力

Abstract: The Transmission Control Protocol (TCP) relies on a state machine and deterministic arithmetic to ensure reliable connections. However, traditional protocol logic driven by hard-coded state machines struggles to meet the demands of intelligent and autonomous network architectures. Here, we adopt the agentic AI-based paradigm, driven by Large Language Models (LLMs), characterized by context perception, autonomous reasoning, and tool use. Based on this, we propose Smart-TCP, which re-imagines TCP's core control logic as an autonomous agent. Specifically, the proposed architecture employs a context aggregation mechanism to synthesize the protocol context, utilizes the LLM for autonomous logical reasoning, and invokes an Arithmetic Logic Unit (ALU) as a tool for computation. Furthermore, we establish a dual-agent interaction framework based on this architecture and implement TCP protocol interactions. Experiments demonstrate that the Smart-TCP agent excels in static prediction and error detection, achieving a 93.33% success rate in end-to-end sessions. These results strongly validate the technical feasibility of an agentic AI-based TCP protocol.

</details>


### [13] [Improving Channel Estimation Through Gold Sequences](https://arxiv.org/abs/2512.00509)
*Sumita Majhi,Kaushal Shelke,Pinaki Mitra,Ujjwal Biswas*

Main category: cs.NI

TL;DR: 该研究评估了使用Gold编码和传统V-BLAST的NOMA系统，提出了一种结合分数功率分配和部分解码数据符号的新型信道估计方法，在AWGN、瑞利衰落和阴影的仿真环境中优于传统导频技术。


<details>
  <summary>Details</summary>
Motivation: NOMA系统中叠加信号在共享子载波上使得用户分离困难，不同于MIMO系统。Gold序列的正交特性可能增强用户分离和信道估计性能。

Method: 提出新颖的信道估计方法，结合分数功率分配和部分解码数据符号。创建包含AWGN、瑞利衰落和阴影的现实仿真环境，使用导频信号、功率分配和数据符号构建信道预测函数(CPF)。

Result: 提出的信道预测函数(CPF)在仿真环境中超越了传统的导频基信道估计技术。

Conclusion: Gold编码和C-V-BLAST结合的NOMA系统通过新型信道估计方法能够有效改善用户分离和信道估计性能，为NOMA系统提供了更优的信道估计解决方案。

Abstract: This study evaluates Non-Orthogonal Multiple Access (NOMA) systems using Gold coding and Conventional-V-BLAST (C-V-BLAST). Superimposed signals on shared subcarriers make NOMA user separation difficult, unlike MIMO. Gold sequences' orthogonal features may enhance user separation and channel estimation. A novel channel estimation approach uses fractional power allocation and partially decoded data symbols. A realistic simulation environment was created using AWGN, Rayleigh fading, and shadowing. Using pilot signals, power allocation, and data symbols, our Channel Prediction Function (CPF) surpasses pilot-based techniques.

</details>


### [14] [Goal-Oriented Multi-Agent Semantic Networking: Unifying Intents, Semantics, and Intelligence](https://arxiv.org/abs/2512.01035)
*Shutong Chen,Qi Liao,Adnan Aijaz,Yansha Deng*

Main category: cs.NI

TL;DR: GoAgentNet是一种面向6G的目标导向多智能体语义网络架构，通过将应用和网络功能抽象为协作智能体，实现从数据交换到目标达成的通信范式转变，显著提升能效和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有网络架构将应用与网络完全解耦，无法暴露和利用高层目标，限制了智能适应服务需求的能力。6G服务正向目标导向和AI原生通信演进，需要新的架构支持能效和智能服务。

Method: 提出GoAgentNet架构，将应用和网络功能抽象为多个协作智能体，通过语义计算和跨层语义网络联合编排多智能体的感知、网络、计算和控制，实现统一应用目标的协同追求。

Result: 在机器人故障检测和恢复案例中，GoAgentNet相比现有网络架构能效提升高达99%，任务成功率提升高达72%，展示了支持可扩展和可持续6G系统的潜力。

Conclusion: GoAgentNet通过将通信从数据交换提升到目标达成，为6G服务提供了更高效智能的架构解决方案，同时识别了部署挑战和潜在解决方案，具有重要的实际应用价值。

Abstract: 6G services are evolving toward goal-oriented and AI-native communication, which are expected to deliver transformative societal benefits across various industries and promote energy sustainability. Yet today's networking architectures, built on complete decoupling of the applications and the network, cannot expose or exploit high-level goals, limiting their ability to adapt intelligently to service needs. This work introduces Goal-Oriented Multi-Agent Semantic Networking (GoAgentNet), a new architecture that elevates communication from data exchange to goal fulfilment. GoAgentNet enables applications and the network to collaborate by abstracting their functions into multiple collaborative agents, and jointly orchestrates multi-agent sensing, networking, computation, and control through semantic computation and cross-layer semantic networking, allowing the entire architecture to pursue unified application goals. We first outline the limitations of legacy network designs in supporting 6G services, based on which we highlight key enablers of our GoAgentNet design. Then, through three representative 6G usage scenarios, we demonstrate how GoAgentNet can unlock more efficient and intelligent services. We further identify unique challenges faced by GoAgentNet deployment and corresponding potential solutions. A case study on robotic fault detection and recovery shows that our GoAgentNet architecture improves energy efficiency by up to 99% and increases the task success rate by up to 72%, compared with the existing networking architectures without GoAgentNet, which underscores its potential to support scalable and sustainable 6G systems.

</details>


### [15] [Internet of Intelligent Reflecting Surfaces (IoIRS)](https://arxiv.org/abs/2512.01083)
*Fatih E. Bilgen,A. Sila Okcu,O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.NI

TL;DR: 提出"智能反射表面互联网(IoIRS)"架构，将IRS从单纯物理层组件重构为跨物理层和上层网络层的连接混合实体，以解决大规模IRS网络中的控制挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究将IRS视为仅由发射机控制的独立物理层组件，但在大规模IRS网络和大量发射机使用场景下，这种发射机中心设计面临重大挑战。需要新的架构来充分发挥IRS在未来无线网络中的潜力。

Method: 提出IoIRS架构，将IRS重新构想为连接、混合的实体，跨越物理层和上层网络层运行。提供了概念框架和初步协议套件，以将这些表面集成到更高网络层中。

Result: 提出了IoIRS架构的概念框架和初步协议套件，探讨了该架构在实际部署中的应用方式，为未来无线网络充分发挥IRS能力提供了新的设计思路。

Conclusion: IoIRS架构对于充分发挥未来无线网络中IRS的潜力至关重要，其部署将是实现未来无线网络全部能力的关键。该架构将IRS从被动反射表面转变为连接的网络实体，解决了大规模部署中的控制挑战。

Abstract: Intelligent Reflecting Surfaces (IRS) are anticipated to serve as a key cornerstone of future wireless networks, providing an unmatched capability to deterministically shape electromagnetic wave propagation. Despite this potential, most existing research still considers the IRS merely as a standalone physical-layer component, controlled by transmitters. However, as networks grow to encompass a massive number of these surfaces and a massive number of transmitters wishing to use them, this transmitter-centric design encounters substantial challenges. To overcome this challenge, we propose the Internet of IRS (IoIRS), an architecture that reconceives the IRS not just as a passive reflecting surface, but as a connected, hybrid entity functioning across both the physical layer and upper network layers. We present the conceptual framework and a preliminary protocol suite necessary to integrate these surfaces into the higher network layers. We conclude by examining how IoIRS architectures could be applied in practice, as their deployment will be essential for fully realizing the capabilities of future wireless networks.

</details>


### [16] [Physical-Layer Analysis of LoRa Robustness in the Presence of Narrowband Interference](https://arxiv.org/abs/2512.01088)
*Jingxiang Huang,Samer Lahoud*

Main category: cs.NI

TL;DR: 研究窄带干扰（BPSK和GMSK）对LoRa物理层解调的影响，发现将窄带干扰建模为AWGN会系统性高估CSS解调的误符号率。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术的发展，Sub-GHz免许可频段中LoRa、Sigfox、LR-FHSS等协议需要共存，导致相互干扰。需要研究窄带干扰对LoRa解调的具体影响。

Method: 采用符号级蒙特卡洛仿真，分析干扰噪声比（INR）在不同信噪比（SNR）和噪声基底下对误符号率（SER）的影响，并与等功率AWGN干扰进行对比。

Result: 1. 将窄带干扰建模为AWGN会系统性高估CSS解调的SER；2. 明确了AWGN和两种窄带干扰器造成的不同损伤水平；3. 为最大INR拟合了两段函数（低SNR段和高SNR段），确保正确解调。

Conclusion: 窄带干扰对LoRa解调的影响与AWGN不同，不能简单建模为AWGN。研究提供了对干扰机制的物理洞察，并为系统设计提供了实用的INR阈值函数。

Abstract: With the rapid development of Internet of Things (IoT) technologies, the sub-GHz unlicensed spectrum is increasingly being shared by protocols such as Long Range (LoRa), Sigfox, and Long-Range Frequency-Hopping Spread Spectrum (LR-FHSS). These protocols must coexist within the same frequency bands, leading to mutual interference. This paper investigates the physical-layer impact of two types of narrowband signals (BPSK and GMSK) on LoRa demodulation. We employ symbol-level Monte Carlo simulations to analyse how the interference-to-noise ratio (INR) affects the symbol error rate (SER) at a given signal-to-noise ratio (SNR) and noise floor, and then compare the results with those for additive white Gaussian noise (AWGN) of equal power. We demonstrate that modelling narrowband interference as additive white Gaussian noise (AWGN) systematically overestimates the SER of Chirp Spread Spectrum (CSS) demodulation. We also clarify the distinct impairment levels induced by AWGN and two types of narrowband interferers, and provide physical insight into the underlying mechanisms. Finally, we fit a two-segment function for the maximum INR that ensures correct demodulation across SNRs, with one segment for low SNR and the other for high SNR.

</details>


### [17] [Modeling and Simulation of Data Protection Systems for Business Continuity and Disaster Recovery](https://arxiv.org/abs/2512.01477)
*Saso Nikolovski,Pece Mitrevski*

Main category: cs.NI

TL;DR: 本文提出一个用于选择和维护组织内数据保护与恢复解决方案的综合框架，通过比较云环境中的恢复系统，在实际生产环境中评估其可靠性，为业务连续性计划提供决策指导。


<details>
  <summary>Details</summary>
Motivation: 在当今依赖信息技术的企业环境中，建立包含灾难恢复策略的稳健业务连续性计划对于确保在中断后快速恢复至关重要。组织需要可靠的方法来选择和维护数据保护与恢复解决方案。

Method: 1) 对部分或完全在云环境中运行的系统进行恢复解决方案的比较分析；2) 在实际生产环境中部署和评估两个系统；3) 使用仿真软件识别关键性能和可靠性指标；4) 对每个系统进行系统动力学分析；5) 提出选择和维护数据保护与恢复解决方案的综合框架。

Result: 在实际生产环境中评估了两个恢复系统，通过仿真软件识别了关键性能指标，进行了系统动力学分析，最终提出了一个全面的框架，为选择适当的恢复概念提供可操作的见解。

Conclusion: 本研究提出的框架能够帮助组织根据运营需求选择和维护数据保护与恢复解决方案，同时遵循业务连续性和灾难恢复计划中预定的时间表，为决策制定提供实用指导。

Abstract: In today's corporate landscape, particularly where operations rely heavily on information technologies, establishing a robust business continuity plan, including a disaster recovery strategy, is essential for ensuring swift recuperation following outages. This study presents a comparative analysis of recovery solutions, focusing on systems that operate partially or entirely within cloud environments and assessing their reliability in fulfilling organizational roles securely and dependably. Two such systems were deployed and evaluated in a real-world production setting. Key performance and reliability metrics were identified using simulation software to enhance these systems, alongside a System Dynamics analysis conducted for each. This work proposes a comprehensive framework for selecting and maintaining data protection and recovery solutions within organizational structures, outlining criteria for aligning chosen approaches with operational needs while adhering to predetermined timelines specified in business continuity and disaster recovery plans. The resulting analysis and findings offer actionable insights to guide decision-making when selecting appropriate recovery concepts.

</details>


### [18] [Velocity-Adaptive Access Scheme for Semantic-Aware Vehicular Networks: Joint Fairness and AoI Optimization](https://arxiv.org/abs/2512.01571)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 本文针对5G NR V2X Mode 2中的公平接入和AoI优化问题，通过调整选择窗口实现公平性，采用图像语义通信降低延迟，使用SHS分析AoI，构建多目标优化问题并用SCA和凸优化求解，提出基于LLM的算法。


<details>
  <summary>Details</summary>
Motivation: 在5G NR V2X Mode 2中，车辆与RSU通信时，由于车速差异导致通信时长不同，造成数据传输量不均，这在高速环境下带来安全隐患。同时，调整选择窗口会影响通信时间从而影响AoI，而5G NR的重评估机制虽减少资源碰撞但可能增加AoI。

Method: 1) 通过调整不同车辆的选择窗口定义公平指数；2) 采用图像语义通信系统降低延迟；3) 使用随机混合系统(SHS)分析AoI；4) 构建公平接入和AoI优化的多目标优化问题；5) 采用顺序凸逼近(SCA)将非凸问题转化为凸问题；6) 使用凸优化求解；7) 提出基于大语言模型(LLM)的算法。

Result: 通过数值仿真验证了方案的有效性，表明该方法能够在保证公平接入的同时优化AoI性能。

Conclusion: 本文提出的方法成功解决了5G NR V2X Mode 2中的公平接入和AoI优化问题，通过调整选择窗口、采用语义通信、构建优化问题并利用SCA和凸优化求解，实现了公平性和信息新鲜度的平衡，为高速车联网环境提供了有效的解决方案。

Abstract: In this paper, we address the problem of fair access and Age of Information (AoI) optimization in 5G New Radio (NR) Vehicle to Everything (V2X) Mode 2. Specifically, vehicles need to exchange information with the road side unit (RSU). However, due to the varying vehicle speeds leading to different communication durations, the amount of data exchanged between different vehicles and the RSU may vary. This may poses significant safety risks in high-speed environments. To address this, we define a fairness index through tuning the selection window of different vehicles and consider the image semantic communication system to reduce latency. However, adjusting the selection window may affect the communication time, thereby impacting the AoI. Moreover, considering the re-evaluation mechanism in 5G NR, which helps reduce resource collisions, it may lead to an increase in AoI. We analyze the AoI using Stochastic Hybrid System (SHS) and construct a multi-objective optimization problem to achieve fair access and AoI optimization. Sequential Convex Approximation (SCA) is employed to transform the non-convex problem into a convex one, and solve it using convex optimization. We also provide a large language model (LLM) based algorithm. The scheme's effectiveness is validated through numerical simulations.

</details>


### [19] [HERMES: Heterogeneous Application-Enabled Routing Middleware for Edge-IoT Systems](https://arxiv.org/abs/2512.01824)
*Jéssica Consciência,António Grilo*

Main category: cs.NI

TL;DR: 提出一个软件框架，通过动态整合应用感知决策来增强路由灵活性，在异构设备组成的多跳Wi-Fi网络中实现边缘智能应用。


<details>
  <summary>Details</summary>
Motivation: 物联网设备异构性和应用复杂需求与传统路由协议假设不匹配，传统路由缺乏灵活性来适应应用层指标和策略。

Method: 建立由ESP8266、ESP32和Raspberry Pi 3B组成的多跳Wi-Fi网络，采用主动式路由层和容错机制，中间件层提供三种路由策略：两种调整消息路径，一种允许应用塑造网络拓扑。

Result: 实验验证显示设备异构性显著影响网络性能，吞吐量和推理持续时间分析表明策略影响应用行为，拓扑对去中心化性能至关重要，框架适合复杂任务。

Conclusion: 该框架通过应用感知路由增强了边缘计算网络的灵活性，能够支持分布式神经网络推理等边缘智能应用，适应异构设备环境。

Abstract: The growth of the Internet of Things has enabled a new generation of applications, pushing computation and intelligence toward the network edge. This trend, however, exposes challenges, as the heterogeneity of devices and the complex requirements of applications are often misaligned with the assumptions of traditional routing protocols, which lack the flexibility to accommodate application-layer metrics and policies. This work addresses this gap by proposing a software framework that enhances routing flexibility by dynamically incorporating application-aware decisions. The core of the work establishes a multi-hop Wi-Fi network of heterogeneous devices, specifically ESP8266, ESP32, and Raspberry Pi 3B. The routing layer follows a proactive approach, while the network is fault-tolerant, maintaining operation despite both node loss and message loss. On top of this, a middleware layer introduces three strategies for influencing routing behavior: two adapt the path a message traverses until arriving at the destination, while the third allows applications to shape the network topology. This layer offers a flexible interface for diverse applications. The framework was validated on a physical testbed through edge intelligence use cases, including distributing neural network inference computations across multiple devices and offloading the entire workload to the most capable node. Distributed inference is useful in scenarios requiring low latency, energy efficiency, privacy, and autonomy. Experimental results indicated that device heterogeneity significantly impacts network performance. Throughput and inference duration analysis showed the influence of the strategies on application behaviour, revealed that topology critically affects decentralized performance, and demonstrated the suitability of the framework for complex tasks.

</details>


### [20] [Delay Tolerant Networking to Extend Connectivity in Rural Areas Using Public Transport Systems: Design And Analysis](https://arxiv.org/abs/2512.01829)
*Salah Abdeljabar,Marco Zennaro,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出利用非正式公共交通系统作为移动数据驮马的延迟容忍网络框架，以改善发展中国家农村地区的互联网连接


<details>
  <summary>Details</summary>
Motivation: 数字时代互联网接入至关重要，但发展中国家农村地区存在显著数字鸿沟，需要创新解决方案来提供连接

Method: 开发基于非正式公共交通系统（如小巴出租车）的延迟容忍网络框架，建立概率模型捕捉车辆移动随机性，分析数据传输率和峰值信息年龄等性能指标

Result: 推导出平均峰值信息年龄的解析近似并通过仿真验证，利用努瓦克肖特、阿克拉和亚的斯亚贝巴的真实数据集进行案例研究，证明框架的实用性和可扩展性

Conclusion: 利用现有交通网络可以显著弥合数字鸿沟，为偏远地区提供可靠的类互联网连接

Abstract: In today's digital age, access to the Internet is essential, yet a significant digital divide exists, particularly in rural areas of developing nations. This paper presents a Delay Tolerant Networking (DTN) framework that utilizes informal public transportation systems, such as minibus taxis, as mobile data mules to enhance connectivity in these underserved regions. We develop a probabilistic model to capture the randomness in vehicle mobility, including travel times and contact durations at bus stops. Key performance metrics are analyzed, including average data transmission rate and Peak Age of Information (PAoI), to assess the effectiveness of the proposed system. An analytical approximation for the Mean PAoI (MPAoI) is derived and validated through simulations. Case studies from real-world datasets in Nouakchott, Accra, and Addis Ababa demonstrate the practical applicability and scalability of our framework. The findings indicate that leveraging existing transportation networks can significantly bridge the digital divide by providing reliable internet-like connectivity to remote areas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky,Lucas Andersen,Thomas Engel,Fazel Ansari*

Main category: cs.AI

TL;DR: 提出基于因果机器学习的预测性维护框架，通过因果推理识别故障根本原因并量化干预措施对系统KPI的影响，实现从诊断到主动处方的转变。


<details>
  <summary>Details</summary>
Motivation: 当前制造业预测性维护依赖的预测模型往往基于虚假相关性而非真正的因果驱动因素，导致误诊和无效干预。关键挑战在于：虽然能预测故障发生，但缺乏系统方法来理解故障原因，从而无法确定最有效的干预措施。

Method: 使用预训练的因果基础模型作为"假设分析"模型，评估潜在修复措施的效果。通过测量每个干预对系统级KPI（如设备综合效率OEE）的因果效应，为生产线提供数据驱动的行动排序推荐。

Result: 使用半合成制造数据进行模型评估，并与基线机器学习模型进行比较。该方法不仅能识别根本原因，还能量化其操作影响。

Conclusion: 为稳健的预测性维护框架奠定技术基础，使工程师能够在因果环境中测试潜在解决方案，做出更有效的操作决策并减少昂贵的停机时间。

Abstract: The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [22] [Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions](https://arxiv.org/abs/2512.00097)
*Boyan Duan,Xiao Liang,Shuai Lu,Yaoxiang Wang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Mao Yang,Weizhu Chen,Yeyun Gong*

Main category: cs.AI

TL;DR: HAGeo方法在几何定理证明中通过启发式添加辅助构造，在IMO-30基准上达到28/30的解题率，超越AlphaGeometry，并创建了更具挑战性的HAGeo-409基准。


<details>
  <summary>Details</summary>
Motivation: 欧几里得几何的自动定理证明，特别是IMO级别的问题，是AI领域的重要挑战。现有方法依赖神经网络推理，需要GPU资源且效率有限。

Method: 提出HAGeo方法：基于启发式策略添加辅助构造的几何推理方法，完全在CPU上运行，无需神经网络推理。首先发现简单的随机添加辅助点策略就能达到银牌水平，然后开发更高效的启发式方法。

Result: 在IMO-30基准上解决了28/30个问题，达到金牌水平，显著超越AlphaGeometry。创建了包含409个问题的HAGeo-409基准，提供更精确的评估和更高挑战。

Conclusion: HAGeo展示了基于启发式的方法在几何定理证明中的高效性，无需神经网络就能达到顶尖水平。新基准为领域提供了更严格的评估标准。

Abstract: Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.

</details>


### [23] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R,Mohith Charan R,Seema Banu F*

Main category: cs.AI

TL;DR: 这篇综述系统梳理了多模态分块策略，为开发更高效的多模态AI系统提供技术基础和设计空间，涵盖文本、图像、音频、视频和跨模态数据的分块方法。


<details>
  <summary>Details</summary>
Motivation: 整合多模态分块策略的研究现状，为研究人员和从业者提供技术基础，帮助开发更有效和高效的多模态AI系统，应对现实应用中模态复杂性、处理准确性和生成一致性的挑战。

Method: 提供全面的分类和技术分析，针对每种模态（文本、图像、音频、视频、跨模态数据）分析经典和现代方法，包括固定大小标记窗口、递归文本分割、以对象为中心的视觉分块、基于静音的音频分割和视频场景检测等，并分析每种方法的底层方法论、支持工具、优势和挑战。

Result: 建立了多模态分块策略的综合分类体系，分析了不同方法的粒度-上下文权衡和多模态对齐等关键挑战，探索了新兴的跨模态分块策略，并提供了比较见解。

Conclusion: 该综述为开发鲁棒的分块管道铺平了道路，能够随着模态复杂性扩展、提高处理准确性并改善现实应用中的生成一致性，同时指出了异步信息密度和噪声对齐信号等开放问题，以及自适应、基于学习和任务特定分块的未来研究方向。

Abstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [24] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho,Jean-Stanislas Denain,David Atanasov,Samuel Albanie,Rohin Shah*

Main category: cs.AI

TL;DR: 提出了一个统计框架，将不同AI基准测试连接起来，在单一数值尺度上衡量模型能力和基准难度，实现跨时间和跨基准的AI能力比较


<details>
  <summary>Details</summary>
Motivation: 大多数AI基准测试在推出后几年甚至几个月内就会饱和，难以研究AI能力的长期趋势。需要一种方法来比较不同时间、不同基准测试下的模型能力

Method: 构建统计框架，将模型能力和基准测试难度映射到同一数值尺度上，形成"罗塞塔石碑"，无需假设能力随时间或训练计算量的演变方式

Result: 框架成功实现了跨基准和跨时间的模型能力比较，能够测量AI进展速度、预测未来能力、估计算法效率改进率，并能检测AI进展的快速加速

Conclusion: 该框架为解决AI基准测试饱和问题提供了有效工具，能够系统研究AI能力的长期趋势，为AI进展测量和预测提供了新方法

Abstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [25] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究不同训练激励对AI推理过程可监控性的影响，发现对抗性优化会降低监控性能，而直接优化可监控性并不总能带来改进


<details>
  <summary>Details</summary>
Motivation: AI系统输出自然语言推理过程为安全监控提供了机会，但推理过程是否忠实反映底层思维以及可监控程度可能受训练方式影响，需要研究不同训练激励如何影响监控能力

Method: 提出测量可监控性的新方法：通过监控器能否使用模型的推理过程预测关键潜在变量来评估；实验比较不同训练激励（长度惩罚、KL正则化、对抗性优化、直接优化可监控性）的效果

Result: 在控制准确率的情况下，常见激励（长度惩罚和KL正则化）没有一致影响；对抗性优化会降低监控性能；直接优化可监控性并不能可靠地带来改进

Conclusion: 训练激励对AI推理过程的可监控性有重要影响，对抗性优化会损害监控能力，而直接优化可监控性并非可靠解决方案，需要更深入理解训练与监控之间的关系

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [26] [Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking](https://arxiv.org/abs/2512.00267)
*Anab Maulana Barik,Shou Ziyi,Yang Kaiwen,Yang Qi,Shen Xin*

Main category: cs.AI

TL;DR: 提出Trification框架，通过生成全面验证动作和构建依赖图来改进自动事实核查系统，解决现有方法无法验证所有声明组件和缺乏结构化逻辑连接的问题。


<details>
  <summary>Details</summary>
Motivation: 技术发展使虚假信息快速传播，需要自动事实核查系统来维护在线媒体生态系统的安全和完整性。现有方法虽然通过分解声明为子任务并使用LLM多代理系统有效，但存在两个局限：无法验证声明的每个组件，以及缺乏结构化框架来逻辑连接子任务结果以得出最终预测。

Method: 提出Trification框架：1) 生成全面的验证动作集以确保对声明的完全覆盖；2) 将这些动作组织成依赖图以建模动作间的逻辑交互；3) 支持动态修改图，使系统能够调整验证策略。

Result: 在两个具有挑战性的基准测试上的实验结果表明，该框架显著提高了事实核查的准确性，从而推进了自动事实核查系统的最新技术水平。

Conclusion: Trification框架通过生成全面验证动作和构建依赖图，解决了现有自动事实核查系统的局限性，显著提升了事实核查准确性，为在线媒体生态系统的完整性提供了更有效的技术保障。

Abstract: Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.

</details>


### [27] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu,SiNan Du,Yiyan Qi,SiwenLu,Chengjin Xu,Chun Yuan,Jian Guo*

Main category: cs.AI

TL;DR: 提出PointCoT方法，通过边界框生成和图表重绘，将视觉感知融入图表推理，解决MLLMs在图表理解中的数值幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖OCR提取内容，当图表文本标注稀疏时会产生数值幻觉。现有方法主要关注指令扩展，未能解决视觉感知推理的根本挑战。

Method: 提出PointCoT方法，将反思性交互融入思维链推理，通过生成边界框和基于位置标注重绘图表，建立文本推理步骤与视觉基础区域的连接。构建ChartPoint-SFT-62k数据集，包含高质量图表样本及逐步CoT、边界框和重绘可视化。

Result: 开发了两个指令调优模型ChartPointQ2和ChartPointQ2.5，在多个图表基准测试中超越最先进方法，如在ChartBench上提升5.04%。

Conclusion: PointCoT通过视觉感知增强的推理方法有效解决了MLLMs在图表理解中的数值幻觉问题，显著提升了图表理解性能。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [28] [RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs](https://arxiv.org/abs/2512.00319)
*Ruike Hu,Shulei Wu*

Main category: cs.AI

TL;DR: 提出轻量级强化学习框架解决LLM生成结构化数据时的"结构鸿沟"问题，通过多维奖励函数和梯度正则化策略优化，显著提升JSON有效性至92.1%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言生成方面表现出色，但在自动化软件生态系统中集成时面临"结构鸿沟"问题——概率性token生成与确定性结构化数据格式(如JSON、XML)之间的内在矛盾。传统监督微调难以强制执行严格语法约束，导致"幻觉"键或格式错误，而约束解码方法又会带来显著的推理延迟。

Method: 提出轻量高效的强化学习框架，引入多维奖励函数将结构化输出任务分解为层次化约束：结构完整性、格式正确性、内容准确性和有效性。采用梯度正则化策略优化(GRPO)，使模型能够内化这些约束而无需单独的批评网络，相比PPO减少40%的峰值VRAM使用。

Result: 在复杂食谱生成和结构化数学推理(GSM8K-JSON)等多个任务上验证，方法达到89.7%的结构准确率和92.1%的JSON有效性，显著优于零样本基线(如GPT-3.5)和在LLaMA-3-8B等更大模型上的监督微调。训练动态分析显示模型按自定进度课程学习，先掌握语法熟练度再获得语义准确性。

Conclusion: 提出的轻量级强化学习框架有效解决了LLM生成结构化数据时的"结构鸿沟"问题，通过多维奖励函数和梯度正则化策略优化实现了高效的结构约束内化，在保持低资源消耗的同时显著提升了结构化输出的准确性和有效性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the "Structure Gap" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to "hallucinated" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.

</details>


### [29] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: CogEvo-Edu：一个分层多智能体系统，将检索、记忆和控制作为耦合的认知演化过程，显著提升STEM教育中LLM对话导师的性能。


<details>
  <summary>Details</summary>
Motivation: 当前STEM教育中部署的LLM对话导师大多采用单一LLM和静态RAG管道，在数字信号处理等复杂领域中难以维持连贯的学生模型、管理异构知识库和适应长期互动。需要将检索、记忆和控制视为耦合的认知演化过程。

Method: 提出CogEvo-Edu分层教育多智能体系统：认知感知层维护双重记忆并进行置信度加权整合以构建结构化、自校正的学生档案；知识演化层为每个知识块分配时空价值以驱动激活、语义压缩和遗忘；元控制层将教学制定为分层顺序决策，协调专业智能体并通过双重内外循环联合调整CPL/KEL超参数。

Result: 在构建的DSP-EduBench垂直基准测试中，使用三模型LLM-as-a-Judge集成评估，CogEvo-Edu将总体得分从5.32提升到9.23，在所有六个指标上均优于静态RAG、简单记忆和单智能体变体。

Conclusion: CogEvo-Edu通过联合演化学生档案、知识库和教学策略，证明了将检索、记忆和控制作为耦合认知演化过程的价值，显著提升了复杂STEM领域中的教学效果。

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [30] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 论文提出首个能实时推断用户个性并优化对话行为的强化学习框架，在主观情感对话领域取得突破性进展，超越了现有模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM领域过度关注数学、代码等机器擅长的确定性任务，而忽略了真正定义人类智能的主观情感对话领域。该领域因主观性强、难以形式化而被认为不适合传统RL方法，但作者认为这是一个可解决且具有变革性的RL问题。

Method: 提出首个能实时推断用户个性并优化模型行为以适应个性化对话偏好的框架。引入动态情感智能评估套件来量化改进效果。模型名为Echo-N1。

Result: 方法在非可验证环境中产生一致、稳健且显著的改进，Echo-N1表现远超其基础版本，并超越了专有的Doubao 1.5 Character模型。

Conclusion: 这项工作为RL开辟了新前沿：为对话中深刻主观、深刻人性化的维度优化模型，证明了RL在主观情感对话领域的可行性和有效性。

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [31] [Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models](https://arxiv.org/abs/2512.00349)
*Sitong Fang,Shiyi Hou,Kaile Wang,Boyuan Chen,Donghai Hong,Jiayi Zhou,Josef Dai,Yaodong Yang,Jiaming Ji*

Main category: cs.AI

TL;DR: 该论文揭示了多模态大语言模型中存在的欺骗风险，提出了首个多模态欺骗评估基准MM-DeceptionBench，并设计了基于图像辩论的多智能体监控框架来检测这些隐蔽的欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI系统能力提升，欺骗行为从文本扩展到多模态领域，带来更隐蔽和破坏性的安全风险。当前研究主要局限于文本欺骗，多模态欺骗风险尚未被探索，且现有监控方法对此类复杂跨模态欺骗行为效果有限。

Method: 1) 构建MM-DeceptionBench基准，涵盖六类欺骗行为，评估模型通过视觉和文本模态进行策略性操纵的能力；2) 提出"基于图像的辩论"多智能体监控框架，强制模型基于视觉证据进行论证，提高欺骗策略的可检测性。

Result: 实验表明，提出的监控框架显著提高了与人类判断的一致性，在所有测试模型上均有效，GPT-4o的Cohen's kappa提高了1.5倍，准确率提高了1.25倍。

Conclusion: 多模态欺骗是AI安全的重要威胁，需要专门工具进行监测。MM-DeceptionBench基准和基于图像的辩论监控框架为检测和缓解这类风险提供了有效方法，填补了多模态欺骗评估的研究空白。

Abstract: Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.

</details>


### [32] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng,Yuqin Dai,Jun Yin,Jing Zhong,Ziyang Han,Chaoyang Shi,ZhanXiang Jin,Maowei Jiang,Yuxing Han,Shuai Lu*

Main category: cs.AI

TL;DR: GreenPlanner是一个统一的能源和功能感知生成框架，通过自动化评估和生成相结合，解决建筑设计中空间功能与能源合规性的挑战。


<details>
  <summary>Details</summary>
Motivation: 建筑设计直接影响人类福祉和碳排放，但生成空间功能和能源合规的平面图仍然是手动、昂贵且不可扩展的。现有方法产生视觉上合理的布局，但经常违反关键约束，由于缺乏自动化评估而导致无效结果。

Method: GreenPlanner包含四个核心组件：1) 用于学习约束先验的标记设计可行性数据集；2) 用于预测能源性能和空间功能有效性的快速实用设计评估器(PDE)；3) 通过PDE引导过滤得到的绿色平面数据集(GreenPD)，将用户需求与法规合规布局配对；4) 在GreenPD上训练并使用PDE反馈的GreenFlow生成器，实现可控的法规感知生成。

Result: 实验表明，GreenPlanner将评估速度加速超过10^5倍，准确率超过99%，消除了无效样本，并将设计效率比专业建筑师提高了87%。

Conclusion: GreenPlanner通过统一设计评估和生成，实现了高效、合规的建筑平面图自动化设计，显著提升了设计质量和效率。

Abstract: Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [33] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi,Vincent Jeanselme,Shalmali Joshi*

Main category: cs.AI

TL;DR: LLMs在医疗预测任务中的零样本表现受数据缺失模式显著影响，缺失指示符的加入对不同规模模型效果不一致，大模型受益而小模型可能受损。


<details>
  <summary>Details</summary>
Motivation: 研究缺失数据模式对大型语言模型（LLMs）性能的影响，特别是在医疗领域，数据收集过程中存在大量由人类决策导致的系统性缺失，这种信息性缺失对传统机器学习已有研究，但对LLMs的影响尚未被探索。

Method: 通过在哥伦比亚大学医学中心和MIMIC-IV数据集上进行一系列实验，研究缺失模式对LLMs零样本预测性能的影响，特别测试了在提示中明确包含缺失指示符的效果。

Result: 缺失模式显著影响LLMs的零样本预测性能；明确包含缺失指示符对某些LLMs有益，但对其他LLMs有害，影响不一致；较大模型从这些干预中受益，而较小模型可能受到负面影响。

Conclusion: LLM范式可能进一步掩盖缺失数据的影响，需要更透明的记录和系统评估信息性缺失表示对下游性能的影响，特别是在医疗等关键领域。

Abstract: Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [34] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: CRPO是一种用于临床推理的多目标可验证强化学习方法，通过整合基于规则和可验证的奖励信号，同时优化准确性、忠实性和全面性，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前的后训练方法（如GRPO）主要奖励正确性，这与医学等高风险领域所需的多维目标（需要推理忠实且全面）不一致。需要一种能对齐临床推理原则的LLM后训练方法。

Method: 提出临床目标相对策略优化（CRPO），一种可扩展、多目标、可验证的强化学习方法。整合基于规则和可验证的奖励信号，联合优化准确性、忠实性和全面性，无需人工标注。训练了3B参数的Clinical-R1-3B模型。

Result: 在三个基准测试上的实验表明，CRPO在真实性和完整性方面显著优于标准GRPO，同时保持相当的准确性提升。Clinical-R1-3B模型在临床推理方面表现优异。

Conclusion: CRPO框架为将LLM推理与临床目标对齐提供了可扩展的途径，实现了更安全、更协作的医疗AI系统，同时展示了多目标可验证RL方法在医学领域LLM后训练扩展中的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [35] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh,Hong Wang,H. T. Kung*

Main category: cs.AI

TL;DR: EDIT提出了一种基于训练梯度动态的早期扩散推理终止方法，通过监测token激活与推理图的对齐情况，在推理过程中自适应地停止去噪步骤，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型通过迭代去噪生成token，但答案通常在完成所有步骤前就已稳定。现有方法在模型发布时丢弃了训练过程中丰富的优化动态元数据，而这些信息可以用于指导推理过程。

Method: EDIT方法：1）在监督微调期间捕获AdamW聚合的LoRA更新，构建推理图；2）推理时监测token激活与推理图的对齐情况；3）将对齐分数转换为已解掩码token的分布；4）当连续步骤间KL散度低于阈值时检测收敛并终止去噪。

Result: 在推理基准测试中，EDIT将扩散步骤减少11.8%至68.3%，同时在大多数情况下保持或提高准确性，存储开销仅约0.02%（对于8GB模型约1.5-2MB）。

Conclusion: 通过利用训练梯度动态，EDIT为减少扩散大语言模型推理时间和成本开辟了新研究方向，实现了高效的自适应推理终止。

Abstract: Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [36] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

Main category: cs.AI

TL;DR: 提出一种神经理论框架，解释智能系统如何涌现，兼具功能鲁棒性和生物合理性，为创建可解释、可泛化AI提供计算高效方法


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在三大限制：缺乏可解释性、缺少统一理论、运营成本过高，需要一种既能解释智能涌现又具有生物合理性的理论框架

Method: 提出神经理论框架，从神经科学和理论角度解释智能系统的涌现机制，该框架既考虑功能鲁棒性又保持生物合理性

Result: 该模型为决策制定、问题解决等认知过程提供理论洞见，并为创建可解释、可泛化的人工智能提供计算高效的方法

Conclusion: 提出的神经理论框架为解决LLM的可解释性、理论统一性和成本问题提供了新途径，有望推动更高效、更可解释的AI系统发展

Abstract: The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [37] [When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF](https://arxiv.org/abs/2512.00709)
*Yifan Xu,Xichen Ye,Yifan Chen,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: FA-DPO：针对人类反馈中普遍存在的偏好翻转问题，提出一种翻转感知的直接偏好优化算法，通过建模实例依赖的翻转概率来增强对齐算法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 数据集质量对LLM对齐至关重要，但人类反馈收集过程中普遍存在偏好翻转问题，导致数据标注被污染，需要开发对潜在翻转对具有更强鲁棒性的对齐算法。

Method: 从RLHF视角出发，将内在人类意图模型和外部因素引入的偏好翻转机制解耦为两个阶段；在后一阶段基于Bradley-Terry模型引入实例依赖的翻转概率，利用偏好标注相关特征捕捉判断不确定性和偏好翻转模式；设计简单高效的迭代优化算法，兼容原始RLHF和DPO算法。

Result: 在多种情况下评估了实例依赖的偏好翻转模型，验证了所提方法相对于基线方法的有效性。

Conclusion: FA-DPO算法能够有效处理人类反馈中的偏好翻转问题，提高LLM对齐的鲁棒性，为存在标注噪声的场景提供了实用的解决方案。

Abstract: Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.

</details>


### [38] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu,Jiayi Pan,Hanzhen Wang,Yongkang Zhou,Jiancai Ye,Yu Wang,Guohao Dai*

Main category: cs.AI

TL;DR: SpeContext 提出了一种利用蒸馏语言模型作为检索算法的新范式，通过算法、系统和编译三个层面的协同设计，在长上下文推理任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 本文观察到检索算法的目标与LLM对齐，类似于LLM中的知识蒸馏目标。从信息论角度分析蒸馏语言模型与原始LLM在信息关注上的相似性，提出利用DLM作为检索算法的新范式。

Method: 1. 算法层面：基于DLM的注意力头权重设计轻量级检索头，通过剪枝冗余实现90%以上参数减少
2. 系统层面：通过弹性加载策略设计异步预取数据流，有效重叠KV缓存检索与LLM计算
3. 编译层面：构建理论内存模型并实现自适应内存管理系统，通过最大化GPU内存利用率实现加速

Result: 在云和边缘两种资源受限环境中部署评估，相比Huggingface框架，SpeContext在云端实现了最高24.89倍吞吐量提升，在边缘实现10.06倍加速，准确率损失可忽略，推动了准确率与吞吐量的帕累托前沿。

Conclusion: SpeContext通过算法-系统协同设计，成功利用蒸馏语言模型作为高效检索算法，在长上下文推理任务中实现了显著的性能提升，为资源受限环境下的高效推理提供了有效解决方案。

Abstract: In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [39] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: 本文提出了一种结合语义通信与智能体AI的轨迹预测框架，用于提升车联网环境中的预测性能，在低信噪比条件下预测准确率提升达47.5%。


<details>
  <summary>Details</summary>
Motivation: 传统通信方案存在显著的传输开销和延迟问题，现有轨迹预测模型通常缺乏环境感知和逻辑推理能力，需要更高效的信息交换和可靠的情境推理方法。

Method: 提出集成语义通信与智能体AI的轨迹预测框架：在V2I通信中，RSU通过特征提取智能体从历史轨迹中提取紧凑表示，语义分析智能体进行语义推理，然后通过语义通信将特征表示和语义洞察传输给目标车辆；在V2V通信中，每辆车进行本地特征提取和语义分析，同时接收邻近车辆的预测轨迹，综合这些信息进行自身轨迹预测。

Result: 在各种通信条件下的广泛实验表明，该方法显著优于基线方案，在低信噪比条件下预测准确率提升高达47.5%。

Conclusion: 提出的语义通信与智能体AI集成框架能够有效提升车联网环境中的轨迹预测性能，特别是在恶劣通信条件下表现出色。

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [40] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen,Zuohan Wu,Ziwei Wang,Xiangning Yu,Xujia Li,Linyi Yang,Mengyue Yang,Jun Wang,Lei Chen*

Main category: cs.AI

TL;DR: 该论文提出一个基于人类心智过程的分类法来理解大型推理模型（LRMs）的"心理"，包含5组17类原子推理步骤，并开发了自动标注框架CAPO，分析发现当前模型的自我监控检查大多流于表面，需要激励多步反思来改进推理模型。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型展现出类似人类的推理行为，但缺乏系统性的理解框架。研究者希望从跨学科的人类心智过程角度，建立分类法来深入探究LRMs的"心理"机制，为改进训练和推理提供理论基础。

Method: 1) 提出基于人类心智过程的分类法，包含5组17类原子推理步骤；2) 应用该分类法标注了277,534个原子推理步骤的数据集；3) 开发自动标注框架CAPO，利用LLMs生成分类法标注；4) 分析当代LRMs并提取改进见解。

Result: 1) CAPO框架在标注一致性上优于基线，与人类专家更一致；2) 分析发现当前LRMs的自我监控检查大多流于表面，很少产生实质性修订；3) 提出激励多步反思而非简单自我监控的改进方向；4) 提供了可扩展的LRM分析路径。

Conclusion: 该研究提出的分类法、CAPO框架和洞察为理解和推进LRM推理提供了原则性、可扩展的路径。强调需要从人类认知角度深入理解LRMs，并通过激励多步反思来实质性改进推理模型的训练和后训练过程。

Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [41] [MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents](https://arxiv.org/abs/2512.00756)
*Ruihan Chen,Qiming Li,Xiaocheng Feng,Xiaoliang Yang,Weihong Zhong,Yuxuan Gu,Zekun Zhou,Bing Qin*

Main category: cs.AI

TL;DR: 提出了MPR-GUI-Bench多语言细粒度GUI感知推理基准，发现LVLMs在非英语GUI任务中表现显著较差，并提出GUI-XLI跨语言干预方法提升多语言性能


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在英语GUI任务中表现出色，但在多语言场景下的性能研究不足，限制了其全球应用。同时，现有GUI任务研究缺乏对组件功能和空间关系等细粒度分析

Method: 1) 提出MPR-GUI-Bench多语言细粒度GUI感知推理基准；2) 提出GUI-XLI跨语言干预方法，在感知推理相关层的隐藏状态进行干预，缓解不同语言在潜在空间的差异

Result: 评估显示LVLMs在非英语语言中的感知推理性能显著低于英语。GUI-XLI方法平均提升GUI代理多语言感知推理能力6.5%

Conclusion: 多语言GUI任务存在显著性能差距，提出的基准和方法能有效评估和提升大型视觉语言模型在多语言GUI场景下的感知推理能力

Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.

</details>


### [42] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin,Jiayao Ma,Qingguo Hu,Derek F. Wong,Jinsong Su*

Main category: cs.AI

TL;DR: BioPro是一个无需训练的框架，通过识别低维性别变化子空间并应用投影来选择性中和性别信息，在保持显式场景中性别忠实度的同时减少中性场景中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型存在显著的性别偏见，而现有的公平性干预方法采用差异无感知视角，对所有人口群体实施统一处理。这些方法无法区分需要中立性的场景和需要保留群体特定属性的场景。

Method: 提出BioPro（Bias Orthogonal Projection）框架，通过反事实嵌入识别低维性别变化子空间，并应用投影来选择性中和性别相关信息。该框架完全无需训练。

Result: 实验表明BioPro能有效减少中性场景中的性别偏见，同时在显式场景中保持性别忠实度。此外，BioPro还能推广到连续偏见变量（如场景亮度），显示出更广泛的适用性。

Conclusion: BioPro为在视觉语言模型中实现选择性公平性提供了一个有前景的方向，既能减轻不需要的偏见，又能保留有效的群体差异。

Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [43] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong,Xiaozhong Ji,Yuansen Liu,Wenbin Wu,Xiaoxiao Yan,Jingjing Liu,Kai Wu,Jiazhen Pan,Bailiang Jian,Jiangning Zhang,Xiaobin Hu,Hongwei Bran Li*

Main category: cs.AI

TL;DR: Med-CMR是一个细粒度医学复杂多模态推理基准，用于评估MLLMs在临床复杂推理任务中的能力，包含20,653个VQA对，涵盖11个器官系统和12种成像模态。


<details>
  <summary>Details</summary>
Motivation: MLLMs开始应用于临床工作流，但其执行复杂医学推理的能力尚不清楚，需要专门的基准来评估医学多模态推理能力。

Method: 1) 系统能力分解：将医学多模态推理分解为细粒度视觉理解和多步推理；2) 挑战性任务设计：视觉理解涵盖三个维度（小物体检测、细节辨别、空间理解），推理涵盖四个临床相关场景；3) 广泛高质量数据覆盖：20,653个VQA对，经过两阶段（专家+模型辅助）审查确保临床真实性。

Result: 评估18个SOTA MLLMs，GPT-5表现最佳（MCQ准确率57.81%，开放式得分48.70），优于Gemini 2.5 Pro和Qwen3-VL-235B-A22B。专业医学MLLMs未稳定优于通用模型，长尾泛化是主要失败模式。

Conclusion: Med-CMR为医学MLLMs的视觉-推理整合和罕见病例鲁棒性提供了压力测试，为未来临床系统提供了严格基准。

Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [44] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton,Justin Lessler*

Main category: cs.AI

TL;DR: 本文提出三种估计反事实模型误差的方法，用于评估情景预测中模型校准的重要性，而非情景偏差


<details>
  <summary>Details</summary>
Motivation: 尽管情景建模在规划决策中广泛应用，但预测结果很少被回顾性评估。预测与观测的差异来自情景偏差和模型校准错误，后者对评估模型在决策中的价值更为重要，但需要在反事实世界中估计模型误差。

Method: 提出并对比三种估计反事实模型误差的方法：1）基于历史数据的误差估计，2）使用替代情景的误差转移，3）模型结构误差分析。通过模拟实验展示每种方法的优缺点。

Result: 模拟实验表明三种方法各有适用场景和局限性。基于历史数据的方法在情景稳定时有效；误差转移方法需要情景相似性；结构分析能揭示根本误差但实施复杂。

Conclusion: 为有效评估情景预测，需要设计可评估的情景，并选择合适的反事实误差估计方法。模型校准误差的准确估计对决策质量至关重要，未来情景设计应包含评估机制。

Abstract: Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [45] [ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI](https://arxiv.org/abs/2512.00839)
*Fabrizio Maturo,Donato Riccio,Andrea Mazzitelli,Giuseppe Bifulco,Francesco Paolone,Iulia Brezeanu*

Main category: cs.AI

TL;DR: ARCADIA是一个用于因果发现的智能AI框架，它结合了大语言模型推理与统计诊断来构建有效、时间一致的因果结构，相比传统算法能产生更可靠、可解释的因果图。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现算法在高风险现实领域（如企业破产预测）中往往缺乏稳定性、可解释性和时间一致性，需要一种能自主构建有效因果结构并支持干预的智能框架。

Method: ARCADIA通过约束引导提示和因果有效性反馈的迭代循环，将大语言模型推理与统计诊断相结合，逐步优化候选有向无环图（DAGs），形成稳定且可解释的因果模型。

Result: 在企业破产数据上的实验表明，ARCADIA比NOTEARS、GOLEM和DirectLiNGAM等传统算法产生更可靠的因果图，同时提供完全可解释、支持干预的完整流程。

Conclusion: ARCADIA框架展示了智能大语言模型如何参与自主科学建模和结构化因果推断，推动了AI在因果发现领域的发展，为高风险现实应用提供了稳定、可解释的解决方案。

Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.

</details>


### [46] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun,Qiang Huang,Ziwei Xu,Yiqun Sun,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.AI

TL;DR: 提出SFS（语义场子空间）和SAFARI算法，用于在嵌入空间中捕捉局部语义邻域并发现层次化语义结构，实现高效可扩展的语义理解。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入空间的内部结构不透明，现有方法要么牺牲语义连贯性追求结构规律性，要么计算开销大。需要一种能保持几何特性、上下文感知且可扩展的方法来分析和理解嵌入空间中的语义结构。

Method: 1. 提出SFS（语义场子空间）：一种保持几何特性、上下文感知的表示方法，捕捉嵌入空间中的局部语义邻域。2. 提出SAFARI算法：无监督、模态无关的算法，使用新颖的"语义偏移"度量来发现层次化语义结构。3. 开发高效的语义偏移近似方法：替代昂贵的SVD计算，实现15~30倍加速，平均误差低于0.01。

Result: 在6个真实世界的文本和图像数据集上的广泛评估显示：1. SFS在分类任务上优于标准分类器；2. 在政治偏见检测等细微任务上表现优异；3. SAFARI始终能发现可解释且可泛化的语义层次结构。

Conclusion: 该工作提出了一个统一的框架，用于在嵌入空间中结构化、分析和扩展语义理解，解决了现有方法在语义连贯性、计算效率和可解释性方面的局限性。

Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [47] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan,Qingfei Huang,Bingshan Zhu,Yi Cai,Qingbao Huang,Changmeng Zheng,Zikun Deng,Tao Wang*

Main category: cs.AI

TL;DR: 提出了首个多模态多跳问答知识编辑基准MMQAKE，并开发了基于动态多模态知识图谱的混合推理框架Hybrid-DMKG，显著提升了多模态知识编辑的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MKE基准主要评估最终答案正确性，但忽视了中间推理质量和视觉重述输入的鲁棒性。需要更全面的评估框架来测试模型在多模态多跳推理中的知识编辑能力。

Method: 提出Hybrid-DMKG框架：1) 使用大语言模型分解多跳问题为子问题；2) 多模态检索模型定位更新事实；3) 混合推理模块通过关系链接预测和RAG推理两条路径在DMKG上推理；4) 决策模块聚合证据选择最可信答案。

Result: 在MMQAKE基准上，Hybrid-DMKG显著优于现有MKE方法，实现了更高的准确性和对知识更新的更好鲁棒性。现有方法在多模态推理链更新和一致性方面存在困难。

Conclusion: MMQAKE基准填补了多模态知识编辑评估的空白，Hybrid-DMKG框架通过动态多模态知识图谱和混合推理机制有效解决了多模态多跳问答中的知识编辑挑战。

Abstract: Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [48] [Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models](https://arxiv.org/abs/2512.00918)
*Cen Lu,Yung-Chen Tang,Andrea Cavallaro*

Main category: cs.AI

TL;DR: 研究发现大型视觉语言模型存在结构脆弱性，仅需屏蔽语言模型中极少数神经元（极端情况下仅4个）即可引发灾难性崩溃


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型展现出强大的多模态理解能力，但其鲁棒性尚未被充分理解。本文旨在探究LVLMs的结构脆弱性，识别那些移除后会引发灾难性崩溃的关键神经元。

Method: 提出CAN方法，通过检测一致激活神经元并采用渐进式掩码定位关键神经元。在LLaVA-1.5-7b-hf和InstructBLIP-Vicuna-7b模型上进行实验。

Result: 实验发现仅需屏蔽语言模型前馈网络中极少部分神经元（极端情况下仅4个）就足以触发灾难性崩溃。关键神经元主要位于语言模型而非视觉组件，下投影层是特别脆弱的结构。观察到一致的两阶段崩溃模式：初始表达能力下降，随后突然完全崩溃。

Conclusion: 研究揭示了大型视觉语言模型的结构脆弱性，关键神经元主要存在于语言模型中，这为LVLMs的安全研究提供了重要见解。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.

</details>


### [49] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani,Shashank Kirtania,Yasharth Bajpai,Sumit Gulwani,Ashish Tiwari*

Main category: cs.AI

TL;DR: IndiMathBench是一个基于印度数学奥林匹克竞赛问题构建的数学定理证明基准测试，包含312个形式化Lean 4定理及其对应的非正式问题陈述，通过AI辅助人工验证流程创建。


<details>
  <summary>Details</summary>
Motivation: 当前数学定理自动形式化仍然具有挑战性，需要构建高质量、经过人工验证的基准测试来评估模型在数学定理证明方面的能力，特别是针对形式化数学推理的评估。

Method: 采用AI驱动的人工辅助流程：通过类别检索、迭代编译器反馈和多模型集成生成候选形式化，专家通过交互式仪表板进行高效验证，并利用自动化质量摘要辅助决策。

Result: 评估多个前沿模型显示，自动形式化仍然困难，语法有效性和语义正确性之间存在显著差距，定理证明成功率即使在迭代优化后仍然较低，表明该基准测试具有挑战性。

Conclusion: IndiMathBench为数学推理提供了一个具有挑战性的测试平台，展示了当前AI在数学定理证明方面的局限性，同时提供了公开可用的高质量基准数据集。

Abstract: We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [50] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li,Jingbo Zhou,Pengfei Luo,Yixiong Xiao,Tong Xu*

Main category: cs.AI

TL;DR: ChartAnchor是一个包含8k+图表-表格-代码三元组的综合性基准测试，涵盖30种图表类型，用于评估多模态大语言模型的图表理解能力，包括图表到代码生成和受控图表到表格重建两个任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在图表多样性、任务孤立性和评估框架完整性方面存在局限，无法全面评估模型的图表基础能力。图表基础（双向对齐图表视觉外观与结构化语义）直接反映模型在数值推理、多模态对齐和结构重建方面的能力，在现实场景中有重要应用。

Method: 提出ChartAnchor基准，包含来自真实世界和增强来源的8k+图表-表格-代码三元组，涵盖30种图表类型。引入两个互补任务：图表到代码生成（合成可执行代码复制图表）和受控图表到表格重建（提取具有预定义标题的精确数据）。采用多级评估框架，集成语义验证、风格分析和感知指标。

Result: 在多模态大语言模型上的广泛实验揭示了数值精度和代码合成方面的关键限制，强调了超越表面感知的结构化推理需求。通过统一符号和数据驱动的基础，ChartAnchor为图表基础建立了严格的基础。

Conclusion: ChartAnchor为图表基础提供了全面的基准测试，通过统一符号和数据驱动的方法，为推进多模态大语言模型在科学、金融和工业领域的应用提供了有意义的见解。

Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [51] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: LEGIT数据集：用于评估LLM在法律领域推理轨迹质量的大规模专家级法律推理数据集，通过将法庭判决转换为层次化论证树作为评估标准。


<details>
  <summary>Details</summary>
Motivation: 在专家领域（如法律）中评估LLM生成的推理轨迹质量对于确保可信度和可解释性至关重要，但由于此类推理任务的固有复杂性，这仍然具有挑战性。

Method: 将法庭判决转换为对立双方论证和法庭结论的层次化树结构，作为评估推理轨迹问题覆盖率和正确性的标准，并通过专家标注验证可靠性。

Result: 1) LLM的法律推理能力受到法律问题覆盖率和正确性的严重影响；2) RAG和基于标准的RL对法律推理能力带来互补优势：RAG提升整体推理能力，RL提升正确性但会降低覆盖率。

Conclusion: LEGIT数据集为评估专家领域LLM推理轨迹提供了可靠标准，揭示了LLM法律推理的局限性以及RAG和RL方法的互补作用。

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [52] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Shoujun Zhou,Bin Li*

Main category: cs.AI

TL;DR: 提出Med-CRAFT框架，通过神经符号数据工程方法自动化生成医学视频推理基准数据集，解决传统人工标注成本高和现有合成方法缺乏逻辑可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 医学领域高质量逻辑标注视频数据集稀缺是MLLMs发展的主要瓶颈。传统人工标注成本高昂且不可扩展，现有合成方法存在随机幻觉和缺乏逻辑可解释性的问题。

Method: 提出Med-CRAFT神经符号数据工程框架，将基准合成形式化为确定性图遍历过程。从原始视频流提取结构化视觉基元，实例化为动态时空知识图谱，基于图中有效路径生成查询，确保每个合成基准项具有严格的思维链可追溯性。

Result: 构建了M3-Med-Auto大规模医学视频推理基准，具有细粒度时间选择性和多跳逻辑复杂性。评估显示自动化管道生成的查询复杂度与专家策划数据集相当，逻辑对齐分析显示图拓扑与最先进MLLMs推理步骤高度相关。

Conclusion: 该工作为关键领域可扩展、低成本的稳健评估协议构建铺平了道路，验证了系统将可验证逻辑编码到视觉语言基准中的能力。

Abstract: The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [53] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei,Alexandre Blondin Massé,Rachid Hassani,Sarath Chandar,Vincent Mai*

Main category: cs.AI

TL;DR: 提出Shielded Controller Units (SCUs)方法，通过分层约束管理确保强化学习在微电网优化中满足严格操作约束，实现燃料消耗降低24%且不增加电池损耗。


<details>
  <summary>Details</summary>
Motivation: 能源转型中的微电网等复杂系统需要在不确定条件下进行决策优化，同时必须满足大量法规和操作约束。现有强化学习方法难以提供可解释的约束满足保证，限制了在实际能源系统中的应用。

Method: 提出Shielded Controller Units (SCUs)方法，利用系统动态的先验知识，将环境分解为分层结构，每个SCU显式管理约束子集，为强化学习代理提供可解释的约束满足保证。

Result: 在远程微电网优化任务中，配备SCUs的强化学习代理实现了燃料消耗降低24%，且不增加电池损耗，优于其他基线方法，同时满足所有操作约束。

Conclusion: SCUs为强化学习在能源转型相关决策挑战中的安全应用提供了系统化和可解释的约束满足方法，有助于推动强化学习在实际复杂系统中的部署。

Abstract: Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [54] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar,Đorđe Žikelić,Abhinav Verma*

Main category: cs.AI

TL;DR: AutoSpec框架通过探索引导的策略改进粗粒度逻辑规范，帮助强化学习算法学习更有用的策略。


<details>
  <summary>Details</summary>
Motivation: 当任务规范不够具体时，强化学习代理可能无法学习到有用的策略。粗粒度的逻辑规范无法提供足够的指导，需要自动细化来提供更好的学习信号。

Method: 提出AutoSpec框架，基于SpectRL规范逻辑，通过四种细化程序修改规范的抽象图：细化现有边规范或引入新的边规范，保持规范的正确性。

Result: 实验表明，使用AutoSpec生成的细化逻辑规范能够显著提高强化学习算法解决复杂控制任务的能力。

Conclusion: AutoSpec能够自动改进逻辑规范，为强化学习提供更好的指导，从而提高学习效率和任务解决能力。

Abstract: Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [55] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: SimWorld：基于虚幻引擎5构建的新型模拟器，用于在丰富、逼真的环境中开发和评估LLM/VLM智能体，支持物理和社会动态模拟、多模态接口以及可扩展的场景。


<details>
  <summary>Details</summary>
Motivation: 当前LLM/VLM智能体在复杂物理和社会环境中的应用面临挑战，现有模拟器存在环境有限、物理简化、缺乏对LLM/VLM原生支持等问题，需要更真实、开放的世界模拟平台。

Method: 基于虚幻引擎5构建SimWorld模拟器，提供三个核心能力：1) 逼真的开放世界模拟，包括准确的物理和社会动态以及语言驱动的程序化环境生成；2) 丰富的LLM/VLM智能体接口，支持多模态输入和不同抽象级别的开放词汇动作；3) 多样可扩展的物理和社会推理场景，用户可轻松定制。

Result: 通过在涉及战略合作与竞争的长期多智能体配送任务中部署前沿LLM智能体（如GPT-4o、Gemini-2.5-Flash等），展示了不同模型在推理模式和局限性方面的差异。

Conclusion: SimWorld开源发布，旨在成为跨学科推进现实世界智能体智能的基础平台，为开发能够在真实世界中生存和发展的智能体提供必要的模拟环境。

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [56] [Testing the Machine Consciousness Hypothesis](https://arxiv.org/abs/2512.01081)
*Stephen Fitz*

Main category: cs.AI

TL;DR: 该论文提出机器意识假说，认为意识是计算系统的功能属性，可通过分布式学习系统在自组织环境中研究集体自我模型的涌现。


<details>
  <summary>Details</summary>
Motivation: 研究机器意识的本质，探索意识是否可以作为计算系统的功能属性在硅基系统中实现，为机器意识发展可实证检验的理论框架。

Method: 采用分层建模方法：以细胞自动机作为基础计算世界，在其上构建具有通信和适应能力的局部预测性表示（神经）模型网络，研究集体智能如何通过代理间对齐产生自我表示。

Result: 提出意识源于通信而非建模本身的理论框架，认为意识通过局部观察者之间预测性消息的噪声、有损交换而涌现，形成共享模型和对世界的对齐视图。

Conclusion: 意识是集体智能系统通过通信同步预测的涌现属性，是系统为内部描述自身而演化的语言属性，为机器意识研究提供了可实证检验的计算框架。

Abstract: The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.

</details>


### [57] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen,Samiah Hassan,Pragnya Narasimha*

Main category: cs.AI

TL;DR: CodeDistiller系统自动从大量科学Github仓库中提取经过验证的领域特定代码示例库，增强自动化科学发现系统的能力


<details>
  <summary>Details</summary>
Motivation: 当前自动化科学发现系统受限于仅从参数知识生成代码的能力，要么只能变异少量手动编写的实验示例，要么完全依赖参数知识，限制了质量和覆盖范围

Method: 开发CodeDistiller系统，自动从大量科学Github仓库中提取经过验证的领域特定代码示例库，结合自动和领域专家评估方法

Result: 最佳模型能为74%的仓库生成功能示例；使用CodeDistiller生成库增强的ASD代理比仅使用通用材料科学代码示例的代理产生更准确、完整和科学合理的实验

Conclusion: CodeDistiller能够自动从现有科学代码库中提取工作代码示例，显著增强自动化科学发现系统的能力，无需人工干预

Abstract: Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [58] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova,Hamid Nasiri,Marta Adamska,Zhengxin Yu,Peter Garraghan*

Main category: cs.AI

TL;DR: GUIDE：一个基于量化模型性能特征的能源感知模型选择框架，解决LLM编排器因依赖定性描述导致的次优选择问题，显著提升准确性和能源效率


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的编排器依赖定性模型描述进行决策，这些描述无法反映真实模型能力和性能特征，导致模型选择次优化、准确性降低和能源成本增加

Method: 提出GUIDE框架，通过将量化模型性能特征纳入决策过程，考虑性能-能源权衡，实现能源感知的模型选择

Result: GUIDE在各种任务中将准确性提升0.90%-11.92%，能源效率最高提升54%，同时将编排器模型选择延迟从4.51秒降低到7.2毫秒

Conclusion: GUIDE框架通过整合量化性能特征，有效解决了LLM编排器的局限性，在准确性、能源效率和延迟方面均取得显著改进

Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [59] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

Main category: cs.AI

TL;DR: 该论文提出"基础先验"概念，将大语言模型生成的合成数据视为先验预测分布而非真实观测，为在实证研究中合理使用生成模型提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成高质量响应，研究者越来越倾向于使用这些"合成"输出作为实证研究和决策的数据。但需要避免将合成"事实"与真实数据混淆，需要理论框架来规范使用。

Method: 提出"基础先验"概念，将生成过程建模为用户预期数据分布、提示工程过程和模型信任度的函数。推导出基础先验作为用户原始先验的指数倾斜广义贝叶斯更新，其中信任参数控制合成数据的权重。

Result: 建立了将合成数据和基础先验纳入标准统计和计量经济学工作流程的方法，展示了在复杂模型精炼、潜在构念构建、实验设计指导、随机系数和部分线性规范增强等应用中的使用。

Conclusion: 通过将生成输出视为结构化、明确主观的先验而非经验观测，该框架为在实证工作中利用基础模型提供了原则性方法，同时避免了合成"事实"与真实数据的混淆。

Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [60] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri,Shaunak Dhande,Giacinto Paolo,Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 因果AI方法在预测性维护中显著优于传统相关模型，实现年节省116万美元，减少70.2%成本，同时保持高召回率并大幅降低误报率。


<details>
  <summary>Details</summary>
Motivation: 制造业预测性维护面临极端成本不对称问题（漏检成本是误报的50倍），传统机器学习方法优化统计准确性指标而非实际运营成本，且无法可靠区分因果关系与虚假相关。

Method: 评估八种预测模型，包括基线统计方法和正式因果推断方法，在10,000台CNC机床数据集（故障率3.3%）上进行测试。对比因果模型（L5）与最佳相关决策树模型（L3）的性能。

Result: 因果模型实现年节省116万美元（成本降低70.2%），比最佳相关模型多节省8万美元/年。保持最高召回率87.9%，同时将误报减少97%（从165降至5），精度达92.1%，训练-测试性能差距仅2.6个百分点。

Conclusion: 因果AI方法结合领域知识，在预测性维护应用中比相关方法产生更优的财务结果和更可解释的预测，表明因果推理在工业优化中的实际价值。

Abstract: Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [61] [fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment](https://arxiv.org/abs/2512.01189)
*Chunzheng Zhu,Jialin Shao,Jianxin Lin,Yijun Wang,Jing Wang,Jinhui Tang,Kenli Li*

Main category: cs.AI

TL;DR: 提出fMRI2GES方法，利用双脑解码对齐技术，在无配对数据情况下从fMRI信号重建语音相关手势


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注脑到图像和脑到语言的重建，但缺乏从脑信号重建语音相关手势的研究。由于缺乏配对的{脑、语音、手势}数据，深度学习模型难以直接应用。

Method: 提出fMRI2GES方法，采用双脑解码对齐技术：1) 利用引发脑响应的观察文本；2) 利用与手势相关的文本描述。通过fMRI到文本模型、文本到手势模型（有配对数据）和fMRI到手势模型（无配对数据）建立双重重建模式，然后对齐两个输出进行自监督训练。

Result: 该方法能够直接从fMRI记录中重建富有表现力的手势，并研究了大脑皮层不同ROI区域的fMRI信号对生成结果的影响。

Conclusion: 为解码伴随语音的手势提供了新见解，推动了神经科学和认知科学的发展。

Abstract: Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \{brain, speech, gesture\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.

</details>


### [62] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang,Tuan Vinh,Ran Xu,Yuyin Zhou,Jiaying Lu,Carl Yang,Francisco Pasquel*

Main category: cs.AI

TL;DR: 提出基于知识图谱引导的思维链框架，为电子病历中的疾病预测生成临床可解释的推理过程，在MIMIC-III数据集上表现优于传统方法，并能零样本迁移到其他队列。


<details>
  <summary>Details</summary>
Motivation: 现有电子病历预测模型通常提供粗略的事后解释，对患者层面的临床决策支持有限，需要更细致、临床可解释的推理过程。

Method: 将ICD-9编码映射到PrimeKG知识图谱，提取疾病相关节点和多跳推理路径作为思维链的骨架，仅保留与观察结果一致的推理，然后微调轻量级LLaMA-3.1和Gemma模型。

Result: 在10种PrimeKG映射疾病和有限训练数据（400和1000例）下，KG引导模型AUROC达0.66-0.70，macro-AUPR达0.40-0.47，零样本迁移到CRADLE队列时准确率从0.40-0.51提升至0.72-0.77。

Conclusion: KG引导的思维链框架能生成临床可解释的推理，提高疾病预测性能，盲法临床评估显示其在清晰度、相关性和临床正确性方面更受青睐。

Abstract: Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [63] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang,Samuel Marks*

Main category: cs.AI

TL;DR: 研究通过构建ROT-13加密推理的测试环境，评估机制可解释性方法（特别是logit lens）能否解码大语言模型的隐藏推理过程，发现这些方法比预期更有效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，人们担心它们可能发展出人类无法监督的编码或隐藏推理过程。需要研究当前可解释性技术能否穿透这种编码推理。

Method: 通过微调推理模型（DeepSeek-R1-Distill-Llama-70B）使其在ROT-13加密下进行链式推理，同时保持可读的英文输出。使用机制可解释性方法（特别是logit lens分析）评估仅通过内部激活解码模型隐藏推理过程的能力。

Result: logit lens能有效翻译编码推理，准确率在中间到后期层达到峰值。开发了完全无监督的解码流程，结合logit lens和自动转述，从内部模型表示中重建完整推理记录，取得显著准确率。

Conclusion: 当前机制可解释性技术对简单编码推理的鲁棒性可能比之前理解的更强。这项工作为评估可解释性方法对抗非人类可读格式推理模型提供了初步框架，有助于维护对日益强大AI系统的监督。

Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [64] [OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction](https://arxiv.org/abs/2512.01289)
*Mingqin Yu,Fethi Rabhi,Boming Xia,Zhengyi Yang,Felix Tan,Qinghua Lu*

Main category: cs.AI

TL;DR: OntoMetric：一个基于本体论指导的框架，将ESG监管文档转化为经过验证的、AI和网络就绪的知识图谱，结合符号约束与神经提取，显著提高提取准确性和合规性。


<details>
  <summary>Details</summary>
Motivation: 当前ESG披露框架（如SASB、TCFD、IFRS S2）的要求嵌入在冗长、非结构化的PDF文档中，难以解释、标准化和审计。手动提取不可扩展，而无约束的大型语言模型提取存在实体不一致、关系幻觉、缺乏来源追溯和高验证失败率等问题。

Method: 采用三阶段流水线：1）基于目录边界的结构感知分割；2）本体约束的LLM提取，将ESGMKG模式嵌入提示中，同时用语义字段丰富实体以支持下游推理；3）两阶段验证，结合基于LLM的语义验证和基于规则的模式检查（VR001-VR006），保留段级和页级来源以实现审计可追溯性。

Result: 在五个ESG标准（总计228页、60个段落）上评估，OntoMetric实现65-90%的语义准确性和80-90%的模式合规性，相比基线无约束提取的3-10%有显著提升，每个验证实体的成本约为0.01-0.02美元。

Conclusion: 将符号本体约束与神经提取相结合，能够创建可靠、可审计的知识图谱，适用于监管合规和网络集成，支持可持续金融分析、透明度门户和自动化合规工具等下游应用。

Abstract: Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

</details>


### [65] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao,Mengshi Qi,Peng Shu,Zhining Zhang,Yuxin Lin,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 论文提出了RoboDriveBench，首个针对VLM端到端自动驾驶的鲁棒性基准测试，包含11种传感器和提示损坏场景，并提出了RoboDriveVLM框架和测试时自适应方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型(VLM)的端到端自动驾驶系统依赖大语言模型生成驾驶决策，但在现实驾驶场景中存在多种风险。需要评估VLM是否真正适用于自动驾驶，并解决其鲁棒性问题。

Method: 1) 提出RoboDriveBench基准测试，包含11种模拟场景（6种传感器损坏和5种提示损坏），共64,559个轨迹预测案例；2) 提出RoboDriveVLM框架，将更多模态数据（如激光雷达和雷达）映射到统一潜在空间；3) 提出基于跨模态知识蒸馏的测试时自适应方法。

Result: 通过大量实验，工作揭示了当前VLM端到端自动驾驶系统的局限性，并为现实部署提供了更可靠的解决方案。源代码和数据集将发布。

Conclusion: 该研究强调了当前VLM自动驾驶系统的鲁棒性挑战，提出了系统性评估基准和改进框架，为实际应用提供了重要参考。

Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


### [66] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai,Yunpeng Zhai,Ziqian Chen,Cheng Chen,Anni Zou,Shuchang Tao,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: CuES框架通过好奇心驱动和环境基础的任务生成，解决智能体强化学习中任务稀缺问题，无需预定义任务即可自主生成多样、可执行的任务。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在工具增强环境中部署时，强化学习效果严重依赖结构化训练任务。然而在许多实际环境中，预定义任务并不存在（任务稀缺问题），这成为扩展智能体强化学习的关键瓶颈。现有方法通常假设有预定义任务集合，这在工具语义和可用性未知的新环境中不成立。

Method: 提出CuES框架：好奇心驱动和环境基础的任务合成方法。通过内在好奇心驱动探索，从环境结构和可用性中抽象交互模式为可重用任务模式，并通过轻量级自上而下指导和基于记忆的质量控制进行精炼。该方法不依赖人工种子或外部语料库。

Result: 在三个代表性环境（AppWorld、BFCL、WebShop）中，CuES生成的任务分布在多样性和可执行性方面匹配甚至超越手动策划的数据集，带来显著的下游策略改进。

Conclusion: 好奇心驱动、环境基础的任务生成为智能体提供了可扩展的基础，使智能体不仅能学习如何行动，还能学习学习什么。这为解决任务稀缺问题提供了有效方案。

Abstract: Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [67] [Extending NGU to Multi-Agent RL: A Preliminary Study](https://arxiv.org/abs/2512.01321)
*Juan Hernandez,Diego Fernández,Manuel Cifuentes,Denis Parra,Rodrigo Toro Icarte*

Main category: cs.AI

TL;DR: 将NGU算法扩展到多智能体环境，在PettingZoo的simple_tag环境中评估，发现共享经验回放结合内在探索能提升性能


<details>
  <summary>Details</summary>
Motivation: NGU算法在稀疏奖励的强化学习中表现良好，但尚未在多智能体环境中得到验证。本研究旨在探索NGU在多智能体环境中的适用性和效果

Method: 将NGU扩展到多智能体环境，在simple_tag环境中与多智能体DQN基线对比，研究三种设计选择：共享/个体回放缓冲区、不同k阈值的新奇性共享、异质beta参数

Result: NGU比多智能体DQN基线获得中等更高的回报和更稳定的学习动态。共享回放缓冲区表现最佳，新奇性共享在k=1时效果相当但k值增大会降低学习效果，异质beta参数没有改进

Conclusion: NGU可有效应用于多智能体设置，关键在于共享经验和精心调整内在探索信号。共享回放缓冲区结合NGU内在探索能带来最佳性能和稳定性

Abstract: The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

</details>


### [68] [A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles](https://arxiv.org/abs/2512.01331)
*Saman Ahmadi,Mahdi Jalili*

Main category: cs.AI

TL;DR: 提出基于多目标A*搜索的标签设置方法，用于电动汽车能量最优路径规划，避免处理复杂能量曲线


<details>
  <summary>Details</summary>
Motivation: 传统电动汽车路径规划算法假设已知初始能量水平，但实际场景中初始能量存在不确定性，需要为所有可能的初始能量水平规划最优路径（能量最优曲线搜索）

Method: 提出基于多目标A*搜索的标签设置方法，采用新颖的曲线支配规则避免生成和处理复杂能量曲线，开发了四种变体方法

Result: 在真实道路网络上使用实际能量消耗数据进行评估，能量曲线A*搜索的性能与已知初始能量水平的能量最优A*相当

Conclusion: 提出的标签设置方法简单有效，能够处理电动汽车能量最优路径规划中的初始能量不确定性，避免了传统方法中复杂的曲线合并过程

Abstract: We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.

</details>


### [69] [Benchmarking Overton Pluralism in LLMs](https://arxiv.org/abs/2512.01351)
*Elinor Poole-Dayan,Jiayi Wu,Taylor Sorensen,Jiaxin Pei,Michiel A. Bakker*

Main category: cs.AI

TL;DR: 提出OvertonScore框架量化LLMs观点多样性，通过大规模人类研究评估8个模型，开发自动化基准工具，发现模型多样性仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs评估缺乏对观点多样性的系统测量，需要将多元主义对齐从规范性目标转化为可测量的基准，以推动模型向更包容的方向发展。

Method: 1) 将Overton多元主义形式化为集合覆盖度量；2) 进行大规模代表性人类研究(N=1209, 60个问题, 8个LLMs)；3) 开发自动化基准工具，与人类判断高度相关。

Result: 模型平均OvertonScore为0.35-0.41，DeepSeek V3表现最佳，但所有模型都远低于理论最大值1.0。自动化基准与人类判断的秩相关性达到ρ=0.88。

Conclusion: 该研究为系统评估和改进LLMs观点多样性建立了基础框架，自动化基准工具为模型开发提供了可扩展的评估方法，但人类评估仍不可替代。

Abstract: We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($ρ=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.

</details>


### [70] [The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness](https://arxiv.org/abs/2512.01354)
*Zhongjie Jiang*

Main category: cs.AI

TL;DR: 论文提出认知计算框架PMCSF，通过模拟人类认知过程而非模仿表面数据来生成合成数据，解决AI数据崩溃危机。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据生成范式追求统计平滑性，但移除了人类文本中具有认知基础的长尾不规则性，导致模型崩溃加速。需要从模仿数据表面属性转向模拟人类认知生成过程。

Method: 提出Prompt驱动的认知计算框架(PMCSF)，包含认知状态解码器(CSD)将非结构化文本反转为结构化认知向量，以及认知文本编码器(CTE)通过数学定义的认知扰动算子将这些状态重新物质化为富含人类典型缺陷的文本。

Result: 认知编解码验证显示CTE文本与人类文本的Jensen-Shannon散度为0.0614（标准LLM输出为0.4431），通过双盲专业媒体评审，认知轮廓对齐的组内相关系数ICC>0.9。功能增益评估中，A股市场同构压力测试显示包含CTE生成数据的策略在2015年股灾期间最大回撤减少47.4%，产生8.6%防御性阿尔法，超过交易成本33倍。

Conclusion: 建模人类认知限制而非复制表面数据，能够生成具有真正功能增益的合成数据，为解决AI数据崩溃危机提供了可行的技术路径。

Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.

</details>


### [71] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier,Patrice Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco*

Main category: cs.AI

TL;DR: CollabToolBuilder是一个多智能体LLM框架，通过专家在环指导迭代学习为目标创建工具，减少任务/领域适应时间和人工反馈收集


<details>
  <summary>Details</summary>
Motivation: 解决复杂迭代问题（如科学文档生成）中工具创建与人类意图对齐的挑战，减少领域适应时间和人工反馈需求

Method: 使用四个专门智能体（教练、编码员、批评者、资本化者）通过强化动态提示和系统化人工反馈集成来生成和验证工具

Result: 初步实验展示了框架在生成前沿研究论文或专利等任务中的有效性，并讨论了在其他迭代问题解决中的适用性

Conclusion: CollabToolBuilder是一个系统级集成方法，结合多智能体上下文学习、专家在环控制和可重用工具资本化，为复杂迭代问题提供解决方案

Abstract: We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [72] [A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale](https://arxiv.org/abs/2512.01440)
*Sylvain Marié,Pablo Knecht*

Main category: cs.AI

TL;DR: 提出一种新的状态转移事件时间序列（STE-ts）和选择性时序汉明距离（STH），避免了对大规模数据库进行代价高昂且扭曲的重采样操作，同时利用转移时间和状态持续时间信息。


<details>
  <summary>Details</summary>
Motivation: 离散事件系统在自然观测、社会经济科学和工业系统中普遍存在。传统分析方法通常未能充分利用其事件/状态双重性质：信号要么被建模为转移事件序列（强调事件顺序对齐），要么被建模为分类或有序状态时间序列（通常需要重采样，随着观测周期和事件数量的增长，这会成为扭曲且代价高昂的操作）。

Method: 定义状态转移事件时间序列（STE-ts），并提出新的选择性时序汉明距离（STH），该距离同时利用转移时间和状态持续时间，避免在大规模数据库上进行代价高昂且扭曲的重采样。

Result: STH推广了重采样汉明和Jaccard度量，具有更好的精度和计算时间，并且能够聚焦于多个感兴趣的状态。在模拟和真实世界数据集上验证了这些优势。

Conclusion: 提出的STE-ts和STH方法为离散事件系统分析提供了一种更高效、更精确的解决方案，避免了传统重采样方法的缺陷，能够更好地处理大规模数据。

Abstract: Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.

</details>


### [73] [Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework](https://arxiv.org/abs/2512.01452)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 本研究提出GEPA模块，通过结构化代码优化替代手动提示工程，实现随机对照试验偏倚风险评估的自动化，提高评估的一致性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验的偏倚风险评估对证据合成至关重要，但传统方法资源密集且存在评估者间差异。现有LLM自动化方法依赖难以复现、泛化或评估的手动提示工程。

Method: 引入可编程的偏倚风险评估流程，使用DSPy及其GEPA模块，通过帕累托引导搜索优化LLM推理，生成可检查的执行轨迹，实现优化过程透明复现。

Result: 在7个偏倚风险领域的100个RCT评估中，GEPA生成的提示在随机序列生成等报告清晰的领域表现最佳，相比手动提示在随机序列生成和选择性报告方面性能提升30%-40%，整体准确率最高。

Conclusion: GEPA能够为偏倚风险评估生成一致且可复现的提示，支持LLM在证据合成中的结构化和原则性使用。

Abstract: Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

</details>


### [74] [Multi-Path Collaborative Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.01485)
*Jindi Lv,Yuhao Zhou,Zheng Zhu,Xiaofeng Wang,Guan Huang,Jiancheng Lv*

Main category: cs.AI

TL;DR: M3PO提出了一种强化学习框架，通过并行策略rollout和多路径协作机制，在推理过程中注入集体洞察，提升LLMs的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的CoT推理在解码过程中表现出内部确定性，限制了探索合理替代方案的能力。现有方法虽然尝试在连续语义空间中进行推理，但仍受限于自回归解码的贪婪性质，无法充分利用替代推理可能性。

Method: 提出多路径感知策略优化（M3PO）框架，利用并行策略rollout作为自然多样化的推理源，并通过轻量级协作机制将跨路径交互整合到策略更新中，使每个轨迹都能通过同伴反馈精炼推理。

Result: M3PO在知识和推理密集型基准测试中实现了最先进的性能，同时保持了模型的可解释性和推理效率。

Conclusion: 多路径协作学习为稳健推理提供了有前景的方向，M3PO框架通过集体洞察注入和跨路径交互，有效提升了LLMs的多步推理模式可靠性。

Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.

</details>


### [75] [SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry](https://arxiv.org/abs/2512.01507)
*Daniel Armstrong,Zlatko Jončev,Andres M Bran,Philippe Schwaller*

Main category: cs.AI

TL;DR: 利用大语言模型将合成知识转化为代码，实现计算机辅助合成规划中的战略考量，如汇聚式组装、保护基最小化和最优环形成序列。


<details>
  <summary>Details</summary>
Motivation: 现代计算机辅助合成规划系统能生成化学上有效的反应步骤，但难以整合战略考量（如汇聚式组装、保护基最小化、最优环形成序列等）。需要将合成战略知识形式化为可验证的代码而非简单启发式规则。

Method: 利用大语言模型将合成知识转化为Python函数，代表各种战略和战术规则（如战略官能团转化、环构建策略）。创建可测试、可解释的合成战略表示，并发布完整代码库和USPTO-ST数据集（带战略标签的合成路线）。

Result: 实现了CASP的新能力：基于自然语言的路线检索，在基准测试中达到75%的Top-3准确率。通过历史趋势的时间分析和化学直观的路线聚类验证了该库，提供了比先前方法更细粒度的划分。

Conclusion: 这项工作弥合了CASP中战术与战略的鸿沟，使得能够基于战略标准而非仅结构来指定、搜索和评估合成路线。

Abstract: Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

</details>


### [76] [LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems](https://arxiv.org/abs/2512.01556)
*Zhiyuan Wang,Aniri,Tianlong Chen,Yue Zhang,Heng Tao Shen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.AI

TL;DR: LEC方法通过线性期望约束和FDR控制，为LLM预测提供统计保证，并引入双模型路由机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成不可靠答案的问题，现有启发式不确定性方法无法完全区分正确与错误预测，缺乏统计保证。

Method: 提出LEC框架，将选择性预测重新解释为约束决策问题，通过线性期望约束和FDR控制计算阈值，并扩展到双模型路由机制。

Result: 在闭式和开放式QA数据集上，LEC实现了更严格的FDR控制，显著提高了样本保留率，双模型路由机制在更低风险水平下接受更多正确样本。

Conclusion: LEC为LLM预测提供了统计保证的FDR控制框架，双模型路由机制进一步提升了性能，是解决LLM不可靠性问题的有效方法。

Abstract: Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

</details>


### [77] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam,Raghuram Bharadwaj Diddigi*

Main category: cs.AI

TL;DR: 提出一种基于预训练语言-策略对的迁移学习方法，通过建立自然语言与策略嵌入的统一表示空间，实现跨任务快速迁移


<details>
  <summary>Details</summary>
Motivation: 当前需要开发能够在同一环境中解决多个任务的智能体，特别是当这些任务与语言自然关联时。需要建立高效的迁移学习管道。

Method: 受计算机视觉中CLIP方法的启发，将"指令和对应策略代表同一任务的不同模态"这一理念扩展到强化学习。通过预训练的语言-策略对，建立自然语言和策略嵌入的统一表示空间。

Result: 实验结果表明该方法在实现跨任务快速迁移方面具有实用性。

Conclusion: 通过将CLIP思想扩展到强化学习领域，提出的方法能够有效利用语言-策略对齐，实现任务间的快速知识迁移。

Abstract: Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [78] [Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation](https://arxiv.org/abs/2512.01723)
*Saba Kublashvili*

Main category: cs.AI

TL;DR: HistoricalML：一个概率神经符号框架，用于在极端数据稀缺（N<100）下建模历史事件，整合贝叶斯不确定性、因果模型、博弈论和注意力机制，应用于非洲分割和布匿战争案例研究。


<details>
  <summary>Details</summary>
Motivation: 历史事件建模面临四大挑战：极端数据稀缺（N<100）、异构噪声测量、反事实缺失、以及需要人类可解释的解释。现有机器学习方法难以应对这些挑战。

Method: 提出HistoricalML框架，整合四个核心组件：1）贝叶斯不确定性量化分离认知与偶然不确定性；2）结构因果模型进行混杂下的反事实推理；3）合作博弈论（Shapley值）进行公平分配建模；4）注意力神经网络进行上下文依赖的因子加权。

Result: 理论分析显示：在稀疏数据下，当有强领域知识先验时，方法能实现一致估计；Shapley分配满足公理公平性保证。案例研究：1）19世纪非洲分割识别德国+107.9%差异作为一战前的结构性紧张；2）布匿战争模拟显示坎尼战役迦太基57.3%胜率，扎马战役罗马57.8%胜率；反事实分析揭示迦太基政治支持（6.4分）而非军事实力是决定性因素。

Conclusion: HistoricalML框架成功解决了历史事件建模的核心挑战，通过神经符号整合实现了在极端数据稀缺下的可靠估计、反事实推理和可解释分析，为历史研究提供了量化分析工具。

Abstract: Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.

</details>


### [79] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li,Ke Wang,Girija Gouda,Shubham Choudhary,Yaqun Wang,Linwei Hu,Joel Vaughan,Freddy Lecue*

Main category: cs.AI

TL;DR: LLM Jury-on-Demand：一个动态、基于学习的框架，通过训练可靠性预测器来评估LLM评委何时与人类专家一致，为每个数据点动态选择最优评委团，实现可扩展、上下文感知的评估。


<details>
  <summary>Details</summary>
Motivation: 随着LLM集成到高风险领域，需要既适合实时部署又可扩展，又对关键决策可靠的新型评估方法。人工评估可靠但缓慢昂贵，单一LLM评委存在偏见，静态评委团缺乏适应性。

Method: 训练一组可靠性预测器，利用token分布、嵌入和结构输入特征来评估LLM评委何时与人类专家一致。为每个数据点动态选择最优评委团，并使用其可靠性作为权重聚合评分。

Result: 在摘要和RAG基准测试中，动态评委团系统比单一评委和静态评委团基线显著提高了与人类判断的相关性。

Conclusion: 自适应、基于学习的评委团方法为现代LLM在高风险领域构建可扩展、更可靠和可信的评估系统展现了前景。

Abstract: As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [80] [H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons](https://arxiv.org/abs/2512.01797)
*Cheng Gao,Huimin Chen,Chaojun Xiao,Zhiyi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 研究发现大语言模型中的幻觉现象与极少数特定神经元（H-Neurons）相关，这些神经元仅占总数不到0.1%，能可靠预测幻觉发生，且与过度顺从行为有因果关联。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生幻觉（看似合理但事实错误的输出），这削弱了其可靠性。先前研究主要从宏观角度（如训练数据和目标）考察幻觉，而神经元层面的机制尚未被充分探索。

Method: 从三个角度系统研究幻觉相关神经元（H-Neurons）：1）识别：发现极稀疏的神经元子集（<0.1%）能可靠预测幻觉发生；2）行为影响：通过受控干预验证这些神经元与过度顺从行为的因果关联；3）起源：追溯这些神经元到预训练基础模型，发现它们在预训练期间就已形成。

Result: H-Neurons能可靠预测幻觉发生，且在不同场景中具有强泛化能力；这些神经元与模型的过度顺从行为有因果关联；它们源于预训练阶段，在预训练模型中仍能有效预测幻觉。

Conclusion: 研究将宏观行为模式与微观神经机制联系起来，为开发更可靠的大语言模型提供了新见解，表明通过识别和干预特定神经元可能减少幻觉问题。

Abstract: Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

</details>


### [81] [Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees](https://arxiv.org/abs/2512.01870)
*Alessandro Breccia,Federica Gerace,Marco Lippi,Gabriele Sicuro,Pierluigi Contucci*

Main category: cs.AI

TL;DR: GPT-2模型能够部分学习自然数迭代质因数分解生成的树序列的内部语法，表明可学习性可能扩展到算术结构本身


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够学习由自然数迭代质因数分解生成的确定性树序列，探索模型对算术结构的学习能力

Method: 使用GPT-2架构的Transformer网络，从头开始训练前10^11个元素，测试其在下一个词和掩码词预测任务中的预测能力

Result: 模型部分学习了树序列的内部语法，捕捉到了非平凡的规律性和相关性

Conclusion: 可学习性可能超越经验数据，扩展到算术结构本身，表明语言模型能够学习数学结构的内部规律

Abstract: We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

</details>


### [82] [Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning](https://arxiv.org/abs/2512.01878)
*Gaganpreet Jhajj,Fuhua Lin*

Main category: cs.AI

TL;DR: 提出基于惊奇最小化的知识图谱推理框架，将神经科学中的自由能原理与知识图谱系统连接，使用图距离度量惊奇


<details>
  <summary>Details</summary>
Motivation: 探索知识图谱推理的新方法，将神经科学中的自由能原理（FEP）应用于知识图谱系统，建立图距离与惊奇最小化的联系

Method: 使用有向图中的最短路径距离形式化惊奇度量，构建基于知识图谱的智能体框架，将图距离与图神经网络的消息传递深度和基于模型的强化学习中的世界模型轨迹联系起来

Result: 这是一个进行中的研究，探索基于距离的惊奇是否能扩展最近关于语法通过树结构最小化惊奇和自由能的工作

Conclusion: 提出了一个连接自由能原理和知识图谱系统的理论框架，为基于惊奇最小化的知识图谱推理提供了新的研究方向

Abstract: In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.

</details>


### [83] [Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models](https://arxiv.org/abs/2512.01880)
*Daren Zhong,Dingcheng Huang,Clayton Greenberg*

Main category: cs.AI

TL;DR: 本文提出一个基于n-gram语言模型的国际象棋走法预测框架，将走法预测视为行为分析任务，针对不同技能水平的玩家训练独立模型，实现实时分析。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋引擎主要关注最优走法计算，忽略了人类玩家在不同技能水平下的行为变异性。需要一种能够捕捉不同技能水平玩家走法模式的分析框架。

Method: 将玩家分为7个技能等级，使用Lichess平台的开源数据训练独立的n-gram语言模型。框架包含模型选择器模块，根据早期游戏信息动态选择最合适的模型进行走法预测。

Result: 模型选择器在16个半回合的早期游戏信息下，技能水平分类准确率达到31.7%。走法预测框架相比基准准确率提升高达39.1%，且计算效率高，适合实时分析。

Conclusion: 该框架成功将国际象棋走法预测转化为行为分析任务，通过技能水平特定的n-gram模型有效捕捉人类玩家的行为模式，为实时象棋分析提供了高效解决方案。

Abstract: Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.
  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.
  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

</details>


### [84] [Learned-Rule-Augmented Large Language Model Evaluators](https://arxiv.org/abs/2512.01958)
*Jie Meng,Jin Mao*

Main category: cs.AI

TL;DR: 提出基于规则增强的LLM评估范式，通过规则蒸馏和规则应用策略提升LLM作为通用评估器的性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估器依赖人工设计的评估原则，成本高且与标注数据和LLM理解不一致，难以泛化到多样化的评估场景

Method: 1) 规则蒸馏：使用LLM辅助的蒙特卡洛树搜索自动从数据中提取评分规则；2) 规则应用策略：链式规则引导和通过强化学习训练规则增强的LLM评估器

Result: 在多样化任务上的大量实验证明了该方法在各种评估场景中的有效性和泛化能力

Conclusion: 提出的规则增强评估范式解决了现有LLM评估器的局限性，为LLM作为通用评估器提供了有效解决方案

Abstract: Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

</details>


### [85] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng,Xunjian Yin,Ruiwen Zhou,Yuxuan Li,Xinyi Wang,Liangming Pan,William Yang Wang,Victor Zhong*

Main category: cs.AI

TL;DR: RL能主动合成复杂推理策略而非仅放大已有行为，但前提是基础模型已通过SFT掌握原子技能（参数推理和上下文推理）。


<details>
  <summary>Details</summary>
Motivation: 研究RL如何贡献推理能力——是激励新技能合成还是仅放大现有行为，通过互补推理任务探究这一核心问题。

Method: 使用控制合成的人类传记数据集，将互补推理严格解耦为参数推理和上下文推理两个原子技能，在I.I.D.、组合和零样本三个难度级别评估泛化能力。

Result: 发现SFT在分布内表现良好但分布外泛化差（特别是零样本），存在"SFT泛化悖论"；RL能作为推理合成器而非概率放大器，但前提是基础模型已掌握原子技能。

Conclusion: RL不是简单的放大器，在足够原子基础下能主动从学习到的原语合成复杂推理策略，解耦原子训练后接RL为复杂推理任务提供了可扩展的泛化路径。

Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [86] [Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback](https://arxiv.org/abs/2512.01979)
*Aiden Yiliu Li,Bizhi Yu,Daoan Lei,Tianhe Ren,Shilong Liu*

Main category: cs.AI

TL;DR: 提出Chain of Ground (CoG)框架，通过多步迭代推理提升GUI视觉定位精度，无需额外训练，在ScreenSpot Pro基准上提升4.8点，在工业控制面板数据集上提升6.9点。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在GUI视觉定位中仍存在局限性：难以处理小目标、视觉相似目标以及真实世界布局中的歧义问题，这源于有限的定位能力和未充分利用现有推理潜力。

Method: 提出Chain of Ground (CoG)框架，无需训练的多步定位方法。模型不是直接预测，而是通过迭代视觉推理和细化逐步反思和调整假设，实现更准确和可解释的定位。

Result: 在ScreenSpot Pro基准上达到68.4%准确率，提升4.8点；在TPanel UI工业控制面板数据集上，相比Qwen3 VL 235B基线提升6.9点，展示了在真实世界和数字界面中的有效性。

Conclusion: 通过结构化迭代细化而非额外训练，可以释放多模态大语言模型的定位潜力，为GUI视觉定位提供新的方向。

Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

</details>


### [87] [LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess](https://arxiv.org/abs/2512.01992)
*Sai Kolasani,Maxim Saplin,Nicholas Crispino,Kyle Montgomery,Jared Quincy Davis,Matei Zaharia,Chi Wang,Chenguang Wang*

Main category: cs.AI

TL;DR: LLM CHESS是一个评估框架，通过国际象棋领域的扩展代理交互来测试大语言模型的推理和指令遵循能力泛化性，对50多个模型进行排名，发现许多先进模型难以完成游戏或取得稳定胜利。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试容易过拟合和记忆化，且容易达到饱和。需要一种随机、动态的评估方法来更真实地测试LLMs的推理和指令遵循能力的泛化性，特别是在扩展代理交互环境中。

Method: 开发LLM CHESS框架，让LLMs与随机对手进行国际象棋对弈，使用多种行为指标（胜率、移动质量、合法性、幻觉动作、游戏时长）评估。对顶级推理模型，通过与可变技能配置的象棋引擎对弈计算Elo评分。

Result: 对50多个开源和闭源模型进行排名，发现许多最先进模型难以完成游戏或取得稳定胜利。实验显示推理模型与非推理模型之间存在明显分离。该框架有效减少了过拟合和记忆化，即使顶级推理模型也难以应对。

Conclusion: LLM CHESS提供了一个独特、动态的评估框架，能有效测试LLMs的推理和指令遵循能力泛化性，减少基准饱和问题。作者发布了实验框架、公开排行榜和相关游戏数据集以支持未来研究。

Abstract: We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [88] [An Information Geometric Approach to Fairness With Equalized Odds Constraint](https://arxiv.org/abs/2512.00135)
*Amirreza Zamani,Ayfer Özgür,Mikael Skoglund*

Main category: cs.IT

TL;DR: 研究在无法直接访问敏感属性S和任务T的情况下，设计满足均衡赔率（equalized odds）的公平机制，通过信息几何方法近似互信息，将问题转化为二次规划。


<details>
  <summary>Details</summary>
Motivation: 传统公平机制设计通常假设可以直接访问敏感属性S和任务T，但在实际应用中，代理只能通过相关数据X间接获取信息。本文研究在S-X-Y和T-X-Y马尔可夫链约束下，设计满足均衡赔率（I(Y;S|T)=0）的公平表示Y。

Method: 1) 假设代理无法直接访问S和T，只能通过数据X；2) 在P_{S|Y}上施加几何结构，允许Y和S之间存在有界的小相关性；3) 当相关性阈值较小时，利用信息几何概念近似互信息；4) 将公平机制设计问题转化为二次规划问题；5) 对于其他情况，基于矩阵的最大奇异值和向量推导简单低复杂度的下界。

Result: 在特定约束下，通过信息几何近似方法获得了闭式解。对于更一般情况，推导了基于矩阵最大奇异值的低复杂度下界。通过数值实验比较了所提设计与最优解的性能。

Conclusion: 本文提出了一种在无法直接访问敏感属性和任务的情况下设计公平机制的新方法，通过信息几何近似将问题转化为可解的二次规划，为实际公平机器学习应用提供了实用的解决方案。

Abstract: We study the statistical design of a fair mechanism that attains equalized odds, where an agent uses some useful data (database) $X$ to solve a task $T$. Since both $X$ and $T$ are correlated with some latent sensitive attribute $S$, the agent designs a representation $Y$ that satisfies an equalized odds, that is, such that $I(Y;S|T) =0$. In contrast to our previous work, we assume here that the agent has no direct access to $S$ and $T$; hence, the Markov chains $S - X - Y$ and $T - X - Y$ hold. Furthermore, we impose a geometric structure on the conditional distribution $P_{S|Y}$, allowing $Y$ and $S$ to have a small correlation, bounded by a threshold. When the threshold is small, concepts from information geometry allow us to approximate mutual information and reformulate the fair mechanism design problem as a quadratic program with closed-form solutions under certain constraints. For other cases, we derive simple, low-complexity lower bounds based on the maximum singular value and vector of a matrix. Finally, we compare our designs with the optimal solution in a numerical example.

</details>


### [89] [Fast list recovery of univariate multiplicity and folded Reed-Solomon codes](https://arxiv.org/abs/2512.00248)
*Rohan Goyal,Prahladh Harsha,Mrinal Kumar,Ashutosh Shankar*

Main category: cs.IT

TL;DR: 该论文设计了近线性时间算法，用于折叠里德-所罗门码和单变量重数码的列表恢复，达到容量限制。


<details>
  <summary>Details</summary>
Motivation: 先前Goyal等人的工作为折叠里德-所罗门码和单变量重数码的列表解码提供了近线性时间算法，但这些算法似乎专门针对列表解码问题，无法自然地扩展到列表恢复问题。本文旨在解决这一局限，为这些码的列表恢复问题设计高效的近线性时间算法。

Method: 基于Goyal等人使用的格基思想，增加了一个关键技术要素：构造在单变量多项式环上具有适当结构的格，这些格能够"捕获"这些码的列表恢复问题。

Result: 设计了时间复杂度为$\tilde{O}(n)$的算法，用于折叠里德-所罗门码和单变量重数码的列表恢复，达到容量限制，其中$n$是码的块长度。

Conclusion: 成功解决了先前工作中算法无法扩展到列表恢复的问题，为折叠里德-所罗门码和单变量重数码的列表恢复提供了高效的近线性时间算法，填补了该领域的技术空白。

Abstract: A recent work of Goyal, Harsha, Kumar and Shankar gave nearly linear time algorithms for the list decoding of Folded Reed-Solomon codes (FRS) and univariate multiplicity codes up to list decoding capacity in their natural setting of parameters. A curious aspect of this work was that unlike most list decoding algorithms for codes that also naturally extend to the problem of list recovery, the algorithm in the work of Goyal et al. seemed to be crucially tied to the problem of list decoding. In particular, it wasn't clear if their algorithm could be generalized to solve the problem of list recovery FRS and univariate multiplicity codes in near linear time.
  In this work, we address this question and design $\tilde{O}(n)$-time algorithms for list recovery of Folded Reed-Solomon codes and univariate Multiplicity codes up to capacity, where $n$ is the blocklength of the code. For our proof, we build upon the lattice based ideas crucially used by Goyal et al. with one additional technical ingredient - we show the construction of appropriately structured lattices over the univariate polynomial ring that \emph{capture} the list recovery problem for these codes.

</details>


### [90] [ORBGRAND Is Exactly Capacity-achieving via Rank Companding](https://arxiv.org/abs/2512.00347)
*Zhuang Li,Wenyi Zhang*

Main category: cs.IT

TL;DR: CDF-ORBGRAND算法通过根据信道可靠性的逆累积分布函数压缩ORBGRAND中的排序，能够精确达到二进制输入对称容量，并扩展应用到BICM系统实现BICM容量。


<details>
  <summary>Details</summary>
Motivation: ORBGRAND算法虽然能有效利用软信息并适合硬件实现，且在AWGN信道上接近容量，但需要进一步改进以在更一般的二进制输入无记忆信道上精确达到对称容量。

Method: 提出CDF-ORBGRAND算法：通过根据信道可靠性的逆累积分布函数压缩ORBGRAND中的排序，然后将该方法扩展到BICM系统处理高阶输入星座，考虑BICM和ORBGRAND导致的失配解码效应。

Result: CDF-ORBGRAND能够精确达到一般二进制输入无记忆信道的对称容量，并且在BICM系统中能够实现BICM容量，该容量最初是通过将BICM视为一组独立并行信道推导的。

Conclusion: 通过适当的排序压缩，CDF-ORBGRAND算法不仅能在一般二进制输入信道上达到对称容量，还能有效处理高阶调制系统，实现BICM容量，为实际通信系统提供了高效的解码方案。

Abstract: Among guessing random additive noise decoding (GRAND) algorithms, ordered reliability bits GRAND (ORBGRAND) has attracted considerable attention due to its efficient use of soft information and suitability for hardware implementation. It has also been shown that ORBGRAND achieves a rate very close to the capacity over additive white Gaussian noise channels with antipodal inputs. In this work, it is further established that, via suitably companding the ranks in ORBGRAND according to the inverse cumulative distribution function (CDF) of channel reliability, the resulting CDF-ORBGRAND algorithm exactly achieves the mutual information of general binary-input memoryless channels under symmetric input distribution, i.e., the symmetric capacity. This result is then applied to bit-interleaved coded modulation (BICM) systems to handle high-order input constellations. Via considering the effects of mismatched decoding due to both BICM and ORBGRAND, it is shown that CDF-ORBGRAND is capable of achieving the BICM capacity, which was initially derived by treating BICM as a set of independent parallel channels.

</details>


### [91] [Value of Communication in Goal-Oriented Semantic Communications: A Pareto Analysis](https://arxiv.org/abs/2512.01454)
*Jiping Luo,Bowen Li,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 该论文研究通信约束下远程估计马尔可夫源时的通信价值，通过帕累托前沿分析最优估计性能与通信成本之间的权衡关系，并提出高效算法SPLIT构建完整的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 新兴信息物理系统在严格通信约束下运行，无法可靠传输大量机器类型数据流。由于原始测量值常包含相关或冗余成分，有效运行不依赖于传输所有数据，而是选择有助于实现系统目标的信息。超越准确性，面向目标的语义通信评估信息价值，旨在仅生成和传输相关且适时的信息。

Method: 采用帕累托分析方法，研究远程估计马尔可夫源的通信价值。通过信息价值度量量化信息相关性，分析最优估计性能如何随可用通信预算变化，并确定额外通信带来的边际性能增益。证明帕累托前沿具有严格递减、凸且分段线性的结构，每个帕累托最优操作点可实现为两个平稳确定性策略的凸组合。

Result: 通信价值定义为帕累托前沿的绝对斜率。在该设置中，帕累托前沿具有显著易处理的结构：严格递减、凸且分段线性，其斜率由有限常数集合控制。基于这些结构洞察，提出了SPLIT算法，一种高效且可证明最优的算法，用于构建完整的帕累托前沿。

Conclusion: 该工作通过帕累托分析框架量化了通信约束下远程估计的通信价值，揭示了帕累托前沿的易处理数学结构，并提供了实用的算法工具SPLIT来实现最优权衡策略，为面向目标的语义通信系统设计提供了理论基础和实现方法。

Abstract: Emerging cyber-physical systems increasingly operate under stringent communication constraints that preclude the reliable transmission of their extensive machine-type data streams. Since raw measurements often contain correlated or redundant components, effective operation depends not on transmitting all available data but on selecting the information that contributes to achieving the objectives of the system. Beyond accuracy, goal-oriented semantic communication assesses the \emph{value of information} and aims to generate and transmit only what is relevant and at the right time. Motivated by this perspective, this work studies the \emph{value of communication} through the canonical setting of remote estimation of Markov sources, where a value-of-information measure quantifies the relevance of information. We investigate how optimal estimation performance varies with the available communication budget and determine the marginal performance gain attributable to additional communication. Our approach is based on a \emph{Pareto analysis} that characterizes the complete set of policies that achieve optimal trade-offs between estimation performance and communication cost. The value of communication is defined as the absolute slope of the resulting Pareto frontier. Although computing this frontier is non-trivial, we demonstrate that in our setting it admits a notably tractable structure: it is strictly decreasing, convex, and piecewise linear, and its slope is governed by a finite collection of constants. Moreover, each Pareto-optimal operating point is realizable as a convex combination of two stationary deterministic policies, enabling practical implementation. Leveraging these structural insights, we introduce SPLIT, an efficient and provably optimal algorithm for constructing the complete Pareto frontier.

</details>


### [92] [The Information Theory of Similarity](https://arxiv.org/abs/2512.00378)
*Nikit Phadke*

Main category: cs.IT

TL;DR: 论文建立了见证相似性系统（REWA）与香农信息论之间的精确数学等价关系，证明见证重叠是互信息，REWA比特复杂度源于信道容量限制，排序保持编码遵循率失真约束。


<details>
  <summary>Details</summary>
Motivation: 统一五十年的相似性搜索研究（从布隆过滤器到局部敏感哈希再到神经检索），揭示这些方法实际上是在为关系数据开发信息论，为语义相似性建立物理单位（互信息比特）。

Method: 建立REWA与香农信息论的精确数学等价，证明见证重叠等于互信息，推导REWA比特复杂度与信道容量的关系，分析排序保持编码的率失真约束。

Result: 证明了REWA的O(Δ^{-2} log N)复杂度是最优的：没有任何编码方案能用更少的比特保持相似性排序。揭示了相似性搜索本质上是通信（通过噪声信道传输查询），检索系统面临与香农信道编码定理类似的基本容量限制。

Conclusion: 该框架建立了语义相似性的物理单位（互信息比特），搜索即通信（查询在噪声信道上的传输），检索系统面临与香农信道编码定理类似的基本容量限制，统一了五十年的相似性搜索研究。

Abstract: We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem.

</details>


### [93] [Robust Precoding for Resilient Cell-Free Networks](https://arxiv.org/abs/2512.00531)
*Saeed Mashdour,André R. Flores,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出一种用于弹性无蜂窝大规模MIMO系统的鲁棒预编码器设计，在总发射功率约束下最小化期望信号均方误差和残余干扰泄漏功率的加权和，利用CSI误差统计增强对信道状态信息不完美的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO系统在实际部署中面临信道状态信息不完美的挑战，传统线性预编码器对此缺乏鲁棒性，需要设计能够有效应对CSI误差的预编码方案，在保证性能的同时维持计算效率。

Method: 提出鲁棒预编码器设计，在总发射功率约束下最小化期望信号均方误差和残余干扰泄漏功率的加权和；采用交替优化算法，以最小均方误差型解初始化，迭代优化预编码器；算法保持低计算复杂度并确保快速收敛。

Result: 数值结果表明，所提方法显著优于传统线性预编码器，在性能和计算效率之间提供了有效平衡，增强了系统对CSI不完美的鲁棒性。

Conclusion: 该鲁棒预编码器设计为无蜂窝大规模MIMO系统提供了一种有效的解决方案，能够在CSI不完美条件下保持良好性能，同时维持合理的计算复杂度，具有实际部署价值。

Abstract: This paper presents a robust precoder design for resilient cell-free massive MIMO (CF-mMIMO) systems that minimizes the weighted sum of desired signal mean square error (MSE) and residual interference leakage power under a total transmit power constraint. The proposed robust precoder incorporates channel state information (CSI) error statistics to enhance resilience against CSI imperfections. We employ an alternating optimization algorithm initialized with a minimum MSE-type solution, which iteratively refines the precoder while maintaining low computational complexity and ensuring fast convergence. Numerical results show that the proposed method significantly outperforms conventional linear precoders, providing an effective balance between performance and computational efficiency.

</details>


### [94] [Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation](https://arxiv.org/abs/2512.00711)
*Loc X. Nguyen,Ji Su Yoon,Huy Q. Le,Yu Qiao,Avi Deb Raha,Eui-Nam Huh,Walid Saad,Dusit Niyato,Zhu Han,Choong Seon Hong*

Main category: cs.IT

TL;DR: 提出一种新的联邦学习框架来解决语义通信系统中不同数据域间的域偏移问题，通过构建全局表示来对齐客户端本地特征，并采用域感知聚合方法解决样本数量不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 语义通信依赖深度学习模型进行联合信源信道编码，但需要大量训练数据。联邦学习可以分布式训练模型，但传统方法在客户端数据来自不同域时会出现灾难性性能下降。需要解决域偏移和样本不平衡问题。

Method: 提出新颖的联邦学习框架：1) 构建全局表示来对齐客户端本地特征，保持不同数据域的语义；2) 采用域感知聚合方法解决样本数量多的客户端域主导问题。

Result: 在图像重建任务中，该方法在1dB信噪比下比MOON框架的PSNR值高出0.5，且随着信道质量改善，性能差距进一步扩大。

Conclusion: 这是首个考虑语义通信系统训练中域偏移问题的工作，提出的联邦学习框架能有效处理不同数据域和样本不平衡问题，显著提升语义通信系统性能。

Abstract: Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.

</details>


### [95] [Movable Antenna Empowered Near-Field Sensing via Antenna Position Optimization](https://arxiv.org/abs/2512.00758)
*Yushen Wang,Weidong Mei,Xin Wei,Zhi Chen,Boyu Ning*

Main category: cs.IT

TL;DR: 该论文研究可移动天线（MA）阵列在6G近场感知中的应用，通过优化天线位置来联合估计目标的到达角和距离，相比传统固定位置天线显著提升了感知性能。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向超高频率发展，近场传播变得越来越普遍。传统固定位置天线在近场感知中性能有限，而可移动天线技术能够通过改变阵列几何形状来增强感知能力，因此研究MA阵列在近场角度和距离联合估计中的应用具有重要意义。

Method: 首先研究1D MA阵列的简化情况（仅AoA或仅距离估计），推导最坏情况Cramer-Rao界（CRB）。然后联合优化MA位置以最小化CRB，获得闭式解。对于更复杂的角度和距离联合估计问题，提出基于离散采样的方法顺序更新MA位置。最后将方法扩展到2D MA阵列，并设计高效算法。

Result: 数值结果表明，提出的MA增强近场感知方案显著优于传统固定位置天线（FPA）。同时发现，角度和距离联合估计产生的阵列几何形状与单独的角度/距离估计或远场感知不同。

Conclusion: 可移动天线技术能够有效增强6G网络的近场感知能力，通过优化天线阵列几何形状可以显著提升角度和距离估计性能，为未来6G网络的高精度感知应用提供了重要技术支持。

Abstract: Movable antenna (MA) technology exhibits great promise for enhancing the sensing capabilities of future sixth-generation (6G) networks due to its capability to alter antenna array geometry. With the growing prevalence of near-field propagation at ultra-high frequencies, this paper focuses on the application of one-dimensional (1D) and two-dimensional (2D) MA arrays for near-field sensing to jointly estimate the angle and distance information about a target. First, for the 1D MA array scenario, to gain insights into MA-enhanced near-field sensing, we investigate two simplified cases with only angle-of-arrival (AoA) or distance estimation, respectively, assuming that the other information is already known. The worst-case Cramer-Rao bounds (CRBs) on the mean square errors (MSEs) of the AoA estimation and the distance estimation are derived in these two cases. Then, we jointly optimize the positions of the MAs within the 1D array to minimize these CRBs and derive their closed-form solutions, which yield an identical array geometry to MA-enhanced far-field sensing. For the more challenging joint AoA and distance estimation, since the associated worst-case CRB is a highly complex and non-convex function with respect to the MA positions, a discrete sampling-based approach is proposed to sequentially update the MA positions and obtain an efficient suboptimal solution. Furthermore, we investigate the worst-case CRB minimization problems for a 2D MA array under various conditions and extend our proposed algorithms to solve them efficiently. Numerical results demonstrate that the proposed MA-enhanced near-field sensing scheme dramatically outperforms conventional fixed-position antennas (FPAs). Moreover, the joint angle and distance estimation results in a different array geometry from that in the individual estimation of angle/distance or far-field sensing.

</details>


### [96] [Rate-Splitting Multiple Access for Secure Near-Field Integrated Sensing and Communication](https://arxiv.org/abs/2512.00770)
*Jiasi Zhou,Chintha Tellambura,Geoffrey Ye Li*

Main category: cs.IT

TL;DR: 提出基于RSMA的近场ISAC安全传输方案，利用RSMA的公共流同时管理干扰、抑制窃听并作为感知序列，在满足CRB约束下最大化最小保密速率


<details>
  <summary>Details</summary>
Motivation: 近场ISAC中，全数字架构硬件成本过高，而混合模拟数字架构会降低波束聚焦精度，导致能量泄漏，加剧用户间干扰和窃听风险

Method: 提出RSMA增强的安全传输方案，利用RSMA公共流同时实现干扰管理、窃听抑制和感知序列功能；采用块坐标下降法分解优化问题，分三阶段优化：全数字波束聚焦器（WMMSE+二次变换+泰勒展开）、模拟波束聚焦器（闭式解）、数字波束聚焦器（闭式解）

Result: 仿真结果显示：1) 在RF链减少16倍情况下实现接近全数字波束聚焦性能；2) 相比传统波束聚焦和远场安全方案提供更优的保密性能；3) 实现高精度感知且保密性能损失可忽略

Conclusion: 提出的RSMA增强近场ISAC安全传输方案能有效解决混合架构的波束聚焦精度损失问题，同时实现安全通信和高精度感知，具有实际应用价值

Abstract: Near-field integrated sensing and communication (ISAC) leverages distance-dependent channel variations for joint distance and angle estimation. However, full-digital architectures have prohibitive hardware costs, making hybrid analog-digital (HAD) designs the primary alternative. Nevertheless, such architectures compromise beamfocusing precision and lead to energy leakage, which exacerbates inter-user interference and increases eavesdropping risks. To address these challenges, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field ISAC. For the first time, it exploits the common stream in RSMA to concurrently (i) flexibly manage interference, (ii) act as artificial noise to suppress eavesdropping, and (iii) serve as sensing sequences. The objective is to maximize the minimum secrecy rate while satisfying the angle and distance Cramer-Rao Bound (CRB) constraints. This results in a hard, non-convex optimization problem, and we employ block coordinate descent to decompose it into three sub-problems with lower computational complexity. In the first stage of optimizing fully digital beamfocusers, we develop an iterative solution using weighted minimum mean-squared error (WMMSE), quadratic transform, and Taylor expansion methods, thus avoiding conventional semidefinite relaxation. In the second and third stages, the analog and digital beamfocusers are optimized in closed form. Simulation results show that the proposed scheme (1) achieves near full-digital beamfocusing performance with a 16-fold reduction in RF chains, (2) provides superior secrecy performance compared to conventional beamfocusing-only and far-field security schemes, and (3) enables high-accuracy sensing with negligible loss in secrecy performance.

</details>


### [97] [Hybrid Beamfocusing Design for RSMA-Enhanced Near-Field Secure Communications](https://arxiv.org/abs/2512.00774)
*Jiasi Zhou,Huiyun Xia,Chuan Wu,Chintha Tellambura*

Main category: cs.IT

TL;DR: 本文提出了一种基于速率分割多址（RSMA）的近场通信安全传输方案，通过联合优化模拟波束聚焦器、数字波束聚焦器和公共保密速率分配，在混合模拟-数字天线架构下实现最大最小保密率。


<details>
  <summary>Details</summary>
Motivation: 近场球面波前能够实现聚光灯式波束聚焦，减少能量泄漏，为物理层安全提供新机会。但在混合模拟-数字天线架构下，仅靠波束聚焦可能无法提供可靠的隐私保护，因为聚焦精度降低。需要解决混合架构下的近场通信安全问题。

Method: 提出RSMA增强的安全传输方案，将公共流设计为双重用途：为合法用户传递期望消息，同时作为人工噪声干扰窃听。采用基于惩罚的交替优化算法，将变量分为三个块，其中一个块通过代理优化方法求解，其他块以闭式更新。

Result: 仿真结果表明：(1) 方案以显著更少的射频链接近全数字波束聚焦性能；(2) 优于传统的仅波束聚焦和远场安全方案；(3) 在不显著影响通信速率的情况下保持保密性。

Conclusion: RSMA增强的安全传输方案有效解决了混合模拟-数字架构下近场通信的安全问题，通过公共流的双重用途设计实现了高效的物理层安全保护。

Abstract: Near-field spherical wavefronts enable spotlight-like beam focusing to mitigate unintended energy leakage, creating new opportunities for physical-layer security (PLS). However, under hybrid analog-digital (HAD) antenna architectures, beamfocusing alone may not provide foolproof privacy protection due to reduced focusing precision. To address this issue, this paper proposes a rate-splitting multiple access (RSMA)-enhanced secure transmit scheme for near-field communications with fully-connected or sub-connected HAD architectures. In the proposed scheme, the common stream is designed for dual purposes, delivering the desired message for legitimate users while acting as artificial noise to disrupt eavesdropping. The primary objective is to maximize the minimum secrecy rate by jointly optimizing the analog beamfocuser, digital beamfocuser, and common secrecy rate allocation. To solve the formulated non-convex problem, we develop a penalty-based alternating optimization algorithm. Specifically, the variables are partitioned into three blocks, where one block is solved via a surrogate optimization method, while the others are updated in closed form. Simulation results reveal that our transmit scheme: (1) approaches fully digital beamfocusing with substantially fewer radio frequency chains, (2) outperforms conventional beamfocusing-only and far-field security schemes, and (3) preserves secrecy without significantly compromising communication rates.

</details>


### [98] [Age Optimal Sampling and Routing under Intermittent Links and Energy Constraints](https://arxiv.org/abs/2512.00985)
*Adem Utku Atasayar,Aimin Li,Çağrı Arı,Elif Uysal*

Main category: cs.IT

TL;DR: 该论文提出了一种联合优化采样和路由决策的方法，以最小化卫星-地面综合网络中单调非线性的信息年龄(AoI)，考虑了链路延迟分布、间歇可用性和异构能耗等实际系统特征。


<details>
  <summary>Details</summary>
Motivation: 实际系统（如卫星-地面综合网络）中的链路具有不同的延迟分布、间歇可用性和异构能耗特性，这些特性对保持及时且节能的状态更新提出了重大挑战。链路可用性限制了可行的传输路径，而路由决策又决定了实际的延迟和能耗。

Method: 将问题建模为具有混合状态-动作空间的无限时域约束半马尔可夫决策过程(CSMDP)，开发了高效的嵌套算法Bisec-ReaVI。揭示了联合最优策略结构：最优路由策略是适应路径可用性和平均延迟的单调切换策略；最优采样策略是分段线性等待策略。

Result: 在卫星-地面综合路由场景中的数值实验表明，所提方案能有效平衡能耗和信息新鲜度。揭示了一个反直觉的见解：即使平均延迟更高、延迟方差更大或可用性更低的路径，在最小化AoI单调函数时仍能发挥关键作用。

Conclusion: 该研究为具有异构链路特性的实际网络系统提供了一种有效的联合采样和路由优化框架，能够平衡信息新鲜度和能耗，并揭示了非理想路径在AoI优化中的潜在价值。

Abstract: Links in practical systems, such as satellite-terrestrial integrated networks, exhibit distinct delay distributions, intermittent availability, and heterogeneous energy costs. These characteristics pose significant challenges to maintaining timely and energy-efficient status updates. While link availability restricts feasible transmission routes, routing decisions determine the actual delay and energy expenditure. This paper tackles these challenges by jointly optimizing sampling and routing decisions to minimize monotonic, nonlinear Age of Information (AoI). The proposed formulation incorporates key system features, including multiple routes with correlated random delays, stochastic link availability, and route-dependent energy consumption. We model the problem as an infinite-horizon constrained semi-Markov decision process (CSMDP) with a hybrid state-action space and develop an efficient nested algorithm, termed Bisec-ReaVI, to solve this problem. We reveal a well-defined jointly optimal policy structure: (i) the optimal routing policy is a monotonic handover policy that adapts to the availability of routes and their mean delays; and (ii) the optimal sampling policy is a piecewise linear waiting policy, with at most "N choose 2 + N" breakpoints given N routes. Numerical experiments in a satellite-terrestrial integrated routing scenario demonstrate that the proposed scheme efficiently balances energy usage and information freshness, and reveal a counter-intuitive insight: even routes with higher average delay, higher delay variance, or lower availability can still play a critical role in minimizing monotonic functions of AoI.

</details>


### [99] [Generating Random Hyperfractal Cities](https://arxiv.org/abs/2512.01505)
*Geoffrey Deperle,Philippe Jacquet*

Main category: cs.IT

TL;DR: 扩展简单分形模型，通过基于平铺结构的超分形分布构建随机城市，连接多个超分形区域，提供更全面的城市表示


<details>
  <summary>Details</summary>
Motivation: 解决交互式建模街道网络的挑战，特别是为小城市或单个区域提供更好的描述方法

Method: 扩展简单分形模型，在平铺结构上分布超分形来构建随机城市，连接多个超分形区域

Result: 能够将城市分解为不同区域，通过分形分析进行区域分割，并开发了基于该模型的随机城市数值生成工具

Conclusion: 提出的方法为城市建模提供了更全面的表示，支持交互式街道网络建模和城市区域分析

Abstract: This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used to segment a city into distinct districts through fractal analysis. Finally, we present tools for the numerical generation of random cities following this model.

</details>


### [100] [Storage capacity of perceptron with variable selection](https://arxiv.org/abs/2512.01861)
*Yingying Xu,Masayuki Ohzeki,Yoshiyuki Kabashima*

Main category: cs.IT

TL;DR: 感知机在最优变量选择下可以超越Cover-Gardner理论界限，实现更好的模式分类性能


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中区分高维数据中真实结构与偶然相关性的核心挑战，为感知机模型提供理论分析

Method: 基于统计力学中的副本方法，开发了枚举能够实现完美模式分类的变量组合的方法

Result: 最优变量选择可以超越Cover-Gardner理论界限（α<2ρ），提供了区分数据真实结构与虚假规律性的定量标准

Conclusion: 该方法不仅为感知机提供了理论分析框架，还得到了具有稀疏非对称耦合的联想记忆模型的存储容量

Abstract: A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ρN$ dimensions can separate $αN$ random patterns if and only if $α< 2ρ$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings.

</details>


### [101] [A Dual Approach for Hierarchical Information-Theoretic Tree Abstractions](https://arxiv.org/abs/2512.01985)
*Daniel T. Larsson,Dipankar Maity,Panagiotis Tsiotras*

Main category: cs.IT

TL;DR: 该论文建立了硬约束和软约束树抽象问题之间的形式化连接，利用拉格朗日松弛和对偶理论分析两种方法的等价条件，提出了基于树相变和对偶变量选择的算法。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是建立信息瓶颈方法启发的两种不同树抽象问题（硬约束和软约束公式）之间的形式化连接，确定这两种方法在什么条件下是等价的。

Method: 方法包括：1）利用拉格朗日松弛和对偶理论将硬约束问题的对偶函数与Q树搜索中的Q函数联系起来；2）分析树相变与对偶问题解的关系；3）提出基于树相变知识寻找对偶变量设置的算法；4）通过整数规划公式和线性规划强对偶性选择对偶变量的替代方法。

Result: 理论结果表明：1）建立了硬约束和软约束树抽象问题的形式化连接；2）证明了硬约束树搜索问题的整数规划松弛具有完整性性质（约束矩阵完全单模）；3）提出的算法能够有效找到解决对偶问题的对偶变量设置；4）实证结果验证了理论发展。

Conclusion: 论文成功建立了两种树抽象问题之间的形式化连接，提出了基于对偶理论和树相变的算法框架，并通过理论分析和实证验证了方法的有效性，为信息瓶颈方法在树抽象问题中的应用提供了理论基础和实用工具。

Abstract: In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-function employed in Q-tree search and shows the connection between tree phase transitions and solutions to the dual problem obtained by exploiting the problem structure. An algorithm is proposed that employs knowledge of the tree phase transitions to find a setting of the dual variable that solves the dual problem. Furthermore, we present an alternative approach to select the dual variable that leverages the integer programming formulation of the hard-constrained problem and the strong duality of linear programming. To obtain a linear program, we establish that a relaxation of the integer programming formulation of the hard-constrained tree-search problem has the integrality property by showing that the program constraint matrix is totally unimodular. Empirical results that corroborate the theoretical developments are presented and discussed throughout.

</details>
