<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: Grace是一个卫星-地面协同系统，用于遥感任务中的近实时大型视觉语言模型推理，通过异步检索增强生成和任务调度算法，在有限卫星-地面接触时间内实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在遥感任务中潜力巨大，但受限于星上计算资源和短暂卫星-地面接触时间，在实际LEO卫星系统中部署仍面临挑战。

Method: 采用卫星-地面协同架构：在卫星上部署紧凑模型进行实时推理，在地面站部署更大模型保证性能；包含异步RAG和基于置信度的任务调度算法。

Result: 基于真实卫星轨道数据的实验表明，Grace相比现有方法平均延迟降低76-95%，且不损害推理精度。

Conclusion: Grace系统成功解决了星上资源限制问题，实现了高效的近实时遥感任务处理，为LVLM在卫星系统中的实际部署提供了可行方案。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [2] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 提出了一种结合数字和模拟预编码的混合预编码方法，通过联合角度和相位熵来优化多用户大规模MIMO系统的数据传输，显著提升了和速率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 混合预编码对于发挥多用户大规模MIMO系统潜力至关重要，需要解决复杂信号中角度和相位变化的相关性问题，以准确确定信道特性。

Method: 将角度和相位建模为服从双变量高斯分布的相关变量，首次定义了联合角度和相位熵来衡量无线信道中角度和相位变化的不确定性，并基于此调整预编码方法。

Result: 仿真验证了分析结果的准确性，相比其他最先进方法，和速率提高了18.31%，鲁棒性提升了11.47%。

Conclusion: 所提出的混合预编码方法通过联合角度和相位熵有效优化了数据传输性能，在提升和速率和抑制旁瓣干扰方面表现出色。

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


### [3] [Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives](https://arxiv.org/abs/2510.24611)
*Azadeh Pourkabirian,Amir Masoud Rahmani,Kai Li,Wei Ni*

Main category: cs.NI

TL;DR: 提出了一种基于经济供需模型和VCG拍卖的物联网任务卸载方法，通过市场平衡机制优化边缘计算资源分配，为延迟敏感型IoT应用提供延迟保证。


<details>
  <summary>Details</summary>
Motivation: 物联网设备处理资源有限，而延迟敏感型应用需要实时响应。任务卸载可以将计算密集型任务转移到资源丰富的边缘服务器，但现有方案缺乏有效的资源分配机制。

Method: 将任务卸载建模为经济供需模型，使用VCG拍卖设计博弈论框架，开发激励机制促进用户和服务提供商参与拍卖。

Result: 仿真结果表明该方法能最大化社会福利、确保真实性、维持市场平衡，并为延迟敏感型IoT应用提供延迟保证。

Conclusion: 所提出的基于经济模型和拍卖机制的任务卸载方法能有效解决边缘计算环境中的资源分配问题，实现用户和服务提供商的双赢。

Abstract: Delay-sensitive Internet of Things (IoT) applications have drawn significant
attention. Running many of these applications on IoT devices is challenging due
to the limited processing resources of these devices and the need for real-time
responses. Task offloading can minimize latency by transferring computationally
intensive tasks from IoT devices to resource-rich edge servers, ensuring delay
and performance guarantees. In this paper, we develop a task-offloading
approach for delay-sensitive IoT applications in edge computing environments.
Unlike existing schemes, we model the task offloading problem as an economic
demand and supply model to achieve market balance. The proposed model avoids
under- and over-supply, ensuring the computational resources at edge servers
(supply) are allocated in a manner that best meets the processing and
computational needs of user devices (demand). Given the multi-agent nature of
task offloading involving users and service providers with different
preferences and objectives, we design a game-theoretic framework using a
Vickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions
and decision-making processes. Additionally, we develop an incentive mechanism
to encourage both parties to participate in the auction. The mechanism
maximizes user task offloading to edge servers and motivates edge servers to
share their computational resources, achieving profitability for both IoT users
and edge servers. Simulations demonstrate our method maximizes social welfare,
ensures truthfulness, maintains market balance, and provides latency guarantees
for delay-sensitive IoT applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 提出多智能体生态系统用于N-of-1决策支持，从单一模型转向协调智能，使医疗AI更加透明、公平且以个体为中心。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统服务于平均患者，但在罕见变异、多病共存和代表性不足人群等边缘情况下表现不佳，这损害了公平性和信任度。

Method: 构建多智能体生态系统，按器官系统、患者群体和分析模式分组的智能体共享模型库和证据合成工具，通过协调层整合结果，提供带置信区间的风险评估、异常值标记和相关证据。

Result: 验证重点从群体平均转向个体可靠性，测量低密度区域的误差、小样本校准和风险-覆盖权衡。

Conclusion: 该方法通过从单一模型转向协调智能，使医疗AI更符合医学首要原则：提供透明、公平且以个体为中心的护理。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [5] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS是一个通用游戏智能体，使用统一、可扩展的键盘鼠标动作空间进行训练，能在操作系统、网页和模拟游戏等异构领域进行大规模持续预训练。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨异构领域（操作系统、网页、模拟游戏）工作的通用游戏智能体，避免API或GUI方法的限制，实现大规模持续预训练。

Method: 采用统一键盘鼠标动作空间，使用超过500B tokens的多样化轨迹和多模态数据进行预训练，关键技术包括衰减持续损失减少因果混淆和高效的稀疏思维策略平衡推理深度与推理成本。

Result: 在开放世界Minecraft任务中成功率是之前最优模型的2倍，在未见过的网页3D游戏中接近人类新手水平，在FPS基准测试中超越GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。

Conclusion: 简单可扩展的动作表示结合大规模预训练为实现具有广泛计算机使用能力的通用智能体提供了有前景的路径。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [6] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文探讨AI在科学问题解决中的作用，重点关注其对学科创造力的影响。通过数学案例表明，虽然计算能扩展学科创造力，但某些AI方法可能取代它，从而改变科学追求的价值。


<details>
  <summary>Details</summary>
Motivation: 研究AI如何影响科学领域的创造力，特别是学科创造力（在特定领域内应用专业知识解决有价值问题的创造性方式），探讨AI可能对科学追求价值产生的潜在影响。

Method: 基于创造力哲学理论，区分创造性方法和创造性产品，引入学科创造力概念。通过两个数学案例进行分析，比较计算和AI方法对学科创造力的不同影响。

Result: 研究发现计算可以扩展学科创造力，但某些AI方法会取代学科创造力，这种取代可能改变科学追求的价值，甚至可能使其贬值。

Conclusion: AI在科学问题解决中具有双重作用：一方面计算能增强学科创造力，另一方面某些AI方法可能取代学科创造力，这需要谨慎考虑AI对科学价值体系的影响。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [7] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: 本文提出了多环境POMDPs（ME-POMDPs）及其扩展形式AB-POMDPs，用于处理具有离散模型不确定性的POMDP问题，并开发了精确和近似的算法来计算鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 当多个领域专家对问题建模存在分歧时，需要一种能够处理模型不确定性的框架。ME-POMDPs扩展了标准POMDPs，能够表示一组共享状态、动作和观测空间但可能任意变化其转移、观测和奖励模型的POMDPs集合。

Method: 首先将ME-POMDPs推广到具有初始信念集合的AB-POMDPs；然后证明任意ME-POMDP可以简化为仅在转移和奖励函数或仅在观测和奖励函数上变化的ME-POMDP；最后设计了精确和近似（基于点的）算法来计算鲁棒策略。

Result: 成功将标准POMDP基准扩展到多环境设置，并能够计算相应的策略。

Conclusion: ME-POMDPs和AB-POMDPs为处理模型不确定性提供了有效的框架，所提出的算法能够在多环境设置下计算鲁棒策略。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [8] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出了一种基于测试时调优的框架，利用预训练transformer模型直接从串联质谱和分子式进行端到端的从头分子结构生成，无需数据库匹配或中间步骤。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖数据库匹配或需要中间片段预测的多步骤流程，难以识别参考数据库中不存在的化合物。

Method: 通过测试时调优增强预训练transformer模型的学习能力，直接从串联质谱和分子式生成分子结构，无需人工标注和中间步骤。

Result: 在两个基准测试NPLIB1和MassSpecGym上分别比现有最佳方法DiffMS提高了100%和20%，测试时调优在MassSpecGym上比传统微调性能提升62%。

Conclusion: 该方法能够动态适应新质谱，即使预测与真实结构有偏差，生成的分子候选仍保持结构准确性，为人工解释提供有价值指导。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [9] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 这篇论文研究了生成式AI在象棋谜题领域的创造力，开发了一个能生成具有美学吸引力、新颖性和反直觉独特解法的AI系统，并由国际象棋专家评估其创造性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，人们对其产生创造性新颖输出的能力存在疑问，特别是在象棋谜题这种需要高度创造性的领域。

Method: 开发了一个专门生成象棋谜题的AI系统，该系统能产生具有美学吸引力、新颖性、反直觉和独特解法的谜题，并邀请三位世界知名象棋专家进行评估。

Result: 三位国际象棋大师（Amatzia Avni、Jonathan Levitt和Matthew Sadler）对AI生成的谜题进行了评估，选择了他们最喜欢的谜题并解释了其吸引力所在。

Conclusion: 研究表明生成式AI在象棋谜题领域能够产生具有创造性和美学价值的输出，专家评估证实了AI系统在创造性方面的潜力。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [10] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 病理学基础模型存在根本性缺陷，包括诊断准确率低、鲁棒性差、几何不稳定、计算需求大和安全漏洞等问题，这些源于通用基础模型假设与人体组织复杂性的概念不匹配。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在非医学领域取得了革命性突破，但在计算病理学中的快速应用并未带来预期的癌症诊断、预后和多模态检索突破，反而暴露出严重缺陷。

Method: 通过系统评估分析病理学基础模型的七个相互关联的根本原因：生物复杂性、无效的自监督学习、过度泛化、过度架构复杂性、缺乏领域特定创新、数据不足以及与组织切片尺寸相关的根本设计缺陷。

Result: 研究发现当前病理学基础模型在概念上与组织形态学本质不匹配，导致性能不佳和安全隐患。

Conclusion: 病理学基础模型需要从根本上重新思考设计范式，而非简单套用通用AI模型。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [11] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP是一个用于大语言模型的递归上下文感知推理和规划框架，通过计划分解、结构化重注入父计划和内存高效执行三个机制，显著提升长时程任务的推理和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序提示方法容易产生上下文漂移、目标信息丢失和循环失败问题，而分层提示方法则削弱跨层连续性或带来大量运行时开销。需要一种能够保持多级上下文一致性的高效规划框架。

Method: 结合三个关键机制：(1) 提前计划分解 - 生成完整子任务列表，执行第一项并优化剩余任务；(2) 结构化重注入父计划 - 在递归返回时保持多级上下文一致性；(3) 内存高效执行 - 限制活动提示，使成本随任务深度线性扩展。

Result: 在各种长时程推理基准测试中显著提高了子目标对齐和成功率，在同步Robotouille上获得32%的提升，在异步Robotouille上获得29%的改进（严格pass@1协议）。

Conclusion: ReCAP框架能够有效对齐高层目标与低层动作，减少冗余提示，并在递归过程中保持连贯的上下文更新，为长时程任务提供了高效的推理和规划解决方案。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [12] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 该论文研究了多智能体路径规划中的去中心化目标分配问题，通过比较贪心启发式、最优分配和基于大语言模型的方法，发现LLM智能体在精心设计的提示下能实现接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化条件下多智能体在共享环境中的协调挑战，探索语言模型在目标分配问题中的应用潜力。

Method: 智能体基于环境结构化表示独立生成目标偏好排名，通过固定冲突解决规则进行目标分配，无需协商或迭代协调。系统比较了贪心启发式、最优分配和LLM智能体方法。

Result: LLM智能体在提供精心设计的提示和相关定量信息时，能实现接近最优的完工时间，并持续优于传统启发式方法。

Conclusion: 语言模型在多智能体路径规划的去中心化目标分配中具有潜力，信息结构在此类系统中至关重要。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [13] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: IBM开发了通用代理CUGA，采用分层规划-执行架构，在企业业务流程外包人才招聘领域进行试点，展示了通用代理在企业规模应用的可能性。


<details>
  <summary>Details</summary>
Motivation: 解决企业部署AI代理系统面临的挑战：框架碎片化、开发缓慢、缺乏标准化评估实践，推动通用代理从原型走向实际企业应用。

Method: 采用分层规划-执行架构，基于强分析基础，在AppWorld和WebArena上实现最先进性能，并在企业业务流程外包人才招聘领域进行试点评估。

Result: CUGA在BPO-TA基准测试中接近专用代理的准确性，同时显示出减少开发时间和成本的潜力，为企业规模应用提供了早期证据。

Conclusion: 通用代理在企业规模应用具有可行性，需要进一步研究将研究级架构转化为企业就绪系统，并总结了技术和组织层面的经验教训。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [14] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的国际象棋谜题生成方法，通过设计基于象棋引擎搜索统计的新型奖励函数，显著提高了生成谜题的创造性、反直觉性和多样性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造真正具有创造性、美学价值和反直觉性的输出方面仍面临挑战，特别是在国际象棋谜题生成领域。

Method: 首先对生成式AI架构进行基准测试，然后引入基于象棋引擎搜索统计的强化学习框架，设计奖励函数来增强谜题的独特性、反直觉性、多样性和真实性。

Result: 强化学习方法将反直觉谜题生成率从0.22%大幅提升至2.5%，超过了现有数据集(2.1%)和最佳Lichess训练模型(0.4%)。生成的谜题在人类专家评估中被认为比传统书籍谜题更具创造性、趣味性和反直觉性。

Conclusion: 该方法成功生成了具有高度创造性的国际象棋谜题，三位世界知名专家认可了这些AI生成谜题的创造力，最终形成了精选的谜题手册。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [15] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 该研究比较了四种预测模型（线性、物理建模、LSTM、混合建模）和三种控制策略（MPC、RL、LLM）在数字孪生系统中的应用，发现HAM模型在精度、泛化性和计算效率方面表现最均衡，MPC控制最稳健，RL适应性最强，LLM控制器在人机交互方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 研究数字孪生在动态系统建模和控制中的应用，整合物理基础、数据驱动和混合方法，比较传统与AI驱动控制器的性能差异。

Method: 使用微型温室作为测试平台，开发四种预测模型（线性、PBM、LSTM、HAM）和三种控制策略（MPC、RL、LLM），在插值和外推场景下进行比较评估。

Result: HAM模型在精度、泛化性和计算效率方面表现最均衡；LSTM精度高但资源消耗大；MPC控制稳健可预测；RL适应性强；LLM控制器结合预测工具可实现灵活的人机交互。

Conclusion: HAM模型在建模中提供最佳平衡性能，MPC控制最可靠，RL在适应性方面表现突出，LLM控制器在人机协作方面具有独特价值，为数字孪生系统的模型和控制选择提供了实用指导。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [16] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文调查了基于大语言模型的智能体AI系统面临的安全威胁，提出了威胁分类法，并讨论了相应的评估方法和防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着具备规划、工具使用、记忆和自主能力的智能体AI系统在自动化中广泛应用，它们带来了传统AI安全和软件安全之外的新型安全风险，需要专门研究。

Method: 通过构建智能体AI特有的威胁分类法，回顾现有基准测试和评估方法，并从技术和治理两个角度分析防御策略。

Result: 系统梳理了智能体AI安全领域的研究现状，识别出关键威胁类型和相应的应对措施。

Conclusion: 智能体AI系统需要安全优先的设计理念，当前研究仍面临诸多开放挑战，需要继续探索以确保系统的安全性。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [17] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 提出基于摊销变分推理的可扩展训练算法，通过多样性强化学习改进LVLMs的推理能力，在七个基准测试中提升效果、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有训练算法（SFT、PPO、GRPO）在未见推理任务上泛化能力不足，且过度依赖有偏奖励模型，需要更鲁棒的推理训练方法。

Method: 将LVLMs推理重新表述为后验推断，使用摊销变分推理和多样性强化学习算法，引入稀疏奖励函数鼓励多样化的潜在CoT，并采用贝叶斯推断缩放策略。

Result: 在七个推理基准测试中，该方法在效果、泛化性和可解释性方面均优于现有最先进LVLMs。

Conclusion: 提出的基于变分推理的训练算法能够有效提升LVLMs的推理能力，克服确定性采样限制和奖励欺骗问题，实现更好的泛化性能。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [18] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出直觉主义分散式因果发现框架judo calculus，使用j-稳定因果推断和j-do-calculus在层拓扑中形式化上下文依赖的因果效应


<details>
  <summary>Details</summary>
Motivation: 现实应用中因果效应依赖于环境（年龄、国家、剂量、基因型等），需要形式化这种上下文依赖性

Method: 结合层理论和Lawvere-Tierney模态算子j，开发judo calculus算法框架，与基于分数、约束和梯度的因果发现方法结合

Result: 在合成和真实数据集上的实验显示，层理论因果发现的分散性质带来计算效率提升，性能优于经典因果发现方法

Conclusion: judo calculus为上下文依赖的因果发现提供了形式化框架，实现了计算效率和性能的改进

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [19] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出了一种名为sign estimator的新方法，通过用二元分类损失替换交叉熵损失，在LLM对齐中提供一致且高效的人口平均效用估计，解决了传统方法对人类偏好异质性的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法对人类偏好的异质性很脆弱，基于成对比较数据的朴素概率模型会产生不一致的人口平均效用估计，这是社会福利的规范衡量标准。

Method: 提出sign estimator方法，在聚合步骤中用二元分类损失替换交叉熵损失，在温和假设下实现一致的序数对齐，并获得该设置中首个多项式有限样本误差界。

Result: 在基于数字孪生的LLM对齐现实模拟中，sign estimator显著减少了模拟人物面板上的偏好扭曲，将（角度）估计误差降低了近35%，与真实人口偏好的不一致性从12%降至8%。

Conclusion: sign estimator方法在保持现有LLM对齐管道实现简单性的同时，优于明确建模用户异质性并需要跟踪个体级偏好数据的面板数据启发式方法。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [20] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 该研究开发了一个条件深度学习模型，通过结合个体的社会基础设施韧性(SIR)和空间上下文，来预测破坏性事件后个体移动模式的变化。


<details>
  <summary>Details</summary>
Motivation: 预测破坏性事件前个体移动模式的变化具有挑战性，因为缺乏衡量个体异质性社会基础设施韧性的方法，复杂交互关系未被充分捕捉，且个体级移动数据稀疏不适合传统预测方法。

Method: 构建条件深度学习模型，将个体的社会基础设施韧性(SIR)与局部空间上下文相结合，使用大规模稀疏个体级数据来捕捉个体移动模式与空间环境的复杂关系。

Result: 实验表明，结合个体的SIR和空间上下文能增强模型预测事件后个体移动模式的能力。条件模型能捕捉到具有相似事前移动模式但SIR不同的个体在移动模式上的差异性变化。

Conclusion: 该研究证明了将个体社会基础设施韧性纳入预测模型的重要性，能够更准确地预测破坏性事件后个体移动模式的变化，特别是对于具有相似事前行为但韧性不同的个体。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [21] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*İbrahim Oğuz Çetinkaya,İ. Esra Büyüktahtakın,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 利用大型语言模型发现新的启发式算法来解决单机总延误问题，提出了EDDC和MDDC两种算法，在100-500个作业的大规模问题上表现优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在解决单机总延误问题时性能有限，而精确方法在大规模问题上计算不可行，需要开发新的高效启发式算法。

Method: 通过人类与LLM协作发现新的启发式算法EDDC和MDDC，基于经典的EDD和MDD规则，使用混合整数规划作为基准进行严格评估。

Result: 对于超过100个作业的实例，EDDC改进了经典EDD规则，MDDC始终优于传统启发式方法，在更大更复杂的实例上与精确方法保持竞争力。

Conclusion: 人类与LLM协作可以有效产生可扩展的高性能启发式算法，用于NP难约束组合优化问题，即使在资源有限的情况下也能有效配置。

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [22] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast是一个结构化、模块化的跨域时间序列预测框架，通过将时间序列分解为季节性和趋势组件，分别使用轻量级投影模块和基于离散扩散的机制进行建模，在多个领域实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨域时间序列预测中面临的领域特定趋势变化和不一致周期性模式的问题，现有方法将时间序列视为未分化的序列，没有显式解耦其内在结构组件。

Method: 提出OneCast框架：1）将时间序列分解为季节性和趋势组件；2）季节性组件通过轻量级投影模块使用可解释基函数重建周期模式；3）趋势组件通过语义感知分词器编码为离散标记，使用掩码离散扩散机制进行推断；4）两个分支输出结合生成最终预测。

Result: 在八个领域上的广泛实验表明，OneCast在大多数情况下优于最先进的基线方法。

Conclusion: 通过显式解耦时间序列的结构组件并分别建模，OneCast能够有效捕捉季节性模式同时跟踪领域特定趋势，在跨域时间序列预测任务中表现出色。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [23] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer是一个基于聚类的日志分析聊天机器人，结合大语言模型和机器学习算法，简化日志分析过程，在多个任务上比现有LLM聊天机器人性能提升39%-68%。


<details>
  <summary>Details</summary>
Motivation: 系统日志是网络安全的核心，但分析大量多样化日志数据面临高成本、缺乏专业知识和时间限制等挑战，许多组织难以进行基本分析。

Method: 采用模块化架构，包括路由器、日志识别器、日志解析器和搜索工具，结合聚类方法和LLM技术，解决LLM的上下文窗口限制和结构化文本处理能力不足的问题。

Result: 在四个不同领域日志和多种任务上的评估显示，相比ChatGPT、ChatPDF和NotebookLM等最先进的LLM聊天机器人，性能显著提升39%-68%，鲁棒性增强，使用ROUGE-1分数的四分位距减少93%。

Conclusion: 该框架通过模块化设计增强LLM在结构化文本分析中的能力，提高准确性和鲁棒性，为网络安全专家和非技术用户提供有价值的资源。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [24] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 比较经典模型和机器学习模型在电动汽车跟车行为建模中的表现，发现随机森林模型在所有场景下都优于物理模型。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，需要理解其驾驶行为以提高交通安全和开发智能驾驶系统。

Method: 使用经典模型（IDM、OVM、OVRV、CACC）和随机森林回归器，基于真实世界EV跟随ICE车辆的数据集进行建模和参数校准。

Result: 随机森林模型表现最佳，RMSE分别为0.0046（中等间距）、0.0016（长间距）和0.0025（超长间距）；经典模型中CACC表现最好，长间距RMSE为2.67。

Conclusion: 机器学习模型在模拟EV行为和分析混合自动驾驶交通动态方面具有重要价值。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [25] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens是一个透明的AI病理学助手，让医生能用自然语言提问并获得带有视觉证明的分析报告，保持医生主导地位的同时提高诊断效率和信心。


<details>
  <summary>Details</summary>
Motivation: 为了让医生真正信任AI，需要解决AI黑盒问题，让医生能理解AI的推理过程，就像咨询同事一样。

Method: 开发了HistoLens系统，允许病理学家用自然语言提问，系统将问题转换为精确查询，提供结构化报告和热图视觉证明，并训练AI专注于患者组织而忽略背景噪声。

Result: 创建了一个工作流程，病理学家保持专家主导地位，使用可信赖的AI助手验证见解，实现更快、更自信的诊断。

Conclusion: 透明的AI系统如HistoLens能够建立医生对AI的信任，通过提供可解释的推理过程，使AI成为真正的协作伙伴而非黑盒工具。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [26] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: OpsAgent是一个轻量级、自演化的多智能体系统，用于云系统事件管理，通过免训练数据处理和多智能体协作实现透明诊断，并在OPENRCA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统手动事件管理劳动密集且易错，现有自动化方法泛化性差、可解释性有限且部署成本高，阻碍实际应用。

Method: 采用免训练数据处理将异构可观测数据转换为结构化文本描述，结合多智能体协作框架实现透明诊断，并引入双自演化机制支持持续能力增长。

Result: 在OPENRCA基准测试中达到最先进性能，证明系统具有泛化性、可解释性、成本效益和自演化能力。

Conclusion: OpsAgent是一个实际可部署且可持续的解决方案，适用于真实云系统的长期运维。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [27] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: 提出了一个自动化框架，用于从半结构化知识源生成高难度、可用于训练的多跳问答数据集，解决现有数据集稀缺且不适合监督微调的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答数据集稀缺且主要设计用于评估，不适合监督微调或强化学习训练。手动构建非平凡可检索问题成本高昂且难以扩展，这成为训练高能力检索推理代理的关键数据瓶颈。

Method: 系统包含三个步骤：(i)通过自然语言推理关系分类和多样性感知扩展生成多样化逻辑标记的证据簇；(ii)应用逆向问题构建来组合间接线索；(iii)通过多模型共识过滤和结构化约束分解的两步评估流程确保质量。

Result: 该系统能够规模化生成复杂、检索抵抗但可验证的问题，适用于监督微调和强化学习训练，同时大幅减少人工标注工作量并保持强评估基准的难度特征。

Conclusion: 该自动化框架有效解决了多跳问答数据集稀缺的问题，为训练高能力检索推理代理提供了可扩展的解决方案。

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [28] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: BLM₁是一个多模态空间基础模型，通过两阶段训练实现跨空间传输、跨任务学习和跨具身泛化，在数字和物理任务中均优于现有模型家族。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在数字-物理空间和不同具身之间的泛化能力差，缺乏统一的跨空间、跨具身模型。

Method: 采用两阶段训练：第一阶段通过精选数字语料注入具身知识，保持语言能力；第二阶段通过意图桥接接口训练策略模块，提取MLLM的高级语义来指导控制。

Result: 单个BLM₁实例在数字任务中提升约6%，在物理任务中提升约3%，优于MLLMs、ELLMs、VLAs和GMLMs四个模型家族。

Conclusion: BLM₁成功实现了跨数字-物理空间的无缝操作，并在不同具身和任务间展现出强大的泛化能力。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [29] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: UniPlanner是首个用于自动驾驶决策的多数据集集成规划框架，通过三个创新组件实现跨数据集统一学习，提升规划鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法局限于单数据集训练，限制了规划鲁棒性。研究发现不同数据集中的车辆轨迹分布和历史-未来相关性具有显著一致性，这为多数据集集成提供了基础。

Method: 1. HFTDN：通过历史轨迹相似性检索相关未来轨迹，生成跨数据集规划指导
2. GFTM：无梯度轨迹映射器，学习鲁棒的历史-未来相关性，转化为通用规划先验
3. S2D范式：训练时选择性抑制规划先验实现鲁棒学习，推理时充分利用先验最大化性能

Result: UniPlanner实现了跨数据集统一学习，能够生成更鲁棒的规划轨迹，在多个数据集上表现出优越的规划性能。

Conclusion: 该研究证明了多数据集集成在自动驾驶规划中的可行性，UniPlanner框架通过创新的组件设计，有效提升了规划系统的鲁棒性和泛化能力。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [30] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: 提出了Memory-Driven GUI Agent (MGA)，通过"先观察后决策"原则解决GUI代理中的历史轨迹依赖和局部探索偏差问题，在多个基准测试中表现出更强的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理存在两个主要问题：对历史轨迹的依赖导致错误传播放大，以及"决策优先、观察滞后"机制忽略了关键界面线索。需要开发更鲁棒和泛化的GUI交互方法。

Method: MGA将GUI交互重构为"先观察后决策"原则，每个步骤建模为独立的环境状态三元组：当前截图、任务无关的空间信息、动态更新的结构化记忆。

Result: 在OSworld基准测试、真实桌面应用（Chrome、VSCode、VLC）和跨任务迁移实验中，MGA相比最先进基线方法在鲁棒性、泛化性和效率方面取得了显著提升。

Conclusion: MGA通过观察优先的方法有效解决了GUI代理中的核心挑战，为开发更可靠的桌面和网页界面自动化系统提供了新思路。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [31] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow是一个自动化web-agent驱动的管道，用于大规模服务器发现、数据合成和模型训练，显著提升了LLM在MCP生态系统中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有MCP研究覆盖服务器少，依赖昂贵的人工整理，缺乏训练支持，阻碍了实际部署。需要克服这些限制来推进LLM在真实世界MCP环境中的能力。

Method: 采用自动化web-agent驱动管道，从1166个服务器和11536个工具中收集和过滤数据，生成68733个高质量指令-函数调用对和6439个轨迹。

Result: 实验证明MCP-Flow在MCP工具选择、函数调用生成和代理任务性能方面表现优异，远超先前工作的规模和多样性。

Conclusion: MCP-Flow为推进LLM代理在真实世界MCP环境中的熟练度提供了可扩展的基础，代码已公开可用。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [32] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文针对MCTS中抽象算法的样本效率问题，提出了多种内部抽象策略来替代随机平局决胜规则，并在多数环境和参数设置中表现优于随机策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的样本效率问题可以通过状态/动作抽象来解决，但现有抽象算法（如pruned OGA）在处理同一抽象节点内多个动作时使用随机平局决胜规则，这可能导致性能不佳。

Method: 提出并实证评估了多种替代的内部抽象策略，用于处理同一抽象节点内多个动作的UCB值相同情况。

Result: 多个提出的内部抽象策略在大多数环境和参数设置中表现优于随机策略。

Conclusion: 内部抽象策略的选择对MCTS抽象算法的性能有显著影响，需要针对具体环境选择合适的策略。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [33] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 提出了一种基于LLM内部行为的自我指示方法，通过计算输入问题与输出推理路径之间的相关矩阵秩来判断推理正确性，无需外部资源即可有效验证LLM输出。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法依赖外部资源（如训练验证器或复杂提示），导致计算开销大且仅适用于特定领域，需要一种更高效通用的LLM输出验证方法。

Method: 利用LLM内部行为，计算输入问题与输出推理路径之间的相关矩阵秩作为推理正确性指标，设计简单的即插即用Self-Indicator方法对候选推理路径进行重加权。

Result: 在多个不同规模和家族的LLM上实验表明，该方法能以超过75%的准确率区分正确与错误推理路径，在三个推理基准上的准确率提升超过8%。

Conclusion: LLM的内部行为已隐含其推理路径的可信度信息，Self-Indicator方法提供了一种高效、无需外部资源的LLM输出验证方案。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [34] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种用于判断预测的多智能体框架，通过不同智能体对主张真实性产生分歧并收集正反证据，使用定量双极论证框架表示，结合多种基于LLM的智能体方法来提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 判断预测作为基于人类判断的未来事件预测任务，可视为主张验证的一种形式。现有方法在证据收集和解释性方面存在局限，需要更有效的多视角证据整合框架。

Method: 提出多智能体主张验证框架，使用三种基于LLM的智能体：ArgLLM（生成和评估QBAF）、RbAM（基于关系论证挖掘生成QBAF）、RAG-ArgLLM（结合检索增强生成的ArgLLM扩展）。

Result: 在两个标准判断预测数据集上的实验表明，结合多个智能体的证据可以提升预测准确性，特别是在三个智能体的情况下，同时为主张验证提供可解释的证据组合。

Conclusion: 多智能体框架能有效提高判断预测的准确性，特别是通过三个智能体的证据组合，同时保持了预测过程的可解释性。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [35] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文探讨了生成式大语言模型在传播学研究内容分析中的应用，指出其相比传统方法具有显著优势，但面临七大挑战，并提出了应对这些挑战的最佳实践指南。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式大语言模型在传播学内容分析中展现出巨大潜力，能够超越众包工作者和训练有素的编码员，但其在研究方法论中的整合仍不充分，需要系统指导来确保研究质量。

Method: 通过综合新兴研究，提出应对七大关键挑战的最佳实践指南：代码本开发、提示工程、模型选择、参数调优、迭代优化、可靠性验证和性能提升。

Result: 研究发现生成式大语言模型能够以更低成本和时间完成内容分析任务，并能解码隐含意义和上下文信息，但需要系统的方法论指导来确保研究质量。

Conclusion: 本文为传播学研究者提供了使用生成式大语言模型进行内容分析的全面指南，旨在使该方法更易用，并确保符合有效性、可靠性、可重复性和研究伦理等学科质量标准。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [36] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于可预测性-可计算性-稳定性(PCS)原则的多智能体系统，用于提升LLM驱动的数据科学系统的可信度和鲁棒性，在多个数据集上优于现有端到端系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学系统仅依赖模型内部推理，缺乏科学和理论原则指导，在处理噪声和复杂真实数据集时可信度和鲁棒性不足。

Method: 基于PCS原则构建多智能体系统，采用模块化工作流程处理数据清洗、特征工程、建模和评估，每个阶段由专门智能体负责，结合扰动分析、单元测试和模型验证。

Result: 在9个不同特征的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents持续优于AutoKaggle和DataInterpreter等最先进的端到端数据科学系统。

Conclusion: 验证了将PCS原则嵌入LLM驱动的数据科学自动化的可行性，为构建更可信和鲁棒的自动化数据科学系统提供了有效途径。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [37] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: 提出了Brain-like Space概念，这是一个统一几何空间，可将不同模态的AI模型映射到人类功能脑网络进行比较，揭示了模型脑相似度的连续几何分布。


<details>
  <summary>Details</summary>
Motivation: 现有脑-AI对齐研究局限于特定输入和任务，缺乏比较不同模态AI模型内在组织的统一框架。

Method: 通过将AI模型的内在空间注意力拓扑组织映射到标准人类功能脑网络，构建Brain-like Space，分析151个Transformer模型。

Result: 发现模型在Brain-like Space中呈现连续的弧状几何分布，脑相似度与预训练范式和位置编码方案相关，且脑相似度与下游任务性能不完全一致。

Conclusion: Brain-like Space为跨领域智能提供了首个统一框架，揭示了连接机器与大脑的深层组织原则。

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [38] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Policy Cards是一种机器可读的部署层标准，用于表达AI代理的操作、监管和伦理约束，作为代理的组成部分在运行时执行约束要求。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI代理在部署和运行过程中需要遵循的操作、监管和伦理约束问题，提供一种将高层治理与工程实践结合的实际机制。

Method: 通过定义包含允许/拒绝规则、义务、证据要求和与NIST AI RMF、ISO/IEC 42001、欧盟AI法案等保证框架映射的规范层，扩展现有的透明度工件。

Result: 每个Policy Card可以自动验证、版本控制，并链接到运行时执行或持续审计管道，为自主代理提供可验证的合规性。

Conclusion: Policy Cards为多代理生态系统中的分布式保证奠定了基础，实现了大规模可问责的自主性。

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [39] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion是一个高效推理框架，通过依赖感知的查询分解和逻辑并行内容扩展，解决了LLM在实时Web应用中推理效率与质量的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到实时Web应用中面临关键挑战：需要同时满足高质量复杂推理和低延迟高吞吐量的交互服务要求。现有方法难以兼顾效率与质量。

Method: Orion将查询推理分解为两个协同阶段：1) 通过检索增强的少样本提示生成逻辑结构化的关键点；2) 基于依赖图并行扩展内容以确保逻辑一致性。还引入了管道调度机制实现跨查询并行。

Result: 实验显示，Orion相比基线实现了4.33倍的token生成速度提升和3.42倍的答案延迟降低，同时通过显式建模点间依赖关系将推理质量提升了18.75%。

Conclusion: Orion框架成功解决了LLM推理在Web服务中的效率与质量平衡问题，通过创新的分解和并行策略显著提升了性能。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [40] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: 提出了APTBench框架，将真实世界智能体任务转换为适合基础模型的多选或文本补全问题，用于评估预训练阶段的智能体能力


<details>
  <summary>Details</summary>
Motivation: 当前预训练基准主要关注孤立静态技能，无法反映模型的智能体能力；而智能体基准通常针对后训练模型，基础模型难以支持多轮任务执行，需要能在预训练阶段评估智能体潜力的基准

Method: 将真实世界智能体任务和成功轨迹转换为多选或文本补全问题，聚焦规划和行动等核心智能体能力，覆盖软件工程和深度研究等关键场景

Result: 相比现有通用基准，APTBench能更准确地预测模型作为智能体的下游性能，同时比后训练的全规模端到端评估更轻量且成本效益更高

Conclusion: APTBench填补了预训练阶段智能体能力评估的空白，为更有效地指导模型训练提供了工具

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [41] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: 提出了MobileRisk-Live动态沙盒环境和OS-Sentinel混合安全检测框架，用于检测移动AI代理的安全风险，在多个指标上比现有方法提升10%-30%。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的计算机代理在移动平台等数字环境中展现出类人能力，但其潜在的安全操作风险（如系统破坏和隐私泄露）引发了重大担忧，而检测这些安全风险在复杂移动环境中的挑战尚未得到充分探索。

Method: 提出了OS-Sentinel混合安全检测框架，结合形式化验证器检测显式系统级违规和基于VLM的上下文判断器评估上下文风险和代理行为，并构建了MobileRisk-Live动态沙盒环境和安全检测基准。

Result: 实验表明OS-Sentinel在多个指标上比现有方法提升10%-30%，为开发更安全可靠的自主移动代理提供了关键见解。

Conclusion: 该研究为移动代理安全研究奠定了基础，提出的混合检测框架能有效识别移动环境中的安全风险，促进更安全可靠的自主移动代理发展。

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [42] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 本研究比较了多个大型语言模型（GPT、Claude、DeepSeek等）的逻辑和抽象推理能力，使用8个定制推理问题，并与人类表现进行基准测试，揭示了LLMs在演绎推理方面的显著差异和困难。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力对于推进人工智能发展至关重要，因为这超越了单纯的语言任务表现，涉及理解模型是否真正理解信息、进行推理以及以逻辑有效的方式得出结论。

Method: 使用8个定制设计的推理问题，比较了多个LLM（包括GPT、Claude、DeepSeek、Gemini等）的逻辑和抽象推理技能，并将LLM结果与人类在相同任务上的表现进行基准测试。

Result: 研究揭示了LLMs在演绎推理方面存在显著差异，并指出了LLMs在推理方面存在困难的领域。

Conclusion: 大型语言模型在逻辑和抽象推理能力方面与人类表现存在差距，特别是在演绎推理方面表现出明显的困难，这为未来模型改进提供了重要方向。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [43] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: Law in Silico是一个基于LLM的法律社会模拟框架，能够模拟个体决策和立法、裁决、执法等制度机制，实验表明该框架能有效复现宏观犯罪趋势并为法律理论验证提供支持。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中的法律实验成本高昂或难以实施，利用AI系统模拟法律社会成为验证和发展法律理论的有效替代方案。LLM凭借其世界知识和角色扮演能力，是构建法律社会模拟的理想基础。

Method: 提出了Law in Silico框架，这是一个基于LLM的智能体框架，能够模拟包含个体决策以及立法、裁决、执法等制度机制的法律场景。

Result: 实验比较模拟犯罪率与现实数据发现，LLM智能体能够很大程度上复现宏观犯罪趋势，并提供与现实观察一致的见解。微观层面模拟显示，运作良好、透明且适应性强的法律系统能更好地保护弱势个体的权利。

Conclusion: LLM能够有效模拟法律系统，为法律理论验证和行政支持提供可行方案，同时揭示了良好法律系统对弱势群体保护的重要性。

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [44] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 提出两种架构模式：DOM转换模式和超媒体功能识别模式，用于从结构化数据构建软件代理的世界模型，解决HTML冗余和API静态集成问题。


<details>
  <summary>Details</summary>
Motivation: 软件代理需要从结构化数据构建可操作的世界模型，但面临两个关键挑战：原始HTML过于冗长难以直接处理，硬编码API集成无法适应服务演化。

Method: DOM转换模式将冗长的原始DOM提炼为紧凑的任务相关表示；超媒体功能识别模式通过解析标准化语义描述来动态发现和集成未知Web服务。

Result: 这两种模式共同为工程化代理提供了稳健框架，能够高效构建和维护准确的世界模型。

Conclusion: 该模式语言使代理能够在Web及其扩展资源上实现可扩展、自适应和互操作的自动化。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [45] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 该论文提出了一种改进脉冲神经网络（SNN）在强化学习中训练的方法，通过分析替代梯度斜率设置和引入特权引导策略，显著提升了SNN在真实世界机器人控制任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在复杂控制任务中面临的两个关键挑战：非可微脉冲神经元需要替代梯度但优化特性不明确，以及SNN的状态动态需要在序列上训练但在强化学习中早期训练序列长度有限，阻碍网络度过预热期。

Method: 系统分析替代梯度斜率设置，发现较浅斜率能增加深层梯度幅度但减少与真实梯度对齐；提出利用特权引导策略来引导学习过程，同时保持与环境的在线交互；结合自适应斜率调度方法。

Result: 在真实世界无人机位置控制任务中，平均回报达到400分，显著优于先前技术（如行为克隆和TD3BC，最多只能达到-200分），训练和最终部署性能提升2.1倍。

Conclusion: 这项工作推进了对SNN中替代梯度学习的理论理解，并为神经形态控制器在实际机器人系统中的训练方法提供了实用进展。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [46] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种成本效益高的两阶段流程，通过跨任务示例和基于图的标签传播方法减少对LLM数据标注的依赖，从而降低ICL的标注成本。


<details>
  <summary>Details</summary>
Motivation: 为新颖或困难任务收集高质量示例成本高昂且劳动密集，需要减少对LLM数据标注的依赖。

Method: 两阶段流程：首先利用跨任务示例提示LLM对少量目标任务实例进行伪标注，然后引入基于图的标签传播方法将标签信息传播到剩余目标示例中，无需额外LLM查询。

Result: 在五个任务上的实验表明，该方法在降低标注成本的同时实现了强劲的性能。

Conclusion: 该流程结合了跨任务监督的灵活性和无LLM传播的可扩展性，为ICL提供了一种成本效益高的解决方案。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [47] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 本文提出了一种以数据为中心的设计范式，将医疗数据生态系统作为生成式AI系统的基础，通过有效的数据处理管道支持医疗AI应用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域具有巨大潜力，但需要深入理解医疗任务和可实现的目标。当前部署需要更好的数据管理方法。

Method: 重新定位数据生命周期，构建医疗数据生态系统作为基础，支持多模态数据的集成、表示和检索，包括语义向量搜索和上下文查询。

Result: 该生态系统能够为上游模型组件提供高质量多模态数据进行预训练和微调，同时作为知识检索后端支持任务特定推理。

Conclusion: 这种以数据为中心的方法能够实现高质量、有效的医疗AI系统部署，改善医疗服务交付。

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [48] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT是一个用于合成多轮工具调用训练数据的新框架，通过环境-API图交互、高级工具查询合成和引导迭代链来解决现有方法的不足，在BFCL基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法（如随机环境采样或多智能体角色扮演）不足以在真实世界环境中生成高质量的多轮工具调用训练数据，存在目标模型训练、工具架构隔离和多轮逻辑依赖等实际挑战。

Method: FunReason-MT框架采用：1）环境-API图交互收集多样化高质量轨迹；2）高级工具查询合成简化困难查询构建；3）引导迭代链生成复杂思维链。

Result: 在Berkeley Function-Calling Leaderboard (BFCLv3)上，基于FunReason-MT生成数据构建的4B模型在同等规模模型中达到最先进性能，超越了大多数闭源模型。在BFCLv4上的进一步性能改进证实了其可靠性。

Conclusion: FunReason-MT为智能体学习提供了可靠且强大的数据源，能够有效解决多轮函数调用数据合成的复杂性障碍。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [49] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 该综述分析了约40篇关于基础模型在作物定点病害管理中的应用文献，重点讨论了大型语言模型和视觉语言模型在自适应学习、强化学习和数字孪生框架中的作用，揭示了该领域的最新发展趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和深度学习在实时计算机视觉中的快速发展，作物定点病害管理从手工特征提取发展到大规模自动特征学习。基础模型的出现为处理作物病害数据集提供了全新的方式，能够整合视觉和文本数据，解释症状文本，推理症状与管理的关系，并为种植者和教育者提供交互式问答支持。

Method: 通过筛选约40篇相关文献，重点分析大型语言模型和视觉语言模型在自适应学习、强化学习和数字孪生框架中的应用，特别关注其在智能喷洒系统中的实现方式。

Result: 主要发现：(a) 基础模型在2023-24年获得广泛关注；(b) 视觉语言模型发展快于大型语言模型，发表量增长5-10倍；(c) 强化学习和自适应学习在智能喷洒中仍处于早期阶段；(d) 结合强化学习的数字孪生可虚拟模拟定点喷洒；(e) 解决模拟到现实的差距对实际部署至关重要；(f) 人机协作仍然有限；(g) 多模态基础模型与实时反馈将推动下一代定点病害管理。

Conclusion: 基础模型正在变革作物定点病害管理领域，特别是视觉语言模型展现出巨大潜力。未来需要解决模拟到现实的差距，加强人机协作，并发展多模态基础模型与实时反馈系统，以实现更有效的田间病害管理。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [50] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: OrchDAG是一个合成数据生成管道，将工具执行建模为具有可控复杂度的有向无环图，用于多轮工具交互的基准测试和强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多忽略了多轮工具交互的复杂性，需要更好的基准测试和训练方法来处理复杂的工具使用场景。

Method: 引入OrchDAG合成数据生成管道，将工具执行建模为有向无环图，并提出基于图的奖励来增强RLVR训练。

Result: 实验表明该数据集提供了具有挑战性但可解决的基准，所提出的奖励与GRPO风格算法结合时效果显著。

Conclusion: 在多轮工具使用中，利用拓扑结构和数据复杂性至关重要，图结构奖励能有效提升模型性能。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [51] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出了一个通过构建工具和文档知识图谱来增强范例工件生成的框架，通过融合工具图谱和领域知识图谱来改进工具增强的推理和规划。


<details>
  <summary>Details</summary>
Motivation: 为了揭示和利用工具与文档之间的依赖关系，从而提升范例工件的生成质量，需要将工具的结构化依赖与程序性知识相结合。

Method: 从工具模式构建工具知识图谱，同时从内部文档和SOP构建补充知识图谱，然后将两者融合，采用深度-稀疏集成策略对齐结构工具依赖与程序知识。

Result: 实验表明该统一框架能有效建模工具交互并改进规划生成。

Conclusion: 将工具图谱与领域知识图谱链接对于工具增强的推理和规划具有显著益处。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [52] [Flexible Intelligent Layered Metasurfaces for Downlink Multi-user MISO Communications](https://arxiv.org/abs/2510.24190)
*Hong Niu,Jiancheng An,Chau Yuen*

Main category: cs.IT

TL;DR: 提出了一种柔性智能分层超表面（FILM）架构，通过使用形状可控的柔性超表面层替代传统刚性超表面，在保持信号处理性能的同时显著减少了所需层数，解决了传统堆叠智能超表面功率衰减严重的问题。


<details>
  <summary>Details</summary>
Motivation: 传统堆叠智能超表面（SIMs）依赖均匀层间距且需要深度堆叠来确保处理能力，导致严重的功率衰减。为了解决这个问题，需要一种能够减少层数同时保持性能的新型架构。

Method: 开发了两层FILM辅助的多用户多输入单输出系统，通过交替优化方法解决信道拟合问题，包括闭式相移更新和基于梯度下降的形状优化。

Result: 仿真结果表明，所提出的透射式FILM架构相比传统的七层SIMs，在总速率上实现了超过200%的提升，在误码率上获得了超过7 dB的增益。

Conclusion: FILM架构通过使用柔性超表面层动态调整传输系数矩阵，在显著减少所需层数的同时保持了信号处理性能，为解决传统SIMs的功率衰减问题提供了有效方案。

Abstract: Stacked intelligent metasurfaces (SIMs) have recently gained attention as a
paradigm for wave-domain signal processing with reduced reliance on costly
radio-frequency (RF) chains. However, conventional SIMs rely on uniform
inter-layer spacing and require deep stacking to ensure processing capability,
resulting in severe power attenuation in practice. To address this issue, we
propose a flexible intelligent layered metasurface (FILM) architecture
consisting of two shape-controllable flexible metasurface layers. By replacing
rigid metasurfaces with flexible ones in both layers, the transmission
coefficient matrix can be dynamically adjusted, significantly decreasing the
number of required layers while maintaining signal processing performance.
Firstly, we develop a two-layer FILM-assisted multi-user multiple-input
single-output (MU-MISO) system, wherein we formulate a channel fitting problem
aimed at reducing the difference between the FILM-induced and target channels.
Then, we solve this non-convex problem by employing an alternating optimization
(AO) method, featuring closed-form phase shift updates and a gradient
descent-based shape optimization. Furthermore, we analyze the upper bound on
sum-rate and the complexity of computation to provide insights into design
trade-offs. Finally, simulation results demonstrated that the proposed
transmissive FILM architecture achieves over 200\% improvement in sum-rate and
more than 7 dB bit-error rate (BER) gain compared to the conventional
seven-layer SIMs.

</details>


### [53] [What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements](https://arxiv.org/abs/2510.24215)
*Vishal Halder,Alexandre Reiffers-Masson,Abdeldjalil Aïssa-El-Bey,Gugan Thoppe*

Main category: cs.IT

TL;DR: 该论文研究了在存在q-稀疏对抗性噪声的情况下，从观测数据y=Ax*+e中恢复x*的最小可恢复集合。主要结果表明，最佳可恢复集合是x*+ker(U)，其中U是投影矩阵，且该集合可以通过最小化y-Ax的ℓ0范数来构造性恢复。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常基于对矩阵A或x*的强结构假设（如受限等距性、稀疏性）来实现精确恢复，但对于任意A和x*的可恢复性问题仍然开放。本文旨在解决在任意矩阵和未知向量情况下，面对稀疏对抗性噪声时的可恢复性界限问题。

Method: 通过分析所有可能删除2q行后得到的A子矩阵的行空间交集，定义投影矩阵U，并证明最小可恢复集合为x*+ker(U)。同时证明了所有最小化y-Ax的ℓ0范数的x都位于该集合中，从而提供了构造性恢复方法。

Result: 证明了在q-稀疏对抗性噪声下，从y中可一致恢复的最小集合是x*+ker(U)，其中U是投影到所有删除2q行后的A子矩阵行空间交集的唯一投影矩阵。

Conclusion: 该工作为任意矩阵和未知向量在稀疏对抗性噪声下的可恢复性提供了理论界限，并给出了构造性恢复方法，填补了现有研究在无强结构假设情况下的空白。

Abstract: Let $\mathbf{A} \in \mathbb{R}^{m \times n}$ be an arbitrary, known matrix
and $\mathbf{e}$ a $q$-sparse adversarial vector. Given $\mathbf{y} =
\mathbf{A} x^* + \mathbf{e}$ and $q$, we seek the smallest set containing
$x^*$-hence the one conveying maximal information about $x^*$-that is uniformly
recoverable from $\mathbf{y}$ without knowing $\mathbf{e}$. While exact
recovery of $x^*$ via strong (and often impractical) structural assumptions on
$\mathbf{A}$ or $x^*$ (for example, restricted isometry, sparsity) is well
studied, recoverability for arbitrary $\mathbf{A}$ and $x^*$ remains open. Our
main result shows that the best that one can hope to recover is $x^* +
\ker(\mathbf{U})$, where $\mathbf{U}$ is the unique projection matrix onto the
intersection of rowspaces of all possible submatrices of $\mathbf{A}$ obtained
by deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the
$\ell_0$-norm of $\mathbf{y} - \mathbf{A} x$ lies in $x^* + \ker(\mathbf{U})$,
which then gives a constructive approach to recover this set.

</details>


### [54] [Precoding-free Hierarchical Rate-Splitting Multiple Access via Stacked Intelligent Metasurface](https://arxiv.org/abs/2510.24246)
*Hiroaki Hashida,Boya Di*

Main category: cs.IT

TL;DR: 提出了一种基于堆叠智能超表面(SIM)的无数字预编码分层速率分割多址接入(HRSMA)架构，通过波域处理实现干扰管理和用户分离，显著降低硬件复杂度和基带计算需求。


<details>
  <summary>Details</summary>
Motivation: 密集多天线无线网络中的干扰管理是核心瓶颈，需要在高频谱效率和用户公平性的同时降低硬件复杂度。

Method: 基站仅执行标量功率分配，多层SIM作为波域处理器通过非线性波前重构实现用户空间分离和干扰抑制。采用交替优化算法联合优化SIM相移、功率分配和用户分组。

Result: 仿真结果表明，SIM辅助的HRSMA在频谱效率和公平性方面相比混合波束成形和无预编码基准方案有显著提升，能以更少的有源天线实现相当或更优的最小速率。

Conclusion: SIM辅助的HRSMA有望成为超6G网络的低成本、高能效和可扩展解决方案。

Abstract: Interference management is a central bottleneck in dense multi-antenna
wireless networks. Therefore, in this study, we present a digital
precoding-free hierarchical rate-splitting multiple access (HRSMA) architecture
assisted by a stacked intelligent metasurface (SIM) to achieve high spectral
efficiency and user fairness with reduced hardware complexity. In the proposed
system, the base station performs only scalar power allocation, while a
multi-layer SIM acts as a wave-domain processor that spatially separates users
and mitigates interference via nonlinear wavefront reconfiguration. This design
eliminates the need for digital or hybrid precoding, drastically reducing the
baseband computations. A joint optimization problem is formulated to maximize
the minimum user rate by jointly optimizing SIM phase shifts, power allocation,
and user grouping. To efficiently solve the resulting non-convex problem, an
alternating optimization algorithm is developed, combining simultaneous
perturbation stochastic approximation (SPSA) for SIM configuration and power
control with clustering-based grouping refinement. Simulation results
demonstrate that the proposed SIM-aided HRSMA achieves substantial gains in
both spectral efficiency and fairness compared to hybrid beamforming and
non-precoding baselines. Specifically, SIM-aided HRSMA attains comparable or
superior minimum rates with significantly fewer active antennas by exploiting
the additional wave-domain degrees of freedom provided by multi-layer SIMs.
These findings highlight the potential of SIM-aided HRSMA as a low-cost,
energy-efficient, and scalable solution for beyond-6G networks.

</details>


### [55] [Joint Active and Passive Beamforming with Sensing-Assisted Discrete Phase Shifts for Dual-RIS ISAC Systems](https://arxiv.org/abs/2510.24480)
*Qing Xue,Yun Lan,Jiajia Guo,Qianbin Chen,Shaodan Ma*

Main category: cs.IT

TL;DR: 本文研究了半被动双RIS辅助的ISAC系统，通过联合主动和被动波束成形解决最大-最小用户SINR问题，提升系统性能并确保用户公平性。


<details>
  <summary>Details</summary>
Motivation: 针对6G需求，研究双RIS辅助的集成感知与通信系统，旨在通过优化波束成形来增强系统性能和用户公平性。

Method: 首先利用双RIS进行用户角度估计简化问题求解，然后开发高效的交替优化算法，包括半定松弛和二分法解决发射波束成形优化子问题，并对RIS离散相移采用感知辅助方法约束搜索空间。

Result: 数值仿真结果表明，所提算法性能接近理想连续相移基准，优于传统离散相移优化算法，相比单RIS系统有显著提升。

Conclusion: 双RIS辅助的ISAC系统通过提出的优化算法能够有效提升系统性能，在离散相移约束下仍能获得接近连续相移的性能表现。

Abstract: Targeting the requirements of 6G, this paper investigates a semi-passive
dual-reconfigurable intelligent surface (RIS)-assisted integrated sensing and
communication (ISAC) system, tackling the max-min user
signal-to-interference-plus-noise ratio (SINR) problem via joint active and
passive beamforming to enhance system performance and ensure user fairness.
Addressing this challenge, we first utilize dual RISs for user angle estimation
to simplify the solution process of the formulated problem, an efficient
alternating optimization algorithm is then developed. Specifically,
semi-definite relaxation and the bisection method are employed to solve the
transmit beamforming optimization subproblem. For the RIS discrete phase
shifts, a sensing-assisted approach is adopted to constrain the optimization
search space, with two distinct low-complexity search strategies introduced for
different RIS sizes. Numerical simulation results demonstrate that the proposed
algorithm achieves performance close to the ideal continuous phase shift
benchmark, outperforms conventional discrete phase shift optimization
algorithms, and exhibits a significant improvement over single-RIS systems.

</details>


### [56] [Dual-Mind World Models: A General Framework for Learning in Dynamic Wireless Networks](https://arxiv.org/abs/2510.24546)
*Lingyi Wang,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.IT

TL;DR: 提出了一种基于双心智世界模型的强化学习框架，通过结合模式驱动的系统1和逻辑驱动的系统2组件，在毫米波V2X场景中优化完整性加权信息年龄，显著提高了数据效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型无关RL和模型基础RL的方法在无线网络中数据效率低、目光短浅，无法泛化到新的网络状态，因为它们只捕捉统计模式而非底层物理和逻辑。在复杂无线网络的高动态性和长期规划需求下，这些限制尤为突出。

Method: 提出双心智世界模型框架，包含模式驱动的系统1组件和逻辑驱动的系统2组件，学习无线网络的动态和逻辑，通过端到端可微分想象轨迹进行链路调度，而非依赖环境交互获取的无线数据。

Result: 在基于Sionna的真实模拟器上进行广泛实验，结果显示该方法在数据效率上显著提升，在未见环境中表现出强大的泛化和适应能力，优于最先进的RL基线和仅使用系统1的世界模型方法。

Conclusion: 双心智世界模型框架通过结合模式识别和逻辑推理，有效解决了无线网络中RL方法的数据效率和泛化问题，为复杂动态环境中的长期规划提供了有效解决方案。

Abstract: Despite the popularity of reinforcement learning (RL) in wireless networks,
existing approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)
are data inefficient and short-sighted. Such RL-based solutions cannot
generalize to novel network states since they capture only statistical patterns
rather than the underlying physics and logic from wireless data. These
limitations become particularly challenging in complex wireless networks with
high dynamics and long-term planning requirements. To address these
limitations, in this paper, a novel dual-mind world model-based learning
framework is proposed with the goal of optimizing completeness-weighted age of
information (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive
psychology, the proposed dual-mind world model encompasses a pattern-driven
System 1 component and a logic-driven System 2 component to learn dynamics and
logic of the wireless network, and to provide long-term link scheduling over
reliable imagined trajectories. Link scheduling is learned through end-to-end
differentiable imagined trajectories with logical consistency over an extended
horizon rather than relying on wireless data obtained from environment
interactions. Moreover, through imagination rollouts, the proposed world model
can jointly reason network states and plan link scheduling. During intervals
without observations, the proposed method remains capable of making efficient
decisions. Extensive experiments are conducted on a realistic simulator based
on Sionna with real-world physical channel, ray-tracing, and scene objects with
material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency and achieves strong
generalization and adaptation to unseen environments, compared to the
state-of-the-art RL baselines, and the world model approach with only System 1.

</details>


### [57] [Feedback Lunch: Deep Feedback Codes for Wiretap Channels](https://arxiv.org/abs/2510.16620)
*Yingyao Zhou,Natasha Devroye,Onur Günlü*

Main category: cs.IT

TL;DR: 该论文提出了一种用于高斯窃听信道的种子模块化代码设计，通过结合通用哈希函数和基于反馈的学习代码，在信道输出反馈下实现了正保密率。


<details>
  <summary>Details</summary>
Motivation: 研究反向退化窃听信道，在没有信道反馈时保密容量为零，因此探索如何利用反馈来克服窃听者的安全优势。

Method: 采用种子模块化代码设计，结合通用哈希函数提供安全性，学习基于反馈的代码提供可靠性，研究通信可靠性与信息泄露之间的权衡。

Result: 反馈使合法方能够协商共享密钥，从而实现了正保密率，克服了窃听者的安全优势。

Conclusion: 研究结果启发了感知辅助安全通信的代码设计，可用于下一代集成感知与通信方法。

Abstract: We consider reversely-degraded wiretap channels, for which the secrecy
capacity is zero if there is no channel feedback. This work focuses on a seeded
modular code design for the Gaussian wiretap channel with channel output
feedback, combining universal hash functions for security and learned
feedback-based codes for reliability to achieve positive secrecy rates. We
study the trade-off between communication reliability and information leakage,
illustrating that feedback enables agreeing on a secret key shared between
legitimate parties, overcoming the security advantage of the wiretapper. Our
findings also motivate code designs for sensing-assisted secure communication,
to be used in next-generation integrated sensing and communication methods.

</details>
