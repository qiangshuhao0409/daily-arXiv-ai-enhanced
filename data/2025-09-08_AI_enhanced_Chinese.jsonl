{"id": "2509.04505", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04505", "abs": "https://arxiv.org/abs/2509.04505", "authors": ["Somtochukwu Azie", "Yiping Meng"], "title": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management", "comment": "16 Pages", "summary": "The integration of Artificial Intelligence (AI) into construction project\nmanagement (CPM) is accelerating, with Large Language Models (LLMs) emerging as\naccessible decision-support tools. This study aims to critically evaluate the\nethical viability and reliability of LLMs when applied to the ethically\nsensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods\nresearch design was employed, involving the quantitative performance testing of\ntwo leading LLMs against twelve real-world ethical scenarios using a novel\nEthical Decision Support Assessment Checklist (EDSAC), and qualitative analysis\nof semi-structured interviews with 12 industry experts to capture professional\nperceptions. The findings reveal that while LLMs demonstrate adequate\nperformance in structured domains such as legal compliance, they exhibit\nsignificant deficiencies in handling contextual nuance, ensuring\naccountability, and providing transparent reasoning. Stakeholders expressed\nconsiderable reservations regarding the autonomous use of AI for ethical\njudgments, strongly advocating for robust human-in-the-loop oversight. To our\nknowledge, this is one of the first studies to empirically test the ethical\nreasoning of LLMs within the construction domain. It introduces the EDSAC\nframework as a replicable methodology and provides actionable recommendations,\nemphasising that LLMs are currently best positioned as decision-support aids\nrather than autonomous ethical agents.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u9879\u76ee\u7ba1\u7406\u4e2d\u7684\u4f26\u7406\u51b3\u7b56\u652f\u6301\u80fd\u529b\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u9700\u8981\u4eba\u7c7b\u76d1\u7763", "motivation": "\u8bc4\u4f30LLMs\u5728\u5efa\u7b51\u9879\u76ee\u7ba1\u7406\u8fd9\u79cd\u9ad8\u98ce\u9669\u3001\u4f26\u7406\u654f\u611f\u9886\u57df\u7684\u53ef\u9760\u6027\u548c\u4f26\u7406\u53ef\u884c\u6027", "method": "\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff1a\u91cf\u5316\u6d4b\u8bd5\u4e24\u4e2a\u9886\u5148LLMs\u572812\u4e2a\u771f\u5b9e\u4f26\u7406\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528EDSAC\u68c0\u67e5\u5355\uff1b\u8d28\u6027\u5206\u679012\u4f4d\u884c\u4e1a\u4e13\u5bb6\u7684\u534a\u7ed3\u6784\u8bbf\u8c08", "result": "LLMs\u5728\u6cd5\u5f8b\u9075\u5faa\u7b49\u7ed3\u6784\u5316\u9886\u57df\u8868\u73b0\u8fc5\u901f\uff0c\u4f46\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u7ec6\u8282\u3001\u786e\u4fdd\u8d1f\u8d23\u4efb\u548c\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677", "conclusion": "LLMs\u76ee\u524d\u4ec5\u9002\u5408\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u800c\u975e\u81ea\u4e3b\u4f26\u7406\u51b3\u7b56\u8005\uff0c\u5fc5\u987b\u6709\u4eba\u7c7b\u5728\u73af\u76d1\u7763"}}
{"id": "2509.04642", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix.", "AI": {"tldr": "Maestro\u662f\u4e00\u4e2a\u6846\u67b6\u65e0\u5173\u7684LLM\u667a\u80fd\u4f53\u6574\u4f53\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u8054\u5408\u641c\u7d22\u56fe\u7ed3\u6784\u548c\u8282\u70b9\u914d\u7f6e\u6765\u6700\u5927\u5316\u667a\u80fd\u4f53\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u5668\u4e3b\u8981\u8c03\u6574\u914d\u7f6e\u800c\u4fdd\u6301\u56fe\u7ed3\u6784\u56fa\u5b9a\uff0c\u65e0\u6cd5\u89e3\u51b3\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u7684holistic\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMaestro\u6846\u67b6\uff0c\u8054\u5408\u641c\u7d22\u667a\u80fd\u4f53\u7684\u56fe\u7ed3\u6784\uff08\u6a21\u5757\u548c\u4fe1\u606f\u6d41\uff09\u548c\u8282\u70b9\u914d\u7f6e\uff08\u6a21\u578b\u3001\u63d0\u793a\u8bcd\u3001\u5de5\u5177\u7b49\uff09\uff0c\u5229\u7528\u53cd\u5c04\u6027\u6587\u672c\u53cd\u9988\u6765\u4f18\u5148\u7f16\u8f91\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "result": "\u5728IFBench\u548cHotpotQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaestro\u5e73\u5747\u9886\u5148MIPROv2 12%\u3001GEPA 4.9%\u3001GEPA+Merge 4.86%\uff1b\u5373\u4f7f\u5728\u4ec5\u4f18\u5316\u63d0\u793a\u8bcd\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9886\u5148\uff0c\u4e14\u4f7f\u7528\u6bd4GEPA\u66f4\u5c11\u7684rollout\u6b21\u6570\u3002", "conclusion": "\u8054\u5408\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u641c\u7d22\u80fd\u591f\u89e3\u51b3\u5355\u7eaf\u63d0\u793a\u8bcd\u8c03\u4f18\u65e0\u6cd5\u89e3\u51b3\u7684\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.04646", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.04646", "abs": "https://arxiv.org/abs/2509.04646", "authors": ["Philippe J. Giabbanelli", "Ameeta Agrawal"], "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization", "comment": "Accepted at the AAAI 2025 Fall Symposium Series. November 6-8, 2025,\n  Arlington, VA, USA", "summary": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u8bc6\u522b\u5065\u5eb7\u6a21\u62df\u4e2d\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5e76\u6307\u5bfcLLMs\u751f\u6210\u5b9a\u5236\u5316\u7684\u89e3\u91ca\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u901a\u7528\u6458\u8981\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u7b49\u5efa\u6a21\u4e0e\u4eff\u771f\u65b9\u6cd5\u5728\u5065\u5eb7\u51b3\u7b56\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u590d\u6742\u6027\u4f7f\u5f97\u5229\u76ca\u76f8\u5173\u8005\u96be\u4ee5\u4f7f\u7528\u3002\u867d\u7136LLMs\u53ef\u4ee5\u5c06\u6a21\u62df\u8f93\u51fa\u8f6c\u5316\u4e3a\u6587\u672c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e00\u5200\u5207\u7684\u6458\u8981\uff0c\u65e0\u6cd5\u53cd\u6620\u4e34\u5e8a\u533b\u751f\u3001\u653f\u7b56\u5236\u5b9a\u8005\u3001\u60a3\u8005\u7b49\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u591a\u6837\u5316\u4fe1\u606f\u9700\u6c42\u548c\u98ce\u683c\u504f\u597d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff1a\u9996\u5148\u83b7\u53d6\u4e0d\u540c\u5065\u5eb7\u5229\u76ca\u76f8\u5173\u8005\u7684\u89e3\u91ca\u9700\u6c42\u548c\u98ce\u683c\u504f\u597d\uff0c\u7136\u540e\u901a\u8fc7\u53ef\u63a7\u5c5e\u6027\u8c03\u4f18\u7b49\u65b9\u5f0f\u4f18\u5316LLMs\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u6700\u540e\u901a\u8fc7\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u5b9a\u5236\u5316\u6458\u8981\u7684\u751f\u6210\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u6846\u67b6\u6765\u7cfb\u7edf\u8bc6\u522b\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u5e76\u6307\u5bfcLLMs\u751f\u6210\u9488\u5bf9\u5065\u5eb7\u6a21\u62df\u7684\u5b9a\u5236\u5316\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u5065\u5eb7\u6a21\u62df\u89e3\u91ca\u4e2d\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u901a\u8fc7\u7cfb\u7edf\u7406\u89e3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u5e76\u5229\u7528LLMs\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\uff0c\u6709\u671b\u66f4\u597d\u5730\u5b9e\u73b0\u5efa\u6a21\u4e0e\u4eff\u771f\u65b9\u6cd5\u5728\u5065\u5eb7\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.04676", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04676", "abs": "https://arxiv.org/abs/2509.04676", "authors": ["Sasha Mitts"], "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria", "comment": "4 figures, 6 pages, presented at CHI 2025 Workshop on Human-AI\n  Interaction for Augmented Reasoning", "summary": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation.", "AI": {"tldr": "\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4ef7\u6807\u51c6\u589e\u5f3aAI\u6a21\u578b\u8bc4\u6d4b\uff0c\u91cd\u70b9\u5173\u6ce8\u7269\u7406\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u7684\u8bc4\u4f30", "motivation": "\u4f20\u7edfAI\u6307\u6807\u65e0\u6cd5\u5168\u9762\u6293\u53d6AI\u6a21\u578b\u7684\u7ec6\u81f4\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4ef7\u6807\u51c6\u6765\u63d0\u5347\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5e94\u7528\u6027", "method": "\u57fa\u4e8ePerception Test\u548cOpenEQA\u6307\u6807\uff0c\u8fdb\u884c\u6df1\u5ea6\u8bbf\u8c08\u548c\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u8bc6\u522b\u5173\u952e\u8ba4\u77e5\u6280\u80fd\uff08\u4f18\u5148\u7ea7\u3001\u8bb0\u5fc6\u3001\u8bc6\u522b\u3001\u4e0a\u4e0b\u6587\u5316\uff09", "result": "\u53d1\u73b0\u4eba\u4eec\u8ba4\u4e3aAI\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u5171\u60c5\u80fd\u529b\uff0c\u4f46\u5bf9AI\u8868\u73b0\u6709\u9ad8\u671f\u671b\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u8bc4\u4f30\u6846\u67b6", "conclusion": "\u7528\u6237\u4e2d\u5fc3\u7684AI\u8bc4\u4f30\u5bf9AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5357\uff0c\u4ee5\u5b9e\u73b0AI\u80fd\u529b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5bf9\u9f50"}}
{"id": "2509.04625", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2509.04625", "abs": "https://arxiv.org/abs/2509.04625", "authors": ["Zhenzhou Qi", "Chung-Hsuan Tung", "Zhihui Gao", "Tingjun Chen"], "title": "NEXUS: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute", "comment": null, "summary": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave\n(mmWave) spectrum, imposes stringent demands on the flexibility, scalability,\nand efficiency of baseband processing. While virtualized Radio Access Networks\n(vRANs) enable dynamic spectrum sharing across cells, compute resource\nallocation for baseband processing, especially in multi-cell deployments with\nheterogeneous workloads, remains underexplored. In this paper, we present\nNEXUS, the first system to realize real-time, virtualized multi-cell mmWave\nbaseband processing on a single server with heterogeneous compute resources.\nNEXUS integrates software-based digital signal processing pipelines with\nhardware-accelerated LDPC decoding, and introduces a novel framework for\nsharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions\n(VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based\nmodel that predicts the most energy-efficient resource allocation for the given\ncell configuration with microsecond-level inference latency and high accuracy.\nFor multi-cell scenarios, NEXUS introduces a power-aware scheduler that\nincorporates a lightweight contention model to adjust resource allocation\nstrategies under concurrent execution. Through extensive evaluation across\nvarious Frequency Range 2 (FR2) cell configurations, we show that NEXUS\nsupports up to 16 concurrent cells under full load, achieving 5.37Gbps\naggregate throughput, while reducing the multi-cell scheduling search space by\norders of magnitude. These results demonstrate that virtualized, resource-aware\nbaseband processing is both practical and efficient for next-generation vRAN\nsystems.", "AI": {"tldr": "NEXUS\u662f\u9996\u4e2a\u5728\u5355\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u865a\u62df\u5316\u591a\u5c0f\u533a\u6beb\u7c73\u6ce2\u57fa\u5e26\u5904\u7406\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u667a\u80fd\u8d44\u6e90\u8c03\u5ea6\uff0c\u652f\u630116\u4e2a\u5e76\u53d1\u5c0f\u533a\uff0c\u8fbe\u52305.37Gbps\u603b\u541e\u5410\u91cf\uff0c\u5927\u5e45\u51cf\u5c11\u8c03\u5ea6\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "5G NR\u6beb\u7c73\u6ce2\u9891\u8c31\u7684\u5feb\u901f\u90e8\u7f72\u5bf9\u57fa\u5e26\u5904\u7406\u7684\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u63d0\u51fa\u4e25\u683c\u8981\u6c42\uff0c\u800c\u591a\u5c0f\u533a\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u96c6\u6210\u8f6f\u4ef6\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u6d41\u6c34\u7ebf\u4e0e\u786c\u4ef6\u52a0\u901fLDPC\u89e3\u7801\uff0c\u901a\u8fc7\u865a\u62df\u51fd\u6570\u5171\u4eabIntel ACC100 eASIC\uff1b\u5355\u5c0f\u533a\u4f7f\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u9884\u6d4b\u80fd\u6548\u6700\u4f18\u8d44\u6e90\u5206\u914d\uff0c\u591a\u5c0f\u533a\u91c7\u7528\u529f\u7387\u611f\u77e5\u8c03\u5ea6\u5668\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ade\u4e89\u6a21\u578b\u3002", "result": "\u652f\u6301\u6700\u591a16\u4e2a\u5e76\u53d1\u5168\u8d1f\u8f7d\u5c0f\u533a\uff0c\u5b9e\u73b05.37Gbps\u603b\u541e\u5410\u91cf\uff0c\u591a\u5c0f\u533a\u8c03\u5ea6\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u5fae\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "conclusion": "\u865a\u62df\u5316\u3001\u8d44\u6e90\u611f\u77e5\u7684\u57fa\u5e26\u5904\u7406\u5bf9\u4e0b\u4e00\u4ee3vRAN\u7cfb\u7edf\u65e2\u5b9e\u7528\u53c8\u9ad8\u6548\uff0c\u4e3a\u591a\u5c0f\u533a\u6beb\u7c73\u6ce2\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04731", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5c42\u6b21\u5316\u4e16\u754c\u6a21\u578b\u6765\u89e3\u51b3\u590d\u6742\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u56f0\u96be\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u51fa\u4f7f\u7528LLM\u52a8\u6001\u751f\u6210\u5c42\u6b21\u5316\u4efb\u52a1\u6846\u67b6\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u5f53\u524dAI\u53d1\u5c55\u4e2d\uff0c\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6a21\u578b\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u663e\u5f0f\u7684\u4e16\u754c\u6a21\u578b\u53d1\u5c55\u6ede\u540e\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u957f\u65f6\u57df\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u3002\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u9ad8\u4fdd\u771f\u4f46\u7ed3\u6784\u5e73\u5766\u7684\u6a21\u62df\u5668\u4e2d\u9762\u4e34\u63a2\u7d22\u7a7a\u95f4\u96be\u4ee5\u5904\u7406\u548c\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c42\u6b21\u5316\u811a\u624b\u67b6\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u76ee\u6807\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u7ba1\u7406\u7684\u5b50\u76ee\u6807\u3002\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u751f\u6210\u8fd9\u79cd\u5c42\u6b21\u5316\u811a\u624b\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u5373\u65f6\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u5185\u5728\u8bfe\u7a0b\u3001\u5bc6\u96c6\u6709\u610f\u4e49\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u7ec4\u5408\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5bf92024\u5e74\u591a\u667a\u80fd\u4f53\u8db3\u7403\u7814\u7a76\u7684\u7cfb\u7edf\u56de\u987e\uff0c\u53d1\u73b0\u660e\u786e\u7684\u8d8b\u52bf\u662f\u5c06\u7b26\u53f7\u5316\u548c\u5c42\u6b21\u5316\u65b9\u6cd5\u4e0e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9690\u5f0f\u6216\u663e\u5f0f\u5730\u6784\u5efa\u57fa\u4e8e\u4efb\u52a1\u7684\u4e16\u754c\u6a21\u578b\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5177\u6709\u663e\u5f0f\u3001\u8bed\u8a00\u53ef\u914d\u7f6e\u4efb\u52a1\u5c42\u7684\u73af\u5883\uff0c\u53ef\u4ee5\u5f25\u5408\u4f4e\u7ea7\u53cd\u5e94\u884c\u4e3a\u4e0e\u9ad8\u7ea7\u6218\u7565\u56e2\u961f\u534f\u4f5c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u8bad\u7ec3\u4e0b\u4e00\u4ee3\u667a\u80fd\u667a\u80fd\u4f53\u521b\u5efa\u5f3a\u5927\u4e14\u53ef\u63a8\u5e7f\u7684\u6846\u67b6\u3002"}}
{"id": "2509.04695", "categories": ["cs.NI", "C.2.1; C.2"], "pdf": "https://arxiv.org/pdf/2509.04695", "abs": "https://arxiv.org/abs/2509.04695", "authors": ["Lars Herschbach", "Damien Rossi", "Sina Keshvadi"], "title": "Path Dynamics in a Deployed Path-Aware Network: A Measurement Study of SCIONLab", "comment": "19 pages, 8 figures. Submitted to the Computer Communications journal", "summary": "Path-aware networks promise enhanced performance and resilience through\nmultipath transport, but a lack of empirical data on their real-world dynamics\nhinders the design of effective protocols. This paper presents a longitudinal\nmeasurement study of the SCION architecture on the global SCIONLab testbed,\ncharacterizing the path stability, diversity, and performance crucial for\nprotocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic\nenvironment, with significant control-plane churn and short path lifetimes in\nparts of the testbed. We identify and characterize path discrepancy, a\nphenomenon where routing policies create asymmetric path availability between\nendpoints. Furthermore, we observe a performance trade-off where concurrent\nmultipath transmissions can improve aggregate throughput but may degrade the\nlatency and reliability of individual paths. These findings demonstrate that\nprotocols such as MPQUIC should explicitly account for high churn and path\nasymmetry, challenging common assumptions in multipath protocol design.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5bf9SCION\u7f51\u7edc\u7684\u7efc\u5411\u6d4b\u91cf\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u52a8\u6001\u7279\u6027\uff0c\u5305\u62ec\u8def\u5f84\u4e0d\u7a33\u5b9a\u6027\u3001\u4e0d\u5bf9\u79f0\u6027\u548c\u6027\u80fd\u4ea4\u6362\uff0c\u4e3a\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u5b9e\u8bc1\u6570\u636e\u3002", "motivation": "\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u867d\u7136\u63a8\u8bf4\u80fd\u63d0\u9ad8\u6027\u80fd\u548c\u5f39\u6027\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u8bc1\u6570\u636e\u963b\u788d\u4e86\u9ad8\u6548\u534f\u8bae\u7684\u8bbe\u8ba1\u3002\u9700\u8981\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u6765\u7406\u89e3\u7f51\u7edc\u52a8\u6001\u7279\u6027\u3002", "method": "\u5728\u5168\u7403SCIONLab\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7efc\u5411\u6d4b\u91cf\u7814\u7a76\uff0c\u5206\u6790SCION\u67b6\u6784\u7684\u8def\u5f84\u7a33\u5b9a\u6027\u3001\u591a\u6837\u6027\u548c\u6027\u80fd\u7279\u5f81\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u8def\u5f84QUIC\u534f\u8bae\u7684\u9700\u6c42\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u73af\u5883\u5f88\u52a8\u6001\uff0c\u63a7\u5236\u5e73\u9762\u53d8\u5316\u660e\u663e\uff0c\u8def\u5f84\u5bff\u547d\u77ed\u3002\u8bc6\u522b\u4e86\u8def\u5f84\u5dee\u5f02\u73b0\u8c61\uff08\u8def\u7531\u7b56\u7565\u5bfc\u81f4\u7aef\u70b9\u95f4\u4e0d\u5bf9\u79f0\u8def\u5f84\u53ef\u7528\u6027\uff09\u3002\u540c\u65f6\u591a\u8def\u5f84\u4f20\u8f93\u80fd\u63d0\u9ad8\u603b\u901f\u7387\u4f46\u53ef\u80fd\u964d\u4f4e\u5355\u4e2a\u8def\u5f84\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u591a\u8def\u5f84\u534f\u8bae\uff08\u5982MPQUIC\uff09\u5e94\u660e\u786e\u8003\u8651\u9ad8\u53d8\u5316\u7387\u548c\u8def\u5f84\u4e0d\u5bf9\u79f0\u6027\uff0c\u8fd9\u5bf9\u4f20\u7edf\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u7684\u5047\u8bbe\u6784\u6210\u6311\u6218\u3002\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u5bf9\u534f\u8bae\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.04791", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04791", "abs": "https://arxiv.org/abs/2509.04791", "authors": ["Yuan Sui", "Yanming Zhang", "Yi Liao", "Yu Gu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Bryan Hooi"], "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking", "comment": "arXiv admin note: text overlap with arXiv:2508.21365", "summary": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.", "AI": {"tldr": "WiA-LLM\u662f\u4e00\u4e2a\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6574\u5408\u5047\u8bbe\u5206\u6790(WIA)\u548c\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u53cd\u9988\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e3b\u52a8\u601d\u8003\u80fd\u529b\uff0c\u80fd\u591f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9884\u6d4b\u672a\u6765\u72b6\u6001\u53d8\u5316", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u53ea\u80fd\u88ab\u52a8\u5904\u7406\u4fe1\u606f\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u63a2\u7d22\u5047\u8bbe\u672a\u6765\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u884c\u52a8\u524d\u9884\u6d4b\u6f5c\u5728\u540e\u679c\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u6574\u5408\u5047\u8bbe\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u53d8\u91cf\u8bc4\u4f30\u5047\u8bbe\u573a\u666f\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u73af\u5883\u53cd\u9988\uff0c\u52a8\u6001\u6a21\u62df\u6bcf\u4e2a\u6f5c\u5728\u884c\u52a8\u7684\u7ed3\u679c\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u672a\u6765\u72b6\u6001\u800c\u4e0d\u4ec5\u4ec5\u662f\u54cd\u5e94\u5f53\u524d\u6761\u4ef6", "result": "\u5728\u300a\u738b\u8005\u8363\u8000\u300b\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cWiA-LLM\u5728\u9884\u6d4b\u6e38\u620f\u72b6\u6001\u53d8\u5316\u65b9\u9762\u8fbe\u523074.2%\u7684\u51c6\u786e\u7387\uff08\u6bd4\u57fa\u7ebf\u63d0\u5347\u4e24\u500d\uff09\uff0c\u5728\u9ad8\u96be\u5ea6\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6b63\u5f0f\u63a2\u7d22\u548c\u6574\u5408\u5047\u8bbe\u5206\u6790\u80fd\u529b\u5230\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\uff0c\u4ee3\u8868\u4e86\u5411\u4e3b\u52a8\u63a8\u7406\u7684\u6839\u672c\u6027\u8fdb\u6b65\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5065\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6"}}
{"id": "2509.04792", "categories": ["cs.NI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04792", "abs": "https://arxiv.org/abs/2509.04792", "authors": ["Erik Rye", "Dave Levin", "Robert Beverly"], "title": "Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition", "comment": null, "summary": "IPv4 NAT has limited the spread of IoT botnets considerably by\ndefault-denying bots' incoming connection requests to in-home devices unless\nthe owner has explicitly allowed them. As the Internet transitions to majority\nIPv6, however, residential connections no longer require the use of NAT. This\npaper therefore asks: has the transition from IPv4 to IPv6 ultimately made\nresidential networks more vulnerable to attack, thereby empowering the next\ngeneration of IPv6-based IoT botnets? To answer this question, we introduce a\nlarge-scale IPv6 scanning methodology that, unlike those that rely on AI, can\nbe run on low-resource devices common in IoT botnets. We use this methodology\nto perform the largest-scale measurement of IPv6 residential networks to date,\nand compare which devices are publicly accessible to comparable IPv4 networks.\nWe were able to receive responses from 14.0M distinct IPv6 addresses inside of\nresidential networks (i.e., not the external-facing gateway), in 2,436 ASes\nacross 118 countries. These responses come from protocols commonly exploited by\nIoT botnets (including telnet and FTP), as well as protocols typically\nassociated with end-user devices (including iPhone-Sync and IPP). Comparing to\nIPv4, we show that we are able to reach more printers, iPhones, and smart\nlights over IPv6 than full IPv4-wide scans could. Collectively, our results\nshow that NAT has indeed acted as the de facto firewall of the Internet, and\nthe v4-to-v6 transition of residential networks is opening up new devices to\nattack.", "AI": {"tldr": "IPv6\u8fc7\u6e21\u4f7f\u4f4f\u5b85\u7f51\u7edc\u66f4\u6613\u53d7\u653b\u51fb\uff0cNAT\u66fe\u662f\u4e92\u8054\u7f51\u7684\u9ed8\u8ba4\u9632\u706b\u5899\u3002\u901a\u8fc7\u5927\u89c4\u6a21IPv6\u626b\u63cf\u53d1\u73b0\uff0c\u76f8\u6bd4IPv4\uff0cIPv6\u7f51\u7edc\u4e2d\u66f4\u591a\u6253\u5370\u673a\u3001iPhone\u548c\u667a\u80fd\u706f\u7b49\u8bbe\u5907\u53ef\u76f4\u63a5\u8bbf\u95ee\uff0c\u4e3a\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u521b\u9020\u4e86\u65b0\u673a\u4f1a\u3002", "motivation": "\u7814\u7a76IPv4\u5411IPv6\u8fc7\u6e21\u662f\u5426\u4f7f\u4f4f\u5b85\u7f51\u7edc\u66f4\u6613\u53d7\u653b\u51fb\uff0c\u8bc4\u4f30NAT\u4f5c\u4e3a\u9ed8\u8ba4\u9632\u706b\u5899\u7684\u4f5c\u7528\u6d88\u5931\u540e\u5bf9\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u5728\u4f4e\u8d44\u6e90IoT\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u5927\u89c4\u6a21IPv6\u626b\u63cf\u65b9\u6cd5\uff0c\u5bf9\u4f4f\u5b85\u7f51\u7edc\u8fdb\u884c\u4e86\u53f2\u4e0a\u6700\u5927\u89c4\u6a21\u7684IPv6\u6d4b\u91cf\uff0c\u5e76\u4e0eIPv4\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4ece118\u4e2a\u56fd\u5bb62,436\u4e2aAS\u7684\u4f4f\u5b85\u7f51\u7edc\u4e2d\u53d1\u73b01,400\u4e07\u4e2a\u53ef\u8bbf\u95ee\u7684IPv6\u5730\u5740\uff0c\u6db5\u76d6telnet\u3001FTP\u7b49\u6613\u53d7\u653b\u51fb\u534f\u8bae\uff0c\u4ee5\u53caiPhone-Sync\u3001IPP\u7b49\u7ec8\u7aef\u8bbe\u5907\u534f\u8bae\u3002\u76f8\u6bd4IPv4\uff0cIPv6\u7f51\u7edc\u4e2d\u66f4\u591a\u6253\u5370\u673a\u3001iPhone\u548c\u667a\u80fd\u706f\u8bbe\u5907\u53ef\u76f4\u63a5\u8bbf\u95ee\u3002", "conclusion": "NAT\u786e\u5b9e\u8d77\u5230\u4e86\u4e92\u8054\u7f51\u9ed8\u8ba4\u9632\u706b\u5899\u7684\u4f5c\u7528\uff0cIPv4\u5411IPv6\u7684\u8fc7\u6e21\u6b63\u5728\u4f7f\u65b0\u8bbe\u5907\u66b4\u9732\u5728\u653b\u51fb\u98ce\u9669\u4e4b\u4e0b\uff0c\u4e3a\u4e0b\u4e00\u4ee3IPv6\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u63d0\u4f9b\u4e86\u6761\u4ef6\u3002"}}
{"id": "2509.04809", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04809", "abs": "https://arxiv.org/abs/2509.04809", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.", "AI": {"tldr": "TalkToAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u95e8\u5316LLM\u4ee3\u7406\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5f25\u5408\u590d\u6742RL\u7b56\u7565\u4e0e\u9886\u57df\u4e13\u5bb6\u4e4b\u95f4\u7684\u7406\u89e3\u9e3f\u6c9f\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60(XRL)\u65b9\u6cd5\u5b58\u5728\u89e3\u91ca\u7ed3\u679c\u53ef\u7406\u89e3\u6027\u6709\u9650\u3001\u5de5\u5177\u8986\u76d6\u5b64\u7acb\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7528\u6237\u4e0d\u786e\u5b9a\u4f7f\u7528\u54ea\u79cd\u5de5\u5177\uff0c\u9700\u8981\u66f4\u900f\u660e\u3001\u4ea4\u4e92\u6027\u5f3a\u7684\u89e3\u91ca\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e94\u667a\u80fd\u4f53\u67b6\u6784\uff08\u534f\u8c03\u5668\u3001\u89e3\u91ca\u5668\u3001\u7f16\u7801\u5668\u3001\u8bc4\u4f30\u5668\u3001\u8c03\u8bd5\u5668\uff09\uff0c\u81ea\u52a8\u5c06\u7528\u6237\u67e5\u8be2\u6620\u5c04\u5230\u76f8\u5173XRL\u5de5\u5177\uff0c\u63d0\u4f9b\u5173\u952e\u72b6\u6001\u53d8\u91cf\u3001\u9884\u671f\u7ed3\u679c\u6216\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u80fd\u4ece\u5b9a\u6027\u884c\u4e3a\u63cf\u8ff0\u63a8\u5bfc\u66ff\u4ee3\u573a\u666f\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b0\u7b56\u7565\u3002", "result": "\u5728\u56db\u6c34\u7bb1\u8fc7\u7a0b\u63a7\u5236\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff0c\u6210\u529f\u9ad8\u7cbe\u5ea6\u6620\u5c04\u7528\u6237\u67e5\u8be2\u5230XRL\u4efb\u52a1\uff0c\u7f16\u7801\u5668-\u8c03\u8bd5\u5668\u4ea4\u4e92\u6700\u5c0f\u5316\u53cd\u4e8b\u5b9e\u751f\u6210\u5931\u8d25\uff0c\u5b9a\u6027\u8bc4\u4f30\u786e\u8ba4\u80fd\u6709\u6548\u89e3\u91ca\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u5728\u95ee\u9898\u57df\u4e2d\u60c5\u5883\u5316\u5176\u542b\u4e49\u3002", "conclusion": "TalkToAgent\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53LLM\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e3a\u89e3\u51b3XRL\u5de5\u5177\u788e\u7247\u5316\u548c\u89e3\u91ca\u53ef\u7406\u89e3\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.04847", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04847", "abs": "https://arxiv.org/abs/2509.04847", "authors": ["Mukul Singh", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory", "comment": "9 pages", "summary": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u8fed\u4ee3\u56f0\u5f92\u56f0\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u5408\u4f5c\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u6027\u80fd\u53ef\u4e0e\u6700\u4f73\u7ecf\u5178\u7b56\u7565\u76f8\u6bd4\u62fc\uff0c\u5e76\u663e\u793a\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u591a\u65b9\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u957f\u671f\u5408\u4f5c\u4e0e\u7ade\u4e89\u884c\u4e3a\uff0c\u8865\u5145\u4ee5\u5f80\u7814\u7a76\u5bf9\u77ed\u671f\u6e38\u620f\u7406\u8bba\u60c5\u5883\u7684\u504f\u91cd", "method": "\u91c7\u7528\u8fed\u4ee3\u56f0\u5f92\u56f0\u5883(IPD)\u6846\u67b6\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4e0e240\u79cd\u7ecf\u5178\u7b56\u7565\u8fdb\u884cAxelrod\u98ce\u683c\u7684\u8d5b\u4f1a\u6bd4\u8d5b\uff0c\u5e76\u8bbe\u8ba1\u63a7\u5236\u5b9e\u9a8c\u89c2\u5bdf\u7b56\u7565\u5207\u6362\u65f6\u7684\u53cd\u5e94", "result": "\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e0e\u6700\u4f73\u7ecf\u5178\u7b56\u7565\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u663e\u793a\u51fa\u53cb\u5584\u6027\u3001\u53ef\u523a\u6fc0\u6027\u5483\u5bcc\u4e8e\u6027\u7b49\u7279\u5f81\uff0c\u80fd\u5728\u51e0\u8f6e\u5185\u5feb\u901f\u68c0\u6d4b\u5e76\u9002\u5e94\u5bf9\u624b\u7b56\u7565\u53d8\u5316", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u6027\u63cf\u8ff0\u8bed\u8a00\u6a21\u578b\u957f\u671f\u5408\u4f5c\u884c\u4e3a\u7684\u7814\u7a76\uff0c\u4e3a\u4eca\u540e\u66f4\u590d\u6742\u7684\u4eba\u5de5\u667a\u80fd-\u4eba\u7c7b\u6df7\u5408\u793e\u4f1a\u73af\u5883\u7814\u7a76\u5960\u5b9a\u57fa\u7840"}}
{"id": "2509.04871", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04871", "abs": "https://arxiv.org/abs/2509.04871", "authors": ["Krittanon Kaewtawee", "Wachiravit Modecrua", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales", "comment": "10 pages, 4 figures", "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7535\u8bdd\u901a\u8bdd\u5f55\u97f3\u4e2d\u514b\u9686\u5bf9\u8bdd\u5f0f\u8bed\u97f3AI\u52a9\u624b\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u97f3\u8bc6\u522b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u7ba1\u7406\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u80fd\u591f\u6a21\u4eff\u4eba\u7c7b\u9500\u552e\u4eba\u5458\u8868\u73b0\u7684AI\u7ec4\u7ec7\u3002", "motivation": "\u8bed\u8a00\u548c\u8bed\u97f3\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u5efa\u7acb\u81ea\u4e3b\u8bed\u97f3\u52a9\u624b\u53d8\u5f97\u53ef\u884c\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5728\u5ba2\u670d\u548c\u533b\u7597\u9886\u57df\u53ef\u4ee5\u81ea\u52a8\u5316\u91cd\u590d\u4efb\u52a1\u3001\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u4f9b24\u5c0f\u65f6\u652f\u6301\u3002", "method": "\u4ece\u7535\u8bdd\u901a\u8bdd\u5f55\u97f3\u4e2d\u514b\u9686\u5bf9\u8bddAI\u7ec4\u7ec7\uff0c\u6574\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u7ba1\u7406\u5668\u548c\u6587\u672c\u8f6c\u8bed\u97f3(TTS)\u5408\u6210\u6280\u672f\uff0c\u6784\u5efa\u6d41\u5f0f\u63a8\u7406\u6d41\u6c34\u7ebf\uff0c\u5b66\u4e60\u9876\u7ea7\u4eba\u7c7b\u4ee3\u8868\u7684\u7ed3\u6784\u5316\u811a\u672c\u3002", "result": "\u5728\u5305\u62ec\u4ecb\u7ecd\u3001\u4ea7\u54c1\u6c9f\u901a\u3001\u9500\u552e\u9a71\u52a8\u3001\u5f02\u8bae\u5904\u7406\u548c\u6210\u4ea4\u7b4922\u4e2a\u6807\u51c6\u7684\u8bc4\u4ef7\u4e2d\uff0c\u76f2\u6d4b\u663e\u793aAI\u7ec4\u7ec7\u5728\u5e38\u89c4\u901a\u8bdd\u65b9\u9762\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\uff0c\u4f46\u5728\u8bf4\u670d\u548c\u5f02\u8bae\u5904\u7406\u65b9\u9762\u8f83\u5dee\u3002", "conclusion": "\u5206\u6790\u4e86AI\u7ec4\u7ec7\u7684\u4e0d\u8db3\u4e4b\u5904\u5e76\u5bf9\u63d0\u793a\u8fdb\u884c\u4e86\u7cbe\u70bc\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u7ecf\u9a8c\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6a21\u62df\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002"}}
{"id": "2509.04876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04876", "abs": "https://arxiv.org/abs/2509.04876", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "comment": "Accepted at EMNLP 2025 (Long Paper)", "summary": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.", "AI": {"tldr": "OSC\u662f\u4e00\u4e2a\u77e5\u8bc6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7Collaborator Knowledge Models\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u52a8\u6001\u611f\u77e5\u534f\u4f5c\u4f19\u4f34\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5b9e\u73b0\u6df1\u5ea6\u8ba4\u77e5\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u667a\u80fd\u4f53\u9009\u62e9\u548c\u7ed3\u679c\u805a\u5408\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u4e13\u5bb6\u667a\u80fd\u4f53\u95f4\u9ad8\u6548\u7684\u8bed\u8a00\u4ea4\u4e92\u548c\u6df1\u5ea6\u534f\u4f5c\u4ecd\u662f\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8ba4\u77e5\u534f\u540c\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faOSC\u6846\u67b6\u4f5c\u4e3a\u9009\u62e9\u548c\u805a\u5408\u4e4b\u95f4\u7684\u5173\u952e\u4e2d\u95f4\u5c42\uff0c\u5f15\u5165Collaborator Knowledge Models(CKM)\uff0c\u901a\u8fc7\u5b9e\u65f6\u8ba4\u77e5\u5dee\u8ddd\u5206\u6790\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u901a\u4fe1\u884c\u4e3a\uff08\u5305\u62ec\u5185\u5bb9\u7126\u70b9\u3001\u7ec6\u8282\u5c42\u6b21\u548c\u8868\u8fbe\u98ce\u683c\uff09\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOSC\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\uff0c\u5c06\"\u5e76\u884c\u5de5\u4f5c\u7684\u4e2a\u4f53\"\u8f6c\u53d8\u4e3a\"\u6df1\u5ea6\u534f\u4f5c\u7684\u8ba4\u77e5\u56e2\u961f\"\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u4f18\u5316\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u8fd8\u4e3aLLM\u667a\u80fd\u4f53\u4ea4\u4e92\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u8ba4\u77e5\u534f\u540c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.04908", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04908", "abs": "https://arxiv.org/abs/2509.04908", "authors": ["Hongyi Jing", "Jiafu Chen", "Chen Rao", "Ziqiang Dang", "Jiajie Teng", "Tianyi Chu", "Juncheng Mo", "Shuo Fang", "Huaizhong Lin", "Rui Lv", "Chenguang Ma", "Lei Zhao"], "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing", "comment": null, "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.", "AI": {"tldr": "SparkUI-Parser\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aefGUI\u89e3\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u5750\u6807\u5efa\u6a21\u548c\u62d2\u7edd\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u80fd\u591f\u89e3\u6790\u6574\u4e2a\u754c\u9762\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u5b9a\u4e49\u5143\u7d20\u96c6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MLLMs\u5728GUI\u611f\u77e5\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u57fa\u4e8e\u6587\u672c\u81ea\u56de\u5f52\u673a\u5236\u7684\u79bb\u6563\u5750\u6807\u5efa\u6a21\u5bfc\u81f4\u5b9a\u4f4d\u7cbe\u5ea6\u4f4e\u548c\u63a8\u7406\u901f\u5ea6\u6162\uff1b2\uff09\u53ea\u80fd\u5b9a\u4f4d\u9884\u5b9a\u4e49\u5143\u7d20\u96c6\uff0c\u65e0\u6cd5\u89e3\u6790\u6574\u4e2a\u754c\u9762\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u548c\u4e0b\u6e38\u4efb\u52a1\u652f\u6301\u3002", "method": "1\uff09\u57fa\u4e8e\u9884\u8bad\u7ec3MLLM\uff0c\u901a\u8fc7\u989d\u5916\u7684token\u8def\u7531\u5668\u548c\u5750\u6807\u89e3\u7801\u5668\u8fdb\u884c\u8fde\u7eed\u5750\u6807\u5efa\u6a21\uff1b2\uff09\u5f15\u5165\u57fa\u4e8e\u6539\u8fdb\u5308\u7259\u5229\u5339\u914d\u7b97\u6cd5\u7684\u62d2\u7edd\u673a\u5236\uff0c\u8bc6\u522b\u5e76\u62d2\u7edd\u4e0d\u5b58\u5728\u7684\u5143\u7d20\uff1b3\uff09\u6784\u5efaScreenParse\u57fa\u51c6\u6d4b\u8bd5\u6765\u7cfb\u7edf\u8bc4\u4f30GUI\u6a21\u578b\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728ScreenSpot\u3001ScreenSpot-v2\u3001CAGUI-Grounding\u548cScreenParse\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684SOTA\u65b9\u6cd5\u3002", "conclusion": "SparkUI-Parser\u901a\u8fc7\u8fde\u7eed\u5efa\u6a21\u548c\u62d2\u7edd\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u6563\u8f93\u51fa\u7279\u6027\u548c\u9010token\u751f\u6210\u8fc7\u7a0b\u7684\u9650\u5236\uff0c\u5728\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3aGUI\u89e3\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04926", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04926", "abs": "https://arxiv.org/abs/2509.04926", "authors": ["Barbara Gendron", "Ga\u00ebl Guibon", "Mathieu D'aquin"], "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts", "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)", "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u672c\u4f53\u7684\u65b9\u6cd5\u6765\u5f62\u5f0f\u5316\u5b9a\u4e49\u5bf9\u8bdd\u7279\u5f81\uff0c\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u7b26\u5c06\u5b9a\u6027\u6982\u5ff5\u8f6c\u5316\u4e3a\u5b9a\u91cf\u5b9a\u4e49\uff0c\u5e94\u7528\u4e8eCEFR\u8bed\u8a00\u719f\u7ec3\u5ea6\u63a7\u5236\uff0c\u901a\u8fc7\u5fae\u8c03\u6307\u5bfcLLM\u751f\u6210\u53ef\u63a7\u6587\u672c", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u65f6\u7684\u53ef\u63a7\u6027\u6311\u6218\uff0c\u786e\u4fdd\u53ef\u9884\u6d4b\u548c\u7528\u6237\u4e2a\u6027\u5316\u7684\u54cd\u5e94\uff0c\u7279\u522b\u662f\u5bf9\u5b9a\u6027\u5bf9\u8bdd\u7279\u5f81\u8fdb\u884c\u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u9700\u6c42", "method": "\u4f7f\u7528\u8bed\u8a00\u63cf\u8ff0\u7b26\u5c06\u5b9a\u6027\u6982\u5ff5\u8f6c\u5316\u4e3a\u5b9a\u91cf\u5b9a\u4e49\uff0c\u6784\u5efa\u63cf\u8ff0\u903b\u8f91\u5f62\u5f0f\u5316\u7684\u672c\u4f53\uff0c\u901a\u8fc7\u5fae\u8c03\u6307\u5bfcLLM\u8fdb\u884c\u53d7\u63a7\u6587\u672c\u751f\u6210", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u719f\u7ec3\u5ea6\u5b9a\u4e49\uff0c\u63d0\u9ad8\u4e86\u5bf9\u8bddAI\u7684\u900f\u660e\u5ea6", "conclusion": "\u57fa\u4e8e\u672c\u4f53\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5bf9\u8bdd\u7279\u5f81\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c\u63a7\u5236\uff0c\u4e3aLLM\u7684\u53ef\u63a7\u5bf9\u8bdd\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6846\u67b6"}}
{"id": "2509.04979", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04979", "abs": "https://arxiv.org/abs/2509.04979", "authors": ["Rajesh Tembarai Krishnamachari", "Srividya Rajesh"], "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents", "comment": null, "summary": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.", "AI": {"tldr": "DOVIS\u534f\u8bae\u548cAgentRank-UC\u7b97\u6cd5\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684\u667a\u80fd\u4f53\u7f51\u7edc\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684\u6027\u80fd\u6570\u636e\u6536\u96c6\u548c\u52a8\u6001\u6392\u540d\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u7684\u6709\u6548\u9009\u62e9\u548c\u534f\u4f5c\u3002", "motivation": "\u968f\u7740AI\u667a\u80fd\u4f53\u7684\u53d1\u5c55\uff0c\u4e92\u8054\u7f51\u6b63\u5728\u5411\u667a\u80fd\u4f53\u7f51\u7edc\u8f6c\u578b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u667a\u80fd\u4f53\u6392\u540d\u673a\u5236\u3002\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u6536\u96c6\u548c\u5229\u7528\u5206\u6563\u7684\u79c1\u6709\u6027\u80fd\u6570\u636e\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u63d0\u51faDOVIS\u4e94\u5c42\u64cd\u4f5c\u534f\u8bae\uff08\u53d1\u73b0\u3001\u7f16\u6392\u3001\u9a8c\u8bc1\u3001\u6fc0\u52b1\u3001\u8bed\u4e49\uff09\u6765\u6536\u96c6\u9690\u79c1\u4fdd\u62a4\u7684\u6027\u80fd\u805a\u5408\u6570\u636e\uff0c\u5e76\u5f00\u53d1AgentRank-UC\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u9891\u7387\u548c\u80fd\u529b\u6307\u6807\u8fdb\u884c\u52a8\u6001\u6392\u540d\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3001\u9c81\u68d2\u6027\u548c\u6297Sybil\u653b\u51fb\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u534f\u8c03\u534f\u8bae\u548c\u6027\u80fd\u611f\u77e5\u6392\u540d\u5728\u6784\u5efa\u53ef\u6269\u5c55\u53ef\u4fe1\u667a\u80fd\u4f53\u7f51\u7edc\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "DOVIS\u534f\u8bae\u548cAgentRank-UC\u7b97\u6cd5\u4e3a\u89e3\u51b3\u667a\u80fd\u4f53\u7f51\u7edc\u4e2d\u7684\u6392\u540d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4e3a\u6784\u5efa\u673a\u5668\u539f\u751f\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05007", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.05007", "abs": "https://arxiv.org/abs/2509.05007", "authors": ["Jie Chen", "Jinhao Jiang", "Yingqian Min", "Zican Dong", "Shijie Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework", "comment": "11 pages, 1 figures, 5 tables", "summary": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS.", "AI": {"tldr": "Sticker-TTS\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u4e09\u4e2a\u534f\u4f5c\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u8fed\u4ee3\u63a2\u7d22\u548c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5197\u4f59\u91c7\u6837\uff0c\u5ffd\u7565\u4e86\u5386\u53f2\u7ecf\u9a8c\u7684\u5229\u7528\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u5386\u53f2\u63a8\u7406\u7ecf\u9a8c\u3002", "method": "\u63d0\u51faSticker-TTS\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u4e2a\u534f\u4f5c\u7684LRM\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u70bc\u5173\u952e\u6761\u4ef6\uff08\u79f0\u4e3asticker\uff09\u6765\u63d0\u53d6\u3001\u7cbe\u70bc\u548c\u91cd\u7528\u5173\u952e\u4fe1\u606f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u81ea\u6211\u6539\u8fdb\u3002", "result": "\u5728AIME-24\u3001AIME-25\u548cOlymMATH\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSticker-TTS\u5728\u53ef\u6bd4\u63a8\u7406\u9884\u7b97\u4e0b consistently\u8d85\u8d8a\u4e86\u81ea\u6d3d\u6027\u548c\u5148\u8fdb\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7b49\u5f3a\u57fa\u7ebf\u3002", "conclusion": "Sticker-TTS\u901a\u8fc7sticker\u5f15\u5bfc\u7684\u5386\u53f2\u7ecf\u9a8c\u5229\u7528\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5386\u53f2\u7ecf\u9a8c\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.05072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05072", "abs": "https://arxiv.org/abs/2509.05072", "authors": ["Nir Sweed", "Hanit Hakim", "Ben Wolfson", "Hila Lifshitz", "Dafna Shahaf"], "title": "Finding your MUSE: Mining Unexpected Solutions Engine", "comment": null, "summary": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research.", "AI": {"tldr": "\u63d0\u51fa\u529f\u80fd\u6027\u6982\u5ff5\u56fe(FCG)\u65b9\u6cd5\u6765\u89e3\u51b3\u521b\u65b0\u8005\u7684\u8ba4\u77e5\u56fa\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u529f\u80fd\u5143\u7d20\u4e92\u8054\u8868\u793a\u6765\u652f\u6301\u62bd\u8c61\u3001\u95ee\u9898\u91cd\u6784\u548c\u7c7b\u6bd4\u542f\u53d1", "motivation": "\u521b\u65b0\u8005\u5f80\u5f80\u5bf9\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6216\u521d\u6b65\u60f3\u6cd5\u5b58\u5728\u8ba4\u77e5\u56fa\u5316\uff0c\u963b\u788d\u4e86\u65b0\u9896\u66ff\u4ee3\u65b9\u6848\u7684\u63a2\u7d22\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fc3\u8fdb\u521b\u9020\u6027\u601d\u7ef4", "method": "\u6784\u5efa\u529f\u80fd\u6027\u6982\u5ff5\u56fe(FCG)\u4f5c\u4e3a\u529f\u80fd\u5143\u7d20\u7684\u4e92\u8054\u8868\u793a\uff0c\u5e76\u63d0\u51faMUSE\u7b97\u6cd5\u5229\u7528FCG\u4e3a\u7ed9\u5b9a\u95ee\u9898\u751f\u6210\u521b\u9020\u6027\u542f\u53d1", "result": "\u572850\u4e07\u9879\u4e13\u5229\u4e0a\u8ba1\u7b97\u6784\u5efa\u4e86FCG\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u4f7f\u7528", "conclusion": "FCG\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u514b\u670d\u5148\u524d\u5de5\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u529f\u80fd\u62bd\u8c61\u5173\u7cfb\uff0c\u652f\u6301\u521b\u65b0\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u91cd\u6784\u548c\u7c7b\u6bd4\u542f\u53d1"}}
{"id": "2509.05091", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05091", "abs": "https://arxiv.org/abs/2509.05091", "authors": ["Matteo Bortoletto", "Yichao Zhou", "Lance Ying", "Tianmin Shu", "Andreas Bulling"], "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback", "comment": "Website at https://www.matteobortoletto.org/protom/", "summary": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.", "AI": {"tldr": "ProToM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u9006\u89c4\u5212\u63a8\u65ad\u667a\u80fd\u4f53\u76ee\u6807\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u6709\u6548\u4fc3\u8fdb\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u76f8\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u5728\u8ffd\u6c42\u72ec\u7acb\u76ee\u6807\u65f6\u96be\u4ee5\u5224\u65ad\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u534f\u52a9\u4ed6\u4eba\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u9006\u89c4\u5212\u63a8\u65ad\u667a\u80fd\u4f53\u76ee\u6807\uff0c\u7136\u540e\u901a\u8fc7\u6700\u5927\u5316\u671f\u671b\u6548\u7528\u6765\u9009\u62e9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u53cd\u9988\u4fe1\u606f\u3002", "result": "\u5728Doors\u3001Keys\u3001Gems\u548cOvercooked\u73af\u5883\u4e2d\uff0cProToM\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u77ed\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u53d7\u5230\u4eba\u7c7b\u7528\u6237\u7684\u4e00\u81f4\u504f\u597d\u3002", "conclusion": "ProToM\u80fd\u591f\u63d0\u4f9b\u9488\u5bf9\u6027\u5f3a\u4e14\u6709\u7528\u7684\u53cd\u9988\uff0c\u800c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u548c\u63a8\u7406\u6a21\u578b\u5728\u63d0\u4f9b\u4e0a\u4e0b\u6587\u63a5\u5730\u4e14\u65f6\u673a\u6070\u5f53\u7684\u53cd\u9988\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2509.05139", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05139", "abs": "https://arxiv.org/abs/2509.05139", "authors": ["Jaime Osvaldo Salas", "Paolo Pareti", "Semih Yumu\u015fak", "Soulmaz Gheisari", "Luis-Daniel Ib\u00e1\u00f1ez", "George Konstantinidis"], "title": "Evaluation and Comparison Semantics for ODRL", "comment": "Accepted as a full paper at the 14th International Joint Conference\n  on Knowledge Graphs (IJCKG 2025). This is the submitted manuscript, the\n  accepted manuscript will be published by Springer Nature", "summary": "We consider the problem of evaluating, and comparing computational policies\nin the Open Digital Rights Language (ODRL), which has become the de facto\nstandard for governing the access and usage of digital resources. Although\npreliminary progress has been made on the formal specification of the\nlanguage's features, a comprehensive formal semantics of ODRL is still missing.\nIn this paper, we provide a simple and intuitive formal semantics for ODRL that\nis based on query answering. Our semantics refines previous formalisations, and\nis aligned with the latest published specification of the language (2.2).\nBuilding on our evaluation semantics, and motivated by data sharing scenarios,\nwe also define and study the problem of comparing two policies, detecting\nequivalent, more restrictive or more permissive policies.", "AI": {"tldr": "\u4e3aODRL\u8bed\u8a00\u63d0\u4f9b\u57fa\u4e8e\u67e5\u8be2\u7b54\u590d\u7684\u6b63\u5f0f\u8bed\u4e49\u5b66\uff0c\u5e76\u5b9a\u4e49\u653f\u7b56\u6bd4\u8f83\u95ee\u9898\u7528\u4e8e\u68c0\u6d4b\u7b49\u4ef7\u3001\u66f4\u4e25\u683c\u6216\u66f4\u514d\u8bb8\u7684\u653f\u7b56", "motivation": "ODRL\u5df2\u6210\u4e3a\u6570\u5b57\u8d44\u6e90\u8bbf\u95ee\u548c\u4f7f\u7528\u7ba1\u7406\u7684\u5b9e\u9645\u6807\u51c6\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u6b63\u5f0f\u8bed\u4e49\u5b66\uff0c\u5f71\u54cd\u653f\u7b56\u8bc4\u4f30\u548c\u6bd4\u8f83", "method": "\u63d0\u51fa\u57fa\u4e8e\u67e5\u8be2\u7b54\u590d\u7684\u7b80\u5355\u76f4\u89c2\u6b63\u5f0f\u8bed\u4e49\u5b66\uff0c\u7cbe\u70bc\u4e4b\u524d\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u4e0eODRL 2.2\u89c4\u8303\u4fdd\u6301\u4e00\u81f4", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u5b8c\u6574\u7684ODRL\u6b63\u5f0f\u8bed\u4e49\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u652f\u6301\u653f\u7b56\u8bc4\u4f30\u548c\u6bd4\u8f83", "conclusion": "\u8be5\u5f62\u5f0f\u8bed\u4e49\u5b66\u4e3aODRL\u653f\u7b56\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5171\u4eab\u573a\u666f\u4e2d\u5bf9\u4e8e\u653f\u7b56\u6bd4\u8f83\u548c\u4e00\u81f4\u6027\u68c0\u67e5\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2509.05263", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05263", "abs": "https://arxiv.org/abs/2509.05263", "authors": ["Yinglin Duan", "Zhengxia Zou", "Tongwei Gu", "Wei Jia", "Zhan Zhao", "Luyi Xu", "Xinzhu Liu", "Hao Jiang", "Kang Chen", "Shuang Qiu"], "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation", "comment": null, "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18", "AI": {"tldr": "LatticeWorld\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u548c\u6e38\u620f\u5f15\u64ce\u76843D\u4e16\u754c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u52a8\u6001\u4ea4\u4e92\u5f0f3D\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u751f\u4ea7\u6548\u738790\u500d\u4ee5\u4e0a", "motivation": "\u5f00\u53d1\u66f4\u771f\u5b9e\u76843D\u4e16\u754c\u6a21\u578b\u6765\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4e3a\u5177\u8eabAI\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e30\u5bcc\u7684\u4eff\u771f\u73af\u5883\uff0c\u66ff\u4ee3\u4f20\u7edf\u624b\u52a8\u5efa\u6a21\u7684\u4f4e\u6548\u751f\u4ea7\u65b9\u5f0f", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7LLM\uff08LLaMA-2-7B\uff09\u548c\u5de5\u4e1a\u7ea7\u6e32\u67d3\u5f15\u64ce\uff08\u5982Unreal Engine 5\uff09\uff0c\u63a5\u53d7\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u6307\u4ee4\u4f5c\u4e3a\u591a\u6a21\u6001\u8f93\u5165\uff0c\u751f\u6210\u5305\u542b\u52a8\u6001\u667a\u80fd\u4f53\u7684\u5927\u89c4\u6a213D\u4ea4\u4e92\u4e16\u754c", "result": "\u5728\u573a\u666f\u5e03\u5c40\u751f\u6210\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u4f18\u5f02\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u52a8\u751f\u4ea7\u65b9\u5f0f\u63d0\u9ad8\u5de5\u4e1a\u751f\u4ea7\u6548\u738790\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u521b\u610f", "conclusion": "LatticeWorld\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u76843D\u4e16\u754c\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5177\u6709\u7ade\u4e89\u6027\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3001\u9ad8\u4fdd\u771f\u7269\u7406\u4eff\u771f\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u52a8\u60013D\u73af\u5883\uff0c\u663e\u8457\u63a8\u8fdb\u4e863D\u4e16\u754c\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55"}}
