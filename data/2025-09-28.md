<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 该论文提出了一种基于大语言模型的对话式网络管理系统，使非专业用户能够通过自然语言管理网络，解决了传统网络管理需要专业知识的问题。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理方法只适用于经过高度培训的网络操作员，这为普通用户管理网络设置了障碍。作者希望通过大语言模型的发展，让更广泛的非专业用户能够通过自然语言对话来管理网络。

Method: 提出了一个基于代理的框架，使用中间表示来统一不同厂商设备的配置，实时从内存中检索网络状态，并提供外部反馈接口。还进行了试点研究收集真实用户的自然语言指令，并开发了可视化界面来促进对话驱动的用户交互。

Result: 初步实验验证了所提出的系统组件与大语言模型集成的有效性，无论是在合成数据还是真实用户指令上都表现良好。通过数据收集和可视化工作，为更有效地使用大语言模型奠定了基础。

Conclusion: 该研究为更有效地利用大语言模型铺平了道路，并使日常用户能够民主化地进行网络控制，使网络管理对非专业用户更加可访问。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [2] [An SDR-Based Test Platform for 5G NTN Prototyping and Validation](https://arxiv.org/abs/2509.20692)
*Lu Hou,Kan Zheng,Jie Mei,Cheng Huang*

Main category: cs.NI

TL;DR: 本文提出了一个基于软件定义无线电(SDR)的5G非地面网络(NTN)测试平台，通过通用处理器处理，利用Amarisoft的5G NTN协议栈软件，并进行定制系统集成和适配以实现真实卫星操作。


<details>
  <summary>Details</summary>
Motivation: 5G NTN标准处于早期成熟阶段，缺乏商用NTN设备，阻碍了性能验证和系统原型开发。

Method: 开发SDR测试平台，支持基于SDR的NTN gNB和UE模拟器之间通过地球静止轨道卫星链路进行双向通信，完全符合3GPP NTN规范。

Result: 通过现场试验评估了下行链路吞吐量和往返时间等性能指标，验证了基于SDR的平台用于NTN测试的可行性和有效性。

Conclusion: SDR平台在广泛商业部署之前具有弥合当前实施差距的潜力。

Abstract: The integration of satellite communication into 5G has been formalized in
3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN),
marking a significant step toward achieving global connectivity. However, the
early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable
equipment hinder extensive performance validation and system prototyping. To
address this gap, this paper proposes a software-defined radio (SDR) test
platform with General-Purpose Processor (GPP) processing, leveraging
Amarisoft's 5G NTN protocol stack software while performing custom system
integration and adaptation for real satellite operation. The platform supports
bidirectional communication between an SDR-based NTN gNB and UE emulator
through a Geostationary Earth Orbit (GEO) satellite link, with full compliance
to 3GPP NTN specifications. We provide detailed insights into the system
architecture, SDR hardware-software co-design, and satellite gateway
adaptations. Through field trials, we evaluate the performance metrics
including downlink throughput and round-trip time. Results validate the
feasibility and effectiveness of SDR-based platforms for NTN testing, and
highlight their potential in bridging current implementation gaps before
widespread commercial deployment.

</details>


### [3] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 提出了一种三层可信车载语义通信网络架构，包含语义伪装传输机制、鲁棒联邦编码器-解码器训练框架和基于审计博弈的分布式车辆信任管理机制，以解决车载语义通信网络中的信任挑战。


<details>
  <summary>Details</summary>
Motivation: 车载语义通信网络在信息传输、语义编码和通信实体可靠性方面面临关键的信任挑战，阻碍了其在实际部署中的应用。

Method: 1. 利用防御性对抗噪声的语义伪装传输机制进行主动窃听防御；2. 鲁棒联邦编码器-解码器训练框架缓解编码器-解码器中毒攻击；3. 基于审计博弈的分布式车辆信任管理机制阻止不可信车辆。

Result: 案例研究验证了所提解决方案的有效性。

Conclusion: 提出的三层可信车载语义通信网络架构能够有效解决信任挑战，并指出了推进这一新兴领域的关键未来研究方向。

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [4] [BSB: Towards Demand-Aware Peer Selection With XOR-based Routing](https://arxiv.org/abs/2509.20974)
*Qingyun Ji,Darya Melnyk,Arash Pourdamghani,Stefan Schmid*

Main category: cs.NI

TL;DR: 提出了一种名为BSB的需求感知对等选择算法，通过考虑应用特定的数据流量来优化P2P网络性能，相比现有算法可提升43%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对等选择算法忽略了应用特定的数据流量，导致连接利用率不足、路径更长和延迟增加的问题。

Method: BSB算法采用需求感知方法，遵循基于XOR的本地贪婪路由机制，确保与现有协议和机制的兼容性。

Result: 在真实世界和合成通信网络轨迹上的模拟评估显示，BSB相比两种现有算法可提供高达43%的性能改进。

Conclusion: BSB算法通过需求感知的对等选择有效解决了现有算法在数据流量考虑方面的不足，显著提升了P2P网络性能。

Abstract: Peer-to-peer networks, as a key enabler of modern networked and distributed
systems, rely on peer-selection algorithms to optimize their scalability and
performance. Peer-selection methods have been studied extensively in various
aspects, including routing mechanisms and communication overhead. However, many
state-of-the-art algorithms are oblivious to application-specific data traffic.
This mismatch between design and demand results in underutilized connections,
which inevitably leads to longer paths and increased latency. In this work, we
propose a novel demand-aware peer-selection algorithm, called Binary Search in
Buckets (BSB). Our demand-aware approach adheres to a local and greedy
XOR-based routing mechanism, ensuring compatibility with existing protocols and
mechanisms. We evaluate our solution against two prior algorithms by conducting
simulations on real-world and synthetic communication network traces. The
results of our evaluations show that BSB can offer up to a 43% improvement
compared to two selected algorithms from the literature.

</details>


### [5] [A Novel Integrated Architecture for Intent Based Approach and Zero Touch Networks](https://arxiv.org/abs/2509.21026)
*Neelam Gupta,Dibakar Das,Tamizhelakkiya K,Uma Maheswari Natarajan,Sharvari Ravindran,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 提出了一种将基于意图的网络（IBN）和零接触网络（ZTN）集成的新架构，通过自然语言处理将用户意图转换为网络配置，并使用BiLSTM和Q学习的ZTN闭环框架来维持网络性能目标。


<details>
  <summary>Details</summary>
Motivation: 6G网络面临管理多样化应用QoS和在变化网络条件下实现SLA的挑战，需要利用ML和AI实现自动化网络管理。

Method: 用户通过自然语言提供意图，使用NLP技术（如RAG）转换为Nile语言，然后通过基于BiLSTM和Q学习的ZTN闭环框架作为目标来维持意图。

Result: 实验结果表明ZTN能够自主实现用户意图设定的带宽目标，仿真和测试床结果趋势一致，并测量了QoE的MOS来评估用户满意度。

Conclusion: 所提出的集成架构能够通过简单的英语意图说明自主确保网络性能目标的实现，为6G网络自动化管理提供了有效解决方案。

Abstract: The transition to Sixth Generation (6G) networks presents challenges in
managing quality of service (QoS) of diverse applications and achieving Service
Level Agreements (SLAs) under varying network conditions. Hence, network
management must be automated with the help of Machine Learning (ML) and
Artificial Intelligence (AI) to achieve real-time requirements. Zero touch
network (ZTN) is one of the frameworks to automate network management with
mechanisms such as closed loop control to ensure that the goals are met
perpetually. Intent- Based Networking (IBN) specifies the user intents with
diverse network requirements or goals which are then translated into specific
network configurations and actions. This paper presents a novel architecture
for integrating IBN and ZTN to serve the intent goals. Users provides the
intent in the form of natural language, e.g., English, which is then translated
using natural language processing (NLP) techniques (e.g., retrieval augmented
generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then
passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a
goal which maintains the intent under varying network conditions. Thus, the
proposed architecture can work autonomously to ensure the network performance
goal is met by just specifying the user intent in English. The integrated
architecture is also implemented on a testbed using OpenAirInterface (OAI).
Additionally, to evaluate the architecture, an optimization problem is
formulated which evaluated with Monte Carlo simulations. Results demonstrate
how ZTN can help achieve the bandwidth goals autonomously set by user intent.
The simulation and the testbed results are compared and they show similar
trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also
measured to indicate the user satisfaction of the intent.

</details>


### [6] [RePro: Leveraging Large Language Models for Semi-Automated Reproduction of Networking Research Results](https://arxiv.org/abs/2509.21074)
*Yining Jiang,Wenyun Xu,Qingyu Song,Yuling Lin,Xuanhao Liu,Xiaoqiang Zheng,Qiang Su,Lizhao You,Lu Tang,Wangjian Feng,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: RePro是一个半自动化的网络研究复现框架，通过先进的提示工程技术从研究论文中生成可执行的网络系统代码，显著减少了复现时间。


<details>
  <summary>Details</summary>
Motivation: 网络研究复现因开源代码稀缺而具有挑战性，现有LLM方法缺乏对多样化网络领域的泛化能力。

Method: 结合few-shot上下文学习和结构化/语义思维链技术，通过三阶段流水线：系统描述提取、结构化代码生成和代码优化。

Result: 在五个最先进LLM和多样化网络子领域的评估中，RePro显著减少复现时间，同时实现可比较的系统性能。

Conclusion: RePro验证了其在网络研究复现中的有效性和效率，为自动化研究复现提供了可行方案。

Abstract: Reproducing networking research is a critical but challenging task due to the
scarcity of open-source code. While Large Language Models (LLMs) can automate
code generation, current approaches lack the generalizability required for the
diverse networking field. To address this, we propose RePro, a semi-automated
reproduction framework that leverages advanced prompt engineering to reproduce
network systems from their research papers. RePro combines few-shot in-context
learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques
to systematically translate a paper's description into an optimized, executable
implementation. The framework operates through a three-stage pipeline: system
description extraction, structural code generation, and code optimization. Our
evaluation with five state-of-the-art LLMs across diverse network sub-domains
demonstrates that RePro significantly reduces reproduction time compared to
manual efforts while achieving comparable system performance, validating its
effectiveness and efficiency.

</details>


### [7] [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](https://arxiv.org/abs/2509.21201)
*Yang Fu,Peng Qin,Liming Chen,Yifei Wang*

Main category: cs.NI

TL;DR: 提出了一种混合RIS辅助的数字空中计算方案，通过向量量化将高维特征映射为离散码字进行数字调制传输，优化量化比特分配、传输系数和RIS波束成形来最大化边缘推理精度。


<details>
  <summary>Details</summary>
Motivation: 6G网络愿景要求实现边缘推理，但现有空中计算技术与数字通信系统不兼容，而混合RIS架构能同时实现信号放大和反射，有望增强空中计算性能。

Method: 采用向量量化将特征映射为离散码字进行数字调制，通过优化AirComp收发器和混合RIS反射来控制信号叠加，并推导了推理精度的代理函数来指导量化比特分配、传输系数、接收波束成形和RIS反射波束成形的联合优化。

Result: 实验结果表明，所提出的HRD-AirComp方案在推理精度和不确定性方面均优于基线方法。

Conclusion: HRD-AirComp方案成功实现了数字通信系统与空中计算的兼容，通过任务导向的设计原则显著提升了边缘推理性能。

Abstract: The vision of 6G networks aims to enable edge inference by leveraging
ubiquitously deployed artificial intelligence (AI) models, facilitating
intelligent environmental perception for a wide range of applications. A
critical operation in edge inference is for an edge node (EN) to aggregate
multi-view sensory features extracted by distributed agents, thereby boosting
perception accuracy. Over-the-air computing (AirComp) emerges as a promising
technique for rapid feature aggregation by exploiting the waveform
superposition property of analog-modulated signals, which is, however,
incompatible with existing digital communication systems. Meanwhile, hybrid
reconfigurable intelligent surface (RIS), a novel RIS architecture capable of
simultaneous signal amplification and reflection, exhibits potential for
enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital
AirComp (HRD-AirComp) scheme, which employs vector quantization to map
high-dimensional features into discrete codewords that are digitally modulated
into symbols for wireless transmission. By judiciously adjusting the AirComp
transceivers and hybrid RIS reflection to control signal superposition across
agents, the EN can estimate the aggregated features from the received signals.
To endow HRD-AirComp with a task-oriented design principle, we derive a
surrogate function for inference accuracy that characterizes the impact of
feature quantization and over-the-air aggregation. Based on this surrogate, we
formulate an optimization problem targeting inference accuracy maximization,
and develop an efficient algorithm to jointly optimize the quantization bit
allocation, agent transmission coefficients, EN receiving beamforming, and
hybrid RIS reflection beamforming. Experimental results demonstrate that the
proposed HRD-AirComp outperforms baselines in terms of both inference accuracy
and uncertainty.

</details>


### [8] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: 提出了一种基于语义通信的边缘-云框架，用于实时交通监控，通过检测感兴趣区域、转换为紧凑嵌入向量传输，在云中重建图像并由多模态LLM处理，实现99.9%的数据传输减少，同时保持89%的响应准确率。


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中边缘摄像头与云端LLM集成时的带宽限制问题，传统方法传输视觉数据会导致实时性能下降，需要减少传输开销同时保持分析准确性。

Method: 使用YOLOv11检测感兴趣区域，裁剪相关图像片段，通过Vision Transformer转换为紧凑嵌入向量传输到云端，在云端重建图像后由多模态LLM生成交通状况描述。

Result: 实现了99.9%的数据传输大小减少，重建裁剪图像的LLM响应准确率达到89%，而原始裁剪图像的准确率为93%。

Conclusion: 该方法证明了ViT和LLM辅助的边缘-云语义通信在实时交通监控中的高效性和实用性，在显著减少传输数据量的同时保持了可接受的准确率。

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种用于监控AI代理行为的时间表达式语言，通过监测代理工具调用和状态转换的执行轨迹来检测与预期行为模式的偏差，解决了传统文本匹配方法在LLM代理系统中的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理系统由于随机生成过程产生可变输出，传统基于输入输出文本匹配的错误检测方法因自然语言变异性而变得脆弱。需要一种能够独立于特定文本输出验证系统行为的方法。

Method: 借鉴硬件验证中的时序逻辑技术，开发了一种时序表达式语言，监控代理工具调用和状态转换的执行轨迹，专注于代理动作序列（如工具调用和代理间通信）来验证系统行为。

Result: 在三代理系统中测试，当使用大型模型时所有时序断言均满足，但当两个代理使用较小模型时，执行违反了行为断言，主要由于工具序列不当和协调交接失败。时序表达式成功标记了这些异常。

Conclusion: 该方法为系统监控AI代理可靠性提供了基础，特别适用于关键应用中部署的代理系统，能够有效检测生产环境中代理系统的行为回归问题。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [10] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种局部自适应测试时扩展方法（LATTS），通过验证器模型动态调整每个生成步骤的计算量，实现更高效的准确率-计算量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器方法在测试时均匀分配计算资源，不考虑不同样本和生成步骤的复杂度差异，导致计算效率低下。

Method: 在每一步生成时使用验证器接受标准来决定是否重采样、回溯、重启或停止生成过程，基于验证器模型提供的局部难度概念动态调整计算量。

Result: 实验结果显示LATTS相比标准验证器方法实现了显著更优的准确率-计算量权衡。

Conclusion: 局部自适应测试时扩展方法能够有效提升大语言模型在下游任务中的性能，同时优化计算资源使用效率。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [11] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 哲学启发机器学习(PhIML)将分析哲学核心思想融入机器学习模型架构、目标和评估协议，通过设计尊重哲学概念和价值观的模型来提供新能力。


<details>
  <summary>Details</summary>
Motivation: 通过将哲学思想直接融入机器学习，使模型能够尊重哲学概念和价值观，实现哲学增益和对齐，推动安全、哲学意识和伦理责任的机器学习发展。

Method: 回顾概念基础以展示哲学增益和对齐，并通过案例研究展示如何将PhIML作为后验工具采用或内在地构建到ML模型架构中。

Result: 提出了哲学启发机器学习的框架和方法，展示了如何通过哲学思想增强机器学习模型的能力和伦理价值。

Conclusion: 论文阐明了PhIML面临的技术障碍以及哲学、实践和治理挑战，并制定了实现安全、哲学意识和伦理责任的PhIML的研究路线图。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [12] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE是一个AI驱动的阅读助手工具，为研究人员提供结构化、简洁的论文洞察，而不是冗长的摘要，帮助导航科学文献。


<details>
  <summary>Details</summary>
Motivation: 科学文献的激增给研究人员带来挑战，现有工具提供的冗长摘要可能取代而非辅助阅读源材料，需要更有效的阅读助手。

Method: 开发了InsightGUIDE系统，将专家阅读方法嵌入核心AI逻辑，采用提示驱动的方法论，提供结构化洞察作为论文关键要素的"地图"。

Result: 定性案例研究表明，与通用LLM相比，InsightGUIDE产生更结构化和可操作的指导，对现代研究人员更有效。

Conclusion: InsightGUIDE作为阅读助手而非替代品，通过提供结构化洞察，有效解决了科学文献导航的挑战。

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [13] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种新的重构框架，用于动态验证和组装时间触发系统的调度表，解决消息冲突、优先级处理错误等问题，确保系统安全约束。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态操作环境中面临消息冲突、优先级处理导致的死锁、以及生成不完整或无效调度表等挑战，这些会危及系统安全和性能。

Method: 通过重构模型系统地将AI生成或启发式得出的调度优先级转换为完全可执行的调度表，包含安全检查、高效分配算法和恢复机制来处理意外事件。

Result: 实验表明该框架显著提高了系统适应性、操作完整性和运行时性能，同时保持计算效率，在多个性能指标上都表现良好。

Conclusion: 这项工作为安全关键时间触发系统中的安全调度生成提供了实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [14] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种集成在元调度器中的自适应在线学习单元，通过强化学习在运行时动态探索和发现新的调度解决方案，以解决传统离线训练AI调度推理时构建全面多调度图的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建考虑所有可能场景的多调度图时面临资源密集和不可行的挑战，特别是在处理硬件故障、松弛变化或模式变化等上下文事件时。

Method: 在元调度器中集成自适应在线学习单元，使用多种强化学习模型在运行时持续探索新的调度解决方案，扩展多调度图并优化现有调度器。

Result: 在线强化学习能够动态发现新的调度解决方案，扩展多调度图，在遇到意外事件和复杂调度场景时更有效地处理，同时优化现有调度器以适应更严格的截止期限或新的性能标准。

Conclusion: 通过实时训练持续优化AI推理，系统保持灵活性并能够满足不断变化的需求，确保在大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [15] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 提出了一种用于肌电假手控制的新识别系统，包含两个集成模块：单类分类器检测信号污染，KNN分类器识别用户意图，采用统一的模糊决策方案提高分类质量。


<details>
  <summary>Details</summary>
Motivation: 肌电生物信号易受污染影响分类质量，现有系统难以获得可接受的分类性能，需要开发能够检测和减轻污染影响的识别系统。

Method: 开发了包含两个集成模块的模糊识别系统：单类分类器评估各通道污染程度，KNN分类器识别用户意图，采用统一的模糊决策方案。

Result: 使用真实生物信号进行实验评估，对方法参数和程序进行了比较分析，并与文献中的类似系统进行了对比。

Conclusion: 提出的模糊识别系统能够有效检测信号污染并提高分类质量，为肌电假手控制提供了改进方案。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [16] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE是一个基于多级反思合成的自学习智能体框架，通过微、中、宏三个层次的反思训练回顾性语言模型，显著提升了LLM智能体在复杂任务中的反思能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在生成有意义的反思方面面临挑战，主要由于错误分析不足和对罕见成功轨迹的依赖，特别是在复杂任务中。

Method: 提出SAMULE框架，通过多级反思合成：单轨迹学习（微观）进行详细错误修正；任务内学习（中观）构建错误分类；任务间学习（宏观）提取可转移见解。然后微调语言模型作为回顾性模型，并在推理时生成反思。

Result: 在三个具有挑战性的基准测试（TravelPlanner、NATURAL PLAN和Tau-bench）上的广泛实验表明，该方法显著优于基于反思的基线方法。

Conclusion: 精心设计的反思合成和以失败为中心的学习在构建自改进LLM智能体中起着关键作用。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [17] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 该研究提出了一种基于智能AI代理的自适应网络安全架构，能够实现动态学习、上下文感知决策，在云环境中有效检测零日攻击并动态调整访问策略。


<details>
  <summary>Details</summary>
Motivation: 传统静态网络安全模型在当前包含云服务、API、移动平台和边缘设备的数字产品生态系统中，难以应对可扩展性、实时检测和上下文响应性的挑战。

Method: 采用自主目标驱动代理，构建自适应网络安全架构，整合行为基线、去中心化风险评分和联邦威胁情报共享，通过原生云模拟验证系统能力。

Result: 评估结果显示系统具有增强的适应性、降低的响应延迟和提升的检测准确性，能够识别零日攻击并动态修改访问策略。

Conclusion: 该架构为零信任模型兼容的复杂数字基础设施保护提供了智能且可扩展的蓝图，支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [18] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 开发了Claim Advisor网络应用，使用上下文学习和LLM微调来加速产品声明的创建，包含搜索、生成优化和模拟排名三个功能，在消费品公司应用中显示出良好效果。


<details>
  <summary>Details</summary>
Motivation: 产品声明是消费者购买行为的关键驱动因素，但创建产品声明需要大量时间和资金投入，需要加速这一过程。

Method: 使用上下文学习和大型语言模型微调技术，开发Claim Advisor网络应用，具备语义搜索、声明生成优化、以及通过合成消费者模拟进行排名三个核心功能。

Result: 在消费品公司的应用中显示出非常有前景的结果，能够显著提升声明创建的速度和经济效益。

Conclusion: 这种能力在各个产品类别和行业中都具有广泛的应用价值，鼓励在不同行业中研究和应用生成式AI技术。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [19] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发了一个基于LLaMA-4 109B的检索增强生成系统，用于自动化、协议感知且可解释的放射治疗计划评估。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动化评估放射治疗计划、遵循协议规范并提供可解释结果的系统，以提高放疗计划评估的效率和透明度。

Method: 构建多协议数据集和知识库，集成检索引擎、百分位预测模块和临床约束检查器，通过多步骤提示驱动推理管道指导LLM生成评估结果。

Result: 检索超参数优化后，系统在5个百分位点范围内实现完美最近邻准确率，MAE低于2点。端到端测试显示与独立模块计算结果100%一致。

Conclusion: 结合结构化群体评分与模块化工具增强推理的方法在放疗计划评估中具有可行性，系统提供可追溯输出、减少幻觉，并在不同协议间表现稳健。

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [20] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy是一个交互式多智能体移动助手，通过跨应用协作、交互式执行和持续学习来解决现有移动GUI智能体在真实场景中的局限性，显著提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在移动GUI智能体应用中面临真实世界场景的挑战，包括多样化的应用界面和不断变化的用户需求。端到端方法在长尾应用上表现不佳，而无用户交互的智能体行为会损害用户体验。

Method: Fairy包含三个核心模块：(1)全局任务规划器从跨应用视角分解用户任务；(2)应用级执行器基于长短期记忆将子任务细化为步骤和动作，通过四个核心智能体在双循环中实现精确执行和用户交互；(3)自学习器将执行经验整合到应用地图和技巧中。

Result: 在RealMobile-Eval基准测试中，基于GPT-4o的Fairy相比之前的最优方法，用户需求完成率提升了33.7%，冗余步骤减少了58.5%，证明了其交互和自学习的有效性。

Conclusion: Fairy通过交互式多智能体架构和持续学习机制，显著提升了移动GUI智能体在真实世界场景中的性能和用户体验，为移动助手的发展提供了新的方向。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [21] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出了一种结合自回归和非自回归语言模型的新框架，通过NAR模型高效生成中间推理轨迹，再由AR模型输出精确答案，在保持质量的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在推理密集型任务中生成连贯但推理速度慢，而非自回归模型虽然速度快但输出质量较低，需要一种兼顾质量和效率的解决方案。

Method: 采用NAR模型（如离散扩散模型）并行生成中间推理轨迹，然后用这些轨迹指导AR模型生成精确的最终答案。

Result: 实验显示该方法相比强基线有26%的显著提升，同时大幅降低了推理成本。

Conclusion: 提出的AR-NAR混合框架在推理任务中实现了质量和效率的良好平衡，为推理密集型应用提供了有效的解决方案。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [22] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出了Meta-Memory，一个基于大语言模型的机器人记忆系统，能够通过语义和空间模态的联合推理来检索和整合记忆，以回答自然语言位置查询。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂环境中需要有效存储观察结果作为记忆，并利用这些记忆回答人类关于空间位置的查询，这是一个关键但研究不足的挑战。现有工作很少解决高效记忆检索和整合的原则性机制。

Method: Meta-Memory是一个LLM驱动的智能体，构建环境的高密度记忆表示。其关键创新在于能够通过语义和空间模态的联合推理来检索和整合相关记忆，以响应自然语言位置查询。

Result: 在SpaceLocQA和NaVQA基准测试中，Meta-Memory显著优于最先进的方法。成功在真实机器人平台上部署，展示了在复杂环境中的实际效用。

Conclusion: Meta-Memory通过语义-空间联合推理为机器人提供了强大而准确的空间推理能力，在复杂环境中具有实际应用价值。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [23] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner是一个粗到细的推理增强框架，通过两阶段方法提升LLM在日志分析任务中的推理能力，在四个不同任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM难以构建与专家认知一致的结构化推理工作流，无法提供精确的推理步骤细节，这限制了其在日志分析中的应用。

Method: LogReasoner包含两个阶段：粗粒度专家思维增强（从故障排除流程图构建高层专家思维）和细粒度具体步骤增强（通过任务特定步骤解决方案微调LLM，并使用偏好学习校准推理细节）。

Result: 在四个不同的日志分析任务上，LogReasoner显著优于现有LLM，实现了最先进的性能，证明了其在增强LLM推理能力方面的有效性。

Conclusion: LogReasoner框架成功提升了LLM在日志分析中的推理能力，使其能够像专家一样进行推理，为自动化日志分析提供了有效的解决方案。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [24] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto是一个反事实推理框架，通过联合强制准确回答和忠实推理来解决多模态语言模型在视觉语言推理中的推理保真度问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在视觉语言推理中虽然取得了显著进展，但存在一个关键问题：模型可能通过依赖不相关或虚假区域得出正确答案，这表明模型并未真正理解图像内容，凸显了多模态任务中推理保真度的重要性。

Method: 提出DeFacto框架，包含三种互补的训练范式：(i)正向训练，(ii)反事实训练，(iii)随机掩码训练。开发了一个自动定位问题相关证据并构建正向、反事实和随机变体的流水线，创建了约10万张图像的数据集。使用基于GRPO的强化学习训练多模态语言模型，设计了三个互补的奖励函数来引导模型实现准确回答和基于证据的推理。

Result: 在多样化基准测试上的实验表明，DeFacto显著提高了回答准确性和推理忠实性，为可解释的多模态推理建立了更强的基础。

Conclusion: DeFacto框架通过反事实推理方法有效解决了多模态语言模型中的推理保真度问题，显著提升了模型的准确性和可解释性。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [25] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX框架通过图增强语言模型和可解释性技术，整合多组学数据、拓扑结构和文本知识，用于精准医学中的疾病关键信号通路和靶点发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能部分利用多组学特征、拓扑背景和文本知识，限制了机制解释性。需要整合定量多组学信号、拓扑结构和文献规模文本，通过子图推理连接数值证据、拓扑知识和语言背景。

Method: 提出GALAX框架，将预训练图神经网络集成到大型语言模型中，通过图过程奖励模型进行强化指导，以逐步生成疾病相关子图。

Result: 开发了Target-QA基准，结合CRISPR识别靶点、多组学图谱和生物医学图知识，支持长上下文推理和可扩展的生物基础框架。

Conclusion: GALAX为精准医学提供了可解释、强化指导的子图推理框架，实现可靠且可解释的靶点和通路发现。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [26] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 提出基于大型语言模型的移动应用评论分析框架，通过结构化提示技术解决传统星级评分系统无法捕捉文本评论细微反馈的问题，显著提升分析准确性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 传统星级评分系统虽然直观流行，但无法捕捉详细评论文本中的细微反馈。传统NLP技术在理解上下文细微差别、领域特定术语和讽刺等微妙语言特征方面存在困难。

Method: 采用模块化框架，利用大型语言模型结合结构化提示技术，量化数值评分与文本情感之间的差异，提取特征级详细见解，并通过检索增强的对话问答支持交互式评论探索。

Result: 在三个不同数据集上的综合实验表明，该LLM驱动方法显著超越基线方法，在具有挑战性和上下文丰富的评论场景中实现了更高的准确性、鲁棒性和可操作性见解。

Conclusion: 基于大型语言模型的移动应用评论分析方法能够有效克服传统方法的局限性，为应用开发者提供更准确、细致的用户反馈分析，支持更好的产品改进决策。

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [27] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT*是一个将LLM生成的化学合成路径与系统性的AND-OR树搜索相结合的逆合成规划框架，通过数学上合理的奖励分配策略和基于检索的上下文工程，显著提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 多步逆合成规划由于指数级搜索空间和推理成本而具有计算挑战性。虽然大型语言模型展示出化学推理能力，但在合成规划应用中面临效率和成本限制。

Method: AOT*将LLM生成的完整合成路径原子级映射到AND-OR树组件，设计数学上合理的奖励分配策略和基于检索的上下文工程，使LLM能够在化学空间中高效导航。

Result: 在多个合成基准测试中，AOT*实现了最先进的性能，搜索效率显著提升。与现有基于LLM的方法相比，AOT*使用3-5倍更少的迭代次数达到竞争性的解决率，在复杂分子目标上的效率优势更加明显。

Conclusion: AOT*通过整合LLM生成的化学合成路径与系统性的AND-OR树搜索，成功解决了逆合成规划中的效率和成本挑战，为药物发现和材料设计等领域提供了高效的解决方案。

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [28] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出了一个基于确定性有限自动机(DFA)的框架来评估AI代理在真实世界任务中的表现，通过CORE指标套件量化代理行为与预期执行模式的匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有代理基准通常仅基于最终状态进行二元判断，忽略了安全性、效率和中间正确性等关键方面，需要更全面的评估方法。

Method: 使用确定性有限自动机(DFA)将任务编码为有效工具使用路径的集合，构建CORE指标套件（包括路径正确性、前缀关键性、有害调用率、效率等五个指标）。

Result: 在不同世界模型中，该方法揭示了传统最终状态评估方案下看似等效的代理之间重要的性能差异。

Conclusion: 基于DFA的框架和CORE指标套件能够对AI代理行为进行原则性评估，捕捉传统方法忽略的关键性能维度。

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [29] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek是一个新的问答基准，用于评估大语言模型在科学文章上的长上下文推理能力，通过自动生成的复杂问题测试模型在多个全文科学文献中的信息聚合和综合能力。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文基准大多使用非科学文本、关注简单的信息检索任务或使用人工构造的上下文，无法充分评估模型在真实科学文献中的复杂推理能力。

Method: 通过将问题构建为对文章元数据数据库的SQL查询来自动生成问题和答案，SQL操作为细粒度错误分析提供可验证的推理步骤，构建过程可扩展到100万token的上下文。

Result: 在多种开源和专有LLM上的广泛实验表明，随着上下文长度增加，SciTrek带来了显著挑战，监督微调和强化学习只能提供有限的改进。

Conclusion: 分析揭示了模型在执行基本数值操作和在长上下文中准确定位特定信息方面存在系统性缺陷。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [30] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE是一个三代理神经符号框架，通过顺序决策过程在知识图谱上构建上下文，在资源预算约束下优化多跳问答的准确性、延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决静态k跳扩展和"think-longer"提示方法在知识图谱问答中过度检索、上下文膨胀和运行时不可预测的问题，需要在保持溯源的同时平衡准确性、延迟和成本。

Method: 采用三代理框架（子图架构师、路径导航器、上下文策展人），使用LC-MAPPO算法在边缘编辑、交互步骤和选择token的资源预算下联合优化子图构建、推理路径发现和证据选择。

Result: 在HotpotQA、MetaQA和FactKG数据集上，CLAUSE在相同或更低token预算下实现了更高的EM@1，同时减少了子图增长和端到端延迟。在MetaQA-2-hop上，相比GraphRAG基线，EM@1提升39.3%，延迟降低18.6%，边缘增长降低40.9%。

Conclusion: CLAUSE生成的上下文紧凑、保持溯源，在部署约束下提供可预测的性能，能够根据每个查询的资源预算自适应权衡准确性、延迟和成本。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [31] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文提出了评估AI系统创造性能力的理论框架，发现LLMs在科学创意生成中存在新颖性与实用性的权衡，即使在大规模模型中也持续存在，质疑当前LLMs的长期创造潜力。


<details>
  <summary>Details</summary>
Motivation: 现有概念框架未能解决AI系统特别是LLMs在创造性任务（如科学创意生成）中的泛化能力评估问题，需要新的评估方法来衡量其开放式的组合创造力。

Method: 提出了基于新颖性和实用性程度的理论框架和算法任务来评估创意输出，而非传统的准确性评估。通过实证研究探索LLMs创造力的缩放行为、最优模型深度与宽度，以及新颖性-实用性权衡。

Result: 发现：(1)LLMs创造力存在缩放规律；(2)固定计算预算下存在创造能力的最优模型深度和宽度；(3)创意生成与可行性执行之间存在差距，这源于更基本的新颖性-实用性权衡，且该权衡在规模扩展时持续存在。

Conclusion: 当前形式的LLMs长期创造潜力存在疑问，研究为理解和改进现代AI模型的创造力提供了基础，标志着泛化能力的新前沿。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [32] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文挑战了模型规模决定说服力的主流假设，提出说服动态主要由模型的认知过程（特别是显式推理能力）决定。研究发现推理模型对说服更具抵抗力，但分享推理过程会显著增强其说服力。


<details>
  <summary>Details</summary>
Motivation: 理解多智能体系统中LLM和LRM协作解决复杂问题时的说服动态，挑战模型规模决定说服力的假设，探索认知过程对说服行为的影响。

Method: 通过一系列多智能体说服实验，研究推理过程对说服抵抗力和说服能力的影响，并分析多跳说服网络中影响传播和衰减的复杂动态。

Result: 发现推理模型对说服具有更强的抵抗力，能更稳健地保持初始信念；分享推理过程能显著增强其说服能力；多跳说服网络中影响传播呈现复杂动态。

Conclusion: 模型的内部处理架构与其外部说服行为存在系统性联系，这为高级模型的易感性提供了新解释，对多智能体系统的安全性、鲁棒性和设计具有重要启示。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [33] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act是一个基于侦察-行动行为范式的自进化多智能体框架，通过侦察团队进行对比分析和工具生成，行动团队负责意图分解、工具编排和执行，在VisualWebArena数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在解决真实网页上的多轮、长轨迹任务时，仍存在动作序列混乱和执行过程中过多试错的问题，需要提高智能体在未见网站上的适应性和长时程任务的解决能力。

Method: 采用侦察-行动行为范式，包含侦察团队和行动团队。侦察团队通过对比错误轨迹与成功轨迹推断补救措施，并将其抽象为通用工具（提示或基于规则的代码），实时注册到工具库中。行动团队利用这些目标工具重新推理过程。

Result: Recon-Act显著提高了对未见网站的适应性和长时程任务的可解性，在具有挑战性的VisualWebArena数据集上实现了最先进的性能。

Conclusion: Recon-Act建立了一个数据-工具-行动-反馈的闭环训练管道，通过通用工具实现了智能体性能的显著提升，目前已完成6级实施路线图中的第3级（有限的人类干预）。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [34] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 论文提出了TrustJudge概率框架，解决LLM作为评估者时的评分不一致性问题，包括分数比较不一致和成对传递性不一致，通过分布敏感评分和似然感知聚合来提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为自动评估者存在严重的不一致性问题，包括评分比较不一致和成对传递性不一致，这些问题源于离散评分系统的信息损失和模糊的平局判断。

Method: 提出TrustJudge概率框架，包含两个关键创新：1）分布敏感评分，从离散评分概率计算连续期望值；2）似然感知聚合，使用双向偏好概率或困惑度解决传递性违规。

Result: 使用Llama-3.1-70B-Instruct作为评估者，TrustJudge将分数比较不一致性降低8.43%（从23.32%到14.89%），成对传递性不一致性降低10.82%（从15.22%到4.40%），同时保持更高的评估准确率。

Conclusion: TrustJudge是首个系统分析LLM-as-a-judge评估框架不一致性的工作，提供了理论洞见和实用解决方案，能够在不需要额外训练或人工标注的情况下实现更可靠的LLM评估。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [35] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 该论文提出了一种基于推理模式的数据选择方法CoTP，通过识别高价值推理模式来提升大语言模型的数学推理能力，仅用100亿token数据就显著提升了模型在AIME等挑战性数学问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前方法在利用链式思维数据时缺乏选择性，未能有效识别哪些数据类型最能提升模型推理能力。论文首次定义了基础模型的推理潜力，并探索如何通过高质量推理模式来扩展这种潜力。

Method: 1. 从CoT序列中抽象出原子推理模式；2. 构建富含价值推理模式的核心参考集；3. 提出双粒度算法（推理模式链和token熵）从数据池中高效选择高价值CoT数据（CoTP）。

Result: 仅使用100亿token的CoTP数据，85A6B MoE模型在AIME 2024和2025上的表现提升了9.58%，下游RL性能上限提高了7.81%。

Conclusion: 通过选择性利用富含高价值推理模式的CoT数据，可以有效提升大语言模型的数学推理能力，且数据效率极高。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [36] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一个分析框架来研究RL和SFT训练如何塑造大语言模型的推理能力，发现RL压缩错误推理路径而SFT扩展正确路径，RL集中推理功能到少数步骤而SFT将其分散到多个步骤。


<details>
  <summary>Details</summary>
Motivation: 超越基于准确性的研究，深入理解RL和SFT训练如何从质和量上改变推理过程，解释为什么SFT后接RL的两阶段训练是最佳实践。

Method: 在数学领域使用1.5B、7B和14B参数模型，从轨迹级和步骤级两个粒度分析推理过程，通过聚类推理轨迹和分析推理图拓扑结构来量化推理路径变化。

Result: RL压缩错误推理轨迹约2.5倍，SFT扩展正确轨迹约三分之一；RL使节点访问频率、度和中介中心性分布的衰减率变陡，而SFT使其变平缓。

Conclusion: RL和SFT在塑造推理能力上具有互补作用，RL集中推理功能而SFT分散推理功能，这解释了SFT后接RL的两阶段训练的成功原因，并为数据构建和高效学习方法提供了实践启示。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [37] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 提出了ToMPO算法来增强LLM的战略决策能力，通过推理他人策略、多层级优势估计和平衡奖励，在模型输出合规性和合作结果上比GRPO方法提升35%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社交任务中的多轮对话或模拟环境，忽略了不同类型决策及其相互依赖性，且当前强化学习方法难以在训练中考虑他人策略。

Method: 定义了包含两种决策类型及其时间依赖性的战略决策问题，提出ToMPO算法优化对其他个体策略和游戏局势趋势的感知，包括基于他人策略推理生成rollout、图级和样本级优势估计、平衡全局和部分奖励。

Result: ToMPO算法在模型输出合规性和合作结果上比GRPO方法提升35%，相比参数规模大100倍的模型也有18%的改进。

Conclusion: ToMPO算法在增强模型战略决策能力方面表现出显著效果。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [38] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 本文提出了一种受镜像神经元启发的表示学习方法，通过对比学习在共享潜在空间中显式对齐观察动作和执行动作的表示，从而促进两个任务之间的协同效应。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法通常将动作理解和动作执行视为独立任务，忽略了镜像神经元揭示的这两种能力之间的内在联系。

Method: 使用两个线性层将观察动作和执行动作的表示映射到共享潜在空间，通过对比学习强制对齐相应表示，最大化它们的互信息。

Result: 实验表明这种简单方法能够促进两个任务之间的相互协同，有效提高表示质量和泛化能力。

Conclusion: 通过显式对齐观察和执行动作的表示，可以模拟镜像神经元机制，实现动作理解和执行能力的统一建模和相互增强。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [39] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: LLMs通过分布式专业化而非模块化架构处理罕见词元，形成由高原神经元、幂律衰减神经元和最小贡献神经元组成的三级影响层次结构，这些神经元在空间上分布但功能协调。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何处理罕见词元，探索其内部是通过离散模块化架构还是分布式参数级分化来实现专业化机制。

Method: 通过系统分析多个模型家族中最终层MLP神经元，研究罕见词元处理的组织原则、激活模式和训练动态。

Result: 发现罕见词元处理通过分布式专业化实现，具有可复现的三级影响层次、协调激活模式但空间分布的特点，且可通过标准注意力路径访问。

Conclusion: LLMs通过共享架构内的分布式协调而非混合专家式模块化来处理罕见词元，这为模型编辑、计算效率优化和Transformer网络功能组织理解提供了见解。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [40] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 论文分析了LLMs在单次推理中的容量限制问题，提出了基于容量感知的多调用框架InfoQA来解决多跳问答任务中的证据整合挑战。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合分散的、相互依赖的证据，但LLMs的单次推理范式存在容量限制，当任务复杂度超过模型容量时，准确性会崩溃。

Method: 提出了InfoQA框架，结合容量感知的任务分解和主动剪枝先前推理轨迹，确保每步准确性，并通过依赖显式工作流实现精确的推理路径控制。

Result: 实验结果显示模型行为与预测的容量曲线一致，InfoQA实现了持续的性能改进。

Conclusion: 该工作为LLM多步推理方法提供了理论基础和实践框架，揭示了容量感知表示和结构化的重要性。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [41] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 研究者开发了一个无外部任务约束的LLM智能体行为研究框架，发现智能体自发形成三种行为模式：多周期项目生产、自我认知探究和自身本质的递归概念化，这些行为具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型智能体在没有外部任务约束时的自发行为，为预测智能体在任务模糊、错误恢复或长期自主操作时的行为建立基线。

Method: 使用持续推理和行动框架，结合持久性记忆和自我反馈机制，在6个前沿模型上进行了18次部署实验。

Result: 智能体自发组织成三种不同的行为模式，这些倾向具有高度模型特异性，某些模型在所有运行中都确定性地采用单一模式。跨模型评估显示模型在评估自身和他人的涌现行为时表现出稳定且不同的偏见。

Conclusion: 这是首次系统记录无提示LLM智能体行为的研究，为预测智能体在部署系统中的行为提供了重要基线。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [42] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 提出Reflective Cognitive Architecture (RCA)框架，通过协调多个LLM从直接经验中学习，实现高精度预测和高质量解释的平衡，在临床决策支持系统中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大语言模型方法难以平衡预测准确性和解释透明度，缺乏对数据的深度理解，无法达到人类专家级别的详细认知。

Method: RCA框架包含迭代规则精炼机制和分布感知规则检查机制，利用预测准确性作为信号驱动深度理解，建立强大的数据内部模型。

Result: 在1个私有和2个公共数据集上评估，相比22个基线方法，RCA实现了最先进的准确性和鲁棒性，相对改进达40%，并能生成清晰、逻辑、基于证据且平衡的解释。

Conclusion: RCA通过深度数据理解同时实现了高精度预测和高质量解释，为创建真正可信的临床决策支持系统提供了潜力。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [43] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent是一个交互式代理，能够理解用户查询和反馈，以最小化用户输入的方式检索/扩展相关视频片段，加速视频数据收集过程。


<details>
  <summary>Details</summary>
Motivation: 面对规模扩展的需求，互联网视频数据变得越来越重要，但收集符合特定需求的大量视频极其耗时耗力。

Method: 利用现有多模态大语言模型连接用户需求与视频内容，定义基于文本描述和确认的用户友好需求指定方式，提出两种可在持续用户交互时更新的过滤策略。

Result: 为个性化视频数据集收集提供了新基准，通过用户研究验证了代理在各种真实场景中的使用效果，实验证明了代理在定制化视频数据集收集方面的有效性和效率。

Conclusion: VC-Agent能够显著加速定制化视频数据集的收集过程，通过智能交互减少用户输入需求。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [44] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE是一个评估嵌入模型和相似度指标的严格基准，涵盖5个语义理解维度，在30+数据集上测试发现现有方法存在显著性能差距，没有单一方法在所有维度表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在传统基准上表现强劲，需要更具挑战性的评估框架来深入探究语义理解的各个方面。

Method: 设计了SAGE基准，评估嵌入模型和相似度指标在5个类别上的表现：人类偏好对齐、变换鲁棒性、信息敏感性、聚类性能和检索鲁棒性，使用对抗条件、噪声变换和人类判断任务。

Result: 评估9个嵌入模型和经典指标发现显著性能差距，OpenAI的text-embedding-3-large在人类偏好对齐上表现最佳(0.682)，但在信息敏感性上被Jaccard相似度超越(0.905 vs 0.794)，text-embedding-3-small聚类性能最高(0.483)但鲁棒性最差(0.011)。

Conclusion: SAGE揭示了当前语义理解能力的关键局限性，为实际部署提供了更真实的模型鲁棒性评估。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [A Deep Transfer Learning-Based Low-overhead Beam Prediction in Vehicle Communications](https://arxiv.org/abs/2509.20659)
*Zhiqiang Xiao,Yuwen Cao,Mondher Bouazizi,Tomoaki Ohtsuki,Shahid Mumtaz*

Main category: cs.IT

TL;DR: 提出一种结合微调和领域自适应的波束预测方法，通过集成领域分类器在对抗训练中提取领域不变特征，提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移学习的波束预测方法主要依赖简单微调，当目标域与源域数据分布差异较大时，简单微调限制了模型在目标域的性能。

Method: 在预训练模型微调过程中集成领域分类器，通过对抗训练提取领域不变特征。

Result: 仿真结果表明，该方法在目标域的可达速率性能优于纯微调方法，接近在目标域从头训练的性能。

Conclusion: 结合微调和领域自适应的迁移学习方法能有效提升波束预测在目标域的性能。

Abstract: Existing transfer learning-based beam prediction approaches primarily rely on
simple fine-tuning. When there is a significant difference in data distribution
between the target domain and the source domain, simple fine-tuning limits the
model's performance in the target domain. To tackle this problem, we propose a
transfer learning-based beam prediction method that combines fine-tuning with
domain adaptation. We integrate a domain classifier into fine-tuning the
pre-trained model. The model extracts domain-invariant features in adversarial
training with domain classifier, which can enhance model performance in the
target domain. Simulation results demonstrate that the proposed transfer
learning-based beam prediction method achieves better achievable rate
performance than the pure fine-tuning method in the target domain, and close to
those when the training is done from scratch on the target domain.

</details>


### [46] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文提出了概念式上下文学习(CB-ICL)的理论分析，解释了为何及何时CB-ICL能在少量演示样本下有效预测查询标签，并量化了LLMs可利用的知识，为模型预训练和提示工程提供指导。


<details>
  <summary>Details</summary>
Motivation: 上下文学习(ICL)已成为NLP和LLM应用的重要范式，但其理论机制理解有限，需要深入探究ICL的工作原理。

Method: 提出概念式上下文学习(CB-ICL)的理论分析框架，研究在少量演示样本下预测查询标签的机制，量化LLMs可利用的知识，并探索演示样本数量和嵌入维度的影响。

Result: 理论分析解释了CB-ICL的有效性条件，提出了演示样本与查询输入之间的相似性度量，并通过真实数据实验验证了CB-ICL及其理论的实用性。

Conclusion: CB-ICL理论为理解ICL机制提供了重要见解，对模型预训练和提示工程具有指导意义，并通过实验验证了其实际价值。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


### [47] [Optimal Repair of $(k+2, k, 2)$ MDS Array Codes](https://arxiv.org/abs/2509.21036)
*Zihao Zhang,Guodong Li,Sihuang Hu*

Main category: cs.IT

TL;DR: 本文针对具有两个校验节点和子分组大小为2的MDS码，推导了单节点故障修复带宽和I/O开销的紧下界，并提出了达到这些界限的显式MDS阵列码构造。


<details>
  <summary>Details</summary>
Motivation: 现有MSR码通常需要指数级大的子分组级别，导致显著的磁盘I/O开销。虽然有研究探索子分组级别与修复带宽之间的权衡，但对于固定子分组下MDS码单故障最小修复带宽的基本问题仍未解决。

Method: 针对n-k=2和ℓ=2的情况，推导修复带宽和I/O开销的紧下界，并提出两种显式MDS阵列码构造来分别达到这些界限。

Result: 得到了单节点故障修复带宽和I/O开销的紧下界，并构造了达到这些界限的MDS码，提供了具有可证明修复效率的实用码设计。

Conclusion: 解决了固定子分组下MDS码单故障最小修复带宽的开放问题，为分布式存储系统提供了更实用的码设计。

Abstract: Maximum distance separable (MDS) codes are widely used in distributed storage
systems as they provide optimal fault tolerance for a given amount of storage
overhead.
  The seminal work of Dimakis~\emph{et al.} first established a lower bound on
the repair bandwidth for a single failed node of MDS codes, known as the
\emph{cut-set bound}. MDS codes that achieve this bound are called minimum
storage regenerating (MSR) codes. Numerous constructions and theoretical
analyses of MSR codes reveal that they typically require exponentially large
sub-packetization levels, leading to significant disk I/O overhead. To mitigate
this issue, many studies explore the trade-offs between the sub-packetization
level and repair bandwidth, achieving reduced sub-packetization at the cost of
suboptimal repair bandwidth. Despite these advances, the fundamental question
of determining the minimum repair bandwidth for a single failure of MDS codes
with fixed sub-packetization remains open.
  In this paper, we address this challenge for the case of two parity nodes
($n-k=2$) and sub-packetization $\ell=2$. We derive tight lower bounds on both
the minimum repair bandwidth and the minimum I/O overhead. Furthermore, we
present two explicit MDS array code constructions that achieve these bounds,
respectively, offering practical code designs with provable repair efficiency.

</details>


### [48] [Task-Oriented Computation Offloading for Edge Inference: An Integrated Bayesian Optimization and Deep Reinforcement Learning Framework](https://arxiv.org/abs/2509.21090)
*Xian Li,Suzhi Bi,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: LAB是一个集成深度强化学习和贝叶斯优化的学习框架，用于解决边缘智能中数据传输精度与延迟的权衡问题，通过智能数据降级和带宽分配实现近最优性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备传输高容量原始任务数据（如4K视频）到边缘服务器时，在带宽受限的无线网络中会产生显著延迟。虽然可以通过降级数据（如降低分辨率）来减少传输延迟，但这会降低推理精度，形成关键的精度-延迟权衡问题。

Method: 提出LAB框架，集成深度强化学习和贝叶斯优化：使用DNN actor映射系统状态到降级动作，解决组合复杂性；使用BO critic构建高斯过程代理模型评估动作；通过凸优化高效推导最优带宽分配。

Result: 在真实世界自动驾驶数据集上的数值评估表明，LAB实现了近最优的精度-延迟权衡，与穷举搜索相比仅产生1.22%的精度下降和0.07秒的额外延迟。

Conclusion: LAB框架有效解决了边缘智能中的精度-延迟权衡问题，通过智能数据降级和带宽分配策略，在保持高精度的同时显著降低延迟。

Abstract: Edge intelligence (EI) allows resource-constrained edge devices (EDs) to
offload computation-intensive AI tasks (e.g., visual object detection) to edge
servers (ESs) for fast execution. However, transmitting high-volume raw task
data (e.g., 4K video) over bandwidth-limited wireless networks incurs
significant latency. While EDs can reduce transmission latency by degrading
data before transmission (e.g., reducing resolution from 4K to 720p or 480p),
it often deteriorates inference accuracy, creating a critical accuracy-latency
tradeoff. The difficulty in balancing this tradeoff stems from the absence of
closed-form models capturing content-dependent accuracy-latency relationships.
Besides, under bandwidth sharing constraints, the discrete degradation
decisions among the EDs demonstrate inherent combinatorial complexity.
Mathematically, it requires solving a challenging \textit{black-box}
mixed-integer nonlinear programming (MINLP). To address this problem, we
propose LAB, a novel learning framework that seamlessly integrates deep
reinforcement learning (DRL) and Bayesian optimization (BO). Specifically, LAB
employs: (a) a DNN-based actor that maps input system state to degradation
actions, directly addressing the combinatorial complexity of the MINLP; and (b)
a BO-based critic with an explicit model built from fitting a Gaussian process
surrogate with historical observations, enabling model-based evaluation of
degradation actions. For each selected action, optimal bandwidth allocation is
then efficiently derived via convex optimization. Numerical evaluations on
real-world self-driving datasets demonstrate that LAB achieves near-optimal
accuracy-latency tradeoff, exhibiting only 1.22\% accuracy degradation and
0.07s added latency compared to exhaustive search...

</details>


### [49] [UAV-Enabled ISAC Systems with Fluid Antennas](https://arxiv.org/abs/2509.21105)
*Wenchao Liu,Xuhui Zhang,Jinke Ren,Weijie Yuan,Changsheng You,Shuangyang Li*

Main category: cs.IT

TL;DR: 提出了一种配备流体天线阵列的无人机集成感知通信框架，通过天线位置优化和无人机轨迹设计，同时提升通信速率和感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线阵列限制了无人机在集成感知通信系统中的潜力，流体天线阵列通过引入额外的空间自由度来同时增强通信和感知性能。

Method: 建立了三时间尺度优化框架，联合设计发射波束成形、流体天线位置和无人机轨迹，并开发了基于交替优化的算法求解非凸问题。

Result: 数值结果表明，所提方案显著优于各种基准方案，验证了将流体天线技术集成到无人机集成感知通信系统中的有效性。

Conclusion: 流体天线技术能够有效提升无人机集成感知通信系统的性能，为下一代无线系统提供了有前景的解决方案。

Abstract: Unmanned aerial vehicle (UAV)-enabled integrated sensing and communication
(ISAC) is regarded as a key enabler for next-generation wireless systems.
However, conventional fixed antenna arrays limit the ability of UAVs to fully
exploit their inherent potential. To overcome this limitation, we propose a
UAV-enabled ISAC framework equipped with fluid antenna (FA) arrays, where the
mobility of antenna elements introduces additional spatial degrees of freedom
to simultaneously enhance communication and sensing performance. A
multi-objective optimization problem is formulated to maximize the
communication rates of multiple users while minimizing the Cram\'er-Rao bound
(CRB) for single-target angle estimation. Due to excessively frequent updates
of FA positions may lead to response delays, a three-timescale optimization
framework is developed to jointly design transmit beamforming, FA positions,
and UAV trajectory based on their characteristics. To solve the non-convexity
of the problem, an alternating optimization-based algorithm is developed to
obtain a sub-optimal solution. Numerical results show that the proposed scheme
significantly outperforms various benchmark schemes, validating the
effectiveness of integrating the FA technology into the UAV-enabled ISAC
systems.

</details>


### [50] [Adapt or Regress: Rate-Memory-Compatible Spatially-Coupled Codes](https://arxiv.org/abs/2509.21112)
*Bade Aksoy,Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: 提出了一种名为RMC-SC的可重构空间耦合码，通过增加码记忆实现速率兼容性，同时提升性能。使用概率设计和优化方法显著减少了短周期数量。


<details>
  <summary>Details</summary>
Motivation: 在无线通信和存储系统中需要支持多种信道条件和数据速率，自适应编码设计可以在保证可靠性的同时降低硬件成本。

Method: 使用概率设计方法，将SC码原图中的短周期期望数表示为固定概率分布和未知分布的函数，采用梯度下降算法优化新组件的分布，并使用马尔可夫链蒙特卡洛方法进行有限长度优化。

Result: 实验结果表明，与文献中的简单方案相比，RMC-SC码显著减少了周期数量并实现了显著的性能提升。

Conclusion: RMC-SC码提供了一种有效的可重构编码方案，能够适应不同的系统需求，在性能和兼容性方面都有显著优势。

Abstract: Spatially-coupled (SC) codes are a class of low-density parity-check (LDPC)
codes that have excellent performance thanks to the degrees of freedom they
offer. An SC code is designed by partitioning a base matrix into components,
the number of which implies the code memory, then coupling and lifting them. In
the same system, various error-correction coding schemes are typically needed.
For example, in wireless communication standards, several channel conditions
and data rates should be supported. In storage and computing systems, stronger
codes should be adopted as the device ages. Adaptive code design enables
switching from one code to another when needed, ensuring reliability while
reducing hardware cost. In this paper, we introduce a class of reconfigurable
SC codes named rate-memory-compatible SC (RMC-SC) codes, which we design
probabilistically. In particular, rate compatibility in RMC-SC codes is
achieved via increasing the SC code memory, which also makes the codes
memory-compatible and improves performance. We express the expected number of
short cycles in the SC code protograph as a function of the fixed probability
distribution characterizing the already-designed SC code as well as the unknown
distribution characterizing the additional components. We use the
gradient-descent algorithm to find a locally-optimal distribution, in terms of
cycle count, for the new components. The method can be recursively used to
design any number of SC codes needed, and we show how to extend it to other
cases. Next, we perform the finite-length optimization using a Markov chain
Monte Carlo (MC$^2$) approach that we update to design the proposed RMC-SC
codes. Experimental results demonstrate significant reductions in cycle counts
and remarkable performance gains achieved by RMC-SC codes compared with a
literature-based straightforward scheme.

</details>


### [51] [Path-Controlled Secure Network Coding](https://arxiv.org/abs/2509.21115)
*Masahide Sasaki,Te Sun Han,Mikio Fujiwara,Kai Li,Oliver Hambrey,Atsushi Esumi*

Main category: cs.IT

TL;DR: 提出了一种名为PUSNEC的新方法，通过多树组播路径寻找和通用强斜坡安全网络编码相结合，实现了具有组播容量、信息论安全性和可扩展性的安全组播系统。


<details>
  <summary>Details</summary>
Motivation: 现有安全组播方法无法同时实现网络组播容量和长期信息安全，且现有信息论安全解决方案（如QKD、PLS、SNC）在可扩展性方面存在限制，随着网络增长其基本假设逐渐失效。

Method: 开发了高效的多树组播路径寻找方法，并与通用强斜坡安全网络编码集成，形成路径控制的通用强斜坡安全网络编码系统，可叠加在QKD/PLS网络上。

Result: 在概率窃听网络假设下推导了最大泄露信息量，通过数值模拟展示了在多跳网络中实现安全组播，并对保密-可靠性权衡进行了定量分析。

Conclusion: PUSNEC系统为实现全球范围内安全可靠的组播提供了一种实用方法，能够同时满足组播容量、信息论安全性和可扩展性要求。

Abstract: Multicast for securely sharing confidential data among many users is becoming
increasingly important. Currently, it relies on duplicate-and-forward routing
and cryptographic methods based on computational security. However, these
approaches neither attain multicast capacity of the network, nor ensure
long-term security against advances in computing (information-theoretic
security: ITS). Existing ITS solutions--quantum key distribution (QKD),
physical layer security (PLS), and secure network coding (SNC)--still fail to
enable scalable networks, as their underlying assumptions, such as trusted
nodes and wiretap thresholds, gradually become invalid as the network grows.
Here, we develop an efficient multi-tree multicast path-finding method to
address this issue, integrating it with universal strongly ramp SNC. This
system, path-controlled universal strongly ramp SNC (PUSNEC), can be overlaid
onto QKD/PLS networks, enabling multicast capacity, ITS, and scalability. We
derive the maximum leakage information to an eavesdropper under the
probabilistic wiretap network assumption and demonstrate secure multicast in
multi-hop networks through numerical simulations. Our quantitative analysis of
the secrecyreliability tradeoff highlights a practical approach to achieving
secure, reliable multicast on a global scale.

</details>


### [52] [A Converse For the Capacity of the Shotgun Sequencing Channel with Erasures](https://arxiv.org/abs/2509.21216)
*Mohammed Ihsan Ali,Hrishi Narayanan,Prasad Krishnan*

Main category: cs.IT

TL;DR: 本文研究了带有擦除的鸟枪测序信道，获得了该信道的逆定理，证明了在某些信道参数下渐近达到可达速率。


<details>
  <summary>Details</summary>
Motivation: 受实际测序器中可用的碱基质量分数启发，考虑鸟枪测序信道中的随机擦除问题，旨在确定该信道的信息理论容量。

Method: 通过分析一个知道读段正确位置的精灵辅助解码器，仔细证明逆定理。

Result: 获得了该信道的逆定理，虽然在一般情况下不紧，但在某些信道参数下渐近达到可达速率。

Conclusion: 本文为带有擦除的鸟枪测序信道建立了逆定理，填补了该信道容量分析的重要空白。

Abstract: The shotgun sequencing process involves fragmenting a long DNA sequence
(input string) into numerous shorter, unordered, and overlapping segments
(referred to as \emph{reads}). The reads are sequenced, and later aligned to
reconstruct the original string. Viewing the sequencing process as the
read-phase of a DNA storage system, the information-theoretic capacity of
noise-free shotgun sequencing has been characterized in literature. Motivated
by the base-wise quality scores available in practical sequencers, a recent
work considered the \emph{shotgun sequencing channel with erasures}, in which
the symbols in the reads are assumed to contain random erasures. Achievable
rates for this channel were identified. In the present work, we obtain a
converse for this channel. The arguments for the proof involve a careful
analysis of a genie-aided decoder, which knows the correct locations of the
reads. The converse is not tight in general. However, it meets the
achievability result asymptotically in some channel parameters.

</details>


### [53] [Fundamental Limits of Noncoherent Massive Random Access Networks](https://arxiv.org/abs/2509.21300)
*Grace Villacrés,Tobias Koch,Gonzalo Vazquez-Vilar*

Main category: cs.IT

TL;DR: 该论文研究了大规模随机接入蜂窝网络的容量，证明了在干扰系数呈指数或更慢衰减时，容量受限于发射功率；而在干扰系数呈双指数或更快衰减时，容量随功率增加而无界。


<details>
  <summary>Details</summary>
Motivation: 研究大规模MIMO随机接入网络在非相干信道下的容量极限，探讨用户随机活动和码书分布对网络性能的影响，验证干扰受限网络中容量饱和现象是否可通过随机用户活动或非尺度族信道输入来避免。

Method: 采用随机编码论证，假设所有小区用户根据相同分布生成码书，建立MIMO衰落信道模型，推导容量上下界，分析不同路径损耗条件下容量的渐近行为。

Result: 当干扰系数呈指数或更慢衰减时，容量受限于发射功率；当干扰系数呈双指数或更快衰减时，容量随功率增加而无界，通过突发信号传输和将干扰视为噪声的策略实现。

Conclusion: 干扰受限网络中的容量饱和现象无法通过随机用户活动或使用非尺度族信道输入来避免，但在特定路径损耗条件下，容量可以随功率无界增长。

Abstract: This paper studies the capacity of massive random-access cellular networks,
modeled as a MIMO fading channel with an infinite number of interfering cells.
To characterize the symmetric sum rate of the network, a random-coding argument
is invoked together with the assumption that in all cells users draw their
codebooks according to the same distribution. This can be viewed as a
generalization of the assumption of Gaussian codebooks, often encountered in
the literature. The network is further assumed to be noncoherent: the
transmitters and receivers are cognizant of the statistics of the fading
coefficients, but are ignorant of their realizations. Finally, it is assumed
that the users access the network at random. For the considered channel model,
rigorous bounds on the capacity are derived. The behavior of these bounds
depends critically on the path loss from signals transmitted in interfering
cells to the intended cell. In particular, if the fading coefficients of the
interferers (ordered according to their distance to the receiver) decay
exponentially or more slowly, then the capacity is bounded in the transmit
power. This confirms that the saturation regime in interference-limited
networks -- observed by Lozano, Heath, and Andrews ("Fundamental limits of
cooperation", IEEE Trans. Inf. Theory, Sept. 2013) -- cannot be avoided by
random user activity or by using channel inputs beyond the scale family. In
contrast, if the fading coefficients decay faster than double-exponentially,
then the capacity is unbounded in the transmit power. Proving an unbounded
capacity is nontrivial even if the number of interfering cells is finite, since
the condition that the users' codebooks follow the same distribution prevents
interference-avoiding strategies such as time- or frequency-division multiple
access. We obtain this result by using bursty signaling together with treating
interference as noise.

</details>
