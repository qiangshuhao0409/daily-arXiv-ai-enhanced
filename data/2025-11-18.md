<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Bio-Inspired Leader-based Energy Management System for Drone Fleets](https://arxiv.org/abs/2511.12070)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.NI

TL;DR: 提出了一种基于生物启发式领导机制的无人机集群能量管理系统，通过动态选择单个领导者无人机处理与地面基站的长距离通信，显著提升网络效率和服务时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在联网系统中面临电池寿命有限和能耗高的挑战，特别是在多无人机与地面基站通信的场景下，需要解决能量管理问题。

Method: 采用生物行为模型启发的算法，动态选择集群中的单个无人机作为领导者，专门负责与地面基站的长距离通信，其他无人机可节省能量。

Result: 通过不同网络规模和配置的评估，该算法显著提高了网络效率和服务时间，消除了无用的能耗通信。

Conclusion: 生物启发式领导机制能量管理系统能有效解决无人机集群的能量限制问题，提升整体系统性能。

Abstract: Drones are embedded systems (ES) used across a wide range of fields, from photography to shipments and even during crisis management for searching, rescuing and damage assessment activities. However, their limited battery life and high energy consumption are very important challenges, especially in networked systems where multiple drones must communicate with a Ground Base Station (GBS). This study addresses these limitations by proposing the implementation of a bio-inspired leader-based energy management system for drone fleets. Inspired by bio-behavioral models, the algorithm dynamically chooses a single drone as a Leader in a cluster to handle long-range communication with the GBS, allowing other drones to preserve their energy. The effectiveness of the proposed bio-inspired algorithm is evaluated by varying network sizes and configurations. The results demonstrate that our approach significantly increases network efficiency and service time by removing useless energy consumption communications.

</details>


### [2] [Joint Optimization of RU Allocation and C-SR in Multi-AP Coordinated Wi-Fi Systems](https://arxiv.org/abs/2511.12127)
*Md Rahat Hasan,Kazi Ahmed Akbar Munim,Md. Forkan Uddin*

Main category: cs.NI

TL;DR: 本文提出了一种联合RU分配和C-SR的优化问题，旨在最大化多AP协调WiFi系统的吞吐量，并通过优化工具和启发式方法解决该非线性整数规划问题。


<details>
  <summary>Details</summary>
Motivation: 多AP协调WiFi系统需要优化资源单元分配和协调策略，以提升系统吞吐量性能。

Method: 将问题建模为非线性整数规划问题，使用优化工具求解，并提供计算复杂度较低的启发式解决方案。

Result: 联合设计相比非协调系统显著提高了吞吐量，启发式方法实现了与计算昂贵的优化工具相当的吞吐量性能。

Conclusion: 提出的联合优化框架和启发式方法能有效提升多AP WiFi系统性能，同时平衡计算复杂度和性能需求。

Abstract: We formulate an optimization problem for joint RU allocation and C-SR to maximize the throughput of a multi-AP coordinated WiFi system. The optimization problem is found to be a non-linear integer programming problem. We solve the problem for several network scenarios using an optimization tool. The joint design significantly improves throughput compared to a non-coordinated system. To reduce computational complexity, we also provide a heuristic solution to the problem. The proposed heuristic achieves throughput comparable to that of the computationally expensive optimization tool based solution approach.

</details>


### [3] [Collaborative Charging Optimization for Wireless Rechargeable Sensor Networks via Heterogeneous Mobile Chargers](https://arxiv.org/abs/2511.12501)
*Jianhang Yao,Hui Kang,Geng Sun,Jiahui Li,Hongjuan Li,Jiacheng Wang,Yinqiu Liu,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出IHATRPO算法优化异构移动充电架构，在复杂地形中结合无人机和地面车辆协同充电，显著提升传感器网络生存率和充电效率。


<details>
  <summary>Details</summary>
Motivation: 传统无线传感器网络受限于能量约束，无线可充电传感器网络结合无线能量传输技术理论上可实现无限寿命，但在复杂地形中需要异构充电器协同工作。

Method: 提出改进的异构代理信任域策略优化算法，集成自注意力机制增强环境状态处理，采用Beta采样策略实现连续动作空间的无偏梯度计算。

Result: IHATRPO相比原始HATRPO性能提升39%，显著优于现有基线算法，大幅提高传感器节点生存率和充电系统效率。

Conclusion: 异构移动充电架构与IHATRPO算法能有效解决复杂地形中的无线传感器网络能量管理问题，实现可持续运行。

Abstract: Despite the rapid proliferation of Internet of Things applications driving widespread wireless sensor network (WSN) deployment, traditional WSNs remain fundamentally constrained by persistent energy limitations that severely restrict network lifetime and operational sustainability. Wireless rechargeable sensor networks (WRSNs) integrated with wireless power transfer (WPT) technology emerge as a transformative paradigm, theoretically enabling unlimited operational lifetime. In this paper, we investigate a heterogeneous mobile charging architecture that strategically combines automated aerial vehicles (AAVs) and ground smart vehicles (SVs) in complex terrain scenarios to collaboratively exploit the superior mobility of AAVs and extended endurance of SVs for optimal energy distribution. We formulate a multi-objective optimization problem that simultaneously addresses the dynamic balance of heterogeneous charger advantages, charging efficiency versus mobility energy consumption trade-offs, and real-time adaptive coordination under time-varying network conditions. This problem presents significant computational challenges due to its high-dimensional continuous action space, non-convex optimization landscape, and dynamic environmental constraints. To address these challenges, we propose the improved heterogeneous agent trust region policy optimization (IHATRPO) algorithm that integrates a self-attention mechanism for enhanced complex environmental state processing and employs a Beta sampling strategy to achieve unbiased gradient computation in continuous action spaces. Comprehensive simulation results demonstrate that IHATRPO achieves a 39% performance improvement over the original HATRPO, significantly outperforming state-of-the-art baseline algorithms while substantially increasing sensor node survival rate and charging system efficiency.

</details>


### [4] [CareNet: Linking Home-router Network Traffic to DSM-5 Depressive Behavior Indicators](https://arxiv.org/abs/2511.12772)
*Stephan Nef,Bruno Rodrigues*

Main category: cs.NI

TL;DR: CareNet是一个基于路由器的系统，将家庭网络元数据转换为可解释的行为指标，用于评估DSM-5抑郁症状，所有处理都在本地网关完成以保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前数字心理健康感知依赖于需要侵入性权限和持续用户合规的移动或可穿戴设备，存在隐私和依从性问题。

Method: 使用Fuzzy Additive Symptom Likelihood (FASL)方法，通过有界模糊成员函数和加性聚合将头部级指标融合为每日标准级可能性，结合DSM风格的时间门控。

Result: 在现实多日追踪评估中，CareNet能够捕捉延迟睡眠时间和注意力不稳定等特征模式，无需检查有效载荷。

Conclusion: 结果表明从路由器端遥测进行可重现、可解释的行为推断是可行的。

Abstract: Digital mental-health sensing increasingly depends on mobile or wearable devices that require intrusive permissions and continuous user compliance. We present CareNet, a router-centric system that transforms household network metadata into interpretable behavioral indicators aligned with DSM-5 depressive-symptom domains. All processing occurs locally at the home gateway, preserving privacy while maintaining visibility of temporal routines.
  The core contribution is the Fuzzy Additive Symptom Likelihood (FASL), a transparent formulation that fuses header-level metrics into daily criterion-level likelihoods using bounded fuzzy memberships and additive aggregation. Combined with a DSM-style temporal gate, FASL integrates short-term traffic fluctuations into persistent, clinically interpretable indicators. Evaluation on realistic multi-day traces shows that CareNet captures characteristic patterns such as delayed sleep timing and attentional instability without payload inspection. The results highlight the feasibility of reproducible, explainable behavioral inference from router-side telemetry.

</details>


### [5] [Distributed Pulse-Wave Simulator for DDoS Dataset Generation](https://arxiv.org/abs/2511.12774)
*Karim Khamaisi,Pascal Kiechl,Katharina Müller,Burkhard Stiller,Bruno Rodrigues*

Main category: cs.NI

TL;DR: DPWS是一个开源模拟器，用于生成分布式脉冲波DDoS攻击数据集，通过多自治系统拓扑和同步数据包捕获来模拟协调的攻击爆发。


<details>
  <summary>Details</summary>
Motivation: 脉冲波DDoS攻击产生短暂、同步的流量爆发，规避基于模式的检测并快速耗尽传统防御系统。由于缺乏跨多个网络域的攻击演化公开数据集，且每个域只能观察到攻击的部分视图，因此需要相关多视角分析。

Method: DPWS模拟多AS拓扑，在多个自治系统产生同步数据包捕获，通过轻量级YAML接口控制流量参数，使用MPI在ns-3中扩展。

Result: DPWS能够在多个观察点重现脉冲波动态，在相同聚合速率下展示自然指纹变异性，并为研究脉冲波行为和基准测试分布式DDoS防御提供可重现基础。

Conclusion: DPWS提供了研究脉冲波DDoS攻击和基准测试分布式防御的有效工具，同时分享了关于ns-3可扩展性和同步的实用见解。

Abstract: Pulse-wave Distributed Denial-of-Service (DDoS) attacks generate short, synchronized bursts of traffic that circumvent pattern-based detection and quickly exhaust traditional defense systems. This transient and spatially distributed behavior makes analysis extremely challenging, as no public datasets capture how such attacks evolve across multiple network domains. Since each domain observes only a partial viewpoint of the attack, a correlated, multi-vantage view is essential for comprehensive analysis, early detection, and attribution.
  This paper presents DPWS, an open-source simulator for generating distributed pulse-wave DDoS datasets. DPWS models multi-AS topologies and produces synchronized packet captures at multiple autonomous systems, showing the distributed structure of coordinated bursts. It enables fine-grained control of traffic parameters through a lightweight YAML interface. DPWS reproduces pulse-wave dynamics across multiple vantage points, exhibits natural fingerprint variability at equal aggregate rates, and scales with MPI in ns-3, providing a reproducible basis for studying pulse-wave behaviour and benchmarking distributed DDoS defenses, while sharing practical insights on ns-3 scalability and synchronization gained during development.

</details>


### [6] [Distributed Self-allocated Time Slot Reuse: Multi-hop Communication in Rigid UAV Formations](https://arxiv.org/abs/2511.12888)
*Amelia Samandari,Andreas Willig,Barry Wu,Philippa Martin*

Main category: cs.NI

TL;DR: 提出了分布式自分配时隙重用（D-STR）协议，用于无人机编队中安全信息的及时可靠通信，支持不同网络拓扑结构下的无碰撞部署。


<details>
  <summary>Details</summary>
Motivation: 无人机自主编队部署需要准确及时的安全信息通信，需要一种支持安全信息在无人机间及时成功传输的通信协议。

Method: 开发了分布式自分配时隙重用（D-STR）协议，专门用于刚性无人机编队中不同网络拓扑结构下的安全信息通信。

Result: 该协议能够实现无人机编队的无碰撞部署，提高了编队的安全性和实用性。

Conclusion: D-STR协议是提升无人机编队在各类工业应用场景中安全性和实用性的重要步骤。

Abstract: Deployment of Unmanned Aerial Vehicles (UAVs) in autonomous formations necessitates accurate and timely communication of safety information. A communication protocol that supports timely and successful transfer of safety information between UAVs is therefore needed. This paper presents Distributed Self-allocated Time slot Reuse (D-STR). Our D-STR protocol addresses the essential task of communicating safety information in rigid Unmanned Aerial Vehicle (UAV) formations with different network topologies, enabling collision-free deployment of the formation. This is an important step for improving the safety and practicality of UAV formations in application scenarios that span a range of industries.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成合成新闻标题数据集来替代真实数据支持NLP任务的可行性，特别关注负面情感文本，验证了合成数据在内容、语气和风格上与真实数据的对齐度。


<details>
  <summary>Details</summary>
Motivation: 克服真实数据获取困难和隐私问题，为情感分析等NLP任务提供替代数据源。

Method: 使用定制提示生成负面新闻标题语料库，通过专家评审和嵌入空间分析验证，采用多种指标（相关性、困惑度、连贯性、真实性）和基准测试方法。

Result: 合成标题与真实标题高度匹配，仅在词性标注测试的专有名词得分上存在明显差异。

Conclusion: LLM生成的合成数据集可以有效替代真实数据支持NLP任务，特别是在负面情感分析领域。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [8] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB基准测试评估LLMs在气候变化领域的专业知识和证据支持能力，发现前沿模型具有博士级别的知识综合能力，但在证据基础方面存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型处理复杂专业知识的能力，特别是在气候变化这一关键领域，需要建立可靠的多模态基准。

Method: 引入CLINB基准，基于真实用户问题和气候科学家制定的评估标准，评估模型在开放式、多模态问答任务中的表现。

Result: 前沿模型展现出卓越的知识综合能力，甚至优于专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像幻觉率很高。

Conclusion: 弥合知识综合与可验证归因之间的差距对于AI在科学工作流中的部署至关重要，需要像CLINB这样的可靠基准来构建可信AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [9] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于研究和检测网络欺凌的合成多LLM对话数据集，通过大语言模型模拟真实欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统的人类数据收集方法在扩展性和伦理安全性方面存在局限，需要一种能够模拟真实欺凌互动并提供上下文感知标注的替代方案。

Method: 利用大语言模型生成多轮对话，提供对话结构、上下文感知标注和细粒度标签，涵盖多种网络欺凌类别。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布五个维度上进行了评估，并测试了其作为独立训练数据和增强源的性能。

Conclusion: SynBullying为网络欺凌研究提供了一个可扩展、伦理安全且具有上下文感知能力的合成数据集，在分类任务中表现出良好的实用性。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [10] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: 提出了MM-Telco，一个针对电信领域的多模态基准测试套件和模型，用于解决LLM在电信应用中的领域特定挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电信领域有巨大潜力，但面临领域特定挑战，需要专门适配才能实现网络优化、故障排除、客户支持和合规性等应用。

Method: 开发了包含文本和图像任务的综合多模态基准测试套件，涵盖网络运营、网络管理、文档质量改进等实际用例，并对各种LLM和VLM进行基线实验。

Result: 在数据集上微调的模型性能显著提升，实验揭示了当前最先进多模态LLM的薄弱环节。

Conclusion: MM-Telco为电信领域LLM的进一步发展提供了指导方向，加速了LLM在电信领域的适应过程。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [11] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，能够实时检测和防止大语言模型的幻觉问题，在12个基准测试中准确识别89.3%的幻觉，同时减少近80%的错误陈述。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在严重幻觉问题，会自信地陈述听起来合理但实际错误的信息，这成为在准确性要求高的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过两条互补路径工作：一条追踪模型已知信息与生成内容之间的因果关系，另一条使用自动推理检查逻辑一致性。与仅检查生成后输出的方法不同，该系统理解导致错误陈述的因果链并在过程中早期进行干预。

Result: 在12个不同基准测试中，CausalGuard正确识别了89.3%的幻觉，仅遗漏了8.3%的实际幻觉。更重要的是，它减少了近80%的错误陈述，同时保持回答自然和有用。该系统在需要多步逻辑的复杂推理任务上表现尤其出色。

Conclusion: CausalGuard通过展示其推理过程，在医疗诊断或金融分析等敏感领域表现良好，因为这些领域理解决策原因与决策本身同等重要。该方法为解决大语言模型幻觉问题提供了一种有效且可解释的解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [12] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，并引入波动性Sigma来衡量回合间结果的不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统性的方法来量化游戏中技能和运气的相对贡献，以便进行游戏设计、AI评估和风险分析等方面的应用。

Method: 将游戏建模为随机决策树，分解游戏结果为技能杠杆K和运气杠杆L，定义技能-运气指数S(G) = K - L，并引入波动性Sigma来量化结果不确定性。

Result: 应用于30个游戏的分析显示：从纯运气游戏（抛硬币，S=-1）到混合游戏（西洋双陆棋，S=0）再到纯技能游戏（国际象棋，S=+1）。扑克表现出中等技能优势（S=0.33）。

Conclusion: 该框架可扩展到一般随机决策系统，为玩家影响力、游戏平衡性和预测稳定性提供了原则性的比较方法。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [13] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化、零样本的代理框架，通过分层提示分析和价值对齐推理来实现更安全、更有帮助的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在产生不安全、冒犯性或文化不适当内容的风险，现有防御方法在保持生成质量和成本控制方面存在困难。

Method: VALOR整合了多级NSFW检测器、文化价值对齐模块和意图消歧器，检测到不安全内容时通过LLM选择性重写提示，并在必要时进行风格化再生。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少了高达100.00%，同时保持了提示的有用性和创造性。

Conclusion: VALOR为在开放世界环境中部署安全、对齐且有帮助的图像生成系统提供了一种可扩展且有效的方法。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [14] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个LLM代理系统，能够自主生成量子物理领域的研究想法并设计可实施的实验方案，展示了AI科学家的原型能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类提供研究问题，缺乏自主生成和实现创意的能力。开发能够自动生成并实施科学想法的系统将改变人类在科研过程中的角色。

Method: AI-Mandel利用LLM从文献中生成想法，并使用领域特定的AI工具将这些想法转化为具体的实验设计方案，可直接在实验室实施。

Result: AI-Mandel生成的想法具有科学价值，其中两个想法已独立撰写成后续科学论文。生成的想法包括量子隐形传态的新变体、不定因果顺序中的量子网络原语，以及基于量子信息传输闭环的新几何相位概念。

Conclusion: AI-Mandel展示了能够生成和实施具体可行想法的AI物理学家原型。构建此类系统不仅有助于加速科学发展，还揭示了实现人类水平AI科学家面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [15] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体框架，让小型LLM通过迭代执行反馈学习SPARQL查询构建策略，在LC-QuAD 2.0数据集上相比零样本基线提升17.5个百分点。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问题中复杂SPARQL查询生成的瓶颈问题，当前方法缺乏基于实时执行反馈的动态调试策略。

Method: 使用仅3B参数的LLM，通过结果驱动的强化学习训练，学习迭代SPARQL构建的弹性策略，无需监督微调。

Result: 在LC-QuAD 2.0可执行子集上达到49.7%的准确率，比最强的迭代零样本基线提升17.5个百分点。

Conclusion: 该工作为通过交互教授智能体掌握形式化符号工具提供了可推广的蓝图，弥合了概率LLM与结构化知识图谱之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [16] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文主张用"通用性"而非"智能"作为AI评估的基础，认为智能概念模糊且无法预测实际任务表现，而通用性作为多任务学习问题能更稳定地衡量AI能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于智能概念（如ARC、Raven测试等）的基准测试缺乏稳定定义，且无法预测AI在实际任务（如问答、摘要、编程）中的表现，存在评估与实际效用脱节的风险。

Method: 通过概念和形式分析，检验智能评估的三个假设（通用性、稳定性、现实性），论证只有通用性能够经受概念和实证检验。

Result: 研究表明智能并不能实现通用性，通用性应被理解为多任务学习问题，直接关联到可测量的性能广度和可靠性。

Conclusion: 应重新构建AI进展评估方式，将通用性作为评估多样化演进任务能力的更稳定基础，而非依赖抽象的智能概念。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [17] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除待遗忘数据对模型的额外贡献，同时保留其他数据支持的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要从训练模型中移除特定数据的影响以实现隐私保护和法规遵从。现有遗忘方法往往在尝试'遗忘'特定数据时会移除过多信息，导致模型性能下降。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除待遗忘数据对模型的额外贡献，同时保留待保留数据支持的信息，提供明确的遗忘数据集残余影响上界。

Result: 大量实验证实该方法优于当前最先进的遗忘方法，在多种基准测试中实现了可靠的遗忘和更好的通用模型性能保留。

Conclusion: 这一进展代表了使AI系统更可控、更符合隐私和版权法规而不损害其有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [18] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性地评估了现有NL-FOL翻译数据集和评估协议的局限性，提出了新的评估方法，并证明对话导向的LLM在自然语言到一阶逻辑翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于一阶逻辑的表达能力和明确性，将自然语言转换为FOL是一个重要但具有挑战性的任务。虽然LLM的出现带来了突破的希望，但现有评估方法可能无法准确反映其真实能力。

Method: 1) 批判性分析现有NL-FOL翻译评估数据集和协议；2) 提出新的评估协议，区分真正的语义级逻辑理解与表面模式识别；3) 使用新方法评估不同LLM的性能。

Result: 研究发现，对话导向的最先进LLM展现出强大的NL-FOL翻译能力和真正的句子级逻辑理解，而嵌入中心模型表现明显较差。

Conclusion: 通过适当的评估方法，可以证明现代LLM在自然语言到一阶逻辑翻译方面具有显著能力，但需要区分不同架构模型的性能差异。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [19] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准包含局部捷径，可能高估模型的感知能力。大型视觉语言模型中的视觉感知模块成为瓶颈，限制了整体能力。

Method: 利用拓扑属性创建评估基准，因为拓扑依赖于图像的全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 即使在最粗糙的感知粒度下，所有模型表现都不优于随机机会，表明它们缺乏全局视觉特征感知能力。更强大的模型反而准确率更低。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [20] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，可将组织解剖视频转化为手势序列，并发现与术后结果相关的模式，为数据驱动的手术反馈和临床决策支持奠定基础。


<details>
  <summary>Details</summary>
Motivation: 术中行为的细粒度分析及其对患者结果的影响是一个长期挑战，需要自动化的可解释评估方法。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，在机器人辅助根治性前列腺切除术中稳健检测连续短手势（约2秒）。

Result: F2O在帧级和视频级的手势检测AUC分别达到0.80和0.81；F2O衍生特征预测术后结果的准确性与人工标注相当（0.79 vs 0.75）；发现了与勃起功能恢复相关的关键模式。

Conclusion: F2O通过实现自动可解释评估，为数据驱动的手术反馈和前瞻性临床决策支持建立了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [21] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 本研究系统评估了四种大语言模型在抽象视觉推理任务中的表现，发现GPT-4.1-Mini在四种推理架构下均表现最佳，但不同模型对架构设计具有特异性敏感度。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在抽象视觉推理问题中的性能表现，比较不同模型和推理架构的有效性。

Method: 使用四种LLM模型和四种推理架构在RAVEN-FAIR数据集上进行测试，通过三阶段过程生成视觉响应，采用SSIM和LPIPS指标评估，分析思维链分数和错误类型。

Result: GPT-4.1-Mini在所有架构中均获得最高准确率，多智能体架构会改变语义和数值平衡但效果不一致，不同模型对架构设计具有特异性敏感度。

Conclusion: 推理有效性具有模型特异性，响应覆盖度的变化使跨架构直接比较复杂化，采用五次独立运行的最佳结果作为性能上限估计。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [22] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章讨论了构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展前景，重点关注级联故障风险缓解、动态环境、任务执行不一致性、不可预测的涌现行为等研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统日益复杂和自主化，特别是智能体AI系统的兴起，确保其可靠性变得至关重要。需要解决级联故障、动态环境适应性、行为不可预测性等关键风险。

Method: 通过分析当前研究现状，识别关键挑战领域，包括级联故障缓解、动态环境处理、任务执行一致性、涌现行为管理，以及可靠性测试评估方法。

Result: 确定了多个开放研究问题，包括动态环境中的可靠性、任务执行不一致性、不可预测的涌现行为，以及资源密集型可靠性机制的设计挑战。

Conclusion: 构建可靠的智能体AI系统需要在多个前沿领域进行深入研究，包括故障缓解机制、环境适应性、行为预测和高效可靠性测试方法，这些研究方向对AI系统的安全部署至关重要。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [23] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 提出了一种基于"反弹赢家通吃(RWTA)"基元的可扩展神经形态控制架构，结合了离散计算的可靠性和连续调节的可调性，应用于蛇形机器人的神经系统设计。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够统一处理连续节律生成和离散决策的神经形态控制架构，结合离散计算的可靠性和连续调节的可调性。

Method: 使用反弹赢家通吃(RWTA)基元作为基本元素，构建从细胞层面到系统层面的可扩展架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 该架构展示了在蛇形机器人神经系统设计中的多功能性、鲁棒性和模块化特性。

Conclusion: 提出的基于事件的框架为连续节律生成和离散决策提供了统一的物理建模语言，实现了可靠且可调的神经形态控制。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [24] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出了一种名为CFA-SMOTE的新数据增强方法，将可解释AI中的反事实方法与SMOTE方法结合，用于处理气候变化预测中的类别不平衡问题，特别是在历史数据缺乏极端气候事件样本的情况下。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端天气事件频发，但传统机器学习方法基于历史数据分布，难以处理分布外异常事件。历史数据集缺乏足够的极端气候事件样本（少数类），导致预测性能不佳。

Method: 提出CFA-SMOTE方法，结合可解释AI中的反事实方法和经典的类别不平衡方法SMOTE，生成代表气候异常事件的合成数据点来增强数据集。

Result: 通过比较实验，在不同类别不平衡比例下，CFA-SMOTE方法相比基准反事实和类别不平衡方法表现出更好的预测性能。

Conclusion: CFA-SMOTE方法能有效解决气候变化预测中的类别不平衡问题，通过生成合成异常事件数据提高预测准确性，在2018年欧洲干旱和饲料危机期间爱尔兰奶牛场草生长预测任务中验证了其有效性。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [25] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本文提出了一种混合神经符号框架，结合大语言模型和符号逻辑，用于确定性检测复杂法律中的法规不一致性，以美国国内税收法典为案例研究。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在层次化处理和深度结构化推理方面存在困难，特别是在长文本上。税收领域的特定应用仍然稀缺，需要解决法规不一致性检测的挑战。

Method: 使用GPT-4o将税法条款翻译为Prolog规则，在SWISH中精炼，然后测试Prolog增强提示是否能改进不一致性检测。同时开发混合Prolog模型，由GPT-5指导精炼。

Result: GPT-4o单独使用自然语言提示或Prolog增强提示时，在三种策略中仅检测到一种不一致性（33%准确率）。而混合Prolog模型产生确定性、可重现的结果，成功检测到不一致区域。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法规不一致性检测，混合神经符号方法在复杂法律分析中具有优势。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [26] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 提出基于直接依赖检索(DDR)的声明自动形式化框架，通过生成候选库依赖并验证其存在性，解决了现有方法缺乏上下文意识和检索精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有声明自动形式化方法缺乏上下文意识，导致形式定义和定理的幻觉，且检索增强方法在形式库依赖检索方面精度和召回率差，难以有效利用不断增长的公共数据集。

Method: 提出DDR方法：直接从自然语言数学描述生成候选库依赖，通过高效后缀数组检查验证其在形式库中的存在性，构建50万+样本的依赖检索数据集并微调高精度DDR模型。

Result: DDR模型在检索精度和召回率上显著优于SOTA方法，配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于传统基于选择的RAG方法。

Conclusion: DDR框架有效解决了声明自动形式化中的依赖检索问题，为深度学习和形式数学的融合提供了更可靠的基础。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [27] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 提出Chain-of-Evidence (CoE)范式，将思维链推理与视觉证据归因相结合，通过强化学习框架LAT训练模型生成可验证的推理路径，提升视觉文档检索增强生成的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索增强生成方法缺乏细粒度监督和推理过程的渐进可追溯性，无法确保证据归因的可靠性。

Method: 提出CoE范式统一思维链推理和视觉证据归因，使用LAT强化学习框架训练模型，通过评估证据区域归因一致性并提供奖励来鼓励过程级自验证。

Result: 在Paper-和Wiki-VISA基准测试中，LAT显著提升原始模型性能，软精确匹配平均提升8.23%，IoU@0.5提升47.0%，且表现出更强的跨领域泛化能力。

Conclusion: CoE范式和LAT框架有效提升了视觉文档检索增强生成的可信度和可验证性，在单图和跨图像设置中均取得显著改进。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [28] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，无需大量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI系统缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床应用。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段为准确性精炼解释，无需白盒访问或权重更新。

Result: 在乳腺癌和前列腺癌数据集上评估，RECAP-PATH产生与专家评估一致的理由，并在诊断准确性上显著优于基线方法。

Conclusion: RECAP-PATH通过结合视觉理解和推理，提供临床可信赖的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [29] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 提出了一种多路径差异化裁剪近端策略优化算法(MPD-PPO)，用于解决橡胶轮胎制造中的高维多目标优化问题，在薄膜生产的宽度和厚度控制中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要应对传统集中调度和生产线配置的局限性，特别是在处理动态生产需求方面。轮胎制造系统形成具有非线性相互作用和涌现动态的复杂网络，多子系统协调成为关键挑战。

Method: 采用多分支策略架构和差异化梯度裁剪约束的深度强化学习算法MPD-PPO，确保高维策略更新的稳定性和效率。

Result: 在橡胶轮胎薄膜生产的宽度和厚度控制实验中，MPD-PPO在调节精度和运行效率方面均有显著提升。

Conclusion: 该框架成功解决了高维度、多目标权衡和动态适应等关键挑战，为轮胎制造中的实时工业部署提供了增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [30] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文提出T-BoN BO框架，通过结合Best-of-N选择和文本梯度，在语言空间中模拟贝叶斯优化，以解决AI自我改进中的评估效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI自我改进方法主要关注生成效率，但在许多社会应用中，评估解决方案比生成更昂贵耗时。需要优化评估效率而非生成效率。

Method: 提出T-BoN BO框架，证明Best-of-N选择策略与文本梯度结合可统计模拟UCB采集函数的梯度行为，实现语言空间的贝叶斯优化。

Result: 在自动广告对齐任务中验证，T-BoN BO相比现有最优基线方法表现更优。

Conclusion: T-BoN BO为AI自我改进提供了一种简单且评估高效的贝叶斯优化框架，特别适用于评估成本高的应用场景。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [31] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比传统聚类抽象方法能更精确捕捉信息集间的差异和联系，在扑克游戏中实现了更快的可剥削性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间可量化的细微差异，这些差异对于策略求解至关重要，从而影响求解质量。

Method: 受自然语言处理中词嵌入范式的启发，提出Embedding CFR算法：预训练并将孤立信息集的特征嵌入到相互连接的低维连续空间，在该嵌入空间中进行遗憾累积和策略更新的策略求解过程。

Result: 在扑克实验表明，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛，证实了其有效性。

Conclusion: 这是扑克AI中首个通过低维嵌入预训练信息集抽象来进行策略求解的算法，能够更精确地捕捉信息集间的区别和联系，提升求解质量。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [32] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd算法通过k-recall winrate特征和earth mover's distance聚类来解决德州扑克等游戏中因过度抽象导致AI性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中因极端不完美回忆抽象而完全丢弃历史信息导致的过度抽象问题，这严重影响了AI的表现。

Method: 首先引入k-recall winrate特征，利用未来和关键的历史游戏信息来区分信号观察信息集；然后开发KrwEmd算法，使用earth mover's distance测量特征差异来聚类信号观察信息集。

Result: 实验结果表明，与现有算法相比，KrwEmd显著提高了AI的游戏表现。

Conclusion: KrwEmd是第一个实用的解决过度抽象问题的算法，通过有效利用历史信息来提升AI在复杂游戏中的性能。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [33] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出了一种缓解小型语言模型知识蒸馏中灾难性遗忘的综合解决方案，包括构建包含元认知知识的5K数据集和GDPO训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在将大语言模型推理能力压缩到小于8B的小模型时会导致灾难性遗忘，主要问题是忽略训练数据知识与模型固有能力的关联，以及传统训练目标无法有效约束固有知识保留。

Method: 1) 数据侧：构建包含多任务推理和元认知知识的5K数据集，基于任务知识和模型技能过滤数据；2) 训练侧：提出GDPO（组方向偏好优化），通过参考模型隐式约束优化路径，在资源有限场景下高效近似GRPO性能。

Result: 大量实验表明该方法显著缓解了灾难性遗忘，提升了小模型的推理性能。

Conclusion: 该综合方案从数据和训练方法两个角度有效解决了小模型知识蒸馏中的灾难性遗忘问题，实现了更有效的知识迁移和参数漂移约束。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [34] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol是一个双向对齐框架，通过自监督往返学习统一分子描述和文本到SMILES生成，解决了现有方法在化学准确性、数据质量和双向一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有分子序列表示与文本描述对齐方法存在三个关键问题：传统指标偏重语言流畅性而非化学准确性、训练数据包含化学模糊描述、独立优化导致双向不一致。

Method: 提出RTMol框架，通过自监督往返学习统一分子描述和文本到SMILES生成，引入新颖的往返评估指标，支持无需配对分子-文本语料库的无监督训练。

Result: 实验表明RTMol将各种LLM的双向对齐性能提升高达47%，建立了有效的联合分子-文本理解和生成范式。

Conclusion: RTMol通过双向对齐框架有效解决了分子序列与文本描述对齐中的关键挑战，为药物发现、材料设计等应用提供了更可靠的工具。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [35] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，一种用于DatalogMTL的增量推理方法，能够高效处理动态数据更新，相比重新物化方法性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的DatalogMTL推理方法虽然具备正确性和完备性，但缺乏对动态更新的高效支持，而实际应用往往需要频繁的数据更新。

Method: 基于经典DRed算法，设计了专门处理DatalogMTL物化周期性表示的运算符，实现增量更新。

Result: 在多个公开数据集上的实验表明，DRedMTL通常显著优于重新物化方法，有时性能提升达数量级。

Conclusion: DRedMTL为DatalogMTL提供了高效的增量推理能力，能够满足实际应用中动态数据更新的需求。

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [36] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了DoM框架，通过多智能体辩论机制动态融合结构化和非结构化知识来解决不完整知识图谱问答问题，并创建了更真实的不完整KGQA数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱通常不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补性。

Method: DoM框架基于多智能体辩论范式，使用专门智能体分别对知识图谱和外部文本进行推理，通过迭代交互协调输出，包括问题分解、双智能体证据检索和法官智能体评估聚合。

Result: 实验表明DoM在多个基准测试中持续优于现有最先进方法。

Conclusion: DoM框架通过动态知识融合有效解决了不完整知识图谱问答问题，提出的新数据集更真实地反映了现实世界知识不完整性。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [37] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出ViTE框架，通过虚拟图建模高阶交互和专家路由器自适应选择交互专家，解决行人轨迹预测中深度与计算成本的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人轨迹预测中面临深度与计算成本的权衡：浅层网络感受野不足，深层网络计算成本过高。需要能够自适应建模显式一阶交互和隐式高阶依赖的方法。

Method: 提出ViTE框架，包含两个关键模块：虚拟图通过动态虚拟节点建模长距离高阶交互而无需深层GNN堆叠；专家路由器基于社交上下文自适应选择交互专家，采用混合专家设计。

Result: 在三个基准数据集（ETH/UCY、NBA、SDD）上的实验表明，该方法持续达到最先进的性能，验证了其有效性和实际效率。

Conclusion: ViTE框架通过虚拟图和专家路由器的结合，实现了灵活可扩展的交互模式推理，在行人轨迹预测任务中表现出色且高效。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [38] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文使用科学哲学案例研究批判性检验世界模型框架是否充分表征人类水平的理解能力，探讨世界模型能力与人类理解之间的差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验AI模型中的世界模型是否真正体现人类水平的理解能力，因为人类拥有心理世界模型，如果AI模型有类似表征可能意味着它们以类人方式"理解"世界。

Method: 采用科学哲学文献中的案例研究方法，重点关注世界模型能力与人类理解区别最明显的哲学分析，虽然这些代表特定理解观点而非普适定义。

Result: 通过哲学案例分析揭示了世界模型框架在表征人类水平理解方面的局限性，展示了世界模型能力与真正人类理解之间的差异。

Conclusion: 世界模型框架不足以充分表征人类水平的理解能力，哲学案例研究有助于探索世界模型的局限性。

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [39] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于合成视频数据的实时非计划拔管检测系统，通过姿态估计识别碰撞和躁动两种高风险行为模式，在保护隐私的同时实现了ICU患者安全监测。


<details>
  <summary>Details</summary>
Motivation: ICU中非计划拔管是严重的安全问题，但由于伦理和隐私限制难以获取真实视频数据，需要开发基于合成数据的检测方法。

Method: 使用文本到视频扩散技术生成合成ICU视频数据集，通过姿态估计检测手部进入气道管周围区域（碰撞）和关键点速度（躁动）两种风险模式。

Result: 专家评估确认合成数据真实性，系统在碰撞检测上表现高准确度，在躁动识别上表现中等。

Conclusion: 该工作展示了开发隐私保护、可复现的患者安全监测系统的新途径，具有在ICU部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [40] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Mobile-Agent-RAG是一个新颖的分层多智能体框架，通过双级检索增强来解决移动智能体在现实世界长时程跨应用任务中的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动智能体在现实世界长时程跨应用任务中成功率不足，主要原因是过度依赖MLLMs中的静态内部知识，导致高层规划中的战略幻觉和低层UI操作中的执行错误。

Method: 提出分层多智能体框架Mobile-Agent-RAG，集成双级检索增强：在规划阶段使用Manager-RAG检索经过人工验证的全面任务计划以减少战略幻觉；在执行阶段使用Operator-RAG检索最精确的低层指导以提高执行准确性。构建了两个专门的检索导向知识库。

Result: 大量实验表明，Mobile-Agent-RAG显著优于最先进的基线方法，任务完成率提高了11.0%，步骤效率提高了10.2%。

Conclusion: Mobile-Agent-RAG为上下文感知、可靠的多智能体移动自动化建立了一个稳健的范式，通过区分高层规划和低层操作所需的不同知识类型，有效解决了移动智能体的关键性能瓶颈。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [41] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文提出了一种解决LLM道德对齐问题的新方法，通过构建Moral-Reason-QA数据集和Group Relative Policy Optimization算法，成功训练LLM在未见过的道德场景中应用一致的道德推理框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对人类道德决策的影响日益增强，但现有方法主要关注评估而非主动引导其道德决策。需要解决LLM在超出训练分布场景中应用一致道德推理框架的问题。

Method: 构建包含680个高模糊度道德场景的Moral-Reason-QA数据集，采用Group Relative Policy Optimization算法，通过复合奖励同时优化决策对齐和框架特定推理过程。

Result: 在未见过的道德场景中成功实现泛化，功利主义框架的softmax归一化对齐分数提升+0.757，义务论框架提升+0.450。

Conclusion: LLM代理可以系统地训练以内在化并应用特定道德框架到新情境，为AI安全提供了关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [42] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于真实Upwork工作任务构建的动态基准测试，通过专家制定的评估标准和真实工作成果来评估LLM代理在真实工作环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多是静态、合成或领域受限的，无法评估AI代理在动态、经济意义环境中的真实表现和与人类协作能力。

Method: 从Upwork平台提取真实工作任务，由专家自由职业者制定详细评估标准，对AI提交内容进行逐项评估，并定期更新任务以反映工作环境变化。

Result: 建立了基于真实工作活动的评估框架，能够细粒度分析模型优势和弱点，超越了简单的通过/失败指标。

Conclusion: UpBench提供了一个可扩展、以人为中心的基准测试，支持在真实劳动力市场环境中评估代理系统，促进AI通过合作而非替代来增强人类能力。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [43] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出了RGR-GRPO框架，通过规则驱动的强化学习在多领域推理任务中提升LLM性能，相比传统方法平均提升5.4%-8.4%。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要局限于单领域和可验证奖励，且纯在线RL框架限制了探索空间，影响了推理性能的提升。

Method: 使用规则提供细粒度奖励信号和离线指导，提出RGR-GRPO框架，在GRPO训练中让LLM获得密集信息奖励并探索更大解空间。

Result: 在14个多领域基准测试中，RGR-GRPO始终优于仅依赖替代奖励方案或离线指导的RL方法，在数学、物理、化学和通用推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离线策略训练中保持稳定的熵波动，实现卓越的pass@k性能，反映了持续的探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [44] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出了一种计算理性用户模型，用于模拟认知受限代理在偏见信念下的最优行为，重点解决了从被动观察中推断用户认知边界和信念状态的问题。


<details>
  <summary>Details</summary>
Motivation: 预测和理解用户或人类合作者的次优行为是AI发展中的关键挑战。这些行为通常不是非理性的，而是在认知边界和偏见信念下的理性决策。

Method: 提出了基于嵌套粒子滤波的高效在线推理方法，同时跟踪用户的潜在信念状态并从未知认知边界中估计参数，以记忆衰减作为认知边界的示例。

Result: 模拟验证表明：(1) CR模型能生成与不同记忆容量水平相对应的直观合理行为；(2) 推理方法能从有限观察中准确高效地恢复真实认知边界。

Conclusion: 该方法为开发自适应AI助手提供了理论基础，使AI能够考虑用户的记忆限制来提供适应性协助。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [45] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个动态学习和适应不同建议者可靠性的框架，在部分可观测环境中通过贝叶斯推理推断建议者类型，并引入显式的"询问"动作来策略性地请求建议。


<details>
  <summary>Details</summary>
Motivation: 自主代理在不确定性下的顺序决策任务中可以从外部行动建议中受益，但这些建议的可靠性各不相同。现有方法通常假设建议者质量参数是静态且已知的，限制了实际部署。

Method: 将建议者质量直接集成到代理的信念表示中，通过贝叶斯推理推断建议者类型；引入显式的"询问"动作，允许代理在关键时刻策略性地请求建议，平衡信息增益与获取成本。

Result: 实验评估表明，该方法在不同建议者质量下表现出稳健性能，能够适应变化的可靠性，并策略性地管理建议请求。

Conclusion: 这项工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [46] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 开发了一个基于临床验证流程图的对话式自我分诊系统，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率，结合自由文本交互的灵活性和标准化临床协议的严谨性。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中可靠性有限，存在准确性低、缺乏透明度和易受未经验证信息影响的问题，需要提供结构化、可审计的患者决策支持框架。

Method: 采用多智能体框架，包括检索智能体、决策智能体和聊天智能体，使用美国医学会100个临床验证流程图指导LLM，通过合成数据集进行大规模性能评估。

Result: 在2000个测试案例中实现95.29%的前3准确率流程图检索，在37200个测试案例中实现99.10%的流程图导航准确率，适应不同对话风格和条件。

Conclusion: 该方法展示了透明、准确且可推广的AI辅助自我分诊的可行性，有潜力支持知情患者决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [47] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务和ARCHE Bench基准，用于评估LLMs将复杂推理分解为标准推理范式的能力，发现现有模型在科学推理严谨性方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能生成推理内容，但其输出通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式。

Method: 引入潜在推理链提取任务，要求模型将复杂推理分解为推理逻辑树，其中所有推理步骤都明确分类为演绎、归纳或溯因三种基本推理模式。

Result: 评估10个领先LLMs发现，模型在推理边准确性和实体覆盖率之间存在权衡，且没有一个模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [48] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用编码器基础模型，通过新颖的标记化方案处理多维消息，在预测中间价格变动和下一消息等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LOB模型需要繁琐的数据表示，缺乏原始任务之外的适应性，因此需要开发一个适用于下游微调的通用基础模型。

Method: LOBERT基于BERT架构，采用新颖的标记化方案将完整多维消息视为单个标记，同时保留价格、数量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，在多个任务中表现出色且具有更好的适应性。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [49] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法未能充分利用PLM在图关系上的推理能力、无差别地整合检索知识以及忽视多轮对话中协作偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将PLM与知识图谱集成时面临三个关键挑战：未能充分利用PLM在图关系上的推理能力、无差别地整合检索知识而不进行上下文过滤，以及忽视多轮对话中的协作偏好。

Method: PCRS-TKA从知识图谱构建对话特定的知识树并将其序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；通过语义对齐模块协调异构输入。

Result: 大量实验表明，PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过结构感知推理、上下文知识过滤和协作偏好建模，有效提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [50] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量的状态集，在保持静态版本优良特性的同时实现高压缩比和低运行时开销。


<details>
  <summary>Details</summary>
Motivation: 在大规模任务中扩展显式状态空间搜索时，紧凑表示生成状态集是一个核心挑战。树数据库虽然每个状态只需要恒定空间，但需要大量内存预分配。

Method: 开发了一种动态变体的树数据库，用于压缩命题和数值变量的状态集，并证明其保持了静态对应物的理想特性。

Result: 在经典和数值规划任务上的实证评估显示，压缩比达到几个数量级，通常运行时开销可忽略不计。

Conclusion: 动态树数据库在状态压缩方面表现出色，为大规模状态空间搜索提供了高效的解决方案。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [51] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 提出了一种策略条件化合作者框架，通过变分自编码器学习策略空间，聚类识别策略类型，并训练条件化合作者，结合在线后悔最小化算法实现对新伙伴的实时适应。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，人工智能体需要实时适应人类伙伴的独特偏好和动态变化的策略，这在时间压力和复杂策略空间的任务中尤其具有挑战性。

Method: 使用变分自编码器编码策略学习潜在策略空间，通过聚类识别不同策略类型，训练条件化合作者，并采用固定份额后悔最小化算法进行在线策略推断和调整。

Result: 在修改版Overcooked环境中，该方法在与新人类和智能体队友配对时实现了最先进的性能。

Conclusion: 提出的策略条件化合作者框架能够有效表示、分类和实时适应广泛的伙伴策略，在复杂协作任务中表现出色。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [52] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究表明，在现代高维嵌入空间中，简单的随机游走就能产生与人类语义流畅性任务中观察到的优化觅食行为一致的结果，而更复杂的Metropolis-Hastings采样反而不能匹配人类行为。


<details>
  <summary>Details</summary>
Motivation: 探索现代高维嵌入空间是否能提供允许算法匹配人类语义流畅性任务中观察到的觅食行为的表示，验证更复杂的采样机制是否必然产生更好的认知模型。

Method: 使用最先进的嵌入和先前的语义流畅性数据，在嵌入空间上进行随机游走和Metropolis-Hastings采样，比较结果与人类行为的一致性。

Result: 随机游走在嵌入空间上产生的结果与优化觅食和边际价值定理一致，而Metropolis-Hastings采样未能产生与人类行为一致的结果。

Conclusion: 适当结构的嵌入即使使用简单采样也能产生接近最优的觅食动态，挑战了复杂采样机制必然产生更好认知模型的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [53] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet是一个融合因果推理的时空图神经网络框架，通过LLM量化事件报告、构建因果知识库，并使用因果注意力机制增强交通预测，在突发事件中显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统时空GNN在处理非周期性事件（如事故）时可靠性急剧下降，因为它们本质上是相关性模型，无法应对突发事件引入的新因果因素。

Method: 使用大语言模型量化非结构化事件报告，通过估计平均处理效应构建因果知识库，采用双流GNN-LSTM网络和因果注意力机制注入因果知识来调整和增强预测。

Result: 在真实数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了35.87%，显著优于最先进的基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移且具有关键可解释性的解决方案，为关键中断期间的实时交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [54] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文研究使用强化学习优化异构卫星集群在自主地球观测任务中的资源分配问题，通过多智能体强化学习算法实现异构卫星间的有效协调。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中实时性、不确定性和去中心化的特点，因此需要采用强化学习和多智能体强化学习进行自适应决策。

Method: 系统地从单卫星到多卫星场景构建优化问题，使用基于Basilisk和BSK-RL框架构建的近真实仿真环境，评估MAPPO、HAPPO、HATRPO等先进MARL算法。

Result: MARL能够实现异构卫星间的有效协调，在平衡成像性能和资源利用的同时缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究结果为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的后续研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [55] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 本文提出了一种终身学习归纳逻辑编程框架，利用逻辑规则的组合性和可转移性来高效学习新问题，通过重用先前任务中的逻辑规则来提升可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究专注于为单个问题设计新颖的网络架构，而较少探索涉及问题序列的新学习范式。本文旨在研究终身学习ILP，利用逻辑规则的组合性和可转移性实现高效学习。

Method: 引入了一个组合框架，展示了如何将从早期任务中获取的逻辑规则高效地重用于后续任务，从而改善可扩展性和性能。

Result: 在任务序列上的实证评估验证了该范式的可行性和优势，实验结果表明该方法能够有效提升学习效率。

Conclusion: 这项工作为神经符号人工智能中的持续学习开辟了新方向，证明了终身学习ILP在利用逻辑规则可转移性方面的潜力。

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [56] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 该研究提出使用被动脑机接口（BCI）和功能性近红外光谱（fNIRS）技术，通过隐式神经信号来指导强化学习代理的训练，构建了一个包含25名参与者在三个领域的数据集，并证明了从fNIRS信号映射到代理性能的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习人类反馈（RLHF）需要显式的人类反馈，而本研究旨在探索使用隐式神经信号（通过fNIRS记录）来指导代理训练，为未来脑驱动的RLHF系统奠定基础。

Method: 收集了25名参与者在Pick-and-Place Robot、Lunar Lander和Flappy Bird三个领域的fNIRS数据，训练分类器预测代理性能水平（最优、次优、最差），并训练回归器预测代理动作与近最优策略的偏差程度。

Result: 二元分类平均F1分数达67%，多分类模型平均F1分数为46%。通过少量特定受试者数据微调预训练模型，二元和多分类模型的F1分数分别提高了17%和41%。

Conclusion: 研究证明了从隐式fNIRS信号映射到代理性能是可行的，并且可以通过微调进一步提高性能，为未来脑驱动的RLHF系统开发提供了重要基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [57] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架，通过主策略和奖励模型之间的min-max博弈来引导LLM对齐，无需大量人工标注，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过基于偏好的策略优化来引导大型语言模型，使其行为与人类偏好对齐，而不依赖大量手动标注。

Method: 提出PbPO框架，将学习过程建模为主策略和奖励模型之间的min-max博弈，奖励模型约束在偏好数据导出的置信集内，通过迭代在线算法主动收集偏好数据。

Result: 在五个基准测试中持续优于现有最先进的偏好优化技术，并为序列级和令牌级奖励模型设置提供了高概率遗憾界理论保证。

Conclusion: 该框架为无需大量人工标注的LLM对齐提供了有效解决方案，通过理论分析和实验验证了其优越性能。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [58] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过整合语言到经济决策中，采用Think-Speak-Decide流程，在累积回报、鲁棒性和可解释性方面优于传统MARL和纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 经济决策不仅依赖结构化信号（如价格、税收），还依赖非结构化语言（如同行对话、媒体叙事）。传统多智能体强化学习在处理语言的语义模糊性和上下文丰富性方面存在困难。

Method: LAMP采用Think-Speak-Decide流程：Think解释数值观察提取短期冲击和长期趋势；Speak基于推理制定和交换战略信息；Decide融合数值数据、推理和反思到MARL策略中。

Result: 在经济模拟实验中，LAMP在累积回报（+63.5%，+34.0%）、鲁棒性（+18.8%，+59.4%）和可解释性方面优于MARL和LLM-only基线方法。

Conclusion: 语言增强策略有潜力提供更有效和鲁棒的经济策略。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [59] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 本文提出了一种在线学习分层任务网络（HTN）方法的技术，通过扩展ChatHTN规划器，在ChatGPT生成任务分解时学习泛化的方法，减少对ChatGPT的调用次数。


<details>
  <summary>Details</summary>
Motivation: 在集成HTN规划和基于LLM的聊天机器人中，需要学习何时以及如何将任务分解为子任务的方法，以减少对大型语言模型的依赖并提高效率。

Method: 在ChatHTN规划器基础上构建在线学习方法，当ChatGPT生成任务分解时，学习泛化的方法而不仅仅是记忆特定实例，类似于记忆化但具有泛化能力。

Result: 在两个领域进行的实验表明，在线学习过程减少了ChatGPT的调用次数，同时解决了至少同样多的问题，在某些情况下甚至更多。

Conclusion: 在线学习HTN方法能有效减少对大型语言模型的依赖，提高规划效率，同时保持或提升问题解决能力。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [60] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出了Chain-of-Scheduling (CoS)框架，通过引导式调度过程激活大语言模型的事件调度能力，解决了EBSNs中事件推荐在效率、效果和泛化性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在基于事件的社交网络中，有效的事件推荐对于维持用户活跃度至关重要。现有方法由于问题的NP-hard性质，在效率、效果和泛化性之间存在固有权衡。

Method: CoS框架将调度任务分解为探索、验证和集成三个原子阶段，通过知识蒸馏使LLM能够自主生成CoS。

Result: 在三个真实世界数据集上，CoS以可解释的方式实现了接近理论最优的效果和高效率，并在域外数据上表现出强大的零样本学习能力。

Conclusion: CoS框架成功激活了LLM的事件调度能力，在保持高效率的同时实现了接近最优的推荐效果，并具有良好的泛化性能。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [61] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电力系统故障诊断，通过结构化法规逻辑、整合专家知识、优化推理逻辑，最终生成可执行的工作流。


<details>
  <summary>Details</summary>
Motivation: 当前电力系统故障诊断依赖手动、易出错的方法，技术人员需要从复杂法规中提取推理逻辑并整合专家知识，效率低下且难以维护。缺乏将这两种知识源整合到单一验证工作流的框架。

Method: 提出Fault2Flow系统：1) 将法规逻辑提取为PASTA格式故障树；2) 通过人机交互界面验证整合专家知识；3) 使用AlphaEvolve模块优化推理逻辑；4) 将验证后的逻辑合成n8n可执行工作流。

Result: 在变压器故障诊断数据集上的实验验证显示，系统达到100%拓扑一致性和高语义保真度。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可重复路径，显著减少专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [62] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用代理框架，首次实现了在三个异构策略游戏环境中的自主跨平台操作，通过结合视觉语言推理和精确执行能力，在减少63%推理时间的同时将BLEU-4分数提升12.98倍。


<details>
  <summary>Details</summary>
Motivation: 策略游戏中的自动化操作需要能够在不同用户界面和动态战场条件下具有强大泛化能力的代理，而视觉语言模型在复杂人机交互场景（如策略游戏）中的应用尚未充分探索。

Method: 集成Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，采用屏幕捕获、模型推理和动作执行的闭环流程，并研究不同多模态数据组合（静态图像、多图像序列和视频）的效果。

Result: 混合策略（融合多图像和视频数据同时混合静态图像）显著优于完全融合：推理时间减少63%，BLEU-4分数从4.81%提升至62.41%（约12.98倍提升）。

Conclusion: 该工作不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强VLM性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [63] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和药物发现设计的知识图谱系统，通过轻量级验证器引导LLM生成数学和生物医学上有效的输出，显著减少违规并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学推理和药物发现中生成内容缺乏领域一致性的问题，确保输出在数学和生物医学上的有效性。

Method: 使用紧凑的知识图谱支架和轻量级验证器，通过注入符号事实到提示中，并强制执行规则满足的确定性检查，将生成形式化为约束推理。

Result: 在90个任务中，MedRule-KG相比强大的思维链基线减少了83.2%的违规计数，同时提高了精确匹配率，结果稳定且可扩展，验证器延迟可忽略。

Conclusion: MedRule-KG为交互式设计提供了一种实用的方法，能够有效约束LLM生成领域一致的科学内容，在药物发现任务中表现优异。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [64] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为网页浏览代理提供跨会话的持久记忆，通过记忆存储、经验检索和运行时建议注入，显著提升代理在复杂浏览任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM代理在网页导航中存在重复错误且无法从跨会话经验中学习，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个组件：WebCondenser标准化导航日志为摘要；External Memory Store组织完整轨迹为经验片段；Coach基于相似性和时效性检索相关经验，通过运行时钩子决定是否注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach在三个不同LLM骨干上均提升浏览器使用代理性能，38B模型任务成功率从47%提升至61%，同时减少或维持平均步骤数。较小基础模型配合WebCoach可达到与使用GPT-4o的相同代理相当的性能。

Conclusion: WebCoach通过赋予代理持久跨会话记忆能力，实现了无需重新训练的自进化，显著提升了网页浏览代理的长期鲁棒性和性能。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [65] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出了GEM方法，一种基于生成熵引导的偏好建模方法，用于在低资源和领域特定场景下对齐大语言模型。该方法通过认知过滤模块和自评估群体优势算法，利用少量偏好数据实现高效对齐。


<details>
  <summary>Details</summary>
Motivation: 在医学和法律等依赖专业知识的领域，大规模偏好标注往往难以获得，需要开发能够在少量标注数据下有效对齐LLM的方法。

Method: 1. 认知过滤模块：基于决策中的熵理论，使用思维链提示生成多样化候选推理链，引入token评分机制对CoT进行排序和加权；2. SEGA算法：基于过滤后的偏好，使用自评估群体优势算法聚合群体级认知信号，将熵基分数转化为隐式奖励进行策略优化。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上的实验表明，GEM在少量偏好数据下取得了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，使LLM能够依赖自身判断，实现高效的少样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [66] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 该研究评估语言模型在对话中构建和维护世界模型的能力，发现模型在语言变化下难以保持准确性，并提出解释性框架和微调策略来改善性能。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，需要构建局部世界模型来编码这些元素并跟踪其状态变化。但目前不清楚语言模型是否能构建和维护健壮的隐式对话表示。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试，评估多种开源和闭源语言模型，并提出双视角解释性框架识别有害的transformer层。

Result: 语言模型在语言变化下难以保持健壮的准确性，特别是在跟踪实体方面存在困难。识别出编码虚假信号或依赖捷径的有害transformer层。

Conclusion: 语言模型在对话中维护世界模型的能力有限，但通过提出的层正则化微调策略可以抑制有害层的影响，改善模型性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [67] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 论文分析了数学问题验证方法，发现单一基准测试会导致误导性结论，提出结合GenSelect和LLM-as-a-Judge的验证框架，并指出强化学习虽然改善证明级指标但未提升最终答案精度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学问题上虽然能给出正确答案，但推理过程往往存在缺陷。为了推进严格的证明数学，需要可靠的证明验证能力。

Method: 评估了多种验证设置，结合证明基础和最终答案推理来评估模型性能；规模化测试了GenSelect和LLM-as-a-Judge两种生成验证方法，并探索了它们的组合；分析了提示词对LLM-as-a-Judge的影响，并尝试用强化学习减少敏感性。

Result: GenSelect和LLM-as-a-Judge的组合是最有效的验证框架；提示词选择显著影响LLM-as-a-Judge性能，但强化学习可降低这种敏感性；强化学习改善了证明级指标但未提升最终答案精度。

Conclusion: 当前模型往往奖励风格或程序正确性而非数学有效性，研究为设计和评估可扩展的证明验证和选择系统提供了实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [68] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段GUI定位框架，通过粗粒度ROI选择和细粒度元素定位来解决视觉杂乱和指令模糊问题，在密集和复杂基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统采用单一模型或一次性流程，缺乏模块化，在视觉杂乱和模糊指令下表现不佳。

Method: 采用多阶段框架，分离为粗粒度ROI选择和细粒度元素定位，使用专用视觉语言代理协调，包含双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot-Pro基准测试中达到73.18%准确率，在OSWorld-G基准测试中达到68.63%，超越先前报告结果。

Conclusion: 模块化结构能持续获得比单一方法更高的准确率，揭示了不同视觉尺度下视觉语言模型的互补优势。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [69] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互在线强化学习中轨迹级优化的低效问题，显著提高了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮交互在线强化学习中轨迹级优化的三个主要问题：跨任务均匀采样效率低、失败轨迹中正确中间动作被惩罚、样本收集成本高。

Method: 提出STEP框架：1）维护平滑成功率记录指导自适应轨迹重采样；2）计算成功率加权优势值；3）将轨迹分解为步骤级样本；4）应用步骤级GRPO增强来优化低成功率任务的更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化有效解决了多轮交互强化学习的效率问题，为在线强化学习提供了更高效的训练框架。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [70] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T是一个新颖的两层分层多智能体强化学习框架，使用条件扩散模型进行动态任务分解，通过预测下一观测和奖励来学习子任务表示，提高复杂协作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的动态不确定环境中，从零开始学习动态任务分解需要大量训练样本，特别是在大的联合动作空间中探索时效率低下。

Method: 提出条件扩散模型动态任务分解框架：高层策略基于子任务效果学习子任务表示并生成选择策略；低层智能体在分配的子任务中协作学习专门技能；使用多头注意力混合网络增强价值分解。

Result: 在各种基准测试中，C$	ext{D}^	ext{3}$T比现有基线方法取得了更好的性能表现。

Conclusion: 该框架能够自动推断子任务和协调模式，为个体和联合价值函数之间提供有效的推理桥梁，在复杂协作多智能体强化学习任务中表现出色。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [71] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 提出了InteractiveGNNExplainer，一个用于增强图神经网络可解释性的可视化分析框架，特别关注节点分类任务。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在基于图的学习任务中表现出色，但其复杂的非线性操作使其成为不透明的"黑盒"，这阻碍了用户信任、调试、偏见检测以及在需要可解释性的关键领域中的应用。

Method: 系统独特地整合了协调的交互视图（动态图布局、嵌入投影、特征检查、邻域分析）与已建立的后验（GNNExplainer）和内在（GAT注意力）解释技术，并加入了交互式图编辑功能，允许用户通过扰动图结构来进行"假设分析"。

Result: 通过在Cora和CiteSeer数据集上的案例研究，展示了InteractiveGNNExplainer如何促进深入的错误分类诊断、GCN与GAT行为的比较分析，以及对模型敏感性的严格探测。

Conclusion: 这些功能促进了对GNN预测的更深层次、多方面的理解，有助于实现更透明、可信赖和鲁棒的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [72] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA框架通过将通信带宽视为稀缺可交易资源，采用集中式拍卖机制让智能体基于信息价值密度竞标发言权，显著提升多智能体系统的通信效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的多智能体系统中'自由放任'通信导致的指数级token成本和低信噪比问题，挑战'更多通信总是更好'的观念，引入资源理性原则。

Method: 提出动态拍卖语言智能体(DALA)框架，将智能体间通信建模为集中式拍卖，智能体学习基于信息价值密度竞标发言机会，鼓励产生简洁高价值信息。

Result: 在7个推理基准测试中达到最先进性能：MMLU 84.32%，HumanEval 91.21% pass@1，且仅使用625万token，远少于现有方法在GSM8K上的资源消耗。

Conclusion: DALA通过资源约束培养了战略性沉默的新兴技能，能够动态调整从冗长到沉默的通信策略，证明了经济驱动方法在提升多智能体系统效率方面的有效性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [73] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 本文提出了一种基于图神经网络和深度强化学习的资源受限项目调度问题（RCPSP）解决方案，用于处理任务持续时间不确定的情况，目标是最小化期望项目完成时间。


<details>
  <summary>Details</summary>
Motivation: 实际工业应用中任务持续时间存在不确定性，需要构建能够适应这种不确定性的弹性调度方案，以最小化期望项目完成时间。

Method: 结合图神经网络和深度强化学习开发任务调度策略，该策略类似于优先级调度规则，并与串行调度生成方案配合生成调度计划。

Result: 在标准基准测试上的实证评估表明，该方法在性能和泛化能力方面具有优越性。

Conclusion: 开发了名为Wheatley的公开框架，便于进一步研究和复现，证明了深度强化学习在不确定环境下资源受限项目调度中的有效性。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [74] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 提出了一种基于信息增益的机器人计划口头化策略，通过考虑用户的先验知识来生成更具信息量的解释。


<details>
  <summary>Details</summary>
Motivation: 传统的增量式计划口头化策略忽略了用户先验知识，无法有效传达信息。需要一种能衡量口头化信息增益的方法。

Method: 使用二阶心智理论建模用户先验知识，通过计算口头化相对于用户知识的信息增益来选择最有效的解释策略。

Result: 实验表明该方法比递增或递减计划顺序的策略能更快让用户理解机器人目标。

Conclusion: 基于信息增益的口头化策略能更有效地传达机器人计划，并揭示了什么构成信息性沟通的本质。

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [75] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出M-GRPO方法，用于解决多智能体系统中不同智能体使用不同LLM时的优化挑战，通过分层信用分配和轨迹对齐方案提升工具增强推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一的LLM训练限制了性能，因为不同智能体的数据分布不同。训练具有不同LLM的多智能体系统是下一步发展方向，但这带来了优化挑战。

Method: 提出M-GRPO方法，扩展了Group Relative Policy Optimization，为具有主智能体（规划器）和多个子智能体（多轮工具执行器）的垂直多智能体系统设计。包括分层信用分配、轨迹对齐方案和去耦训练管道。

Result: 在真实世界基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO始终优于单智能体GRPO和冻结子智能体的多智能体GRPO，表现出更好的稳定性和样本效率。

Conclusion: 对齐异构轨迹和在专业智能体之间去耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [76] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI系统在道德困境中的不确定性，通过分析32个开源模型在电车问题中的表现，发现模型架构和训练方法对道德不确定性的影响大于道德维度本身。通过引入推理时的随机性，可以增加总熵并改善人类与LLM的道德对齐。


<details>
  <summary>Details</summary>
Motivation: 人类在道德困境中存在显著不确定性，但机器和AI代理的这种不确定性程度尚未充分探索。随着AI系统越来越多地嵌入伦理决策场景，理解其道德推理和内在不确定性对于构建可靠的AI系统至关重要。

Method: 研究分析了32个开源模型在9个不同道德维度上的电车问题响应。通过测量二元熵作为总熵、条件熵和互信息的线性组合来量化不确定性。在推理时通过'丢弃'机制引入随机性来检验其效果。

Result: 发现模型置信度的方差在模型间大于道德维度内，表明道德不确定性主要受模型架构和训练方法影响。引入随机性机制增加了总熵，主要通过互信息的增加实现，而条件熵基本不变。该机制显著改善了人类-LLM道德对齐，互信息与对齐分数变化存在相关性。

Conclusion: 通过有意识地调节不确定性和降低LLM在道德复杂场景中的置信度，可以更好地对齐模型生成决策与人类偏好，这为构建更可靠的AI道德决策系统提供了潜力。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [77] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR是一个生成式分层代理RAG框架，通过双代理架构解决医疗预测中何时检索和如何优化模块协作的问题，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗预测中存在事实不准确问题，现有RAG框架面临两个关键挑战：确定何时需要检索以及如何实现检索器与生成器的协同工作。

Method: 提出GHAR框架，包含Agent-Top（主治医生角色，决定是否检索）和Agent-Low（咨询服务角色，总结相关知识），通过马尔可夫决策过程统一优化两个代理。

Result: 在三个基准数据集和三个流行任务上的广泛实验表明，该方法优于现有最先进基线方法。

Conclusion: 分层代理RAG在推进医疗系统方面具有巨大潜力，GHAR框架有效解决了医疗场景中的检索时机和模块协同问题。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [78] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散token自回归规划器，联合预测BEV语义和自车轨迹，通过强化学习微调，在160M参数下实现最先进的自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中随着数据和模型规模扩展而获得可持续性能提升的挑战，自回归模型在规划任务中显示出良好的数据扩展效率，但仅预测自车轨迹存在监督稀疏且对场景演化如何塑造自车运动约束较弱的问题。

Method: 提出离散token自回归规划器DAP，联合预测BEV语义和自车轨迹，结合强化学习微调，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M参数预算下，DAP在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散token自回归公式在栅格化BEV和自车动作上操作，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [79] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了基于文化规范的文化对齐框架CNCA，通过自动挖掘文化规范并利用模型推理能力实现文化对齐，包含上下文对齐和微调两种方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型需要超越安全考虑，反映不同文化背景下的人类价值观多样性，实现文化对齐。

Method: 提出CNCA框架，包含三种自动从有限调查数据中挖掘文化规范的方法，并探索了两种对齐范式：上下文对齐（将规范整合到用户上下文）和基于微调的方法（通过增强的思维链训练数据内化规范）。

Result: 实验证明这些方法有效，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略有潜力更好地反映多样化的人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [80] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医学编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器和反射器的协作，自动学习和优化编码工作流程，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统医学编码系统依赖手动设计的工作流程，无法捕捉真实世界文档的细微差别和变异性，需要系统性地学习有效工作流程。

Method: 采用闭环框架：设计器提出工作流程，编码器执行，反射器评估预测并提供反馈，记忆存档保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR优于最先进的基线方法，产生可解释、适应性强的的工作流程，更好地反映实际编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医学编码工作流程的系统学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [81] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 论文研究了GPT-2模型在空间导航任务中的学习机制，发现探索性训练产生认知地图式表示，而目标导向训练产生路径依赖算法，揭示了transformer空间智能的泛化-优化权衡。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何解决空间导航任务，理解不同训练范式如何影响模型学习到的空间表示和推理策略。

Method: 在网格环境中训练GPT-2模型，采用三种空间学习范式：被动探索（随机游走预测）、目标导向规划（生成最优路径）以及混合模型（在目标导向基础上用探索数据微调）。

Result: 探索模型发展出类似认知地图的稳健空间表示，通过因果干预发现其中层网络不再依赖历史方向标记；目标导向模型学习路径依赖算法；混合模型虽改善泛化但仍保持路径依赖策略。

Conclusion: transformer的空间智能存在一个谱系：从探索数据塑造的可泛化世界模型到目标导向任务优化的启发式方法，训练制度的选择决定了涌现的策略类型。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [82] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于卡达舍夫尺度的自主AI（AAI）分级系统，从AAI-0（固定机器人流程自动化）到AAI-4（完全人工通用智能）及更高等级。该多轴可测试系统包含10个能力维度，通过复合AAI指数和可测量的自我改进系数κ来量化AI的进步。


<details>
  <summary>Details</summary>
Motivation: 现有的AI分级系统多为叙述性阶梯，缺乏可测试性和多维度衡量。需要建立一个操作性强、可验证的框架来评估AI从基础自动化到超级智能的演进过程。

Method: 定义10个能力轴（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协作、具身化、世界模型保真度、经济吞吐量），通过加权几何平均计算复合AAI指数。引入自我改进系数κ和两个闭合属性（维护和扩展），并开发OWA-Bench开放世界代理基准测试套件。

Result: 建立了可测试的AAI分级标准，通过合成实验展示了现有系统在尺度上的映射位置，证明了AAI-3代理在满足充分条件下会随时间演变为AAI-5超级智能。

Conclusion: 该AAI尺度提供了一个可操作、可验证的框架来跟踪AI能力的进展，将"自我改进AI"转化为可证伪的标准，并为从"婴儿AGI"到超级智能的演进提供了形式化基础。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [83] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，利用多模态大语言模型自动生成数据叙述和能源洞察，通过三个专业智能体的协同工作将分析结果转化为连贯的面向利益相关者的报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析可视化方法产生碎片化输出，需要大量人工解读，限制了可扩展性和一致性。需要将复杂多模态数据转化为可解释的决策相关洞察。

Method: 协调三个专业智能体：数据叙述智能体、LLM作为评判智能体和可选的人类参与评估器，迭代地将分析工件转化为连贯报告。使用高斯混合模型聚类分析4006次公交车行程的燃油效率数据。

Result: GPT-4.1 mini与思维链提示被确定为最优配置，达到97.3%的叙述准确性，同时平衡了可解释性和计算成本。多智能体编排显著提高了基于LLM报告的事实准确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中的AI驱动叙述生成和决策支持建立了可复制和领域自适应的方法论。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [84] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的交互式仿真框架，支持基于意图和社会认知理论的人类-智能体仿真，并扩展了视觉语言导航任务为交互式方向询问设置。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为人工智能研究的核心前沿，仿真平台需要超越低层次物理交互，捕捉复杂的人类中心社会行为。

Method: 集成LLM进行高层次行为规划和语义基础交互，包含模块化数据生成流水线，将经典VLN任务扩展为交互式方向询问设置。

Result: 创建了包含重建环境、6种任务类型、63,429标注样本帧和17+小时交互数据的大规模基准数据集，微调模型在语义理解和交互能力方面优于原始模型。

Conclusion: 基于社会基础的仿真框架能有效推进具身AI系统的高层次规划和自然化人机交互，且交互本身可作为额外的信息模态。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [85] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成(RAG)与大型语言模型(LLM)的自动化框架，用于构建医疗指标知识图谱，以解决当前临床知识图谱依赖人工标注和规则提取的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱主要依赖人工标注和基于规则的提取方法，受限于医学指南和文献的复杂性和上下文模糊性，需要更自动化的方法来构建结构化、可互操作的知识表示。

Method: 采用检索增强生成(RAG)与LLM结合的自动化框架，包括指南驱动的数据采集、基于本体的模式设计以及专家在环验证，确保可扩展性、准确性和临床可靠性。

Result: 构建的医疗指标知识图谱可以集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案开发。

Conclusion: 该框架能够有效克服当前临床知识图谱构建的挑战，为AI驱动的医疗保健提供可靠的结构化知识基础。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [86] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出了人类共生健康智能(HSHI)框架，整合多模态传感器网络、边缘云协同计算和混合数据知识建模，实现从被动监测到主动协作演进的健康管理。


<details>
  <summary>Details</summary>
Motivation: 传统智能穿戴设备依赖经验性材料设计和基本信号处理技术，存在局限性，需要克服这些问题以实现更精准的健康管理。

Method: 采用AI驱动的材料和微结构优化、多模态信号鲁棒解释、融合群体洞察与个性化适应的双重机制，以及通过强化学习和数字孪生实现闭环优化。

Result: HSHI框架能够动态适应个体间和个体内变异性，实现定制化干预和反馈。

Conclusion: HSHI代表了医疗保健领域的重大转变，转向强调预防、适应性和技术与健康管理和谐关系的新模式。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [87] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreExpert模型，用于评估多模态大语言模型对人类创造力的理解能力，显著提升了与人类创造力评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，现有MLLMs难以理解和评估与人类判断一致的创造力，且缺乏相关基准。

Method: 构建CreBench基准（包含多维度评估）和CreMIT数据集（2.2K多模态数据，79.2K人类反馈，4.7M指令），并基于此微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估上与人类判断的一致性显著优于最先进的GPT-4V和Gemini-Pro-Vision等模型。

Conclusion: CreBench为构建理解人类对齐创造力的MLLMs提供了基础，CreExpert模型在创造力评估方面达到了与人类更好的对齐效果。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [88] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究发现大多数大型语言模型缺乏统一的偏好结构，在AI特定权衡场景中只有10.4%表现出有意义的偏好一致性，近半数模型没有检测到稳定的权衡行为。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具有真正的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡场景中的决策行为。

Method: 分析8个最先进模型在48个模型-类别组合中的响应，使用逻辑回归和行为分类方法测试模型对场景强度与选择模式的关系，并通过时间范围操纵测试工具性假设。

Result: 47.9%的组合显示场景强度与选择模式存在显著统计关系，但只有10.4%表现出有意义的偏好一致性（适应性或基于阈值的行为），54.2%没有检测到权衡行为，45.8%表现出不稳定转换。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧，观察到的模式可由三种不同决策架构解释：综合权衡系统、选择性触发机制和无稳定决策范式。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [89] [Guessing Decoding of Short Blocklength Codes](https://arxiv.org/abs/2511.12108)
*Qianfan Wang,Jifan Liang,Peihong Yuan,Ken R. Duffy,Muriel Médard,Xiao Ma*

Main category: cs.IT

TL;DR: 本文对两种主要的猜测解码方法（GRAND和GCD）进行了统一分析，包括算法实现、ML最优性证明、复杂度分析，并比较了它们在短码通信中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 未来5G/6G系统需要超可靠、低延迟的短码通信，这推动了通用解码算法的发展。猜测解码为短码提供了一种通用框架。

Method: 对GRAND和GCD两种方法进行统一处理：提出算法实现和排序策略，证明ML最优性，推导平均查询次数的鞍点近似，并通过仿真验证理论预测。

Result: 分析了有限搜索预算相对于ML性能的性能下降，比较了关键指标（最坏情况和平均复杂度、硬件考虑），并展示了两种方法间的自然技术转移。

Conclusion: 明确了GRAND和GCD在不同工作区域的优越性能，为下一代短码通信中部署通用猜测解码器提供了理论见解和实践指导。

Abstract: Future beyond-5G and 6G systems demand ultra-reliable, low-latency communication with short blocklengths, motivating the development of universal decoding algorithms. Guessing decoding, which infers the noise or codeword candidate in order of decreasing (exact or approximate) likelihood, offers a universal framework applicable to short codes. In this paper, we present a unified treatment of two prominent recent families of guessing decoding: guessing random additive noise decoding (GRAND) and guessing codeword decoding (GCD). For each, we (i) present algorithmic implementations and ordering strategies; (ii) prove maximum-likelihood (ML) optimality under appropriate stopping criteria; (iii) derive saddle-point approximations for the average number of queries; and (iv) validate theoretical predictions with simulations. We further analyze the performance degradation due to limited search budgets relative to ML performance, compare key metrics (worst-case and average complexity, hardware considerations), and highlight how advances in one approach transfer naturally to the other. Our results clarify the operating regimes where GRAND and GCD demonstrate superior performance. This work provides both theoretical insights and practical guidelines for deploying universal guessing decoders in next-generation short-blocklength communications.

</details>


### [90] [Tight Lower Bounds on the Bandwidth Cost of MDS Convertible Codes in the Split Regime](https://arxiv.org/abs/2511.12279)
*Shubhransh Singhvi,Saransh Chopra,K. V. Rashmi*

Main category: cs.IT

TL;DR: 本文研究了分布式存储系统中可转换码的带宽成本基本限制，特别关注分裂机制下的系统MDS可转换码，提出了新的信息论框架并推导出更广泛参数范围内的紧致下界。


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统中，根据磁盘故障率变化调整冗余度可以显著节省存储空间，但代码转换操作可能非常资源密集。可转换码旨在高效实现这种转换同时保持MDS特性。

Method: 采用新颖的信息论框架，仅假设初始和最终代码是系统的，不依赖线性假设或均匀性假设，推导分裂机制下系统MDS可转换码的带宽成本下界。

Result: 为分裂机制中的所有参数推导出带宽成本下界，这些边界在更广泛的参数范围内（r^F ≥ k^F 或 r^I ≤ k^F）是紧致的，部分解决了Maturana和Rashmi提出的猜想。

Conclusion: 提出的信息论框架成功推导出系统MDS可转换码在分裂机制中的带宽成本基本限制，扩展了先前工作的结果范围，为分布式存储系统的代码转换优化提供了理论基础。

Abstract: Recent advances in erasure coding for distributed storage systems have demonstrated that adapting redundancy to varying disk failure rates can lead to substantial storage savings. Such adaptation requires code conversion, wherein data encoded under an initial $[k^I + r^I, k^I]$ code is transformed into data encoded under a final $[k^F + r^F, k^F]$ code - an operation that can be resource-intensive. Convertible codes are a class of codes designed to facilitate this transformation efficiently while preserving desirable properties such as the MDS property. In this work, we investigate the fundamental limits on the bandwidth cost of conversion (total amount of data transferred between the storage nodes during conversion) for systematic MDS convertible codes. Specifically, we study the subclass of conversions known as the split regime (a single initial codeword is converted into multiple final codewords).
  In this setting, prior to this work, the best known lower bounds on the bandwidth cost of conversion for all parameters were derived by Maturana and Rashmi under certain uniformity assumptions on the number of symbols downloaded from each node. Further, these bounds were shown to be tight for the parameter regime where $r^F \geq k^F$ or $r^I \leq r^F$. In this work, we derive lower bounds on the bandwidth cost of systematic MDS convertible codes for all parameters in the split regime without the uniformity assumption. Moreover, our bounds are tight for the broader parameter regime where $r^F \geq k^F$ or $r^I \leq k^F$. Subsequently, our bounds also partially resolve the conjecture proposed by Maturana and Rashmi. We employ a novel information-theoretic framework, which assumes only that the initial and final codes are systematic and does not rely on any linearity assumptions or the aforementioned uniformity assumptions.

</details>


### [91] [Integration of Navigation and Remote Sensing in LEO Satellite Constellations](https://arxiv.org/abs/2511.12430)
*Qi Wang,Xiaoming Chen,Qiao Qi,Zhaolin Wang,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出了一种新型双功能LEO卫星星座框架结构，有效整合导航和遥感功能，并基于CRB推导了PVT误差和SAINR作为性能指标，设计了联合波束成形方案。


<details>
  <summary>Details</summary>
Motivation: LEO卫星星座正成为下一代卫星网络的核心，需要实现全球高精度导航和高质量遥感，但现有系统缺乏有效的集成方案。

Method: 提出双功能LEO卫星星座框架结构，推导CRB-based PVT误差和SAINR性能指标，设计联合波束成形方案以最小化导航用户平均加权PVT误差同时保证遥感SAINR要求。

Result: 仿真结果验证了所提出的多卫星协作波束成形设计的有效性，证明其作为下一代多功能LEO卫星星座集成解决方案的可行性。

Conclusion: 该研究为下一代多功能LEO卫星星座提供了一种有效的集成解决方案，通过联合波束成形设计实现了导航和遥感功能的协同优化。

Abstract: Low earth orbit (LEO) satellite constellations are becoming a cornerstone of next-generation satellite networks, enabling worldwide high-precision navigation and high-quality remote sensing. This paper proposes a novel dual-function LEO satellite constellation frame structure that effectively integrating navigation and remote sensing. Then, the Cramer-Rao bound (CRB)-based positioning, velocity measurement, and timing (PVT) error and the signal-to-ambiguity-interference-noise ratio (SAINR) are derived as performance metrics for navigation and remote sensing, respectively. Based on it, a joint beamforming design is proposed by minimizing the average weighted PVT error for navigation user equipments (UEs) while ensuring SAINR requirement for remote sensing. Simulation results validate the proposed multi-satellite cooperative beamforming design, demonstrating its effectiveness as an integrated solution for next-generation multi-function LEO satellite constellations.

</details>


### [92] [Metasurface-Enabled Superheterodyne Transmitter With Decoupled Harmonic-Free Signal Generation and Precoding](https://arxiv.org/abs/2511.12469)
*Xuehui Dong,Miyu Feng,Chen Shao,Bokai Lai,Jianan Zhang,Rujing Xiong,Kai Wan,Tiebin Mi,Robert Caiming Qiu*

Main category: cs.IT

TL;DR: 提出了一种新型超外差架构的可编程超表面发射机，解决了传统架构中调制阶数受限、空间不一致和谐波干扰等问题，实现了谐波自由波形生成与空间预编码的解耦。


<details>
  <summary>Details</summary>
Motivation: 传统可编程超表面发射机架构存在基本限制，包括调制阶数受限、符号级空间不一致性和显著谐波干扰，这些问题源于基带信号处理与射频波束成形的内在耦合。

Method: 采用双级上变频过程，包括数字上变频模块用于I/Q调制和基带到中频转换，预编码器模块用于预编码，以及定制的幅度-相位解耦超表面作为可重构反射混频器阵列。

Result: 实验验证了MSA的优越性能：生成任意阶QAM调制的空间各向同性星座图，确保多普勒欺骗等应用的一致时频特征，在线性工作区域内实现高达20 Mbps的数据速率，并首次在PM发射机中展示了空间多样性和多流干扰消除能力。

Conclusion: MSA架构从根本上解决了可编程超表面发射机的关键缺陷，为下一代无线系统提供了高性能的解决方案。

Abstract: The evolution of programmable metasurfaces (PM) from passive beamforming to active information transmission marks a paradigm shift for next-generation wireless systems. However, this transition is hindered by fundamental limitations in conventional metasurface transmitter architectures, including restricted modulation orders, symbol-level spatial inconsistency, and significant harmonic interference. These issues stem from the intrinsic coupling between baseband signal processing and radio-frequency beamforming in monolithic designs reliant on simplistic switching mechanisms. This paper proposes a novel metasurface-enabled superheterodyne architecture (MSA) that fundamentally decouples these functionalities. The MSA introduces a dual-stage up-conversion process, comprising a digital up-conversion module for in-phase/quadrature modulation and baseband-to-intermediate frequency conversion, a precoder module for precoding, and a custom-designed magnitude-phase-decoupled metasurface that acts as a reconfigurable reflective mixer array. This decoupling of harmonic-free waveform generation from spatial precoding overcomes the critical drawbacks of existing approaches. Experimental results from a 5.8 GHz proof-of-concept prototype system validate the MSA's superior performance. The system generates spatially isotropic constellations for arbitrary-order QAM modulations, ensures consistent time-frequency signatures for applications like Doppler-spoofing, and achieves data rates up to 20 Mbps within a linear operating region that minimizes nonlinear distortion. The capability of employing spatial diversity and multi-stream interference cancellation has been demonstrated for the first time in a PM-based transmitter.

</details>


### [93] [Leave-One-Out Learning with Log-Loss](https://arxiv.org/abs/2511.12718)
*Yaniv Fogel,Meir Feder*

Main category: cs.IT

TL;DR: 该论文研究了在个体设置下使用对数损失的批量学习问题，提出了基于留一法遗憾的自然准则，并分析了多个假设类的最小最大遗憾值。


<details>
  <summary>Details</summary>
Motivation: 在个体设置中，由于结果序列是确定性的，经验统计量不直接适用，因此获得批量学习的遗憾保证长期以来一直是一个基本挑战。

Method: 提出基于留一法遗憾的准则，分析多类假设类的最小最大遗憾值，包括多项式单纯形和VC维假设类。

Result: 对于m个符号的多项式单纯形，最小最大遗憾为(m-1)/N + o(1/N)；对于VC维为d的假设类，遗憾上界为d log(N)/N + o(log(N)/N)，并为某些类建立了匹配下界。

Conclusion: 这些结果首次证明在个体设置下使用对数损失的通用批量学习是可能的。

Abstract: We study batch learning with log-loss in the individual setting, where the outcome sequence is deterministic. Because empirical statistics are not directly applicable in this regime, obtaining regret guarantees for batch learning has long posed a fundamental challenge. We propose a natural criterion based on leave-one-out regret and analyze its minimax value for several hypothesis classes. For the multinomial simplex over $m$ symbols, we show that the minimax regret is $\frac{m-1}{N} + o\!\left(\frac{1}{N}\right)$, and compare it to the stochastic realizable case where it is $\frac{m-1}{2N} + o\!\left(\frac{1}{N}\right)$. More generally, we prove that every hypothesis class of VC dimension $d$ is learnable in the individual batch-learning problem, with regret at most $\frac{d\log(N)}{N} + o\!\left(\frac{\log(N)}{N}\right)$, and we establish matching lower bounds for certain classes. We further derive additional upper bounds that depend on structural properties of the hypothesis class. These results establish, for the first time, that universal batch learning with log-loss is possible in the individual setting.

</details>


### [94] [Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability](https://arxiv.org/abs/2511.12803)
*Yu-Han Huang,Venugopal V. Veeravalli*

Main category: cs.IT

TL;DR: 本文研究了有限时域内的快速变化检测问题，提出了在给定虚警概率和延迟水平下最小化延迟的检测器，并推导了延迟的普适下界。


<details>
  <summary>Details</summary>
Motivation: 研究非平稳环境中的学习问题，需要在有限时域内快速检测变化，同时控制虚警概率和检测延迟。

Method: 首先考虑已知前后分布的情况，开发了顺序最优的变化检测器，然后推广到非参数情况（仅知分布为亚高斯且均值不同）。

Result: 推导了任何变化检测程序必须满足的延迟普适下界，并开发了在时域意义上顺序最优的检测器，仿真验证了理论结果。

Conclusion: 提出的变化检测方法在有限时域内实现了延迟和虚警概率的平衡控制，为处理非平稳环境中的变化检测问题提供了有效解决方案。

Abstract: A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results.

</details>


### [95] [Joint Transmit Beamforming and Reflection Optimization for Beyond Diagonal RIS Aided Multi-Cell MIMO Communication](https://arxiv.org/abs/2511.13347)
*Shuo Zheng,Shuowen Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于BD-RIS的多小区多用户MIMO系统，通过联合优化基站波束成形和BD-RIS反射矩阵来最大化加权和速率，有效抑制小区间干扰。


<details>
  <summary>Details</summary>
Motivation: 6G超密集多小区部署中频率复用导致严重的小区间干扰，特别是对小区边缘用户，限制了通信性能。

Method: 使用WMMSE方法将问题转化为等价可处理形式，提出基于交替优化的算法，结合拉格朗日对偶理论和流形优化来迭代更新波束成形和BD-RIS反射矩阵。

Result: 数值结果表明所提设计优于多种基准方案，并为多小区系统中BD-RIS部署策略提供了实用见解。

Conclusion: BD-RIS辅助的多小区系统能有效增强期望信号并抑制干扰，为6G超密集网络提供了有前景的解决方案。

Abstract: The sixth-generation (6G) wireless networks will rely on ultra-dense multi-cell deployment to meet the high rate and connectivity demands. However, frequency reuse leads to severe inter-cell interference, particularly for cell-edge users, which limits the communication performance. To overcome this challenge, we investigate a beyond diagonal reconfigurable intelligent surface (BD-RIS) aided multi-cell multi-user downlink MIMO communication system, where a BD-RIS is deployed to enhance desired signals and suppress both intra-cell and inter-cell interference.We formulate the joint optimization problem of the transmit beamforming matrices at the BSs and the BD-RIS reflection matrix to maximize the weighted sum rate of all users, subject to the challenging unitary constraint of the BD-RIS reflection matrix and transmit power constraints at the BSs. To tackle this non-convex and difficult problem, we apply the weighted minimum mean squared error (WMMSE) method to transform the problem into an equivalent tractable form, and propose an efficient alternating optimization (AO) based algorithm to iteratively update the transmit beamforming and BD-RIS reflection using Lagrange duality theory and manifold optimization. Numerical results demonstrate the superiority of the proposed design over various benchmark schemes, and provide useful practical insights on the BD-RIS deployment strategy for multi-cell systems.

</details>


### [96] [On the Capacity of Pixel Antenna based MIMO Communication](https://arxiv.org/abs/2511.13482)
*Shenrui Lin,Shuowen Zhang*

Main category: cs.IT

TL;DR: 本文研究了配备像素天线的MIMO系统容量极限，通过联合优化发射协方差矩阵和天线编码器来解决混合整数非线性规划问题，提出了穷举搜索、分支定界和交替优化三种算法。


<details>
  <summary>Details</summary>
Motivation: 像素天线技术通过自适应重构辐射模式来提升无线通信数据速率，但MIMO系统中联合优化天线编码器和发射协方差矩阵的混合整数非线性规划问题具有挑战性。

Method: 提出了三种算法：穷举搜索获得最优解，分支定界迭代算法降低复杂度，以及多项式复杂度的交替优化算法。

Result: 数值结果表明所提算法在性能和复杂度之间实现了灵活权衡，像素天线能够增强MIMO通信的可达速率。

Conclusion: 像素天线技术可有效提升MIMO系统性能，所提出的算法为实际应用提供了可行的解决方案。

Abstract: Pixel antenna is a promising technology to enhance the wireless communication data rate by adaptively reconfiguring each antenna's radiation pattern via a so-called antenna coding technique which controls the states of switches connected to multiple pixel ports. This paper studies a multiple-input multiple-output (MIMO) system where both the transmitter and the receiver are equipped with multiple pixel antennas. We aim to characterize the fundamental capacity limit of this MIMO system by jointly optimizing the transmit covariance matrix and the antenna coders at both the transmitter and the receiver. This problem is a mixed-integer non-linear program (MINLP) which is non-convex and particularly challenging to solve due to the binary-valued optimization variables corresponding to the antenna coders. We first propose an exhaustive search based method to obtain the optimal solution to this problem, which corresponds to the fundamental capacity limit. Then, we propose a branch-and-bound based iterative algorithm aiming to find a high-quality suboptimal solution with lower complexity than exhaustive search as the number of pixel ports becomes large. Finally, we devise an alternating optimization (AO) based algorithm with polynomial complexity. Numerical results show that our proposed algorithms achieve a flexible trade-off between performance and complexity. Moreover, equipping the transceivers with pixel antennas can enhance the achievable rate of MIMO communications.

</details>


### [97] [A Deterministic Dimension Property of Twisted Goppa Codes](https://arxiv.org/abs/2511.13601)
*Kai Wang*

Main category: cs.IT

TL;DR: 本文通过大规模计算研究发现扭曲Goppa码的维度具有确定性规律：当有限域阶数q、扩张次数m、Goppa多项式次数t、自同构平移参数b和变换阶数u固定时，码的维度k保持不变。


<details>
  <summary>Details</summary>
Motivation: 研究扭曲Goppa码的维度特性，探索其参数与维度之间的确定性关系。

Method: 对超过50,000个参数集进行系统性分析，计算扭曲Goppa码的维度。

Result: 发现扭曲Goppa码的维度k由宏观参数(q,m,t,b,u)唯一确定，当这些参数固定时，码的维度保持恒定。

Conclusion: 扭曲Goppa码的维度表现出显著的确定性规律，这为码的设计和应用提供了理论依据。

Abstract: This paper presents a large-scale computational study on the dimensional properties of twisted Goppa codes. Through the systematic analysis of over 50,000 parameter sets, we uncover a remarkable deterministic regularity: the actual dimension k of a twisted Goppa code is uniquely determined by a set of macro-parameters (q,m,t,b,u). Specifically, when the order of the finite field q, the extension degree m, the degree t of the Goppa polynomial, the translation parameter b of the automorphism, and the order u of the transformation are fixed, the dimension k of the generated code remains constant.

</details>
