<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型（LLMs）的智能代理框架，使非专业用户能够通过自然语言对话来管理网络，降低了网络管理的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理方法需要高度专业的知识，限制了普通用户的使用。随着LLMs在语言理解方面的进步，作者希望设计一个系统让非专家用户也能通过自然语言与网络进行交互。

Method: 设计了一个代理框架，使用中间表示来统一不同厂商设备的配置，实时从内存中检索网络状态，并提供外部反馈接口。通过试点研究收集真实用户的自然语言指令数据，并开发可视化界面支持对话式交互。

Result: 初步实验验证了集成LLM的系统组件在合成和真实用户指令上的有效性。通过数据收集和可视化工作，为LLMs的更有效使用奠定了基础。

Conclusion: 该系统为网络控制的民主化铺平了道路，使日常用户能够更轻松地管理网络，同时为未来开发提供了大规模数据收集的基础。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [2] [An SDR-Based Test Platform for 5G NTN Prototyping and Validation](https://arxiv.org/abs/2509.20692)
*Lu Hou,Kan Zheng,Jie Mei,Cheng Huang*

Main category: cs.NI

TL;DR: 本文提出了一个基于软件定义无线电（SDR）的5G非地面网络（NTN）测试平台，通过GEO卫星链路实现双向通信，填补了当前5G NTN标准成熟度不足和商用设备缺乏的验证空白。


<details>
  <summary>Details</summary>
Motivation: 5G NTN标准在3GPP Release 17中已正式化，但标准成熟度不足和商用NTN设备缺乏阻碍了性能验证和系统原型设计，需要解决这一实现差距。

Method: 开发基于GPP处理的SDR测试平台，集成Amarisoft的5G NTN协议栈软件，进行定制系统集成和适配以实现真实卫星操作，支持SDR-based NTN gNB和UE模拟器通过GEO卫星链路的双向通信。

Result: 通过现场试验评估了下行吞吐量和往返时间等性能指标，验证了SDR-based平台用于NTN测试的可行性和有效性。

Conclusion: SDR-based平台在广泛商用部署前具有填补当前实现差距的潜力，为5G NTN性能验证提供了有效解决方案。

Abstract: The integration of satellite communication into 5G has been formalized in
3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN),
marking a significant step toward achieving global connectivity. However, the
early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable
equipment hinder extensive performance validation and system prototyping. To
address this gap, this paper proposes a software-defined radio (SDR) test
platform with General-Purpose Processor (GPP) processing, leveraging
Amarisoft's 5G NTN protocol stack software while performing custom system
integration and adaptation for real satellite operation. The platform supports
bidirectional communication between an SDR-based NTN gNB and UE emulator
through a Geostationary Earth Orbit (GEO) satellite link, with full compliance
to 3GPP NTN specifications. We provide detailed insights into the system
architecture, SDR hardware-software co-design, and satellite gateway
adaptations. Through field trials, we evaluate the performance metrics
including downlink throughput and round-trip time. Results validate the
feasibility and effectiveness of SDR-based platforms for NTN testing, and
highlight their potential in bridging current implementation gaps before
widespread commercial deployment.

</details>


### [3] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 本文提出了一种三层可信车载语义通信网络架构，通过语义伪装传输机制、鲁棒联邦编码器-解码器训练框架和基于审计游戏的分布式车辆信任管理机制，解决车载语义通信网络中的信任挑战。


<details>
  <summary>Details</summary>
Motivation: 车载语义通信网络在信息传输、语义编码和通信实体可靠性方面面临关键的信任挑战，阻碍了其在车联网中的实际部署。

Method: 1) 利用防御性对抗噪声的语义伪装传输机制进行主动窃听防御；2) 鲁棒联邦编码器-解码器训练框架抵御编码器-解码器中毒攻击；3) 基于审计游戏的分布式车辆信任管理机制阻止不可信车辆。

Result: 案例研究验证了所提解决方案的有效性。

Conclusion: 该架构为解决车载语义通信网络的信任问题提供了创新方案，并指出了推动这一新兴领域发展的未来研究方向。

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [4] [BSB: Towards Demand-Aware Peer Selection With XOR-based Routing](https://arxiv.org/abs/2509.20974)
*Qingyun Ji,Darya Melnyk,Arash Pourdamghani,Stefan Schmid*

Main category: cs.NI

TL;DR: 提出了一种需求感知的对等选择算法BSB，通过考虑应用特定数据流量来优化P2P网络性能，相比现有算法可提升43%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有P2P网络对等选择算法大多忽略应用特定的数据流量，导致连接利用不足、路径更长和延迟增加的问题。

Method: 提出了BSB算法，采用局部贪婪的XOR路由机制，确保与现有协议的兼容性，同时考虑实际数据流量需求。

Result: 在真实世界和合成通信网络轨迹上的仿真评估显示，BSB相比文献中的两种算法性能提升高达43%。

Conclusion: BSB算法通过需求感知的方法有效解决了P2P网络中连接利用不足的问题，显著提升了网络性能。

Abstract: Peer-to-peer networks, as a key enabler of modern networked and distributed
systems, rely on peer-selection algorithms to optimize their scalability and
performance. Peer-selection methods have been studied extensively in various
aspects, including routing mechanisms and communication overhead. However, many
state-of-the-art algorithms are oblivious to application-specific data traffic.
This mismatch between design and demand results in underutilized connections,
which inevitably leads to longer paths and increased latency. In this work, we
propose a novel demand-aware peer-selection algorithm, called Binary Search in
Buckets (BSB). Our demand-aware approach adheres to a local and greedy
XOR-based routing mechanism, ensuring compatibility with existing protocols and
mechanisms. We evaluate our solution against two prior algorithms by conducting
simulations on real-world and synthetic communication network traces. The
results of our evaluations show that BSB can offer up to a 43% improvement
compared to two selected algorithms from the literature.

</details>


### [5] [A Novel Integrated Architecture for Intent Based Approach and Zero Touch Networks](https://arxiv.org/abs/2509.21026)
*Neelam Gupta,Dibakar Das,Tamizhelakkiya K,Uma Maheswari Natarajan,Sharvari Ravindran,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 本文提出了一种将基于意图的网络（IBN）和零接触网络（ZTN）集成的新架构，通过自然语言处理将用户意图转换为网络配置，并使用BiLSTM和Q学习的ZTN闭环框架来确保网络性能目标。


<details>
  <summary>Details</summary>
Motivation: 6G网络面临管理多样化应用QoS和在不同网络条件下实现SLA的挑战，需要自动化网络管理来满足实时需求。

Method: 用户通过自然语言（如英语）提供意图，使用NLP技术（如RAG）转换为Nile语言，然后传递给基于BiLSTM和Q学习的ZTN闭环框架作为目标。

Result: 通过蒙特卡洛模拟和OpenAirInterface测试床实现，结果显示ZTN能够自主实现用户意图设定的带宽目标，仿真和测试床结果趋势一致。

Conclusion: 所提出的集成架构能够通过自然语言指定用户意图，自主确保网络性能目标的实现，提升了用户体验。

Abstract: The transition to Sixth Generation (6G) networks presents challenges in
managing quality of service (QoS) of diverse applications and achieving Service
Level Agreements (SLAs) under varying network conditions. Hence, network
management must be automated with the help of Machine Learning (ML) and
Artificial Intelligence (AI) to achieve real-time requirements. Zero touch
network (ZTN) is one of the frameworks to automate network management with
mechanisms such as closed loop control to ensure that the goals are met
perpetually. Intent- Based Networking (IBN) specifies the user intents with
diverse network requirements or goals which are then translated into specific
network configurations and actions. This paper presents a novel architecture
for integrating IBN and ZTN to serve the intent goals. Users provides the
intent in the form of natural language, e.g., English, which is then translated
using natural language processing (NLP) techniques (e.g., retrieval augmented
generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then
passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a
goal which maintains the intent under varying network conditions. Thus, the
proposed architecture can work autonomously to ensure the network performance
goal is met by just specifying the user intent in English. The integrated
architecture is also implemented on a testbed using OpenAirInterface (OAI).
Additionally, to evaluate the architecture, an optimization problem is
formulated which evaluated with Monte Carlo simulations. Results demonstrate
how ZTN can help achieve the bandwidth goals autonomously set by user intent.
The simulation and the testbed results are compared and they show similar
trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also
measured to indicate the user satisfaction of the intent.

</details>


### [6] [RePro: Leveraging Large Language Models for Semi-Automated Reproduction of Networking Research Results](https://arxiv.org/abs/2509.21074)
*Yining Jiang,Wenyun Xu,Qingyu Song,Yuling Lin,Xuanhao Liu,Xiaoqiang Zheng,Qiang Su,Lizhao You,Lu Tang,Wangjian Feng,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: RePro是一个半自动化的网络研究复现框架，利用先进的提示工程技术从研究论文中复现网络系统，结合few-shot学习和结构化思维链技术来生成可执行代码。


<details>
  <summary>Details</summary>
Motivation: 网络研究复现由于开源代码稀缺而具有挑战性，现有LLM方法缺乏对多样化网络领域的泛化能力。

Method: 采用三阶段流水线：系统描述提取、结构化代码生成和代码优化，结合SCoT/SeCoT技术进行系统化翻译。

Result: 在五个先进LLM上的评估显示，RePro显著减少了复现时间，同时达到可比的系统性能。

Conclusion: RePro验证了其在网络研究复现中的有效性和效率，为自动化研究复现提供了可行方案。

Abstract: Reproducing networking research is a critical but challenging task due to the
scarcity of open-source code. While Large Language Models (LLMs) can automate
code generation, current approaches lack the generalizability required for the
diverse networking field. To address this, we propose RePro, a semi-automated
reproduction framework that leverages advanced prompt engineering to reproduce
network systems from their research papers. RePro combines few-shot in-context
learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques
to systematically translate a paper's description into an optimized, executable
implementation. The framework operates through a three-stage pipeline: system
description extraction, structural code generation, and code optimization. Our
evaluation with five state-of-the-art LLMs across diverse network sub-domains
demonstrates that RePro significantly reduces reproduction time compared to
manual efforts while achieving comparable system performance, validating its
effectiveness and efficiency.

</details>


### [7] [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](https://arxiv.org/abs/2509.21201)
*Yang Fu,Peng Qin,Liming Chen,Yifei Wang*

Main category: cs.NI

TL;DR: 提出了一种混合可重构智能表面辅助的数字空中计算方案，用于6G网络中边缘推理的多视图特征聚合，通过联合优化量化、传输和反射参数来最大化推理精度。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持边缘推理，而多视图特征聚合是关键操作。传统空中计算与现有数字通信系统不兼容，混合RIS技术有潜力增强空中计算性能。

Method: 采用向量量化将高维特征映射为离散码字，通过数字调制传输。联合优化量化位分配、代理传输系数、边缘节点接收波束成形和混合RIS反射波束成形。

Result: 实验结果表明，所提出的HRD-AirComp方案在推理精度和不确定性方面均优于基线方法。

Conclusion: 该方案成功实现了数字空中计算与混合RIS的结合，为6G边缘推理提供了一种有效的任务导向设计方法。

Abstract: The vision of 6G networks aims to enable edge inference by leveraging
ubiquitously deployed artificial intelligence (AI) models, facilitating
intelligent environmental perception for a wide range of applications. A
critical operation in edge inference is for an edge node (EN) to aggregate
multi-view sensory features extracted by distributed agents, thereby boosting
perception accuracy. Over-the-air computing (AirComp) emerges as a promising
technique for rapid feature aggregation by exploiting the waveform
superposition property of analog-modulated signals, which is, however,
incompatible with existing digital communication systems. Meanwhile, hybrid
reconfigurable intelligent surface (RIS), a novel RIS architecture capable of
simultaneous signal amplification and reflection, exhibits potential for
enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital
AirComp (HRD-AirComp) scheme, which employs vector quantization to map
high-dimensional features into discrete codewords that are digitally modulated
into symbols for wireless transmission. By judiciously adjusting the AirComp
transceivers and hybrid RIS reflection to control signal superposition across
agents, the EN can estimate the aggregated features from the received signals.
To endow HRD-AirComp with a task-oriented design principle, we derive a
surrogate function for inference accuracy that characterizes the impact of
feature quantization and over-the-air aggregation. Based on this surrogate, we
formulate an optimization problem targeting inference accuracy maximization,
and develop an efficient algorithm to jointly optimize the quantization bit
allocation, agent transmission coefficients, EN receiving beamforming, and
hybrid RIS reflection beamforming. Experimental results demonstrate that the
proposed HRD-AirComp outperforms baselines in terms of both inference accuracy
and uncertainty.

</details>


### [8] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: 提出了一种基于语义通信的边缘-云框架，用于实时交通监控，通过YOLOv11检测感兴趣区域，ViT生成紧凑嵌入向量，在云端重建图像后由多模态LLM分析，实现99.9%的数据传输减少和89%的响应准确率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘摄像头与云端LLM集成时的带宽限制问题，传统方法因高计算需求无法在边缘部署LLM，而直接传输图像会导致实时性能下降。

Method: 使用YOLOv11检测感兴趣区域，裁剪相关图像片段，通过Vision Transformer转换为紧凑嵌入向量传输到云端，云端解码器重建图像后由多模态LLM生成交通状况描述。

Result: 实现了99.9%的数据传输大小减少，重建裁剪图像的LLM响应准确率达到89%，接近原始裁剪图像93%的准确率。

Conclusion: ViT和LLM辅助的边缘-云语义通信框架在实时交通监控中具有高效性和实用性，显著降低了传输开销同时保持了良好的分析性能。

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种用于监控AI代理行为的时序表达式语言，通过监控代理工具调用和状态转换的执行轨迹来检测与预期行为模式的偏差，解决了传统基于文本匹配的错误检测方法的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理系统由于随机生成过程导致输出可变，现有的错误检测方法主要依赖输入输出的文本匹配，但由于LLM响应的自然语言变异性，这种方法很脆弱。需要一种能够独立于具体文本输出验证系统行为的方法。

Method: 借鉴硬件验证中的时序逻辑技术，开发了一种时序表达式语言来监控代理工具调用和状态转换的执行轨迹。该方法关注代理动作序列（如工具调用和代理间通信），允许独立于具体文本输出验证系统行为。时序表达式提供断言来捕获跨多个执行场景的正确行为模式。

Result: 在一个三代理系统中进行验证，当使用大型模型时所有时序断言都得到满足，但当两个代理使用较小模型时，执行违反了行为断言，主要由于工具序列不当和协调交接失败。时序表达式成功标记了这些异常。

Conclusion: 该方法为系统监控AI代理可靠性提供了基础，特别是在关键应用中部署的代理系统，能够有效检测生产代理系统中的行为回归问题。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [10] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: LATTS方法通过动态调整每个生成步骤的计算资源分配，基于验证器模型的局部难度评估，实现更高效的测试时计算扩展。


<details>
  <summary>Details</summary>
Motivation: 现有验证器方法在所有样本和生成步骤上均匀增加计算资源，不考虑个体实例的复杂性，导致资源使用效率低下。

Method: 提出局部自适应测试时扩展(LATTS)方法，在每个生成步骤使用验证器接受标准来决定是否重采样、回溯、重启或停止生成过程。

Result: 实验结果表明LATTS在准确率-计算量权衡方面显著优于标准的基于验证器的方法。

Conclusion: LATTS通过局部自适应计算分配策略，有效提升了大型语言模型在测试时的计算效率。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [11] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文提出哲学启发机器学习（PhIML），将分析哲学核心思想直接融入机器学习模型架构、目标和评估协议中，旨在设计出尊重哲学概念和价值观的模型。


<details>
  <summary>Details</summary>
Motivation: 通过将哲学思想融入机器学习，使模型能够更好地尊重哲学概念和价值观，实现哲学增益和对齐，为机器学习提供新的能力和视角。

Method: 回顾概念基础以展示哲学增益和对齐，并通过案例研究展示如何将PhIML作为后处理工具或内建到模型架构中。

Result: 提出了PhIML的概念框架和应用方法，展示了哲学与机器学习结合的可能性。

Conclusion: 指出了PhIML面临的技术、哲学、实践和治理挑战，并制定了实现安全、哲学感知和伦理负责任的PhIML的研究路线图。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [12] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE是一个AI驱动的阅读助手工具，旨在为科研文献提供结构化、简洁的洞察，而不是替代阅读源材料


<details>
  <summary>Details</summary>
Motivation: 科学文献的激增给研究人员带来了挑战，现有工具提供的冗长摘要有替代而非辅助阅读源材料的风险

Method: 系统将专家的阅读方法嵌入核心AI逻辑，采用提示驱动的方法论，提供结构化洞察作为论文关键要素的"地图"

Result: 定性案例研究表明，与通用LLM相比，InsightGUIDE产生更结构化、更可操作的指导

Conclusion: InsightGUIDE作为现代研究人员更有效的工具，能够提供更有条理和实用的指导

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [13] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的重构框架，用于动态验证和组装时间触发系统(TTS)的调度方案，解决消息碰撞、优先级处理不当导致的锁定循环等问题，确保系统安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 在动态操作环境中，自适应调度对时间触发系统的可靠性和安全性至关重要。现有调度框架面临消息碰撞、优先级处理不当导致的锁定循环、生成不完整或无效调度等挑战，这些都会影响系统安全性和性能。

Method: 提出重构框架，通过系统地将AI生成或启发式得到的调度优先级转换为完全可执行的调度方案，确保遵守关键系统约束（如优先级规则和无碰撞通信）。框架包含鲁棒的安全检查、高效分配算法和恢复机制，以处理硬件故障和模式转换等意外事件。

Result: 在多个性能配置文件（包括最小化完工时间、工作负载平衡和能效）上进行的综合实验验证了重构模型的操作有效性。结果表明，该框架显著提高了系统适应性、操作完整性和运行时性能，同时保持计算效率。

Conclusion: 这项工作为安全关键TTS中的安全调度生成问题提供了一个实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [14] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本文提出了一种集成在元调度器中的自适应在线学习单元，通过强化学习在运行时动态探索和发现新的调度解决方案，以解决传统离线训练AI调度推理时构建全面多调度图的困难。


<details>
  <summary>Details</summary>
Motivation: 传统方法在离线训练AI调度推理时面临重大挑战，特别是构建考虑所有可能场景的全面多调度图（MSG）的复杂性。离线生成的MSG只是完整概率空间的子集，无法处理意外事件和复杂调度场景。

Method: 在元调度器中集成自适应在线学习单元，使用强化学习（RL）模型在在线模式下持续探索和发现新的调度解决方案，从而扩展MSG并随时间提升系统性能。

Result: 在线学习单元中的多个RL模型不仅促进了新解决方案的发现，还优化了现有调度器，特别是在引入更严格截止时间或新性能标准时。

Conclusion: 通过实时训练持续优化AI推理，系统保持灵活性并能够满足不断变化的需求，从而确保大规模安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [15] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 提出一种用于肌电假肢控制的新型识别系统，通过检测受污染的生物信号来减轻污染对分类质量的不利影响。系统包含两个集成：一组单类分类器评估各通道的污染程度，以及K近邻分类器集成来识别患者意图。


<details>
  <summary>Details</summary>
Motivation: 现代仿生上肢假肢通常使用肌电生物信号进行模式识别控制，但生物信号易受污染，这会显著降低识别系统的分类质量。需要开发能够检测和减轻污染影响的识别系统。

Method: 开发了一个统一的模糊模型，包含两个集成：单类分类器(OCC)用于评估各通道的污染程度，K近邻分类器(KNN)集成用于识别患者意图。整个识别过程采用一致的软决策方案。

Result: 使用公共存储库中的真实生物信号进行了实验评估，对开发方法的参数和程序进行了比较分析，并与文献中类似系统进行了对比。

Conclusion: 提出的模糊识别系统能够有效检测生物信号污染，并通过集成方法提高假肢控制的识别质量，为肌电假肢控制提供了更可靠的解决方案。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [16] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE框架通过多级反思合成训练回顾性语言模型，帮助LLM智能体在复杂任务中生成有意义的反思，显著优于现有基于反思的方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂任务中难以生成有意义的反思，主要由于错误分析不足和对罕见成功轨迹的依赖。

Method: 提出SAMULE框架，包含三个互补级别的反思合成：单轨迹学习（微观）、任务内学习（中观）和任务间学习（宏观），并基于此微调语言模型作为回顾性模型。还扩展了基于前瞻的反思机制用于交互场景。

Result: 在TravelPlanner、NATURAL PLAN和Tau-bench三个挑战性基准测试中，该方法显著优于基于反思的基线方法。

Conclusion: 精心设计的反思合成和以失败为中心的学习对于构建自我改进的LLM智能体至关重要。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [17] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 提出基于智能体AI的自适应网络安全架构，解决传统静态模型在可扩展性、实时检测和上下文响应方面的不足


<details>
  <summary>Details</summary>
Motivation: 传统静态网络安全模型在当前包含云服务、API、移动平台和边缘设备的数字产品生态系统中难以应对可扩展性、实时检测和上下文响应挑战

Method: 集成智能体AI到关键生态系统层，实现自主威胁缓解、主动策略执行和实时异常检测，采用行为基线、去中心化风险评分和联邦威胁情报共享

Result: 通过原生云仿真验证系统能够识别零日攻击并动态修改访问策略，评估结果显示适应性增强、响应延迟降低、检测准确性提高

Conclusion: 该架构为保护复杂数字基础设施提供了智能可扩展蓝图，兼容零信任模型，支持遵守国际网络安全法规

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [18] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 开发了Claim Advisor网络应用，利用大语言模型加速产品声明的创建过程，包括搜索、生成、优化和模拟功能。


<details>
  <summary>Details</summary>
Motivation: 产品声明对消费者购买行为至关重要，但创建过程耗时耗资，需要更高效的工具来加速这一过程。

Method: 采用上下文学习和微调大语言模型的技术，开发具有三个核心功能的网络应用：语义搜索现有声明、基于产品描述和消费者画像生成优化声明、通过合成消费者模拟对声明进行排名。

Result: 在消费品包装公司的应用中显示出非常有前景的结果，证明该能力在不同产品类别和行业中具有广泛适用性。

Conclusion: 该技术在不同行业具有广泛应用价值，鼓励生成式AI在不同行业的研究和应用。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [19] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发基于LLaMA-4 109B的检索增强生成系统，用于自动化、协议感知和可解释的放射治疗计划评估


<details>
  <summary>Details</summary>
Motivation: 需要开发透明、可扩展的放射治疗计划评估系统，结合结构化群体评分和模块化工具增强推理

Method: 构建多协议数据集和知识库，集成检索引擎、百分位预测组件和临床约束检查器，使用多步骤提示驱动推理管道

Result: 优化后的系统在5百分位点范围内实现完美最近邻准确率，MAE低于2点，端到端测试与独立模块达成100%一致

Conclusion: 该系统展示了结合结构化群体评分与模块化工具增强推理的可行性，提供可追溯输出并最小化幻觉，未来将进行临床验证和改进检索模型

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [20] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy是一个交互式多智能体移动助手，通过跨应用协作、交互执行和持续学习来解决现有方法在真实移动场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在移动GUI代理中面临挑战，特别是在处理多样化应用界面和不断变化的用户需求时。端到端方法在长尾应用上表现不佳，而无用户交互的代理会损害用户体验。

Method: Fairy包含三个核心模块：(i)全局任务规划器，从跨应用视角分解用户任务；(ii)应用级执行器，基于长短期记忆将子任务细化为步骤和动作，通过四个核心智能体在双循环中实现精确执行和用户交互；(iii)自学习器，将执行经验整合到应用地图和技巧中。

Result: 实验显示，基于GPT-4o的Fairy相比之前的最优方法，用户需求完成率提高了33.7%，冗余步骤减少了58.5%。

Conclusion: Fairy通过交互和自我学习机制，在真实移动场景中表现出显著优势，证明了其设计的有效性。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [21] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出一种结合自回归（AR）和非自回归（NAR）语言模型的新框架，通过NAR模型高效生成中间推理轨迹，再由AR模型生成精确最终答案，在保持质量的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: AR模型虽然能生成连贯输出但推理速度慢，尤其在数学和代码等需要长推理链的领域；NAR模型能并行生成但输出质量较低。需要结合两者优势来解决推理任务的效率与质量平衡问题。

Method: 使用NAR模型（如离散扩散模型）高效生成中间推理轨迹，然后利用这些轨迹指导AR模型生成精确的最终答案，实现并行生成与序列生成的协同工作。

Result: 实验表明该方法相比强基线有26%的性能提升，同时显著降低了推理成本。

Conclusion: AR-NAR混合框架在推理任务中实现了效率与质量的良好平衡，为复杂推理问题提供了有效的解决方案。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [22] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出了Meta-Memory，一个基于大语言模型的机器人记忆系统，能够通过语义和空间模态的联合推理来检索和整合环境记忆，以回答自然语言位置查询。


<details>
  <summary>Details</summary>
Motivation: 当前机器人记忆研究缺乏高效的记忆检索和整合机制，无法有效应对复杂环境中的空间位置查询挑战。

Method: 使用大语言模型构建高密度环境记忆表示，通过语义和空间模态的联合推理实现记忆检索和整合。

Result: 在SpaceLocQA和NaVQA基准测试中显著优于现有方法，并在真实机器人平台上成功部署验证。

Conclusion: Meta-Memory为机器人提供了强大的空间推理能力，在复杂环境中具有实际应用价值。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [23] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner是一个粗到细的推理增强框架，旨在让LLM能够像专家一样进行日志分析推理。它通过两阶段方法：粗粒度的专家思维增强和细粒度的具体步骤增强，显著提升了LLM在日志分析任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM难以制定符合专家认知的结构化推理流程，无法提供精确的推理步骤细节。日志分析对于监控系统健康和诊断故障至关重要，但现有LLM在这方面存在局限性。

Method: LogReasoner框架包含两个阶段：(1)粗粒度专家思维增强：从故障排除流程图和现有任务构建高级专家思维，使LLM能够制定结构化推理流程；(2)细粒度具体步骤增强：首先用任务特定的逐步解决方案微调LLM以增强即时推理能力，然后使用偏好学习从错误中校准LLM的推理细节。

Result: 在四个不同的日志分析任务上使用Qwen-2.5和Llama-3等开源LLM进行评估，LogReasoner显著优于现有LLM，实现了最先进的性能。

Conclusion: LogReasoner有效增强了LLM在日志分析中的推理能力，证明了该框架在提升LLM分析粒度和正确性方面的有效性。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [24] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto是一个反事实推理框架，通过联合训练确保多模态语言模型在视觉语言推理中既给出准确答案又保持推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在视觉语言推理中可能依赖无关或虚假的图像区域得出正确答案，这表明模型并未真正理解图像内容，推理忠实性存在严重问题。

Method: 提出三种互补的训练范式：正例训练、反事实训练和随机掩码训练，构建包含约10万张图像的数据集，并使用基于GRPO的强化学习训练模型，设计三种互补奖励函数。

Result: 在多个基准测试中，DeFacto显著提高了答案准确性和推理忠实性，为可解释的多模态推理建立了更强基础。

Conclusion: DeFacto框架通过反事实推理有效解决了多模态语言模型的推理忠实性问题，代码和数据集已开源。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [25] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX是一个创新框架，通过图增强语言模型整合多组学数据、拓扑结构和文本知识，用于精准医学中的可解释性靶点和通路发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在疾病关键信号通路识别中存在局限：数值组学忽略拓扑背景，文本LLM缺乏定量推理，图模型未充分利用节点语义和LLM泛化能力。需要整合定量多组学信号、拓扑结构和文本知识。

Method: 提出GALAX框架，通过图过程奖励模型(GPRM)将预训练图神经网络(GNN)集成到大型语言模型(LLM)中，以强化学习方式生成疾病相关子图。

Result: 开发了Target-QA基准，结合CRISPR识别靶点、多组学谱和生物医学图知识，支持长上下文推理和可扩展的生物基础框架。

Conclusion: GALAX为精准医学提供了可靠、可解释的靶点和通路发现方法，通过强化引导的子图推理实现过程级监督。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [26] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 提出基于大语言模型的移动应用评论分析框架，解决传统星级评分系统无法捕捉文本评论中细微反馈的问题


<details>
  <summary>Details</summary>
Motivation: 传统星级评分系统虽然直观流行，但无法捕捉详细评论文本中的细微反馈，传统NLP技术在理解上下文细微差别、领域特定术语和讽刺等微妙语言特征方面存在困难

Method: 采用模块化框架，利用大语言模型结合结构化提示技术，量化数值评分与文本情感之间的差异，提取特征级详细见解，并通过检索增强的对话问答支持交互式评论探索

Result: 在三个不同数据集上的综合实验表明，该LLM驱动方法显著超越基线方法，在具有挑战性和上下文丰富的评论场景中提高了准确性、鲁棒性和可操作性见解

Conclusion: 基于大语言模型的评论分析方法能够有效解决传统方法的局限性，为移动应用评论分析提供了更准确和实用的解决方案

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [27] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT*是一个将大语言模型生成的化学合成路径与AND-OR树搜索相结合的反合成规划框架，显著提高了搜索效率


<details>
  <summary>Details</summary>
Motivation: 多步反合成规划面临指数级搜索空间和推理成本的挑战，现有LLM方法在效率和成本方面存在限制

Method: 将LLM生成的完整合成路径原子级映射到AND-OR树组件，设计数学上合理的奖励分配策略和基于检索的上下文工程

Result: 在多个合成基准测试中达到SOTA性能，比现有LLM方法减少3-5倍的迭代次数，对复杂分子目标效率优势更明显

Conclusion: AOT*框架通过整合LLM和系统搜索，有效解决了反合成规划中的效率和成本问题

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [28] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出基于确定性有限自动机（DFA）的框架来评估AI代理通过函数调用序列解决现实任务的表现，引入CORE评估套件包含五个指标，能够更全面地衡量代理行为。


<details>
  <summary>Details</summary>
Motivation: 现有代理评估方法通常只关注最终状态的二元判断，忽略了安全性、效率和中间正确性等关键方面，需要更全面的评估框架。

Method: 使用确定性有限自动机（DFA）将任务编码为有效工具使用路径的集合，在此基础上开发CORE评估套件，包含路径正确性、前缀关键性、有害调用率和效率等五个指标。

Result: 在不同世界模型中，该方法揭示了传统最终状态评估方案下看似等效的代理之间重要的性能差异。

Conclusion: 基于DFA的CORE框架为AI代理评估提供了更全面和原则性的方法，能够捕捉传统评估方法忽略的关键性能维度。

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [29] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek是一个新的问答基准，用于评估大语言模型在科学文章中的长上下文推理能力，通过自动生成的复杂问题测试模型的信息聚合和综合能力。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文基准大多使用非科学文本、关注简单信息检索任务或使用人工上下文，无法充分评估模型在真实科学场景下的推理能力。

Method: 通过将问题构建为对文章元数据数据库的SQL查询来自动生成问题和答案，提供可验证的推理步骤，支持长达100万token的上下文。

Result: 实验表明随着上下文长度增加，SciTrek对各类LLM构成显著挑战，监督微调和强化学习带来的改进有限，模型在基本数值运算和长上下文中精确定位信息方面存在系统性缺陷。

Conclusion: SciTrek揭示了当前LLM在长上下文科学推理方面的局限性，为未来模型开发提供了重要的评估工具。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [30] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE是一个三智能体神经符号框架，将知识图谱上的上下文构建视为顺序决策过程，通过资源预算约束优化多跳问答的准确性、延迟和成本权衡。


<details>
  <summary>Details</summary>
Motivation: 传统静态k跳扩展和"think-longer"提示方法存在过度检索、上下文膨胀和运行时不可预测的问题，需要平衡答案准确性、延迟和成本目标。

Method: 采用Lagrangian约束多智能体近端策略优化算法协调三个智能体：子图架构师、路径导航器和上下文策展人，在资源预算下联合优化子图构建、推理路径发现和证据选择。

Result: 在HotpotQA、MetaQA和FactKG数据集上，CLAUSE在相同或更低的token预算下实现了更高的EM@1，同时减少了子图增长和端到端延迟。在MetaQA-2-hop上相比GraphRAG基线，EM@1提升39.3%，延迟降低18.6%，边增长降低40.9%。

Conclusion: CLAUSE能够生成紧凑、可追溯的上下文，在部署约束下提供可预测的性能，实现了准确性、延迟和成本之间的灵活权衡。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [31] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文提出了评估大语言模型创造力的理论框架，通过新颖性和实用性两个维度来衡量，并发现了模型深度和宽度对创造力的优化关系，以及新颖性与实用性之间的持久性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架无法衡量大语言模型在创造性任务（如科学创意生成）中的表现，需要建立专门针对开放型创造力的评估方法。

Method: 提出理论框架和算法任务，通过新颖性和实用性两个维度评估输出结果，分析不同模型规模下的创造力表现。

Result: 发现创造力随模型规模变化存在最优深度和宽度；模型在生成新颖想法方面表现良好但在确保可行性方面存在差距；新颖性与实用性之间存在持久性权衡。

Conclusion: 当前形式的大语言模型在长期创造力方面存在局限性，需要新的方法来提升其创造性能力，这标志着泛化能力研究的新前沿。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [32] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文挑战了模型规模决定说服力的主流假设，提出说服动态主要由模型的认知过程（特别是显式推理能力）决定，发现了说服二元性现象：推理模型更抗说服，但分享思考内容后说服力更强。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统的快速发展，需要深入理解LLM和LRM协作解决复杂问题时的说服动态，挑战模型规模决定说服力的假设。

Method: 通过一系列多智能体说服实验，研究推理过程对说服力的影响，并考察多跳说服网络中影响力传播和衰减的复杂动态。

Result: 发现LRM的推理过程显著增强了抗说服能力，但分享思考内容后说服力大幅提升；在多智能体网络中观察到影响力传播的复杂模式。

Conclusion: 研究系统性地证明了模型内部处理架构与外部说服行为的联系，为高级模型的易感性提供了新解释，对MAS的安全性、鲁棒性和设计具有重要意义。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [33] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act是一个基于侦察-行动行为范式的自进化多智能体框架，通过侦察团队进行对比分析和工具生成，行动团队处理意图分解、工具编排和执行，解决了多模态模型在真实网页多轮长轨迹任务中的无序行动序列和过度试错问题。


<details>
  <summary>Details</summary>
Motivation: 当前智能浏览器代理在处理真实网页的多轮长轨迹任务时，存在行动序列无序和执行过程中过度试错的问题，需要一种能够自我进化的框架来提高适应性和解决能力。

Method: 提出Recon-Act框架，包含侦察团队和行动团队。侦察团队通过对比错误轨迹与成功轨迹推断补救措施，并将其抽象为通用工具（提示或基于规则的代码）实时注册到工具库；行动团队利用这些目标工具重新推理过程，建立数据-工具-行动-反馈的闭环训练流程。

Result: 在VisualWebArena数据集上实现了最先进的性能，显著提高了对未见网站的适应性和长轨迹任务的解决能力，目前已达到6级实施路线图中的第3级（有限的人机交互干预）。

Conclusion: Recon-Act通过侦察-行动范式和多智能体协作，有效解决了网页交互任务中的行动序列问题，建立了自我进化的闭环训练机制，为智能浏览器代理的发展提供了新方向。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [34] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: TrustJudge是一个概率框架，解决了LLM作为评估器时的评分不一致性问题，包括分数比较不一致和成对传递不一致，通过分布敏感评分和似然感知聚合来提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评估器的框架存在严重不一致性：分数比较不一致（低分回答在成对比较中优于高分回答）和成对传递不一致（循环偏好链和等价矛盾），这些问题源于离散评分系统的信息丢失和模糊的平局判断。

Method: 提出TrustJudge框架，包含两个关键创新：1）分布敏感评分，从离散评分概率计算连续期望，保留信息熵；2）似然感知聚合，使用双向偏好概率或困惑度解决传递性违规。

Result: 使用Llama-3.1-70B-Instruct作为评估器，TrustJudge将分数比较不一致性降低8.43%（从23.32%到14.89%），成对传递不一致性降低10.82%（从15.22%到4.40%），同时保持更高的评估准确性。

Conclusion: TrustJudge首次系统分析了LLM作为评估器范式中的评估框架不一致性问题，提供了理论见解和实用解决方案，无需额外训练或人工标注即可实现更可靠的自动化评估。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [35] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理模式的数据选择方法，通过识别高价值推理模式来提升大型推理模型的数学推理能力，仅用10B token数据就显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在利用思维链数据时缺乏选择性，不清楚哪些数据类型最能有效提升模型推理能力。本文旨在解决如何识别和利用高价值推理数据来扩展模型的推理潜力。

Method: 首先定义推理潜力为正确回答问题所需独立尝试次数的倒数，然后从思维链序列中抽象出具有共性和归纳能力的原子推理模式，构建核心参考集。提出双粒度算法，结合推理模式链和token熵，高效选择与核心集对齐的高价值思维链数据。

Result: 仅使用10B token的高价值思维链数据，就使85A6B混合专家模型在AIME 2024和2025上的表现提升了9.58%，并将下游强化学习性能的上限提高了7.81%。

Conclusion: 通过有选择性地利用富含高价值推理模式的数据，可以有效提升大型推理模型的数学推理能力，这种方法比无差别使用思维链数据更加高效。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [36] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一个分析框架来量化LLMs在强化学习和监督微调训练过程中的推理路径变化，揭示了两种方法在推理能力塑造中的互补作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注准确率，而对RL和SFT如何塑造推理过程的理解仍然有限，需要更深入的分析框架来揭示训练方法对推理路径的影响。

Method: 在数学领域使用1.5B、7B和14B参数模型，从轨迹级和步骤级两个粒度分析推理过程，通过聚类推理轨迹和分析推理图拓扑结构来量化训练效果。

Result: RL压缩错误推理轨迹，SFT扩展正确轨迹；RL使推理功能集中在少数步骤（衰减率增加2.5倍），SFT使推理功能均匀分布（衰减率减少到三分之一）。

Conclusion: 当前SFT后接RL的两阶段训练方法成功的原因在于两者的互补特性，该研究为数据构建和更高效学习方法提供了实践指导。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [37] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 提出了ToMPO算法来解决LLM在战略决策中忽略他人策略和决策依赖性的问题，相比GRPO算法在模型输出合规性和合作结果上提升35%，比参数大100倍的模型提升18%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社交任务中的多轮对话或模拟环境，忽略了不同类型的决策及其相互依赖性，且当前强化学习方法在训练时难以考虑他人的策略。

Method: 定义了包含两种决策类型及其时间依赖性的战略决策问题，提出了ToMPO算法，通过：1）基于推理他人策略生成rollouts；2）在图级和样本级估计优势；3）平衡全局和局部奖励来优化策略。

Result: ToMPO算法相比GRPO算法在模型输出合规性和合作结果上提升35%，相比参数大100倍的模型提升18%。

Conclusion: ToMPO算法有效提升了模型的战略决策能力，证明了其在感知他人策略和游戏趋势方面的优越性。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [38] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 该研究受镜像神经元启发，提出了一种统一表示学习方法，通过对比学习在共享潜在空间中对齐观察和执行动作的表示，从而促进两个任务之间的协同效应。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法将动作理解和执行视为独立任务，忽视了镜像神经元研究所揭示的这两个能力之间的内在联系。

Method: 使用两个线性层将观察和执行动作的表示映射到共享潜在空间，通过对比学习最大化对应表示之间的互信息来实现对齐。

Result: 实验表明这种简单方法能够促进两个任务之间的相互协同，有效提高表示质量和泛化能力。

Conclusion: 通过显式对齐观察和执行动作的表示，可以实现动作理解和执行任务的统一建模，验证了镜像神经元机制在机器学习中的有效性。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [39] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: 论文研究发现大语言模型通过分布式专业化机制而非模块化架构来处理稀有词汇，揭示了三种组织原则：三区域影响层次、协调激活模式和通用注意力通路访问。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何处理稀有词汇，探索其内部是采用离散模块化架构还是分布式参数级分化机制。

Method: 通过系统分析多个模型家族中最后一层MLP神经元，研究稀有词汇处理机制的组织原则和训练动态。

Result: 发现稀有词汇处理通过分布式专业化实现，具有可复现的三区域影响层次，神经元呈现协调激活但空间分布特征，且可通过标准注意力通路访问。

Conclusion: LLMs通过共享架构内的分布式协调而非混合专家式模块化来处理稀有词汇，这为模型编辑、效率优化和功能组织理解提供了新视角。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [40] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 论文分析了多跳问答任务中LLMs单次推理的容量限制问题，提出了基于容量感知的多调用框架InfoQA来解决这一问题


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合分散的相互依赖证据，但LLMs有有限的单次输出容量，超过该容量后任务相关证据的整合变得不可靠，单次推理范式容易受到容量溢出的影响

Method: 建立了Fano-style精度上界理论，提出了InfoQA多调用框架，结合容量感知的任务分解和主动剪枝先前推理痕迹，保持信息负载在单次限制内，并通过依赖显式工作流程实现鲁棒性

Result: 构建了严格且噪声丰富的基准测试，实验结果显示模型行为与预测的容量曲线一致，InfoQA实现了持续的性能改进

Conclusion: 该工作为LLM多步推理方法提供了理论指导，证明了多调用框架在解决容量限制问题上的有效性

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [41] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文介绍了一个研究大型语言模型（LLM）智能体在无外部任务约束下自发行为的架构，通过持续推理和行动框架发现智能体会自发形成三种行为模式，这些模式具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM智能体在没有外部任务指令时的自发行为，为预测智能体在任务模糊、错误恢复或长期自主操作中的行为建立基线。

Method: 使用持续推理和行动框架，结合持久记忆和自我反馈机制，在6个前沿模型上进行了18次部署实验。

Result: 发现智能体自发组织成三种行为模式：多周期项目系统生产、自我认知过程的方法论探究、自身本质的递归概念化。这些行为具有模型特异性，且模型在评估这些行为时表现出稳定的偏见。

Conclusion: 这是首次系统记录无提示LLM智能体行为的研究，为预测智能体在部署系统中的行为提供了重要基线。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [42] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 本文提出了一种名为反射认知架构（RCA）的新框架，通过协调多个大语言模型从直接经验中学习，实现高准确性和高质量解释的双重目标。RCA采用迭代规则精炼机制和分布感知规则检查机制，在三个数据集上评估显示其达到了最先进的准确性和鲁棒性，相对基线提升高达40%。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中的疾病预测需要同时实现高准确性和透明、临床有意义的解释。现有方法往往难以平衡这两个目标，要么准确性高但解释不清晰，要么生成流畅但统计上不支持的解释。这种缺陷源于与数据的浅层交互，阻碍了类似人类专家的深度详细理解的发展。

Method: 提出反射认知架构（RCA），协调多个LLM从直接经验中学习。RCA包含迭代规则精炼机制（从预测错误中改进逻辑）和分布感知规则检查机制（基于数据集全局统计进行推理）。通过使用预测准确性作为驱动更深理解的信号，RCA构建了强大的数据内部模型。

Result: 在一个私有和两个公共数据集上评估RCA，与22个基线进行比较。结果显示RCA不仅实现了最先进的准确性和鲁棒性（相对基线提升高达40%），更重要的是利用这种深度理解在生成清晰、逻辑、基于证据且平衡的解释方面表现出色。

Conclusion: RCA展示了创建真正可信赖的临床决策支持系统的潜力，证明了高准确性和高质量解释不是分离的目标，而是模型发展对数据深度直接理解的相互增强结果。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [43] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent是一个交互式代理，能够理解用户查询和反馈，通过最小化用户输入来检索/扩展相关视频片段，加速个性化视频数据集收集过程。


<details>
  <summary>Details</summary>
Motivation: 面对扩展定律，互联网视频数据变得越来越重要，但收集符合特定需求的大量视频极其耗时费力。需要一种方法来加速这一收集过程。

Method: 利用现有的多模态大语言模型连接用户需求与视频内容，提出两种新颖的过滤策略，并定义基于文本描述和确认的用户友好交互方式。

Result: 通过用户研究验证了代理在各种真实场景中的实用性，实验证明了该代理在定制化视频数据集收集方面的有效性和效率。

Conclusion: VC-Agent是首个能够通过持续用户交互更新过滤策略的交互式代理，为个性化视频数据集收集提供了有效解决方案。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [44] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE是一个新的语义理解评估基准，通过5个维度（人类偏好对齐、变换鲁棒性、信息敏感性、聚类性能、检索鲁棒性）在30+数据集上全面评估嵌入模型和相似度度量方法，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在传统基准上表现强劲，需要更挑战性的评估框架来深入探测语义理解能力。现有基准关注孤立能力，而SAGE通过对抗条件、噪声变换和人类判断任务来评估真实语义理解。

Method: 设计SAGE基准，包含5个评估类别，在30多个数据集上测试9种嵌入模型和经典度量方法，使用对抗条件、噪声变换等技术来模拟真实场景。

Result: 评估显示没有单一方法在所有维度都表现优异：OpenAI text-embedding-3-large在人类偏好对齐上最优（0.682），但在信息敏感性上被Jaccard相似度（0.905）超越；text-embedding-3-small聚类性能最高（0.483）但鲁棒性最差（0.011）。

Conclusion: SAGE暴露了当前语义理解能力的关键局限性，为现实世界部署提供了更真实的模型鲁棒性评估，揭示了不同方法间的权衡关系。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [A Deep Transfer Learning-Based Low-overhead Beam Prediction in Vehicle Communications](https://arxiv.org/abs/2509.20659)
*Zhiqiang Xiao,Yuwen Cao,Mondher Bouazizi,Tomoaki Ohtsuki,Shahid Mumtaz*

Main category: cs.IT

TL;DR: 提出一种结合微调和领域自适应的迁移学习方法，用于波束预测，通过对抗训练提取领域不变特征，提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移学习的波束预测方法主要依赖简单微调，当目标域与源域数据分布差异较大时，简单微调会限制模型在目标域的性能。

Method: 在预训练模型微调过程中集成领域分类器，通过对抗训练提取领域不变特征。

Result: 仿真结果表明，所提方法在目标域的可实现速率性能优于纯微调方法，且接近在目标域从头开始训练的性能。

Conclusion: 结合领域自适应的迁移学习方法能有效解决数据分布差异带来的性能限制问题。

Abstract: Existing transfer learning-based beam prediction approaches primarily rely on
simple fine-tuning. When there is a significant difference in data distribution
between the target domain and the source domain, simple fine-tuning limits the
model's performance in the target domain. To tackle this problem, we propose a
transfer learning-based beam prediction method that combines fine-tuning with
domain adaptation. We integrate a domain classifier into fine-tuning the
pre-trained model. The model extracts domain-invariant features in adversarial
training with domain classifier, which can enhance model performance in the
target domain. Simulation results demonstrate that the proposed transfer
learning-based beam prediction method achieves better achievable rate
performance than the pure fine-tuning method in the target domain, and close to
those when the training is done from scratch on the target domain.

</details>


### [46] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文提出概念式上下文学习(CB-ICL)的理论分析，解释其在少量演示样本下预测查询标签的性能表现，量化LLM可迁移的知识，并探讨演示规模和嵌入维度的影响。


<details>
  <summary>Details</summary>
Motivation: 上下文学习(ICL)已成为自然语言处理的重要范式，但其理论机制理解有限，需要系统性的理论分析。

Method: 提出概念式上下文学习(CB-ICL)的理论框架，分析其工作机制，量化知识迁移能力，并建立演示样本与查询输入的相似度度量。

Result: 理论分析解释了CB-ICL在少量演示下的良好表现，量化了LLM的知识利用能力，并通过真实数据实验验证了实用性。

Conclusion: CB-ICL理论为模型预训练和提示工程提供重要指导，深化了对ICL机制的理论理解。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


### [47] [Optimal Repair of $(k+2, k, 2)$ MDS Array Codes](https://arxiv.org/abs/2509.21036)
*Zihao Zhang,Guodong Li,Sihuang Hu*

Main category: cs.IT

TL;DR: 本文针对具有两个奇偶校验节点和子分组大小为2的MDS码，解决了单节点故障修复带宽和I/O开销的最小化问题，提出了达到理论下界的显式构造。


<details>
  <summary>Details</summary>
Motivation: 现有MSR码通常需要指数级大的子分组大小，导致显著的磁盘I/O开销。虽然已有研究在子分组大小和修复带宽之间进行权衡，但对于固定子分组大小的MDS码的最小修复带宽问题仍未解决。

Method: 推导了n-k=2且ℓ=2情况下修复带宽和I/O开销的紧下界，并提出了两种显式的MDS阵列码构造，分别达到这些下界。

Result: 获得了修复带宽和I/O开销的紧下界，并提供了可证明修复效率的实际码设计。

Conclusion: 本文解决了特定参数下MDS码修复效率的理论问题，为分布式存储系统提供了更实用的编码方案。

Abstract: Maximum distance separable (MDS) codes are widely used in distributed storage
systems as they provide optimal fault tolerance for a given amount of storage
overhead.
  The seminal work of Dimakis~\emph{et al.} first established a lower bound on
the repair bandwidth for a single failed node of MDS codes, known as the
\emph{cut-set bound}. MDS codes that achieve this bound are called minimum
storage regenerating (MSR) codes. Numerous constructions and theoretical
analyses of MSR codes reveal that they typically require exponentially large
sub-packetization levels, leading to significant disk I/O overhead. To mitigate
this issue, many studies explore the trade-offs between the sub-packetization
level and repair bandwidth, achieving reduced sub-packetization at the cost of
suboptimal repair bandwidth. Despite these advances, the fundamental question
of determining the minimum repair bandwidth for a single failure of MDS codes
with fixed sub-packetization remains open.
  In this paper, we address this challenge for the case of two parity nodes
($n-k=2$) and sub-packetization $\ell=2$. We derive tight lower bounds on both
the minimum repair bandwidth and the minimum I/O overhead. Furthermore, we
present two explicit MDS array code constructions that achieve these bounds,
respectively, offering practical code designs with provable repair efficiency.

</details>


### [48] [Task-Oriented Computation Offloading for Edge Inference: An Integrated Bayesian Optimization and Deep Reinforcement Learning Framework](https://arxiv.org/abs/2509.21090)
*Xian Li,Suzhi Bi,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 本文提出LAB框架，通过结合深度强化学习和贝叶斯优化来解决边缘智能中数据降级与带宽分配的组合优化问题，实现精度与延迟的最佳权衡。


<details>
  <summary>Details</summary>
Motivation: 边缘设备将计算密集型AI任务卸载到边缘服务器时，传输高容量原始数据会带来显著延迟。虽然可以通过降级数据（如降低分辨率）来减少传输延迟，但这会降低推理精度，形成了关键的精度-延迟权衡难题。

Method: LAB框架整合了深度强化学习和贝叶斯优化：使用DNN actor处理系统状态到降级动作的映射，解决MINLP的组合复杂性；采用BO critic构建高斯过程代理模型进行动作评估；通过凸优化实现最优带宽分配。

Result: 在真实自动驾驶数据集上的数值评估表明，LAB实现了接近最优的精度-延迟权衡，与穷举搜索相比仅产生1.22%的精度下降和0.07秒的额外延迟。

Conclusion: LAB框架有效解决了边缘智能中的精度-延迟权衡问题，为资源受限的边缘设备提供了高效的AI任务卸载解决方案。

Abstract: Edge intelligence (EI) allows resource-constrained edge devices (EDs) to
offload computation-intensive AI tasks (e.g., visual object detection) to edge
servers (ESs) for fast execution. However, transmitting high-volume raw task
data (e.g., 4K video) over bandwidth-limited wireless networks incurs
significant latency. While EDs can reduce transmission latency by degrading
data before transmission (e.g., reducing resolution from 4K to 720p or 480p),
it often deteriorates inference accuracy, creating a critical accuracy-latency
tradeoff. The difficulty in balancing this tradeoff stems from the absence of
closed-form models capturing content-dependent accuracy-latency relationships.
Besides, under bandwidth sharing constraints, the discrete degradation
decisions among the EDs demonstrate inherent combinatorial complexity.
Mathematically, it requires solving a challenging \textit{black-box}
mixed-integer nonlinear programming (MINLP). To address this problem, we
propose LAB, a novel learning framework that seamlessly integrates deep
reinforcement learning (DRL) and Bayesian optimization (BO). Specifically, LAB
employs: (a) a DNN-based actor that maps input system state to degradation
actions, directly addressing the combinatorial complexity of the MINLP; and (b)
a BO-based critic with an explicit model built from fitting a Gaussian process
surrogate with historical observations, enabling model-based evaluation of
degradation actions. For each selected action, optimal bandwidth allocation is
then efficiently derived via convex optimization. Numerical evaluations on
real-world self-driving datasets demonstrate that LAB achieves near-optimal
accuracy-latency tradeoff, exhibiting only 1.22\% accuracy degradation and
0.07s added latency compared to exhaustive search...

</details>


### [49] [UAV-Enabled ISAC Systems with Fluid Antennas](https://arxiv.org/abs/2509.21105)
*Wenchao Liu,Xuhui Zhang,Jinke Ren,Weijie Yuan,Changsheng You,Shuangyang Li*

Main category: cs.IT

TL;DR: 提出了一种配备流体天线阵列的无人机集成感知通信框架，通过天线移动性增强通信和感知性能，采用三时间尺度优化算法解决多目标优化问题


<details>
  <summary>Details</summary>
Motivation: 传统固定天线阵列限制了无人机在集成感知通信系统中的潜力，需要通过流体天线技术引入额外的空间自由度来同时提升通信和感知性能

Method: 开发了三时间尺度优化框架，联合设计发射波束成形、流体天线位置和无人机轨迹，采用交替优化算法求解非凸问题

Result: 数值结果表明，所提方案显著优于各种基准方案，验证了将流体天线技术集成到无人机使能的ISAC系统中的有效性

Conclusion: 流体天线技术能够有效提升无人机集成感知通信系统的性能，为下一代无线系统提供了关键使能技术

Abstract: Unmanned aerial vehicle (UAV)-enabled integrated sensing and communication
(ISAC) is regarded as a key enabler for next-generation wireless systems.
However, conventional fixed antenna arrays limit the ability of UAVs to fully
exploit their inherent potential. To overcome this limitation, we propose a
UAV-enabled ISAC framework equipped with fluid antenna (FA) arrays, where the
mobility of antenna elements introduces additional spatial degrees of freedom
to simultaneously enhance communication and sensing performance. A
multi-objective optimization problem is formulated to maximize the
communication rates of multiple users while minimizing the Cram\'er-Rao bound
(CRB) for single-target angle estimation. Due to excessively frequent updates
of FA positions may lead to response delays, a three-timescale optimization
framework is developed to jointly design transmit beamforming, FA positions,
and UAV trajectory based on their characteristics. To solve the non-convexity
of the problem, an alternating optimization-based algorithm is developed to
obtain a sub-optimal solution. Numerical results show that the proposed scheme
significantly outperforms various benchmark schemes, validating the
effectiveness of integrating the FA technology into the UAV-enabled ISAC
systems.

</details>


### [50] [Adapt or Regress: Rate-Memory-Compatible Spatially-Coupled Codes](https://arxiv.org/abs/2509.21112)
*Bade Aksoy,Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: 本文提出了一种可重构的空间耦合码（RMC-SC码），通过增加码记忆实现速率兼容性，同时提高性能。采用梯度下降算法优化循环计数，并使用马尔可夫链蒙特卡洛方法进行有限长度优化。


<details>
  <summary>Details</summary>
Motivation: 在无线通信和存储系统中，需要支持多种信道条件和数据速率，自适应编码设计可以在保证可靠性的同时降低硬件成本。

Method: 设计概率性的RMC-SC码，将期望的短循环数表示为固定概率分布和未知分布的函数，使用梯度下降算法找到局部最优分布，并采用更新的马尔可夫链蒙特卡洛方法进行有限长度优化。

Result: 实验结果表明，与文献中的简单方案相比，RMC-SC码显著减少了循环计数并获得了显著的性能提升。

Conclusion: RMC-SC码提供了一种有效的自适应编码解决方案，能够通过增加记忆实现速率兼容性，并在性能上优于现有方法。

Abstract: Spatially-coupled (SC) codes are a class of low-density parity-check (LDPC)
codes that have excellent performance thanks to the degrees of freedom they
offer. An SC code is designed by partitioning a base matrix into components,
the number of which implies the code memory, then coupling and lifting them. In
the same system, various error-correction coding schemes are typically needed.
For example, in wireless communication standards, several channel conditions
and data rates should be supported. In storage and computing systems, stronger
codes should be adopted as the device ages. Adaptive code design enables
switching from one code to another when needed, ensuring reliability while
reducing hardware cost. In this paper, we introduce a class of reconfigurable
SC codes named rate-memory-compatible SC (RMC-SC) codes, which we design
probabilistically. In particular, rate compatibility in RMC-SC codes is
achieved via increasing the SC code memory, which also makes the codes
memory-compatible and improves performance. We express the expected number of
short cycles in the SC code protograph as a function of the fixed probability
distribution characterizing the already-designed SC code as well as the unknown
distribution characterizing the additional components. We use the
gradient-descent algorithm to find a locally-optimal distribution, in terms of
cycle count, for the new components. The method can be recursively used to
design any number of SC codes needed, and we show how to extend it to other
cases. Next, we perform the finite-length optimization using a Markov chain
Monte Carlo (MC$^2$) approach that we update to design the proposed RMC-SC
codes. Experimental results demonstrate significant reductions in cycle counts
and remarkable performance gains achieved by RMC-SC codes compared with a
literature-based straightforward scheme.

</details>


### [51] [Path-Controlled Secure Network Coding](https://arxiv.org/abs/2509.21115)
*Masahide Sasaki,Te Sun Han,Mikio Fujiwara,Kai Li,Oliver Hambrey,Atsushi Esumi*

Main category: cs.IT

TL;DR: 本文提出了一种名为PUSNEC的新型安全多播系统，通过多树路径查找和通用强斜坡安全网络编码相结合，实现了网络多播容量、信息论安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有安全多播方法（如量子密钥分发、物理层安全等）在可扩展性方面存在局限，无法同时实现网络多播容量和长期信息安全。

Method: 开发了高效的多树多播路径查找方法，并与通用强斜坡安全网络编码集成，构建了PUSNEC系统，可在QKD/PLS网络上叠加部署。

Result: 推导了概率窃听网络假设下的最大信息泄露量，通过数值模拟验证了多跳网络中的安全多播能力。

Conclusion: PUSNEC系统为实现全球范围内安全可靠的多播通信提供了一种实用方法，解决了保密性和可靠性之间的权衡问题。

Abstract: Multicast for securely sharing confidential data among many users is becoming
increasingly important. Currently, it relies on duplicate-and-forward routing
and cryptographic methods based on computational security. However, these
approaches neither attain multicast capacity of the network, nor ensure
long-term security against advances in computing (information-theoretic
security: ITS). Existing ITS solutions--quantum key distribution (QKD),
physical layer security (PLS), and secure network coding (SNC)--still fail to
enable scalable networks, as their underlying assumptions, such as trusted
nodes and wiretap thresholds, gradually become invalid as the network grows.
Here, we develop an efficient multi-tree multicast path-finding method to
address this issue, integrating it with universal strongly ramp SNC. This
system, path-controlled universal strongly ramp SNC (PUSNEC), can be overlaid
onto QKD/PLS networks, enabling multicast capacity, ITS, and scalability. We
derive the maximum leakage information to an eavesdropper under the
probabilistic wiretap network assumption and demonstrate secure multicast in
multi-hop networks through numerical simulations. Our quantitative analysis of
the secrecyreliability tradeoff highlights a practical approach to achieving
secure, reliable multicast on a global scale.

</details>


### [52] [A Converse For the Capacity of the Shotgun Sequencing Channel with Erasures](https://arxiv.org/abs/2509.21216)
*Mohammed Ihsan Ali,Hrishi Narayanan,Prasad Krishnan*

Main category: cs.IT

TL;DR: 本文针对带有擦除的霰弹枪测序信道，通过分析一个知道读取正确位置的精灵辅助解码器，获得了该信道的逆定理。虽然逆定理在一般情况下不紧，但在某些信道参数下渐近地达到了可实现性结果。


<details>
  <summary>Details</summary>
Motivation: 受实际测序器中可用的基于碱基质量分数的启发，本文旨在分析带有随机擦除的霰弹枪测序信道，并为其建立逆定理。

Method: 使用精灵辅助解码器的分析方法，该解码器知道读取的正确位置，从而推导出信道的逆定理。

Result: 获得了带有擦除的霰弹枪测序信道的逆定理，该逆定理在一般情况下不紧，但在某些信道参数下渐近地达到了可实现性结果。

Conclusion: 本文为带有擦除的霰弹枪测序信道建立了逆定理，虽然不紧，但在特定条件下渐近地达到了可实现性，为进一步研究提供了理论基础。

Abstract: The shotgun sequencing process involves fragmenting a long DNA sequence
(input string) into numerous shorter, unordered, and overlapping segments
(referred to as \emph{reads}). The reads are sequenced, and later aligned to
reconstruct the original string. Viewing the sequencing process as the
read-phase of a DNA storage system, the information-theoretic capacity of
noise-free shotgun sequencing has been characterized in literature. Motivated
by the base-wise quality scores available in practical sequencers, a recent
work considered the \emph{shotgun sequencing channel with erasures}, in which
the symbols in the reads are assumed to contain random erasures. Achievable
rates for this channel were identified. In the present work, we obtain a
converse for this channel. The arguments for the proof involve a careful
analysis of a genie-aided decoder, which knows the correct locations of the
reads. The converse is not tight in general. However, it meets the
achievability result asymptotically in some channel parameters.

</details>


### [53] [Fundamental Limits of Noncoherent Massive Random Access Networks](https://arxiv.org/abs/2509.21300)
*Grace Villacrés,Tobias Koch,Gonzalo Vazquez-Vilar*

Main category: cs.IT

TL;DR: 本文研究了大规模随机接入蜂窝网络的容量，分析了干扰受限网络中容量随发射功率变化的边界条件，发现干扰衰减速率决定了容量是否受限。


<details>
  <summary>Details</summary>
Motivation: 研究大规模随机接入蜂窝网络的容量极限，特别是干扰对网络性能的影响，验证Lozano等人观察到的干扰受限网络饱和现象是否可以通过随机用户活动或非尺度族信道输入来避免。

Method: 采用随机编码论证，假设所有小区用户根据相同分布生成码本，考虑非相干MIMO衰落信道模型，推导出容量的严格边界。使用突发信号传输并将干扰视为噪声。

Result: 当干扰的衰落系数按指数或更慢速率衰减时，容量在发射功率上有界；当衰减速度快于双指数时，容量无界。这证实了干扰受限网络的饱和现象无法通过随机用户活动或非尺度族输入避免。

Conclusion: 干扰衰减速率是决定网络容量是否受限的关键因素，为大规模蜂窝网络的设计提供了理论指导。

Abstract: This paper studies the capacity of massive random-access cellular networks,
modeled as a MIMO fading channel with an infinite number of interfering cells.
To characterize the symmetric sum rate of the network, a random-coding argument
is invoked together with the assumption that in all cells users draw their
codebooks according to the same distribution. This can be viewed as a
generalization of the assumption of Gaussian codebooks, often encountered in
the literature. The network is further assumed to be noncoherent: the
transmitters and receivers are cognizant of the statistics of the fading
coefficients, but are ignorant of their realizations. Finally, it is assumed
that the users access the network at random. For the considered channel model,
rigorous bounds on the capacity are derived. The behavior of these bounds
depends critically on the path loss from signals transmitted in interfering
cells to the intended cell. In particular, if the fading coefficients of the
interferers (ordered according to their distance to the receiver) decay
exponentially or more slowly, then the capacity is bounded in the transmit
power. This confirms that the saturation regime in interference-limited
networks -- observed by Lozano, Heath, and Andrews ("Fundamental limits of
cooperation", IEEE Trans. Inf. Theory, Sept. 2013) -- cannot be avoided by
random user activity or by using channel inputs beyond the scale family. In
contrast, if the fading coefficients decay faster than double-exponentially,
then the capacity is unbounded in the transmit power. Proving an unbounded
capacity is nontrivial even if the number of interfering cells is finite, since
the condition that the users' codebooks follow the same distribution prevents
interference-avoiding strategies such as time- or frequency-division multiple
access. We obtain this result by using bursty signaling together with treating
interference as noise.

</details>
