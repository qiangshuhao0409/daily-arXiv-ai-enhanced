<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 13]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question](https://arxiv.org/abs/2509.19337)
*Stefanos Bakirtzis,Paul Almasan,José Suárez-Varela,Gabriel O. Ferreira,Michail Kalntis,André Felipe Zanella,Ian Wassell,Andra Lutu*

Main category: cs.NI

TL;DR: 本文通过大规模实验评估了可微分光线追踪与深度学习模型在无线电传播建模中的性能，发现深度学习模型在准确性和适应性方面优于可微分光线追踪。


<details>
  <summary>Details</summary>
Motivation: 可微分光线追踪在无线电传播建模中显示出潜力，但缺乏在生产级网络上的实验验证。本文旨在填补这一空白，为移动网络运营商和研究社区提供实际指导。

Method: 使用可微分光线追踪和深度学习模型，基于从主要移动网络运营商收集的覆盖13个城市、超过10,000个天线的真实世界数据，进行无线电覆盖仿真。

Result: 可微分光线追踪在泛化能力和实时应用方面存在困难，而深度学习模型在城市、郊区和农村部署中表现出更高的准确性和更快的适应性，准确度提升可达3 dB。

Conclusion: 深度学习模型在当前阶段比可微分光线追踪更适合大规模实际应用，为无线生态系统和未来研究提供了重要见解。

Abstract: Differentiable ray tracing has recently challenged the status quo in radio
propagation modelling and digital twinning. Promising unprecedented speed and
the ability to learn from real-world data, it offers a real alternative to
conventional deep learning (DL) models. However, no experimental evaluation on
production-grade networks has yet validated its assumed scalability or
practical benefits. This leaves mobile network operators (MNOs) and the
research community without clear guidance on its applicability. In this paper,
we fill this gap by employing both differentiable ray tracing and DL models to
emulate radio coverage using extensive real-world data collected from the
network of a major MNO, covering 13 cities and more than 10,000 antennas. Our
results show that, while differentiable ray-tracing simulators have contributed
to reducing the efficiency-accuracy gap, they struggle to generalize from
real-world data at a large scale, and they remain unsuitable for real-time
applications. In contrast, DL models demonstrate higher accuracy and faster
adaptation than differentiable ray-tracing simulators across urban, suburban,
and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental
results aim to provide timely insights into a fundamental open question with
direct implications on the wireless ecosystem and future research.

</details>


### [2] [Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks](https://arxiv.org/abs/2509.19341)
*Yang Fu,Peng Qin,Yueyue Zhang,Yifei Wang*

Main category: cs.NI

TL;DR: 提出了一种细粒度的AI模型缓存和下载系统，利用参数可重用性来优化6G网络中边缘节点的模型存储和传输效率。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持按需AI模型下载以满足用户多样化的推理需求，但现有AI模型体积庞大，在边缘节点有限存储容量下缓存困难，且无线信道中异构模型的并发传输存在挑战。

Method: 基于参数可重用性原理，选择性缓存模型参数块，消除不同模型间可重用参数的冗余存储；采用协调多点广播技术同时向多个用户传输可重用参数块；提出分布式多智能体学习框架优化参数块缓存、迁移和广播波束成形。

Result: 理论分析和仿真实验验证了所提学习框架具有优越的收敛性能，能够有效降低模型下载延迟。

Conclusion: 该系统通过参数重用和协调传输机制，解决了6G网络中AI模型边缘缓存和传输的挑战，为按需AI服务提供了可行解决方案。

Abstract: 6G networks are envisioned to support on-demand AI model downloading to
accommodate diverse inference requirements of end users. By proactively caching
models at edge nodes, users can retrieve the requested models with low latency
for on-device AI inference. However, the substantial size of contemporary AI
models poses significant challenges for edge caching under limited storage
capacity, as well as for the concurrent delivery of heterogeneous models over
wireless channels. To address these challenges, we propose a fine-grained AI
model caching and downloading system that exploits parameter reusability,
stemming from the common practice of fine-tuning task-specific models from a
shared pre-trained model with frozen parameters. This system selectively caches
model parameter blocks (PBs) at edge nodes, eliminating redundant storage of
reusable parameters across different cached models. Additionally, it
incorporates coordinated multipoint (CoMP) broadcasting to simultaneously
deliver reusable PBs to multiple users, thereby enhancing downlink spectrum
utilization. Under this arrangement, we formulate a model downloading delay
minimization problem to jointly optimize PB caching, migration (among edge
nodes), and broadcasting beamforming. To tackle this intractable problem, we
develop a distributed multi-agent learning framework that enables edge nodes to
explicitly learn mutual influence among their actions, thereby facilitating
cooperation. Furthermore, a data augmentation approach is proposed to
adaptively generate synthetic training samples through a predictive model,
boosting sample efficiency and accelerating policy learning. Both theoretical
analysis and simulation experiments validate the superior convergence
performance of the proposed learning framework.

</details>


### [3] [TinyAC: Bringing Autonomic Computing Principles to Resource-Constrained Systems](https://arxiv.org/abs/2509.19350)
*Wojciech Kalka,Ruitao Xue,Kamil Faber,Aleksander Slominski,Devki Jha,Rajiv Ranjan,Tomasz Szydlo*

Main category: cs.NI

TL;DR: 本文探讨了在物联网设备中使用自主计算的问题与挑战，提出了一种结合自下而上智能（TinyML和设备上学习）与自上而下指导（LLMs）的混合方法，以实现可扩展和可解释的智能自适应微小系统开发。


<details>
  <summary>Details</summary>
Motivation: 自主计算是开发深度网络边缘智能自适应自管理系统的一种有前景的方法，但在物联网设备应用中面临特定问题和挑战。

Method: 采用混合方法，结合自下而上智能（TinyML和设备上学习）和自上而下指导（大型语言模型），开发可扩展和可解释的智能自适应微小系统。

Result: 提出TinyAC系统需要具备自适应特性来处理运行中可能出现的问题，并识别了现有差距和挑战。

Conclusion: 讨论了现有挑战和未来研究方向，强调了自主计算在物联网设备中的潜力和发展需求。

Abstract: Autonomic Computing (AC) is a promising approach for developing intelligent
and adaptive self-management systems at the deep network edge. In this paper,
we present the problems and challenges related to the use of AC for IoT
devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML
and on-device learning) and top-down guidance (LLMs) to achieve a scalable and
explainable approach for developing intelligent and adaptive self-management
tiny systems. Moreover, we argue that TinyAC systems require self-adaptive
features to handle problems that may occur during their operation. Finally, we
identify gaps, discuss existing challenges and future research directions.

</details>


### [4] [A User-to-User Resource Reselling Game in Open RAN with Buffer Rollover](https://arxiv.org/abs/2509.19392)
*Ruide Cao,Marie Siew,David Yau*

Main category: cs.NI

TL;DR: 提出了一种基于游戏的用户间物理资源块（PRB）转售模型，在O-RAN环境中实现频谱资源共享，通过纳什均衡和迭代竞价机制优化资源利用。


<details>
  <summary>Details</summary>
Motivation: O-RAN框架的灵活性为未使用PRB资源的转售提供了机会，旨在提高频谱效率并满足用户动态异构的服务需求。

Method: 构建用户间的策略性博弈模型，考虑未满足需求的跨时隙传递和用户内部缓冲区状态，提出收敛到纳什均衡的迭代竞价机制。

Result: 仿真显示，该方法减少数据丢失30.5%，降低频谱资源浪费50.7%，显著提升社会福利。

Conclusion: 游戏理论模型和竞价机制有效优化了O-RAN环境中的资源分配，证明了该方法的实用性和效率优势。

Abstract: The development of the Open RAN (O-RAN) framework helps enable network
slicing through its virtualization, interoperability, and flexibility. To
improve spectral efficiency and better meet users' dynamic and heterogeneous
service demands, O-RAN's flexibility further presents an opportunity for
resource reselling of unused physical resource blocks (PRBs) across users. In
this work, we propose a novel game-based user-to-user PRB reselling model in
the O-RAN setting, which models the carryover of unmet demand across time
slots, along with how users' internal buffer states relate to any PRBs
purchased. We formulate the interplay between the users as a strategic game,
with each participant aiming to maximize their own payoffs, and we prove the
existence and uniqueness of the Nash equilibrium (NE) in the game. We
furthermore propose an iterative bidding mechanism that converges to this NE.
Extensive simulations show that our best approach reduces data loss by 30.5%
and spectrum resource wastage by 50.7% while significantly improving social
welfare, compared to its absence.

</details>


### [5] [FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks](https://arxiv.org/abs/2509.19398)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: FedOC是一个多服务器联邦学习框架，利用重叠区域的客户端作为中继节点和动态模型选择器，实现边缘服务器间的模型共享和间接数据融合，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 多服务器联邦学习中，不同边缘服务器覆盖区域可能存在重叠，这些重叠区域的客户端可以访问多个边缘服务器。充分利用这些重叠客户端的潜力来缓解通信瓶颈和加速训练过程。

Method: 提出FedOC框架：1）中继重叠客户端（ROCs）实时转发边缘模型促进服务器间模型共享；2）普通重叠客户端（NOCs）根据模型交付时间动态选择初始模型进行本地训练。每轮训练包含客户端本地训练、边缘服务器聚合、ROC中继传输和二次聚合。

Result: 实验结果表明，与现有方法相比，FedOC方案取得了显著的性能提升，特别适合延迟敏感的边缘环境。

Conclusion: FedOC通过充分利用重叠客户端的双重角色，实现了去中心化的模型传播和间接数据融合，有效加速了多服务器联邦学习的训练过程。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to
mitigate communication bottlenecks of single-server FL. We focus on a typical
multi-server FL architecture, where the regions covered by different edge
servers (ESs) may overlap. A key observation of this architecture is that
clients located in the overlapping areas can access edge models from multiple
ESs. Building on this insight, we propose FedOC (Federated learning with
Overlapping Clients), a novel framework designed to fully exploit the potential
of these overlapping clients. In FedOC, overlapping clients could serve dual
roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models
between neighboring ESs in real time to facilitate model sharing among
different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically
select their initial model for local training based on the edge model delivery
time, which enables indirect data fusion among different regions of ESs. The
overall FedOC workflow proceeds as follows: in every round, each client trains
local model based on the earliest received edge model and transmits to the
respective ESs for model aggregation. Then each ES transmits the aggregated
edge model to neighboring ESs through ROC relaying. Upon receiving the relayed
models, each ES performs a second aggregation and subsequently broadcasts the
updated model to covered clients. The existence of ROCs enables the model of
each ES to be disseminated to the other ESs in a decentralized manner, which
indirectly achieves intercell model and speeding up the training process,
making it well-suited for latency-sensitive edge environments. Extensive
experimental results show remarkable performance gains of our scheme compared
to existing methods.

</details>


### [6] [Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation](https://arxiv.org/abs/2509.19405)
*Tony Chahoud,Lorenzo Mario Amorosa,Riccardo Marini,Luca De Nardis*

Main category: cs.NI

TL;DR: 提出了一种轻量级的移动数据增强框架，通过核密度估计和K近邻方法增强多小区指纹定位性能，特别适用于稀疏采样区域


<details>
  <summary>Details</summary>
Motivation: 解决蜂窝网络中室外定位因测量数据稀疏、异构且现场勘测成本高而面临的挑战

Method: 使用核密度估计（KDE）建模空间分布生成地理一致的合成位置，结合KNN方法增强每个小区的无线电指纹，采用无训练、可解释的模块化架构

Result: 在真实MDT数据集上的实验表明，该框架能持续提升定位性能，在稀疏采样或结构复杂区域效果最显著，但存在区域依赖的饱和效应

Conclusion: 该框架为运营商提供了一种实用、低复杂度的路径，可利用现有移动数据痕迹增强定位服务

Abstract: Accurate outdoor positioning in cellular networks is hindered by sparse,
heterogeneous measurement collections and the high cost of exhaustive site
surveys. This paper introduces a lightweight, modular mobile data augmentation
framework designed to enhance multi-cell fingerprinting-based positioning using
operator-collected minimization of drive test (MDT) records. The proposed
approach decouples spatial and radio-feature synthesis: kernel density
estimation (KDE) models the empirical spatial distribution to generate
geographically coherent synthetic locations, while a k-nearest-neighbor
(KNN)-based block produces augmented per-cell radio fingerprints. The
architecture is intentionally training-free, interpretable, and suitable for
distributed or on-premise operator deployments, supporting privacy-aware
workflows. We both validate each augmentation module independently and assess
its end-to-end impact on fingerprinting-based positioning using a real-world
MDT dataset provided by an Italian mobile network operator across diverse urban
and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation
consistently improves positioning performance, with the largest benefits in
sparsely sampled or structurally complex regions; we also observe
region-dependent saturation effects as augmentation increases. The framework
offers a practical, low-complexity path to enhance operator positioning
services using existing mobile data traces.

</details>


### [7] [Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database](https://arxiv.org/abs/2509.19411)
*Vasilis Andritsoudis,Pavlos Sermpezis,Ilias Dimitriadis,Athena Vakali*

Main category: cs.NI

TL;DR: ChatIYP是一个基于检索增强生成(RAG)的系统，允许用户通过自然语言查询互联网黄页(IYP)，解决了传统查询需要Cypher语言和IYP模式知识的技术门槛问题。


<details>
  <summary>Details</summary>
Motivation: IYP聚合了互联网路由信息，但查询需要掌握Cypher语言和精确的IYP模式，这限制了非专家用户的使用。

Method: 提出了ChatIYP，一个领域特定的检索增强生成(RAG)系统，通过自然语言问题实现IYP查询。

Result: 评估显示在简单查询上表现良好，同时指出了改进方向，并为选择更适合IYP查询AI代理的评估指标提供了见解。

Conclusion: ChatIYP系统有效降低了IYP查询的技术门槛，为领域特定的自然语言查询系统提供了有价值的实践经验和评估方法。

Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources
about Internet routing into a unified, graph-based knowledge base. However,
querying it requires knowledge of the Cypher language and the exact IYP schema,
thus limiting usability for non-experts. In this paper, we propose ChatIYP, a
domain-specific Retrieval-Augmented Generation (RAG) system that enables users
to query IYP through natural language questions. Our evaluation demonstrates
solid performance on simple queries, as well as directions for improvement, and
provides insights for selecting evaluation metrics that are better fit for IYP
querying AI agents.

</details>


### [8] [Where 6G Stands Today: Evolution, Enablers, and Research Gaps](https://arxiv.org/abs/2509.19646)
*Salma Tika,Abdelkrim Haqiq,Essaid Sabir,Elmahdi Driouch*

Main category: cs.NI

TL;DR: 本文对6G移动通信系统进行了全面概述，包括其严格需求、关键技术、应用场景以及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着5G的全球部署，产业界和学术界开始构想6G，以满足日益增长的高级数字社会需求。虽然5G相比LTE有显著进步，但仍难以满足超高可靠性、无缝自动化和无处不在覆盖等所有要求。

Method: 论文从6G的主要严格需求入手，重点分析了太赫兹通信、智能反射表面、大规模MIMO和AI驱动网络等关键使能技术。

Result: 6G将构建一个高度智能、自动化和超可靠的通信系统，能够处理大量连接设备，并支持各种应用场景。

Conclusion: 实现6G承诺需要解决一系列潜在挑战，包括技术实现、标准化和部署等方面的困难。

Abstract: As the fifth-generation (5G) mobile communication system continues its global
deployment, both industry and academia have started conceptualizing the 6th
generation (6G) to address the growing need for a progressively advanced and
digital society. Even while 5G offers considerable advancements over LTE, it
could struggle to be sufficient to meet all of the requirements, including
ultra-high reliability, seamless automation, and ubiquitous coverage. In
response, 6G is supposed to bring out a highly intelligent, automated, and
ultra-reliable communication system that can handle a vast number of connected
devices. This paper offers a comprehensive overview of 6G, beginning with its
main stringent requirements while focusing on key enabling technologies such as
terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO
and AI-driven networking that will shape the 6G networks. Furthermore, the
paper lists various 6G applications and usage scenarios that will benefit from
these advancements. At the end, we outline the potential challenges that must
be addressed to achieve the 6G promises.

</details>


### [9] [RIS-assisted Data Collection and Wireless Power Transfer in Low-altitude Wireless Networks](https://arxiv.org/abs/2509.19651)
*Wenwen Xie,Geng Sun,Jiahui Li,Jiacheng Wang,Yinqiu Liu,Dusit Niyato,Dong In Kim,Shiwen Mao*

Main category: cs.NI

TL;DR: 本文提出了一种基于RIS辅助的无人机数据收集和无线能量传输系统，采用深度强化学习方法优化RIS相位、无人机轨迹等参数，以最小化信息年龄和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决低空无线网络中物联网设备面临的能量限制和信道质量差的问题，确保在偏远地区及时收集数据。

Method: 采用交替优化改进参数化深度Q网络（AO-IPDQN）方法，先优化RIS相位以降低动作空间维度，再处理混合动作空间。

Result: 仿真结果表明，AO-IPDQN方法在各种仿真场景下相对于多种比较方法表现出优异性能。

Conclusion: 所提出的RIS辅助无人机系统能有效提升数据收集效率，降低能耗，并保持数据的新鲜度。

Abstract: Low-altitude wireless networks (LAWNs) have become effective solutions for
collecting data from low-power Internet-of-Things devices (IoTDs) in remote
areas with limited communication infrastructure. However, some outdoor IoTDs
deployed in such areas face both energy constraints and low-channel quality
challenges, making it challenging to ensure timely data collection from these
IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent
surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection
and wireless power transfer system in LAWN. Specifically, IoTDs first harvest
energy from a low-altitude UAV, and then upload their data to the UAV by
applying the time division multiple access (TDMA) protocol, supported by an RIS
to improve the channel quality. To maintain satisfactory data freshness of the
IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age
of information (AoI) and energy consumption of the UAV by jointly optimizing
the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD
scheduling. We propose a deep reinforcement learning (DRL)-based approach,
namely the alternating optimization-improved parameterized deep Q-network
(AO-IPDQN). Specifically, considering that RIS typically contains a large
number of reflecting elements, we first adopt an alternating optimization (AO)
method to optimize the RIS phase shifts to reduce the dimension of the action
space. Then, we propose the improved parameterized deep Q-network (IPDQN)
method to deal with the hybrid action space. Simulation results indicate that
AO-IPDQN approach achieves excellent performance relative to multiple
comparison methods across various simulation scenarios.

</details>


### [10] [Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement](https://arxiv.org/abs/2509.19669)
*Yifan Wang,Minzhao Lyu,Vijay Sivaraman*

Main category: cs.NI

TL;DR: 本文提出了一种通过分析网络流量来实时测量云游戏用户体验的方法，能够识别游戏标题和玩家活动阶段，为网络运营商提供有效的服务质量评估工具。


<details>
  <summary>Details</summary>
Motivation: 随着云游戏市场的发展，网络运营商需要创建可盈利的保障服务来动态配置网络资源。然而，如果没有准确测量云游戏用户体验的方法，他们无法评估资源配置方法的有效性。基本的带宽和帧率指标本身不足以衡量用户体验，需要结合游戏内容和玩家活动背景来解读。

Method: 通过分析网络流量来实时测量云游戏体验，包括游戏标题和玩家活动阶段等上下文因素。该方法能够在游戏启动的前5秒内分类游戏标题，并持续评估玩家活动阶段（活跃、被动或空闲）。

Result: 该方法在托管NVIDIA云游戏服务器的ISP中部署，通过对三个月内数十万个云游戏流会话的分析，揭示了带宽消耗和体验水平对游戏情境的依赖性。

Conclusion: 该方法为网络运营商提供了一种有效的实时云游戏体验测量工具，能够帮助优化网络资源配置，提升服务质量。

Abstract: To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.

</details>


### [11] [SPARQ: An Optimization Framework for the Distribution of AI-Intensive Applications under Non-Linear Delay Constraints](https://arxiv.org/abs/2509.19913)
*Pietro Spadaccino,Paolo Di Lorenzo,Sergio Barbarossa,Antonia M. Tulino,Jaime Llorca*

Main category: cs.NI

TL;DR: 本文提出SPARQ算法，用于边缘云基础设施上分布式AI应用的队列延迟感知编排，解决了现有模型无法捕捉AI密集型工作负载中延迟与资源使用非线性关系的问题。


<details>
  <summary>Details</summary>
Motivation: 下一代实时计算密集型应用（如扩展现实、多用户游戏、自动驾驶）包含异构AI功能，具有多样化资源需求和严格延迟约束。现有模型无法准确捕捉AI工作负载中延迟与资源使用的非线性关系。

Method: 扩展云网络流优化框架，引入保证资源(GR)和共享资源(SR)两种执行模型，使用M/M/1和M/G/1队列动态表示专用和共享资源使用。开发SPARQ迭代近似算法，将非凸优化问题分解为两个凸子问题。

Result: 仿真结果表明，SPARQ不仅更准确地表示系统延迟，而且相比现有最先进方法显著提高了资源效率和整体成本延迟权衡。

Conclusion: SPARQ算法为分布式AI应用提供了更真实的延迟建模和更优的资源编排方案，在边缘云环境中实现了更好的性能表现。

Abstract: Next-generation real-time compute-intensive applications, such as extended
reality, multi-user gaming, and autonomous transportation, are increasingly
composed of heterogeneous AI-intensive functions with diverse resource
requirements and stringent latency constraints. While recent advances have
enabled very efficient algorithms for joint service placement, routing, and
resource allocation for increasingly complex applications, current models fail
to capture the non-linear relationship between delay and resource usage that
becomes especially relevant in AI-intensive workloads. In this paper, we extend
the cloud network flow optimization framework to support queuing-delay-aware
orchestration of distributed AI applications over edge-cloud infrastructures.
We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource
(SR), that more accurately capture how computation and communication delays
emerge from system-level resource constraints. These models incorporate M/M/1
and M/G/1 queue dynamics to represent dedicated and shared resource usage,
respectively. The resulting optimization problem is non-convex due to the
non-linear delay terms. To overcome this, we develop SPARQ, an iterative
approximation algorithm that decomposes the problem into two convex
sub-problems, enabling joint optimization of service placement, routing, and
resource allocation under nonlinear delay constraints. Simulation results
demonstrate that the SPARQ not only offers a more faithful representation of
system delays, but also substantially improves resource efficiency and the
overall cost-delay tradeoff compared to existing state-of-the-art methods.

</details>


### [12] [A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network](https://arxiv.org/abs/2509.20068)
*Bilal Dalgic,Betul Sen,Muge Erel-Ozcevik*

Main category: cs.NI

TL;DR: 提出了一种基于SDN和数字孪生的新型IIoT短时异常检测框架SD-TWIN，通过时间感知特征标记和机器学习模型评估，发现GPU加速的LightGBM模型在实时部署中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前文献缺乏SDN-based数字孪生的实现细节和针对IIoT威胁的时间感知智能模型训练方法，需要解决IIoT环境中的安全监控和动态控制需求。

Method: 集成软件定义网络(SDN)和数字孪生(DT)范式，使用综合数据集进行时间感知特征标记，评估多种机器学习模型，提出SD-TWIN异常检测算法。

Result: 在实时SD-TWIN部署中，GPU加速的LightGBM模型表现特别有效，实现了高召回率和强分类性能的平衡。

Conclusion: 基于SDN的数字孪生框架能够有效实现IIoT环境的动态安全监控，GPU加速的LightGBM模型是短时异常检测的优选方案。

Abstract: Secure monitoring and dynamic control in an IIoT environment are major
requirements for current development goals. We believe that dynamic, secure
monitoring of the IIoT environment can be achieved through integration with the
Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current
literature lacks implementation details for SDN-based DT and time-aware
intelligent model training for short-term anomaly detection against IIoT
threats. Therefore, we have proposed a novel framework for short-term anomaly
detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware
labeling of features, and a comprehensive evaluation of various machine
learning models, we propose a novel SD-TWIN-based anomaly detection algorithm.
According to the performance of a new real-time SD-TWIN deployment, the GPU-
accelerated LightGBM model is particularly effective, achieving a balance of
high recall and strong classification performance.

</details>


### [13] [Can LLMs Forecast Internet Traffic from Social Media?](https://arxiv.org/abs/2509.20123)
*Jonatan Langlet,Mariano Scazzariello,Flavio Luciani,Marta Burocchi,Dejan Kostić,Marco Chiesa*

Main category: cs.NI

TL;DR: 该论文提出了一种利用公共讨论信号（如新闻标题、论坛和社交媒体）来预测互联网流量异常峰值的社会技术系统方法。


<details>
  <summary>Details</summary>
Motivation: 传统流量预测系统仅依赖常规流量模式，无法预测由社会事件（如名人去世、软件发布、体育赛事）引发的突发需求高峰，这些异常事件会淹没对等点和内容分发网络。

Method: 开发了一个概念验证系统，该系统自主抓取在线讨论，推断现实世界事件，对事件进行语义聚类和丰富，并将其与主要互联网交换点的流量测量数据进行关联分析。

Result: 原型系统在抓取适量在线讨论后，成功预测了56-92%的社会驱动流量峰值。

Conclusion: 这种方法为跨领域预测、调度、需求预期和社会知情决策开辟了新的研究机会。

Abstract: Societal events shape the Internet's behavior. The death of a prominent
public figure, a software launch, or a major sports match can trigger sudden
demand surges that overwhelm peering points and content delivery networks.
Although these events fall outside regular traffic patterns, forecasting
systems still rely solely on those patterns and therefore miss these critical
anomalies.
  Thus, we argue for socio-technical systems that supplement technical
measurements with an active understanding of the underlying drivers, including
how events and collective behavior shape digital demands. We propose traffic
forecasting using signals from public discourse, such as headlines, forums, and
social media, as early demand indicators.
  To validate our intuition, we present a proof-of-concept system that
autonomously scrapes online discussions, infers real-world events, clusters and
enriches them semantically, and correlates them with traffic measurements at a
major Internet Exchange Point. This prototype predicted between 56-92% of
society-driven traffic spikes after scraping a moderate amount of online
discussions.
  We believe this approach opens new research opportunities in cross-domain
forecasting, scheduling, demand anticipation, and society-informed decision
making.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 用户模拟是克服AGI发展瓶颈的关键催化剂，通过创建模拟人类交互的计算代理来提供可扩展的评估环境和交互学习数据


<details>
  <summary>Details</summary>
Motivation: AGI发展面临复杂交互系统评估和大量交互数据获取的瓶颈，需要新的方法来加速AGI开发进程

Method: 提出用户模拟技术，即创建计算代理来模拟人类与AI系统的交互，为AGI提供可扩展的评估环境和数据生成能力

Result: 论证了用户模拟技术对AGI发展的关键作用，并提出了未来研究议程

Conclusion: 用户模拟技术与智能任务代理研究具有深度协同效应，必须携手并进，这是推动AGI发展的关键路径

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [15] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: 本文提出了评估感知强化学习（EvA-RL）框架，在训练策略时同时考虑最小化评估误差，使策略既高效又易于评估。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在策略评估时面临高方差（数据有限、长时程任务）和高偏差（支持度不均、环境模型不准确）的问题，这些挑战源于训练时未显式考虑评估需求。

Method: 设计EvA-RL框架，策略训练目标同时最大化期望回报和最小化评估误差；扩展方法共同学习评估条件状态价值预测器。

Result: 实验表明EvA-RL能显著降低评估误差，同时保持有竞争力的回报性能，缓解了评估准确性与策略性能之间的权衡。

Conclusion: 这项工作为将可靠评估作为训练首要原则的新RL方法奠定了基础，开辟了评估感知RL的新研究方向。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [16] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 本文分析了在固定计算预算下，LLM自一致性估计器及其权衡，建议将预算大致按平方根比例分配给提示采样和重复调用。


<details>
  <summary>Details</summary>
Motivation: 系统通常重复相同提示给大语言模型并聚合响应以提高可靠性，需要分析在固定计算预算下的最优分配策略。

Method: 在固定计算预算B=mn的条件下，分析自一致性估计器的性能，其中m是任务分布中采样的提示数量，n是每个提示的重复LLM调用次数。

Result: 分析结果表明，最优的分配策略是m和n大致与B的平方根成比例，即m,n∝√B。

Conclusion: 在固定计算预算下，将预算大致按平方根比例分配给提示采样和重复调用可以获得更好的自一致性估计效果。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [17] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 该研究提出了计算认知负荷理论，通过ICE基准测试发现上下文饱和和注意力残留会显著降低LLMs在多跳推理任务中的性能，表明认知负荷是AI系统推理失败的关键因素


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在静态基准测试中表现出色，但在动态信息丰富环境中的脆弱性暴露了性能差距，需要理解认知负荷对推理能力的限制机制

Method: 引入计算认知负荷理论，设计ICE基准测试系统操纵上下文饱和和注意力残留因素，在200个多跳推理问题上进行10次重复测试，评估5个指令调优模型

Result: 小型开源模型在所有条件下准确率为0%，Gemini-2.0-Flash-001在控制条件下达到85%准确率，但在上下文饱和条件下性能显著下降

Conclusion: 认知负荷是推理失败的关键因素，动态的认知感知压力测试对于评估先进AI系统的真实韧性和安全性至关重要

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [18] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: 提出StepEval评估框架，使用视觉语言模型自动评估机器人操作任务中的子目标完成情况，倡导子目标级别的性能报告标准化


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习论文仅报告单一二进制成功率，无法显示多步骤操作任务中各子目标的完成情况，掩盖了部分能力

Method: 设计StepEval框架，利用VLMs作为自动评判器，从记录图像或视频中评估子目标结果，支持单视图或多视图输入，保持模型无关性

Result: 提出子目标成功率向量作为主要评估指标，同时考虑延迟和成本估计等辅助指标，为框架优化提供诊断信息

Conclusion: StepEval是一个轻量级、可扩展的开源项目蓝图，旨在推动子目标级别评估成为标准化和可复现的实践

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [19] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 研究应用小型语言模型（<100亿参数）通过智能体框架解决基因组问答中的幻觉问题和计算成本挑战，实现与大型模型相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决基因组问答中大型语言模型存在的幻觉问题和计算成本高昂的挑战，探索小型模型在专业领域的应用潜力。

Method: 开发Nano Bio-Agent（NBA）框架，整合任务分解、工具编排和API访问（NCBI、AlphaGenome等系统），将小型语言模型与专业工具结合。

Result: 在GeneTuring基准测试中达到98%准确率，3-100亿参数的小型模型实现85-97%准确率，显著降低计算资源需求。

Conclusion: 小型语言模型结合智能体框架在基因组学领域展现出巨大潜力，可在保持高性能的同时实现效率提升、成本节约和工具民主化。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [20] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 本文提出了一个将AI评估视为推理的框架，强调基准测试分数应该基于能力理论进行推断，而不是简单的测量。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估存在可靠性问题，基准测试分数往往被当作简单测量，但实际上应该被视为基于能力理论的推断。

Method: 提出了评估即推理的框架：从能力理论出发，推导出估计能力的方法；针对敏感性挑战，引入了考虑不确定性的方法，包括显著降低样本复杂度的自适应算法。

Result: 该框架为通过基准测试获得更可靠、可信的AI能力估计奠定了基础。

Conclusion: 评估应该被明确视为推理过程，需要从能力理论出发来设计评估方法，这样才能获得真正反映模型性能的可靠估计。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [21] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: SteinerSQL是一个统一框架，将复杂的Text-to-SQL查询中的数学推理和模式导航挑战整合为图优化问题，在LogicCat和Spider2.0-Lite基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立处理复杂Text-to-SQL查询中的数学推理和模式导航挑战，导致推理过程碎片化，影响逻辑和结构正确性。

Method: SteinerSQL采用三阶段方法：数学分解识别所需表格（终端）、通过Steiner树问题构建最优推理支架、多级验证确保正确性。

Result: 在LogicCat和Spider2.0-Lite基准上分别达到36.10%和40.04%的执行准确率，使用Gemini-2.5-Pro模型。

Conclusion: SteinerSQL为Text-to-SQL提供了新的统一范式，为复杂推理任务开辟了更稳健和原则性的解决方案路径。

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [22] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的成对解释验证器，通过生成校准的置信度分数和自然语言推理来改进推理模型的测试时计算策略。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型的自评估能力较差，限制了测试时计算策略的有效性，需要更好的验证机制来提升准确性和效率。

Method: 使用强化学习（GRPO）训练成对解释验证器，生成校准的置信度分数和相关的自然语言推理。

Result: 验证器显著提升了最佳选择（best-of-n）和自我反思等测试时策略的准确性和效率，特别是在识别具有挑战性的失败模式方面表现优异。

Conclusion: 该方法在标准方法（如多数投票）失败的情况下仍能成功识别困难案例，为推理模型的测试时优化提供了有效解决方案。

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [23] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL是一个统一的框架，通过标准化gym环境和模拟用户来训练和评估以用户为中心的智能体能力，研究发现奖励设计和用户模拟选择对开发稳健的用户中心智能体模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练智能体模型方面显示出潜力，但智能体的最终价值在于协助用户的能力，而用户交互的多样性和动态性带来了挑战。

Method: 提出UserRL框架，系统性地变化回合级奖励分配和轨迹级分数计算，分析不同公式在GRPO算法下对学习的影响，在Qwen3模型上进行实验。

Result: 三个关键发现：(i)SFT冷启动对解锁初始交互能力和实现持续RL改进至关重要；(ii)有意识的轨迹评分能产生更高效的多轮交互；(iii)虽然更强的模拟用户有助于训练，但开源模拟器是成本效益高且可迁移的选择。

Conclusion: 精心设计奖励塑造和用户模拟选择与模型规模同等重要，UserRL为开发稳健的用户中心智能体模型提供了实用途径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [24] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CEPO是一种优化的推理工作流程，通过减少模型冗余和改善指令跟随，使小型开源模型能够超越比它们大数倍的模型。


<details>
  <summary>Details</summary>
Motivation: 现代LLM推理依赖于大量的测试时计算，但模型冗余和指令跟随不佳导致计算效率低下，需要优化能力与成本之间的权衡。

Method: 引入CEPO优化推理工作流程，通过协同设计编排框架与底层模型能力，减少模型冗余和改善指令跟随效率。

Result: 小型开源模型使用CEPO后能够超越比它们大数倍的模型性能。

Conclusion: 这项工作展示了通过协同设计编排框架与模型能力，为中小型模型解锁强大推理能力的清晰路径，并将开源此工作流程以促进进一步研究。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [25] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 提出了一种在低代码/无代码环境中集成元认知层的架构模式，通过监控主代理并预测任务失败来主动发起人工交接，提高系统可靠性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 自主代理的非确定性特性在LCNC环境中导致可靠性问题，如陷入循环、生成错误输出或不可恢复故障，造成用户挫败感和信任缺失。

Method: 引入次级元认知层监控主LCNC代理，基于延迟过长或重复动作等触发器预测任务失败，并主动发起人工交接，提供代理的"思考过程"摘要和失败原因解释。

Result: 原型系统的实证分析表明该方法显著提高了整体任务成功率，但带来了显著的计算开销增加。

Conclusion: 将人工交接重新定义为增强系统弹性、改善用户体验和建立信任的核心设计特征，而非失败的承认，并讨论了该方法的实践和伦理影响及未来研究方向。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [26] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 本文提出了一种基于对数障碍函数的新方法，将MDP的线性规划问题转化为无约束优化问题，使得可以通过梯度下降获得近似解。


<details>
  <summary>Details</summary>
Motivation: 线性规划方法在解决马尔可夫决策问题时使用较少，主要因为其涉及不等式约束优化问题，比基于贝尔曼方程的方法更难有效求解。本文旨在为LP-based MDPs建立更有效实用的理论基础。

Method: 利用不等式约束优化中广泛使用的对数障碍函数，将MDP的LP公式转化为无约束优化问题，从而可以通过梯度下降轻松获得近似解。

Result: 该方法虽然看似简单，但此前缺乏系统的理论解释。本文旨在填补这一空白。

Conclusion: 通过对数障碍函数转换LP公式，为LP-based MDPs提供了一种更实用有效的求解方法，并建立了相应的理论基础。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [27] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD是一个三阶段框架，结合行为对齐和监督潜空间控制，实现可解释的安全导向，在保持模型实用性的同时提高安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在表示层面平衡全面安全性和细粒度可控性，需要在保持LLM实用性的同时实现鲁棒的安全对齐。

Method: 三阶段方法：1）在包含对抗提示和良性查询的有理据数据集上微调LLM；2）训练结构化变分自编码器（VAE）学习解耦潜表示；3）通过潜维度操作实现选择性拒绝行为。

Result: 在Qwen3-8B上实验显示安全可控性和响应可解释性显著提升，在Mistral-7B上的跨架构验证证实了方法的通用性。

Conclusion: 结构化表示级干预为构建更安全实用的LLM系统提供了有前景的途径。

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [28] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA是一个混合隐私保护框架，专门用于企业合同的安全问答，结合本地和云端LLM，通过三阶段流程保护敏感信息。


<details>
  <summary>Details</summary>
Motivation: 企业越来越多地将云端LLM集成到法律文档工作流中，保护敏感合同信息（如PII和商业敏感条款）成为关键挑战。

Method: 三阶段框架：1）语义查询分解和查询感知文档块检索；2）通过结构化一对多映射方案匿名化敏感实体；3）云端LLM生成匿名化响应，本地使用会话一致的多对一反向映射准确重建原始答案。

Result: 在CUAD-QA语料库（85k问答对）上的实证评估显示，CON-QA有效维护隐私和实用性，保持答案质量，保护法律条款语义保真度，显著降低隐私风险。

Conclusion: CON-QA展示了在企业级合同文档中实现安全问答的实践适用性。

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [29] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本文全面综述了具身人工智能（Embodied AI）领域，重点探讨了大型语言模型（LLMs）和世界模型（WMs）在实现人工通用智能（AGI）中的关键作用，并提出了联合MLLM-WM架构的未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 具身AI作为实现AGI的重要范式，近年来因LLMs和WMs的突破性进展而受到广泛关注。本文旨在系统梳理该领域从基础到前沿的研究进展，为构建能够在物理世界中执行复杂任务的智能系统提供理论指导。

Method: 采用文献综述方法，首先介绍具身AI的历史、关键技术、核心组件和硬件系统，然后重点分析LLMs/MLLMs和WMs在具身AI中的具体应用，最后提出联合MLLM-WM架构的创新思路。

Result: 系统梳理了具身AI的发展脉络，明确了LLMs在语义推理和任务分解、WMs在世界表示和物理规律预测方面的互补优势，为构建更强大的具身智能系统奠定了理论基础。

Conclusion: 联合MLLM-WM驱动的具身AI架构是实现复杂物理世界任务的关键方向，未来需要在多模态融合、物理规律建模等方面进行深入研究，以推动AGI的发展。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [30] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: 提出了一种多智能体临床诊断（MACD）框架，使LLM能够通过总结、提炼和应用诊断见解的多智能体管道自我学习临床知识，显著提高了诊断准确性，并在某些情况下达到或超过人类医生的水平。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理复杂真实世界临床诊断时面临挑战，传统提示方法无法积累可重用的临床经验，需要一种能够模拟医生通过经验发展专业知识的方法。

Method: MACD框架采用多智能体管道，包括总结、提炼和应用诊断见解的智能体，并扩展到MACD-人类协作工作流，其中多个基于LLM的诊断智能体进行迭代咨询，由评估智能体和人类监督支持。

Result: 在4,390个真实世界患者病例上评估，MACD显著提高了主要诊断准确性，比既定临床指南提升高达22.3%，在某些数据子集上达到或超过人类医生水平（比仅医生诊断提升高达16%）。MACD-人类工作流比仅医生诊断提升18.6%。

Conclusion: 这项工作为LLM辅助诊断提供了一个可扩展的自我学习范式，弥合了LLM内在知识与真实世界临床实践之间的差距。

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [31] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 该研究建立了线虫信息素介导的聚集行为与强化学习之间的理论等价性，证明信息素信号可作为分布式奖励机制，并探索了在动态环境中通过引入探索性个体来恢复群体适应性的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解群体智能如何通过去中心化交互实现集体问题解决，特别是探索信息素介导的聚集行为与强化学习算法的数学等价性，为可编程生物系统在动态环境中的弹性决策提供理论基础。

Method: 方法包括：1）建立线虫群体觅食任务的数学模型；2）证明信息素动态与交叉学习更新算法的数学等价性；3）使用文献数据进行实验验证；4）在多臂老虎机场景中进行计算实验；5）引入对信息素不敏感的探索性个体来测试群体适应性。

Result: 结果显示：1）模型能准确复制静态条件下的线虫觅食模式；2）动态环境中持久的信息素轨迹会阻碍群体适应；3）引入少数探索性个体可恢复集体可塑性，实现快速任务切换；4）行为异质性平衡了探索-利用权衡。

Conclusion: 结论表明信息素系统固有地编码了分布式强化学习过程，环境信号作为集体信用分配的外部记忆。这项工作通过结合合成生物学与群体机器人学，推进了在波动环境中具有弹性决策能力的可编程生命系统的发展。

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [32] [Steerable Adversarial Scenario Generation through Test-Time Preference Alignment](https://arxiv.org/abs/2509.20102)
*Tong Nie,Yuewen Mei,Yihong Tang,Junlin He,Jie Sun,Haotian Shi,Wei Ma,Jian Sun*

Main category: cs.AI

TL;DR: 本文提出了一种可调控的对抗性场景生成框架SAGE，通过多目标偏好对齐方法实现对抗性和真实性的动态权衡，无需重新训练即可在推理时灵活控制场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性场景生成方法通常只能在单一固定的对抗性与真实性权衡下工作，缺乏灵活性和效率，无法满足多样化的训练和测试需求。

Method: 提出分层组偏好优化方法，将硬可行性约束与软偏好解耦，通过训练两个偏好相反的专家模型，在推理时通过线性插值权重构建连续策略谱。

Result: 实验表明SAGE不仅能生成对抗性与真实性平衡更好的场景，还能更有效地进行驾驶策略的闭环训练。

Conclusion: SAGE框架为自动驾驶系统的安全评估提供了一种高效灵活的场景生成方法，通过理论证明和实验验证了其有效性。

Abstract: Adversarial scenario generation is a cost-effective approach for safety
assessment of autonomous driving systems. However, existing methods are often
constrained to a single, fixed trade-off between competing objectives such as
adversariality and realism. This yields behavior-specific models that cannot be
steered at inference time, lacking the efficiency and flexibility to generate
tailored scenarios for diverse training and testing requirements. In view of
this, we reframe the task of adversarial scenario generation as a
multi-objective preference alignment problem and introduce a new framework
named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator
(SAGE). SAGE enables fine-grained test-time control over the trade-off between
adversariality and realism without any retraining. We first propose
hierarchical group-based preference optimization, a data-efficient offline
alignment method that learns to balance competing objectives by decoupling hard
feasibility constraints from soft preferences. Instead of training a fixed
model, SAGE fine-tunes two experts on opposing preferences and constructs a
continuous spectrum of policies at inference time by linearly interpolating
their weights. We provide theoretical justification for this framework through
the lens of linear mode connectivity. Extensive experiments demonstrate that
SAGE not only generates scenarios with a superior balance of adversariality and
realism but also enables more effective closed-loop training of driving
policies. Project page: https://tongnie.github.io/SAGE/.

</details>


### [33] [PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs](https://arxiv.org/abs/2509.20105)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.AI

TL;DR: 本文提出了一种量子启发的保真度奖励方法，通过投影纠缠对态(PEPS)结合近端策略优化，改善大语言模型在多步推理中的连贯性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要结构化逻辑流的任务中，往往难以保持连贯的多步推理轨迹。现有方法如直接监督或对比目标存在局限性，需要新的方法来增强推理轨迹的全局一致性。

Method: 采用量子启发的保真度奖励机制，基于投影纠缠对态(PEPS)构建结构一致性指导，并将其集成到近端策略优化中，以增强推理轨迹的连贯性。

Result: 在GSM8K、StrategyQA和EntailmentBank等多样化数据集上的评估表明，该方法在算术、直觉和蕴含推理任务中，相比监督、对比和预训练基线方法都有显著提升。

Conclusion: 量子启发的保真度方法为改善大语言模型推理轨迹的连贯性提供了有效基础，展示了量子计算概念在自然语言处理中的潜在应用价值。

Abstract: Large Language Models (LLMs) often struggle with maintaining coherent
multi-step reasoning traces, particularly in tasks that require a structured
logical flow. This work introduces a quantum-inspired approach to address the
challenge by incorporating a fidelity-based reward derived from Projected
Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior
approaches that use direct supervision or contrastive objectives, the proposed
method guides learning through structural consistency, offering a novel
approach to enforce global coherence in generated reasoning traces. The
proposed framework is evaluated using multiple coherence-determining metrics on
diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning
arithmetic, intuitive, and entailment-based reasoning. Results show that the
proposed quantum-inspired approach offers significant improvements over
supervised, contrastive, and pretrained baseline approaches, highlighting the
effectiveness of quantum-inspired fidelity as a foundation to improve reasoning
trace coherence in LLMs.

</details>


### [34] [Formal Verification of Minimax Algorithms](https://arxiv.org/abs/2509.20138)
*Wieger Wesselink,Kees Huizing,Huub van de Wetering*

Main category: cs.AI

TL;DR: 使用Dafny验证系统对多种极小极大搜索算法进行形式化验证，包括带alpha-beta剪枝和置换表的变体。针对带置换表的深度受限搜索，提出了基于见证的正确性标准，并应用于两种代表性算法。


<details>
  <summary>Details</summary>
Motivation: 极小极大搜索算法在游戏AI和决策系统中广泛应用，但其正确性验证具有挑战性，特别是涉及剪枝和缓存优化时。需要形式化方法来确保这些复杂算法的正确性。

Method: 采用Dafny验证系统进行形式化验证，针对带alpha-beta剪枝和置换表的极小极大算法。为深度受限搜索引入基于见证的正确性标准，并在两种代表性算法上应用验证。

Result: 成功验证了多种极小极大搜索算法的正确性，包括带优化技术的变体。所有验证工件（包括证明和Python实现）已公开可用。

Conclusion: 提出的基于见证的正确性标准为带置换表的深度受限搜索提供了有效的验证框架，证明了形式化验证在复杂搜索算法正确性保证中的可行性。

Abstract: Using the Dafny verification system, we formally verify a range of minimax
search algorithms, including variations with alpha-beta pruning and
transposition tables. For depth-limited search with transposition tables, we
introduce a witness-based correctness criterion and apply it to two
representative algorithms. All verification artifacts, including proofs and
Python implementations, are publicly available.

</details>


### [35] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA是一个分布式编排框架，通过版本化能力向量将静态多智能体协调转变为动态、能力驱动的协作，在HealthBench上相比单模型基线实现了13倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统多智能体系统中静态协调的局限性，实现基于智能体能力的动态协作，释放异构AI智能体联邦的集体智能。

Method: 结合三个关键创新：语义路由（通过分片HNSW索引匹配任务与智能体）、动态任务分解（兼容智能体通过共识合并将复杂任务分解为DAG子任务）、智能聚类（将处理相似子任务的智能体分组进行k轮优化）。

Result: 在HealthBench基准测试中，相比单模型基线实现了13倍性能提升，特别是在需要多视角的复杂推理任务中，聚类增强的协作效果显著。

Conclusion: 基于MQTT发布-订阅语义的FoA框架通过语义编排和结构化协作，能够有效解锁异构AI智能体联邦的集体智能，同时保持水平扩展性和一致性能。

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [36] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 该研究通过真实硬件部署探索协作式车道变换预测，分享了实施和测试中的实践经验，重点讨论了实际挑战和限制。


<details>
  <summary>Details</summary>
Motivation: 现有车道变换预测研究多在仿真环境或预录数据集上进行，依赖简化的假设，而真实世界部署较少且实践经验不足。

Method: 在混合交通环境中进行真实硬件部署，实施协作式车道变换预测系统。

Result: 识别了系统实施中的瓶颈、可靠性问题和操作限制等实际挑战。

Conclusion: 通过记录这些实践经验，为类似系统的开发提供指导。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


### [37] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: 提出基于大语言模型的智能体框架，用于辅助CT扫描协议的自然语言配置管理，提高工作流程效率


<details>
  <summary>Details</summary>
Motivation: CT扫描协议管理（包括调整采集参数、配置重建算法等）耗时且需要专业知识，同时放射学领域面临熟练劳动力短缺问题

Method: 结合上下文学习、指令跟随和结构化工具调用能力的LLM智能体框架，识别相关协议元素并应用准确修改

Result: 实验结果表明该智能体能有效检索协议组件、生成设备兼容的协议定义文件，并忠实执行用户请求

Conclusion: 尽管面临设备API不统一导致的语法语义有效性限制，以及模糊复杂请求的挑战，但研究展示了LLM智能体支持CT成像扫描协议管理的可行路径

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [38] [Analyzing α-divergence in Gaussian Rate-Distortion-Perception Theory](https://arxiv.org/abs/2509.19572)
*Martha V. Sourla,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.IT

TL;DR: 本文研究了高斯源在均方误差失真和α散度感知度量下的信息率失真感知函数估计问题，提出了参数化解并建立了数值求解方法。


<details>
  <summary>Details</summary>
Motivation: 研究信息率失真感知函数对于目标导向的有损压缩和语义信息重建具有重要意义，特别是在高斯源和特定感知度量下的理论分析。

Method: 假设联合高斯RDPF形成凸优化问题，推导出参数化上界解，通过求解α次简化指数多项式的根来获得最优参数，并使用二分法进行数值计算。

Result: 建立了RDPF的参数化解析解，证明了最优参数求解等价于寻找特定多项式的根，并确定了根的分布区间以便数值求解。

Conclusion: 数值结果验证了理论分析的正确性，并与现有研究成果建立了联系，为高斯源在α散度感知度量下的RDPF估计提供了有效方法。

Abstract: The problem of estimating the information rate distortion perception function
(RDPF), which is a relevant information-theoretic quantity in goal-oriented
lossy compression and semantic information reconstruction, is investigated
here. Specifically, we study the RDPF tradeoff for Gaussian sources subject to
a mean-squared error (MSE) distortion and a perception measure that belongs to
the family of {\alpha} divergences. Assuming a jointly Gaussian RDPF, which
forms a convex optimization problem, we characterize an upper bound for which
we find a parametric solution. We show that evaluating the optimal parameters
of this parametric solution is equivalent to finding the roots of a reduced
exponential polynomial of degree {\alpha}. Additionally, we determine which
disjoint sets contain each root, which enables us to evaluate them numerically
using the well-known bisection method. Finally, we validate our analytical
findings with numerical results and establish connections with existing
results.

</details>


### [39] [Efficient $\varepsilon$-approximate minimum-entropy couplings](https://arxiv.org/abs/2509.19598)
*Spencer Compton*

Main category: cs.IT

TL;DR: 本文提出了一种针对最小熵耦合问题的多项式时间近似方案（PTAS），能够在常数m的情况下，以n^O(poly(1/ε)·exp(m))的运行时间实现H(ALG) ≤ H(OPT) + ε的近似保证。


<details>
  <summary>Details</summary>
Motivation: 最小熵耦合问题是NP难问题，之前最好的多项式时间算法只能达到H(ALG) ≤ H(OPT) + c的近似保证（c≈0.53 for m=2, c≈1.22 for general m）。一个主要开放问题是该问题是否APX难，或者是否存在PTAS。

Method: 设计了一种算法，通过构造耦合来最小化联合分布的熵，同时保持边际分布与输入分布一致。算法运行时间为n^O(poly(1/ε)·exp(m))。

Result: 证明了对于常数m，最小熵耦合问题存在PTAS，即可以在多项式时间内获得任意精度的近似解。

Conclusion: 这项工作解决了最小熵耦合问题的一个重要开放问题，表明对于固定数量的分布（常数m），该问题存在有效的近似方案，为后续研究提供了新的方向。

Abstract: Given $m \ge 2$ discrete probability distributions over $n$ states each, the
minimum-entropy coupling is the minimum-entropy joint distribution whose
marginals are the same as the input distributions. Computing the
minimum-entropy coupling is NP-hard, but there has been significant progress in
designing approximation algorithms; prior to this work, the best known
polynomial-time algorithms attain guarantees of the form $H(\operatorname{ALG})
\le H(\operatorname{OPT}) + c$, where $c \approx 0.53$ for $m=2$, and $c
\approx 1.22$ for general $m$ [CKQGK '23].
  A main open question is whether this task is APX-hard, or whether there
exists a polynomial-time approximation scheme (PTAS). In this work, we design
an algorithm that produces a coupling with entropy $H(\operatorname{ALG}) \le
H(\operatorname{OPT}) + \varepsilon$ in running time
$n^{O(\operatorname{poly}(1/\varepsilon) \cdot \operatorname{exp}(m) )}$:
showing a PTAS exists for constant $m$.

</details>


### [40] [Agentic AI for Low-Altitude Semantic Wireless Networks: An Energy Efficient Design](https://arxiv.org/abs/2509.19791)
*Zhouxiang Zhao,Ran Yi,Yihan Cang,Boyang Jin,Zhaohui Yang,Mingzhe Chen,Chongwen Huang,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于智能AI的低空语义无线网络框架，通过优化无人机位置、语义压缩比、传输功率等关键参数，解决无人机辅助自主系统的能效问题，显著降低总能耗。


<details>
  <summary>Details</summary>
Motivation: 解决无人机辅助自主系统中的能源效率问题，提高任务续航能力，通过智能协调感知-通信-决策-控制工作流程来优化系统能耗。

Method: 构建系统级能耗最小化问题，综合考虑无人机位置、语义压缩比、传输功率和AI推理任务卸载等关键变量，在严格延迟和服务质量约束下，开发低复杂度算法通过二维搜索获得全局最优解。

Result: 仿真结果表明，所提出的设计相比传统基线方法能显著降低总能耗，验证了方案的有效性。

Conclusion: 该框架为无人机辅助自主系统提供了一种高效的能源管理解决方案，通过智能优化关键操作参数实现了显著的能耗降低。

Abstract: This letter addresses the energy efficiency issue in unmanned aerial vehicle
(UAV)-assisted autonomous systems. We propose a framework for an agentic
artificial intelligence (AI)-powered low-altitude semantic wireless network,
that intelligently orchestrates a sense-communicate-decide-control workflow. A
system-wide energy consumption minimization problem is formulated to enhance
mission endurance. This problem holistically optimizes key operational
variables, including UAV's location, semantic compression ratio, transmit power
of the UAV and a mobile base station, and binary decision for AI inference task
offloading, under stringent latency and quality-of-service constraints. To
tackle the formulated mixed-integer non-convex problem, we develop a
low-complexity algorithm which can obtain the globally optimal solution with
two-dimensional search. Simulation results validate the effectiveness of our
proposed design, demonstrating significant reductions in total energy
consumption compared to conventional baseline approaches.

</details>


### [41] [Understanding the ratio of the partition sum to its Bethe approximation via double covers](https://arxiv.org/abs/2509.19910)
*Pascal O. Vontobel*

Main category: cs.IT

TL;DR: 该论文研究了图模型中配分函数与其Bethe近似之间的比率关系，特别是发现配分函数与Bethe近似的比率通常接近配分函数与二阶Bethe近似比率的平方。


<details>
  <summary>Details</summary>
Motivation: 观察到一个现象：配分函数与Bethe近似的比率通常接近配分函数与二阶Bethe近似比率的平方，这一关系对于分析配分函数具有重要意义，因为二阶Bethe近似通常更容易分析和量化。

Method: 对观察到的比率关系进行理论论证，并针对两类对数超模图模型分析这些比率。

Result: 为观察到的比率关系提供了理论依据，并对两类对数超模图模型的比率进行了分析。

Conclusion: 该研究为图模型中配分函数近似比率的分析提供了理论基础，特别是对于对数超模图模型，验证了配分函数与Bethe近似比率和二阶Bethe近似比率之间的平方关系。

Abstract: For various classes of graphical models it has been observed that the ratio
of the partition sum to its Bethe approximation is often close to being the
square of the ratio of the partition sum to its degree-2 Bethe approximation.
This is of relevance because the latter ratio can often better be analyzed
and/or quantified than the former ratio. In this paper, we give some
justifications for the observed relationship between these two ratios and then
analyze these ratios for two classes of log-supermodular graphical models.

</details>


### [42] [Constrained Higher-Order Binary Optimization for Wireless Communications Systems Using Ising Machines](https://arxiv.org/abs/2509.20092)
*Gan Zheng,Ioannis Krikidis*

Main category: cs.IT

TL;DR: 本文提出了一种基于Ising机器的迭代算法，用于解决无线通信系统中具有不等式约束的大规模高阶二进制优化问题，通过增广拉格朗日方法和泰勒展开将高阶多项式近似为二次型。


<details>
  <summary>Details</summary>
Motivation: 传统QUBO方法在无线通信资源优化中应用受限，因为实际问题通常包含高阶多项式项和严格不等式约束，需要开发能够处理这些复杂约束的新方法。

Method: 采用增广拉格朗日方法处理约束，使用泰勒展开将高阶多项式近似为二次型，在每个迭代步骤中求解单个QUBO问题，无需辅助变量。

Result: 在同时无线信息和能量传输系统的相位优化案例研究中，算法表现出满意性能，优于启发式基准方案。

Conclusion: 所提算法成功克服了QUBO在无线通信应用中的瓶颈，为利用Ising机器解决复杂资源优化问题提供了有效途径。

Abstract: This paper develops an algorithmic solution using Ising machines to solve
large-scale higher-order binary optimization (HOBO) problems with inequality
constraints for resource optimization in wireless communications systems.
Quadratic unconstrained binary optimization (QUBO) aims to solve a special
category of these problems widely encountered in engineering and science. To
solve QUBO instances, specialized Ising machines have been designed, while
sophisticated quantum annealing algorithm and quantum-inspired classical
heuristics have been developed. However, the application of QUBO in wireless
communications has limited practical interest mainly due to the complexity of
resource optimization problems which are often characterized by high-order
polynomial terms and strict inequality constraints. To overcome these
bottlenecks and take advantage of recent advancements in Ising machines, in
this paper, we propose an iterative algorithmic solution to solve HOBO
problems, which is based on the augmented Lagrangian method to handle
constraints. Specifically, Taylor expansion is employed to approximate
higher-order polynomials to quadratic ones in the augmented Lagrangian
function, which enables the solution of a single QUBO problem at each iteration
without auxiliary variables. As an illustrative case study, we consider the
problem of phase optimization in a simultaneous wireless information and power
transfer system, where a reconfigurable intelligent surface with 1-bit phase
resolution is used to facilitate information/energy transfer. Simulation
results verify that the proposed algorithm achieves satisfactory performance
and outperforms heuristic benchmark schemes.

</details>
