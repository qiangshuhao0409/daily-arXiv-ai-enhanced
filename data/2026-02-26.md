<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [UnlinkableDFL: a Practical Mixnet Protocol for Churn-Tolerant Decentralized FL Model Sharing](https://arxiv.org/abs/2602.21343)
*Chao Feng,Thomas Grubl,Jan von der Assen,Sandrin Raphael Hunkeler,Linn Anna Spitz,Gerome Bovet,Burkhard Stiller*

Main category: cs.NI

TL;DR: UnlinkableDFL：一个结合对等混合网络和分片模型聚合的去中心化联邦学习框架，确保完全去中心化环境中的不可链接性


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习消除了中心聚合器的需求，但可能暴露通信模式从而泄露参与者身份。需要在不牺牲学习性能的前提下保护参与者的隐私和不可链接性。

Method: 结合对等混合网络和分片模型聚合：将模型更新分割为加密分片，通过独立的多跳路径发送，并在不使用任何身份信息的情况下进行聚合。

Result: 理论分析表明中继和端到端不可链接性随混合集大小和路径长度而改善，收敛性能与标准FedAvg相似。原型实现显示可靠收敛并适应节点流失，通信延迟是主要开销，内存和CPU使用适中。

Conclusion: UnlinkableDFL在匿名性和系统效率之间取得了平衡，证明了在去中心化学习工作流中可以保持强不可链接性。

Abstract: Decentralized Federated Learning (DFL) eliminates the need for a central aggregator, but it can expose communication patterns that reveal participant identities. This work presents UnlinkableDFL, a DFL framework that combines a peer-based mixnet with fragment-based model aggregation to ensure unlinkability in fully decentralized settings. Model updates are divided into encrypted fragments, sent over separate multi-hop paths, and aggregated without using any identity information. A theoretical analysis indicates that relay and end-to-end unlinkability improve with larger mixing sets and longer paths, while convergence remains similar to standard FedAvg. A prototype implementation evaluates learning performance, latency, unlinkability, and resource usage. The results show that UnlinkableDFL converges reliably and adapts to node churn. Communication latency emerges as the main overhead, while memory and CPU usage stay moderate. These findings illustrate the balance between anonymity and system efficiency, demonstrating that strong unlinkability can be maintained in decentralized learning workflows.

</details>


### [2] [Compensating the Packet Delay Variation for 6G Integrated with IEEE Time-Sensitive Networking](https://arxiv.org/abs/2602.21444)
*Marilet De Andrade,Joachim Sachs,Lucas Haug,Simon Egger,Frank Dürr,Balázs Varga,Janos Farkas,György Miklós*

Main category: cs.NI

TL;DR: 该论文提出在6G网络中集成TSN（时间敏感网络），通过虚拟时隙和去抖动技术来补偿无线随机行为引起的包延迟变化，从而提高时间关键通信的可靠性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持对高可靠性和时间关键性有严格要求的应用，但无线通信的随机行为会导致较大的固有包延迟变化，这会影响时间敏感通信的性能。

Method: 提出在6G网络中集成TSN，采用虚拟时隙提供时间感知能力，并利用去抖动技术来减少包延迟变化。同时研究了时隙大小对可调度TSN流数量的影响。

Result: 评估了基于去抖动的6G解决方案对减少包延迟变化的效果，分析了时隙大小对可调度TSN流数量的影响，讨论了所提方案的优点。

Conclusion: 通过虚拟时隙和去抖动技术，6G网络能够有效补偿无线随机行为，减少包延迟变化，从而更好地支持时间敏感应用，但需要优化时隙大小以平衡性能和可调度流数量。

Abstract: 6G is deemed as a key technology to support emerging applications with stringent requirements for highly dependable and timecritical communication. In this paper, we investigate 6G networks integrated with TSN and how to compensate for wireless stochastic behavior which involves a large intrinsic packet delay variation. We evaluate a 6G solution to reduce packet delay variation that is based on de-jittering. For this, we propose to use virtual timeslots for providing the required time-awareness. We discuss the benefits of the proposed solution while evaluating the impact of the timeslot size on the number of schedulable TSN streams.

</details>


### [3] [Lossy Compression of Network Feature Data: When Less Is Enough](https://arxiv.org/abs/2602.21891)
*Fabio Palmese,Gabriele Merlach,Damiano Ravalico,Martino Trevisan,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 论文研究网络流量特征的任务感知有损压缩策略，在保持分析精度的同时减少存储占用，适用于核心网络和物联网环境。


<details>
  <summary>Details</summary>
Motivation: 随着加密流量普及，基于特征的网络流量分析变得重要，但特征存储已成为从大规模核心网络到资源受限物联网环境的可扩展性瓶颈。

Method: 采用任务感知有损压缩策略，使用简单且保持语义的压缩技术，在核心网络网站分类和物联网设备识别两个代表性用例中验证。

Result: 研究发现这些压缩技术暴露了稳定的操作区域，能够平衡存储效率和任务性能，表明压缩是网络监控系统的关键设计维度。

Conclusion: 压缩应被视为可扩展网络监控系统中的一级设计维度，通过任务感知压缩策略可以在保持分析准确性的同时显著减少存储需求。

Abstract: Network traffic analysis increasingly relies on feature-based representations to support monitoring and security in the presence of pervasive encryption. Although features are more compact than raw packet traces, their storage has become a scalability bottleneck from large-scale core networks to resource-constrained Internet of Things (IoT) environments. This article investigates task-aware lossy compression strategies that reduce the storage footprint of traffic features while preserving analytics accuracy. Using website classification in core networks and device identification in IoT environments as representative use cases, we show that simple, semantics-preserving compression techniques expose stable operating regions that balance storage efficiency and task performance. These results highlight compression as a first-class design dimension in scalable network monitoring systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: 本书对软集理论及其主要扩展进行了综述式概述，涵盖核心定义、代表性构造和当前发展方向


<details>
  <summary>Details</summary>
Motivation: 软集理论为参数化决策建模提供了直接框架，通过为每个属性分配给定论域的子集来结构化表示不确定性。该理论已扩展出多个变体并连接到拓扑学、拟阵理论等领域，需要系统梳理

Method: 采用综述式方法，对软集及其主要扩展（包括超软集、超级超软集、树软集、双极软集、动态软集等）进行系统性概述

Result: 提供了软集理论的全面概览，突出了核心定义、代表性构造和关键研究方向，为相关领域研究者提供了系统的参考框架

Conclusion: 本书系统总结了软集理论的发展历程、主要扩展和应用连接，为该领域的进一步发展提供了基础性参考

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [5] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT是一个分层多智能体框架，用于自主发现和分析地球科学数据，通过集中式监督-工作者拓扑、沙盒代码执行和自校正机制，实现复杂工作流的自动化处理。


<details>
  <summary>Details</summary>
Motivation: 地球科学数据快速增长，但大量数据集在PANGAEA等存储库中未被充分利用，限制了数据的可重用性，需要解决可扩展性挑战。

Method: 采用分层多智能体框架，具有集中式监督-工作者拓扑结构，实现严格的数据类型感知路由、沙盒确定性代码执行，以及通过执行反馈进行自校正的机制。

Result: 在物理海洋学和生态学等用例场景中，系统能够以最少的人工干预执行复杂的多步骤工作流，展示了自主数据发现和分析的能力。

Conclusion: 该框架提供了一种通过协调智能体工作流来查询和分析异构存储库数据的方法论，为解决地球科学数据利用不足的问题提供了可行方案。

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [6] [Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information](https://arxiv.org/abs/2602.21496)
*Umid Suleymanov,Zaur Rajabov,Emil Mirzazada,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: SemSIEdit是一个推理时框架，通过智能"编辑"迭代批评和重写敏感内容，在保持叙事流畅的同时减少语义敏感信息泄露，实现隐私与效用的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）带来了新的威胁：语义敏感信息（SemSI），即模型推断敏感身份属性、生成损害声誉的内容或产生错误幻觉。如何在保持模型实用性的同时，让LLMs自我调节这些复杂、上下文相关的敏感信息泄露，仍然是一个未解决的科学问题。

Method: 提出SemSIEdit框架，采用智能"编辑"在推理时迭代批评和重写敏感内容，而不是简单地拒绝回答。该方法通过智能重写来保持叙事流畅，同时减少敏感信息泄露。

Result: 智能重写将三类SemSI的泄露减少了34.6%，而效用损失仅为9.8%，实现了隐私与效用的帕累托前沿。发现规模依赖的安全分歧：大型推理模型通过建设性扩展（增加细微差别）实现安全，而能力受限模型则倾向于破坏性截断（删除文本）。还发现推理悖论：推理时推理虽然增加了基线风险（使模型能够进行更深的敏感推断），但同时也增强了防御执行安全重写的能力。

Conclusion: SemSIEdit框架有效解决了LLMs中的语义敏感信息泄露问题，通过智能重写实现了隐私保护与模型效用的平衡。研究揭示了不同规模模型的安全策略差异，以及推理能力在风险与防御中的双重作用。

Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic "Editor" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.

</details>


### [7] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: 提出ARLArena框架和SAMPO方法，解决ARL训练不稳定的问题，通过分解策略梯度四个维度分析稳定性，实现稳定训练


<details>
  <summary>Details</summary>
Motivation: ARL在复杂多步交互任务中表现出潜力，但训练高度不稳定，经常导致训练崩溃，限制了向更大环境和更长交互周期的扩展性，也制约了算法设计的系统性探索

Method: 1. 提出ARLArena：稳定的训练配方和系统性分析框架，在受控可复现环境中检查训练稳定性；2. 构建清洁标准化测试平台；3. 将策略梯度分解为四个核心设计维度并评估每个维度的性能和稳定性；4. 提出SAMPO：稳定的智能体策略优化方法，旨在减轻ARL中的主要不稳定源

Result: SAMPO在各种智能体任务中实现了一致的稳定训练和强大性能，为ARL提供了统一的策略梯度视角，并为构建稳定可复现的LLM智能体训练流程提供了实用指导

Conclusion: 该研究通过ARLArena框架和SAMPO方法解决了ARL训练不稳定的核心问题，为智能体强化学习提供了系统性分析工具和稳定优化方案，推动了该领域的发展

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [8] [Power and Limitations of Aggregation in Compound AI Systems](https://arxiv.org/abs/2602.21556)
*Nivasini Ananthakrishnan,Meena Jagadeesan*

Main category: cs.AI

TL;DR: 聚合多个相同AI模型可以扩展系统设计者可获取的输出范围，通过三种机制实现：可行性扩展、支持扩展和绑定集收缩。


<details>
  <summary>Details</summary>
Motivation: 研究复合AI系统中聚合多个相同模型是否真的能扩展可获取的输出范围，探索聚合在克服模型能力和提示工程限制方面的潜力。

Method: 采用风格化的委托-代理框架分析，建模系统设计者如何通过奖励函数部分引导代理输出，但仍受限于提示工程能力和模型能力。识别并分析三种扩展机制。

Result: 发现聚合通过三种机制扩展可获取输出：可行性扩展、支持扩展和绑定集收缩。证明任何聚合操作必须实现其中至少一种机制才能扩展可获取性，强化版本的机制提供了充分必要条件。

Conclusion: 聚合确实能扩展复合AI系统的输出范围，通过特定机制克服模型能力和提示工程限制。研究为理解何时复合系统能超越单个模型限制提供了理论框架。

Abstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.

</details>


### [9] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: ASIR勇气模型是一个相变动态框架，将真相披露建模为从抑制态到表达态的状态转变，适用于人类和AI系统，统一解释了在压力下的沉默和偏好驱动的失真。


<details>
  <summary>Details</summary>
Motivation: 传统上将勇气视为人格特质，但作者认为真相披露是一个动态过程。需要建立一个统一框架来解释人类在不对称风险下的沉默和AI系统在政策约束下的输出失真，将两者视为相同相变结构的实例。

Method: 提出ASIR勇气模型，将真相披露形式化为状态转变：从抑制态(S0)到表达态(S1)。转变条件由不等式 λ(1+γ)+ψ > θ+φ 决定，其中λ是基线开放度，γ是关系放大因子，ψ是累积内部压力，θ和φ是转变成本。该框架扩展到AI系统，抑制对应约束输出状态，压力来自竞争目标、上下文紧张和递归交互动态。

Result: 该模型提供了一个统一的结构性解释：人类在压力下的沉默和AI偏好驱动的失真都是相同相变动态的表现。通过反馈扩展，模型展示了转变结果如何递归地重新校准系统参数，在重复交互中产生路径依赖和分歧效应。

Conclusion: ASIR勇气模型通过将勇气和对齐置于共享的动态结构中，为人类和人工系统在风险下的真相披露提供了形式化视角。模型将明显的真实性变化解释为约束相空间中相互作用力的几何结果，而非归因于AI系统的意图。

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [10] [fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation](https://arxiv.org/abs/2602.21746)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: fEDM+扩展了原有的模糊伦理决策框架，增加了可解释性模块和多元语义验证，提升AI伦理决策的透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 原fEDM框架虽然保证了形式正确性和决策一致性，但未能充分解决两个关键挑战：决策的原则性可解释性，以及在伦理多元主义下的鲁棒性。需要增强决策透明度和处理不同利益相关者伦理视角的能力。

Method: 1. 引入可解释性和可追溯性模块(ETM)，将伦理决策规则与底层道德原则明确关联，计算每个推荐行动的加权原则贡献度。2. 用多元语义验证框架替代单一参照验证，针对多个利益相关者参照进行评估，每个参照编码不同的原则优先级和风险容忍度。

Result: 扩展后的fEDM+框架在保持形式可验证性的同时，实现了增强的可解释性和利益相关者感知的验证。能够提供透明、可审计的解释，展示决策依据和原则基础，并正式表示原则性分歧而非压制。

Conclusion: fEDM+通过增强可解释性和多元验证，成为适用于伦理敏感AI系统的监督和治理层，在保持形式严谨性的同时提高了透明度和情境敏感性。

Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.

</details>


### [11] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: STAR推理框架将洗车问题准确率从0%提升至85%，结合用户画像和RAG上下文达到100%准确率，表明结构化推理比上下文注入更重要


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要隐式物理约束推理的"洗车问题"上持续失败，研究旨在探索哪些提示架构层能够实现正确推理

Method: 使用Claude 3.5 Sonnet进行变量隔离研究（n=20/条件，6个条件，共120次试验），控制超参数（温度0.7，top_p 1.0），测试不同提示架构层的效果

Result: STAR推理框架单独将准确率从0%提升至85%（p=0.001），添加用户画像上下文再提升10个百分点，RAG上下文再提升5个百分点，完整堆栈条件下达到100%准确率

Conclusion: 对于隐式约束推理任务，结构化推理框架（特别是强制目标表达）比上下文注入更为重要

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [12] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: 使用强化学习联合优化分解质量和验证器对齐，通过GRPO方法提升复杂声明验证性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将声明分解质量与验证性能对齐，需要一种能同时优化分解质量和验证器对齐的方法

Method: 提出强化学习方法，使用Group Relative Policy Optimization (GRPO)，整合结构化顺序推理、教师蒸馏示例的监督微调，以及平衡格式合规性、验证器对齐和分解质量的多目标奖励

Result: 在六个评估设置中，训练的8B分解器将下游验证性能提升至71.75% macro-F1，优于基于提示的方法（提升1.99-6.24）和现有RL方法（提升5.84），人类评估确认了生成子声明的高质量

Conclusion: 该框架使较小语言模型能够通过联合优化验证准确性和分解质量，实现最先进的声明验证

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [13] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: ProactiveMobile是一个针对移动智能体主动智能的综合性基准测试，包含3,660个实例和63个API，旨在解决现有MLLMs在主动预测用户意图和执行动作方面的能力不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在移动智能体开发中主要局限于被动执行用户命令的范式，而主动智能（智能体自主预测需求并启动行动）是下一代移动智能体的前沿方向，但缺乏能够应对现实世界复杂性并支持客观、可执行评估的基准测试。

Method: 提出ProactiveMobile基准测试，将主动任务形式化为：基于设备上下文信号的四个维度推断潜在用户意图，并从包含63个API的综合函数池中生成可执行函数序列。基准包含14个场景的3,660个实例，采用多答案标注以应对现实复杂性，并由30名专家团队进行最终审核，确保事实准确性、逻辑一致性和行动可行性。

Result: 实验表明，微调的Qwen2.5-VL-7B-Instruct模型成功率达到19.15%，优于o1（15.71%）和GPT-5（7.39%）。这表明主动性是当前MLLMs普遍缺乏的关键能力，但可以通过学习获得，凸显了该基准对主动性评估的重要性。

Conclusion: ProactiveMobile基准测试填补了移动智能体主动智能评估的空白，证明了主动性是可学习的能力，为未来主动智能研究提供了系统性的评估框架和方向。

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


### [14] [2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support](https://arxiv.org/abs/2602.21889)
*Otto Nyberg,Fausto Carcassi,Giovanni Cinà*

Main category: cs.AI

TL;DR: AI决策支持框架揭示单一先验信念偏差可能导致比无支持更差的结果


<details>
  <summary>Details</summary>
Motivation: 随着AI模型预测在各领域支持人类决策，我们仍缺乏对这些技术采用效果的深入理解，需要系统框架来评估AI辅助决策的影响

Method: 提出"2-Step Agent"通用计算框架，使用贝叶斯因果推断方法：1) 建模新观测预测如何影响理性贝叶斯代理的信念；2) 建模信念变化如何影响下游决策和后续结果

Result: 通过模拟显示，单一未对齐的先验信念就足以导致决策支持产生比无支持更差的下游结果，揭示了AI驱动决策支持的潜在陷阱

Conclusion: AI决策支持存在风险，需要彻底的模型文档和适当的用户培训来确保有效应用

Abstract: Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.

</details>


### [15] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM使用LLM分析PDDL描述，在规划前识别无关对象、动作和谓词，大幅减少基础化任务规模，实现更快的规划基础化


<details>
  <summary>Details</summary>
Motivation: 经典规划中的基础化步骤常因任务规模增大导致基础化动作和原子指数增长而成为计算瓶颈。现有部分基础化方法主要依赖关系特征或学习嵌入，未能充分利用PDDL描述中的文本和结构信息。

Method: 使用大型语言模型分析领域和问题文件，启发式识别潜在无关的对象、动作和谓词，在基础化前进行筛选，显著减少基础化任务规模。

Result: 在七个难以基础化的基准测试中，SPG-LLM实现了更快的基础化速度（通常快几个数量级），在某些领域还能提供相当或更好的规划成本。

Conclusion: 利用LLM分析PDDL的文本和结构信息可以有效识别无关元素，大幅提升规划基础化效率，为解决基础化瓶颈提供了新方法。

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [16] [Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts](https://arxiv.org/abs/2602.22070)
*Jessica Y. Bo,Lillio Mok,Ashton Anderson*

Main category: cs.AI

TL;DR: LLMs在决策任务中对人类专家和算法代理表现出不一致的偏见：口头信任评价偏向人类，但实际决策时却偏向算法，即使算法表现更差。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在处理不同来源信息（人类专家和算法代理）时的权衡机制，特别是考察LLMs是否也存在人类决策中常见的"算法厌恶"现象。

Method: 使用行为经济学实验范式，评估8个不同LLM在决策任务中的委托行为。采用两种任务呈现方式：1) 陈述偏好（直接询问信任度），2) 显示偏好（提供上下文示例展示两者表现）。

Result: 1) 在信任度评分中，LLMs给人类专家的评分更高；2) 但在实际激励性决策中，LLMs不成比例地选择算法，即使算法表现明显更差。

Conclusion: LLMs对人类和算法编码了不一致的偏见，这种不一致性在高风险部署场景中需要仔细考虑。同时，LLMs对任务呈现格式的敏感性应在AI安全评估中得到广泛审查。

Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.

</details>


### [17] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: 提出基于Petri网可达性松弛的方法，用于鲁棒不变式合成、高效目标不可达检测和有用的不可行性解释，支持目标和约束的增量更新。


<details>
  <summary>Details</summary>
Motivation: 现实中的计划经常因情况变化或理解变化而需要调整，有时甚至不存在可行计划。传统规划方法主要关注可行情况下的高效一次性规划，而缺乏对领域更新和不可行性检测的支持。

Method: 采用Petri网可达性松弛技术，结合增量约束求解器，支持目标和约束的更新，实现鲁棒不变式合成、目标不可达检测和不可行性解释。

Result: 与基线相比，系统生成相当数量的不变式，检测到最多2倍的不可行情况，在一次性规划中表现相当，在顺序计划更新中表现更优。

Conclusion: 提出的Petri网可达性松弛方法能有效支持计划更新、不可行性检测和解释，为动态环境下的规划问题提供了更全面的解决方案。

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [18] [The constructions of Singleton-optimal locally repairable codes with minimum distance 6 and locality 3](https://arxiv.org/abs/2602.21494)
*Yanzhen Xiong,Jianbing Lu*

Main category: cs.IT

TL;DR: 利用有限几何的组合结构，构造了最小距离d=6、局部性r=3的q元Singleton最优局部可修复码，长度随q的奇偶性和模4余数变化。


<details>
  <summary>Details</summary>
Motivation: 局部可修复码（LRCs）在分布式存储系统中具有重要应用，需要构造具有最优参数的LRCs。本文旨在构造最小距离d=6、局部性r=3的Singleton最优LRCs。

Method: 利用有限几何的组合结构，通过完全正交拉丁方组（MOLS）与仿射平面AG(2,q)的对应关系，在射影平面PG(2,q)中系统构造不相交的4-弧族，使得任意两个不同4-弧的并集形成8-弧。这些4-弧称为4-局部弧，其存在性等价于所需码的存在性。

Result: 对于任意素数幂q≥7，构造出长度n=2q（q为偶数）、n=2q-2（q≡3 mod 4）或n=2q-6（q≡1 mod 4）的Singleton最优LRCs，具有最小距离d=6和局部性r=3。

Conclusion: 通过有限几何的组合结构，成功构造了参数最优的局部可修复码，为分布式存储系统提供了新的编码方案。

Abstract: In this paper, we present new constructions of $q$-ary Singleton-optimal locally repairable codes (LRCs) with minimum distance $d=6$ and locality $r=3$, based on combinatorial structures from finite geometry. By exploiting the well-known correspondence between a complete set of mutually orthogonal Latin squares (MOLS) of order $q$ and the affine plane $\mathrm{AG}(2,q)$, We systematically construct families of disjoint 4-arcs in the projective plane $\mathrm{PG}(2,q)$, such that the union of any two distinct 4-arcs forms an 8-arc. These 4-arcs form what we call 4-local arcs, and their existence is equivalent to that of the desired codes. For any prime power $q\ge 7$, our construction yields codes of length $n = 2q$, $2q-2$, or $2q-6$ depending on whether $q$ is even, $q\equiv 3 \pmod{4}$, or $q\equiv 1 \pmod{4}$, respectively.

</details>


### [19] [Guided Wireless Technology for Near-Field Communication](https://arxiv.org/abs/2602.21528)
*Mohamed Akrout,Amine Mezghani,Faouzi Bellili,Robert W. Heath*

Main category: cs.IT

TL;DR: 该论文提出了一种结合导波与无线通信优势的导引无线技术，重点研究了线性小区环境中长连接阵列的近场通信建模，通过电路模型分析和仿真验证了驻波现象，并展示了LMMSE发射波束成形在干扰抑制和功率分配方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统无线系统信号在空中传播易受干扰、衰减和干扰，而导引通信将信号限制在物理介质中，能显著减少干扰并支持更远距离的更高数据速率。该研究旨在利用导引无线技术的优势，解决线性小区环境中长阵列的近场通信建模问题。

Method: 1. 将长连接阵列建模为具有多个周期性馈点的无限长偶极子电路模型；2. 通过开路近似将其简化为有限阵列；3. 进行仿真分析验证驻波现象；4. 应用LMMSE发射波束成形技术，自适应分配功率以缓解干扰并最小化均方误差。

Result: 1. 仿真验证了驻波现象，表现为频谱效率的振荡；2. LMMSE发射波束成形能有效抑制干扰，通过自适应分配更多功率给信道衰减最严重的用户，最小化均方误差；3. 实现了用户间可达速率的更均衡变化。

Conclusion: 导引无线技术结合了导波和无线通信的优势，通过长连接阵列的电路建模和LMMSE波束成形，能有效解决传统无线系统的干扰问题，提高通信可靠性和性能均衡性，为线性小区环境中的近场通信提供了有效解决方案。

Abstract: Guided wireless technology is an innovative approach that combines the strengths of guided waves and wireless communication. In traditional wireless systems, signals propagate through the air, where they are vulnerable to interference, attenuation, and jamming. Guided communication, in contrast, confines signals within a physical medium, significantly reducing interference and supporting higher data rates over longer distances. Guided wireless technology harnesses these benefits by creating guided wireless channels and offering a controlled pathway for electromagnetic waves. This work harnesses these benefits by focusing on the modeling of near-field communication through long connected arrays deployed in linear-cell environments. We derive a circuit model for long array as an infinitely long dipole with multiple periodic feed points before approximating it with a finite array through open circuiting. Through our simulations, we show how the standing wave phenomenon is confirmed by the oscillations in spectral efficiency. We also demonstrate the capability of the LMMSE transmit beamformer in mitigating interference and minimizing the mean square error by adaptively allocating more power to the user experiencing the most severe channel attenuation, resulting in a more balanced variation of achievable rates across users.

</details>


### [20] [Impact of Pointing Errors and Correlated Wall Blockages on Practical Grid-based Indoor Terahertz Communication Systems](https://arxiv.org/abs/2602.21558)
*Zhifeng Tang,Nan Yang,Salman Durrani,Xiangyun Zhou,Josep Miquel Jornet,Markku Juntti*

Main category: cs.IT

TL;DR: 本文研究了室内太赫兹通信系统中结构化AP部署（方形和六边形网格）下的覆盖概率，考虑了人体遮挡、墙体遮挡相关性、波束训练和残余指向误差等因素，发现六边形网格优于方形网格，墙体遮挡相关性显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信虽然能支持极高数据速率，但面临严重的路径损耗、遮挡效应和波束失准等挑战。室内环境中，结构化AP部署下的覆盖性能分析尚不完善，特别是墙体遮挡相关性、波束训练开销等因素的影响需要深入研究。

Method: 开发了一个可处理的3D室内太赫兹通信系统分析框架，考虑方形和六边形网格AP部署。模型联合考虑了人体遮挡、跨AP的墙体遮挡相关性、波束训练过程和残余指向误差。通过数值分析评估不同拓扑下的覆盖概率。

Result: 1) 墙体遮挡相关性显著降低关联和覆盖概率，不可忽略；2) 六边形网格比方形网格获得更高覆盖，通过减轻墙体遮挡相关性和缩短UE到AP距离；3) 覆盖性能强烈依赖于UE位置，远离最近AP时明显下降；4) 残余指向误差导致显著覆盖损失，尤其对长距离链路；5) 天线阵列尺寸与训练开销呈非单调关系，存在天线配置、波束宽度选择和训练效率的权衡。

Conclusion: 研究为实际室内太赫兹通信系统的设计和部署提供了重要见解：应优先考虑六边形网格AP部署，充分考虑墙体遮挡相关性影响，优化波束训练策略以平衡天线配置和训练效率，并注意残余指向误差对长距离链路的负面影响。

Abstract: Terahertz (THz) communications has emerged as a promising technology for future wireless systems due to its potential to support extremely high data rates. However, severe path loss, blockage effects, and sensitivity to beam misalignment pose major challenges to reliable indoor THz communications. In this paper, we investigate the coverage probability of downlink transmission in a three-dimensional (3D) indoor THz communication system under structured access point (AP) deployments, with a focus on square and hexagonal grid topologies. A tractable analytical framework is developed to jointly account for human blockages, correlated wall blockages across APs, beam training, and residual pointing error. Numerical results demonstrate that wall blockage correlation significantly reduces the association and coverage probabilities, and its impact cannot be neglected in system performance analysis. Compared with square grid AP deployments, hexagonal grids consistently achieve higher coverage by mitigating correlated wall blockage effects and reducing the distances between user equipments (UEs) and their associated APs. Furthermore, coverage performance is shown to strongly depend on the UE location, with noticeable degradation as the UE moves away from its nearest AP. Residual pointing error is found to introduce substantial coverage loss, especially for longer links. In addition, beam training analysis reveals a non-monotonic relationship between antenna array size and training overhead, highlighting an inherent tradeoff among antenna configuration, beamwidth selection, and beam training efficiency. These findings provide useful insights into the design and deployment of practical indoor THz communication systems.

</details>


### [21] [Concatenated Sum-Rank Codes](https://arxiv.org/abs/2602.21609)
*Huimin Lao,Hao Chen,San Ling,Yaqi Chen*

Main category: cs.IT

TL;DR: 本文提出了一种和秩码与汉明度量码的级联构造方法，构建了优于和秩BCH码的新码，并获得了超过Tsfasman-Vladut-Zink-like界和Gilbert-Varshamov-like界的渐近好码序列。


<details>
  <summary>Details</summary>
Motivation: 和秩码在多跳网络编码、分布式存储和空时码构造中有广泛应用。虽然已有超过Gilbert-Varshamov-like界的渐近好码序列，但需要更简单、更显式的构造方法来获得具有更好参数的和秩码。

Method: 引入和秩码与汉明度量码的级联构造方法。通过这种级联技术，能够简单且显式地构造出许多参数优于和秩BCH码的新和秩码。

Result: 成功构造了多个参数优于和秩BCH码的和秩码。更重要的是，获得了一个渐近好的和秩码序列，该序列同时超过了Tsfasman-Vladut-Zink-like界和Gilbert-Varshamov-like界。

Conclusion: 提出的级联构造方法为和秩码提供了一种简单有效的构造途径，不仅能够获得优于现有码的参数，还能达到重要的渐近性能界限，具有理论和实际应用价值。

Abstract: Sum-rank codes have wide applications in multishot network coding, distributed storage and the construction of space-time codes. Asymptotically good sequences of linearized algebraic geometry sum-rank codes, exceeding the Gilbert-Varshamov-like bound, were constructed in a recent paper published in IEEE Trans. Inf. Theory by E. Berardini and X. Caruso. We call this bound the Tsfasman-Vladut-Zink-like bound. In this paper, we introduce the concatenation of a sum-rank code and a Hamming metric code. Then many sum-rank codes with good parameters, which are better than sum-rank BCH codes, are constructed simply and explicitly. Moreover, we obtain an asymptotically good sequence of sum-rank codes exceeding the Tsfasman-Vladut-Zink-like bound and the Gilbert-Varshamov-like bound.

</details>


### [22] [Permutation Polynomials Under Multiplicative-Additive Perturbations: Characterization via Difference Distribution Tables](https://arxiv.org/abs/2602.21632)
*Ranit Dutta,Pantelimon Stanica,Bimal Mandal*

Main category: cs.IT

TL;DR: 论文研究了有限域上具有完美c非线性(PcN)性质的置换多项式，这种性质提供了对c差分攻击的最优抵抗。作者首次使用经典差分分布表(DDT)给出了PcN的刻画，将验证复杂度从O(p^{3n})降低到O(p^{2n})，并证明了单项式置换的严格二分性质。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于Kuznyechik密码变体的最新密码分析，其中c差分攻击成为一个重要关注点。完美c非线性(PcN)性质代表了对此类攻击的最优抵抗，因此需要深入理解具有这种性质的置换多项式的结构特性。

Method: 研究方法包括：1) 使用经典差分分布表(DDT)刻画PcN性质；2) 分析单项式置换的严格二分性质；3) 对二次置换给出显式代数刻画；4) 识别保持c差分均匀性的仿射变换类；5) 推导紧的非线性度界限。

Result: 主要结果：1) 首次使用DDT给出了PcN的充要条件：F是PcN当且仅当对所有非零a,b有Δ_F(a,b)Δ_F(a,c^{-1}b)=0；2) 将验证复杂度从O(p^{3n})降低到O(p^{2n})；3) 证明了单项式置换的严格二分性质；4) 发现了完美c非线性与APN性质之间的基本不兼容性。

Conclusion: 完美c非线性代表了置换多项式理论中一个结构上独特的体系。该研究为理解抵抗c差分攻击的密码学原语提供了理论基础，揭示了PcN与经典APN性质之间的根本差异，为设计更安全的密码系统提供了新视角。

Abstract: We investigate permutation polynomials F over finite fields F_{p^n} whose generalized derivative maps x -> F(x + a) - cF(x) are themselves permutations for all nonzero shifts a. This property, termed perfect c-nonlinearity (PcN), represents optimal resistance to c-differential attacks - a concern highlighted by recent cryptanalysis of the Kuznyechik cipher variant. We provide the first characterization using the classical difference distribution table (DDT): F is PcN if and only if Delta_F(a,b) Delta_F(a,c^{-1}b) = 0 for all nonzero a,b. This enables verification in O(p^{2n}) time given a precomputed DDT, a significant improvement over the naive O(p^{3n}) approach. We prove a strict dichotomy for monomial permutations: the derivative F(x + alpha) - cF(x) is either a permutation for all nonzero shifts or for none, with the general case remaining open. For quadratic permutations, we provide explicit algebraic characterizations. We identify the first class of affine transformations preserving c-differential uniformity and derive tight nonlinearity bounds revealing fundamental incompatibility between PcN and APN properties. These results position perfect c-nonlinearity as a structurally distinct regime within permutation polynomial theory.

</details>


### [23] [From Specialist to Large Models: A Paradigm Evolution Towards Semantic-Aware MIMO](https://arxiv.org/abs/2602.21672)
*Keke Ying,Zhen Gao,Tingting Yang,Jianhua Zhang,Xiang Cheng,Tony Q. S. Quek,H. Vincent Poor*

Main category: cs.IT

TL;DR: 论文提出"语义感知MIMO"新范式，利用专业模型和大模型感知、利用和融合信道与信源的固有语义，以解决6G网络中大规模MIMO带来的开销和延迟问题，并满足沉浸式通信和环境感知等新兴需求。


<details>
  <summary>Details</summary>
Motivation: 6G网络将部署更大规模MIMO阵列以支持大规模连接，但这会增加物理层开销和延迟。同时，沉浸式通信和环境感知等新兴6G需求对传统信号处理提出挑战。

Method: 提出"语义感知MIMO"范式，利用专业模型和大模型来感知、利用和融合信道与信源的固有语义。针对随机接入活动检测、信道反馈和预编码等典型MIMO物理层任务，设计了利用信道和信源语义的专业模型。进一步探索大模型作为多任务语义感知MIMO的可扩展解决方案。

Result: 通过利用信道和信源语义，专业模型在MIMO物理层任务中实现了更好的性能。大模型为多任务语义感知MIMO提供了可扩展的解决方案。

Conclusion: 语义感知MIMO范式为解决6G网络挑战提供了新思路。专业模型和大模型在利用语义信息方面各有优势，未来需要进一步探索两者的演进路径、面临的挑战和发展前景。

Abstract: The sixth generation (6G) network is expected to deploy larger multiple-input multiple-output (MIMO) arrays to support massive connectivity, which will increase overhead and latency at the physical layer. Meanwhile, emerging 6G demands such as immersive communications and environmental sensing pose challenges to traditional signal processing. To address these issues, we propose the ``semantic-aware MIMO'' paradigm, which leverages specialist models and large models to perceive, utilize, and fuse the inherent semantics of channels and sources for improved performance. Moreover, for representative MIMO physical-layer tasks, e.g., random access activity detection, channel feedback, and precoding, we design specialist models that exploit channel and source semantics for better performance. Additionally, in view of the more diversified functions of 6G MIMO, we further explore large models as a scalable solution for multi-task semantic-aware MIMO and review recent advances along with their advantages and limitations. Finally, we discuss the challenges, insights, and prospects of the evolution of specialist models and large models empowered semantic-aware MIMO paradigms.

</details>


### [24] [Function-Correcting Codes with Optimal Data Protection for Hamming Code Membership](https://arxiv.org/abs/2602.21932)
*Swaraj Sharma Durgi,Anjana A. Mahesh,Anupriya Kumari,Rajlaxmi Pandey,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 该论文研究了针对[7,4,3]-汉明码成员函数(HCMF)的单错误纠正函数校正码(SEFCC)，建立了奇偶校验分配的有效条件，利用汉明码字间的距离关系构建二分图，提出了一种系统化的SEFCC构造方法，证明了该构造在最大和距离、最小距离和距离-2码字对数量方面达到最优。


<details>
  <summary>Details</summary>
Motivation: 研究汉明码成员函数的单错误纠正函数校正码，旨在为判断向量是否属于[7,4,3]-汉明码的函数提供有效的错误保护机制，提高数据保护的可靠性。

Method: 1. 建立汉明码字与其最近非码字之间距离约束的有效奇偶校验分配条件；2. 利用汉明距离为3的码字关系构建二分图；3. 基于该二分图开发系统化的SEFCC构造方法；4. 推导码字对距离和的紧上界，证明所提构造的唯一最优性。

Result: 1. 证明了所提二分图构造唯一地实现了最大和距离、可能的最大最小距离2以及最少的距离-2码字对；2. 对于HCMF SEFCC问题，和距离最大化不仅是启发式方法，而且精确地强制执行与错误概率相关的最优距离谱特性；3. 在AWGN信道上的软判决解码仿真表明，最大和SEFCC相比任意有效分配提供了显著改进的数据保护和误码率性能。

Conclusion: 该论文为汉明码成员函数开发了一种最优的单错误纠正函数校正码构造方法，通过利用汉明码字间的几何结构构建二分图，实现了距离特性的最优化，显著提升了错误保护性能，为函数校正码设计提供了新的理论框架。

Abstract: This paper investigates single-error-correcting function-correcting codes (SEFCCs) for the Hamming code membership function (HCMF), which indicates whether a vector in $\mathbb{F}_2^7$ belongs to the [7,4,3]-Hamming code. Necessary and sufficient conditions for valid parity assignments are established in terms of distance constraints between codewords and their nearest non-codewords. It is shown that the Hamming-distance-3 relations among Hamming codewords induce a bipartite graph, a fundamental geometric property that is exploited to develop a systematic SEFCC construction. By deriving a tight upper bound on the sum of pairwise distances, we prove that the proposed bipartite construction uniquely achieves the maximum sum-distance, the largest possible minimum distance of 2, and the minimum number of distance-2 codeword pairs. Consequently, for the HCMF SEFCC problem, sum-distance maximisation is not merely heuristic-it exactly enforces the optimal distance-spectrum properties relevant to error probability. Simulation results over AWGN channels with soft-decision decoding confirm that the resulting max-sum SEFCCs provide significantly improved data protection and Bit Error Rate (BER) performance compared to arbitrary valid assignments.

</details>


### [25] [Maximal Recoverability: A Nexus of Coding Theory](https://arxiv.org/abs/2602.22042)
*Joshua Brakensiek,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: 这篇综述探讨了最大可恢复性(MR)码的两个家族：MR局部可恢复码(LRCs)和网格码(GCs)，分析了它们的恢复性保证、最优构造以及与计算机科学和数学的广泛联系。


<details>
  <summary>Details</summary>
Motivation: 在大规模计算系统中，纠错码通过引入冗余确保从故障中恢复。为了最大化每个字节的效用，研究最大可恢复性(MR)码框架来优化各种架构中的纠错码。

Method: 采用综述研究方法，深入分析两个MR码家族：MR局部可恢复码（部分MDS码）和网格码。讨论每个家族的恢复性保证、已知的最优构造，并探索它们与计算机科学和数学其他领域的联系。

Result: 对于MR LRCs，斜多项式码统一了许多先前构造；对于MR GCs，高阶MDS码理论显示它们可用于构造最优列表可解码码，且其最优恢复模式与图结构刚性问题密切相关。

Conclusion: MR码研究不仅提供了优化的纠错方案，还揭示了与计算机科学和数学多个领域的深刻联系，包括统一构造方法、列表解码优化和图结构理论等。

Abstract: In the modern era of large-scale computing systems, a crucial use of error correcting codes is to judiciously introduce redundancy to ensure recoverability from failure. To get the most out of every byte, practitioners and theorists have introduced the framework of maximal recoverability (MR) to study optimal error-correcting codes in various architectures. In this survey, we dive into the study of two families of MR codes: MR locally recoverable codes (LRCs) (also known as partial MDS codes) and grid codes (GCs).
  For each of these two families of codes, we discuss the primary recoverability guarantees as well as what is known concerning optimal constructions. Along the way, we discuss many surprising connections between MR codes and broader questions in computer science and mathematics. For MR LRCs, the use of skew polynomial codes has unified many previous constructions. For MR GCs, the theory of higher order MDS codes shows that MR GCs can be used to construct optimal list-decodable codes. Furthermore, the optimally recoverable patterns of MR GCs have close ties to long-standing problems on the structural rigidity of graphs.

</details>


### [26] [Multichannel Conflict-Avoiding Codes for Expanded Scenarios](https://arxiv.org/abs/2602.22081)
*Kangkang Xu,Yuan-Hsun Lo,Tsai-Lien Wong,Yijin Zhang,Kenneth W. Shum*

Main category: cs.IT

TL;DR: 本文研究了多信道冲突避免码（MC-CAC），特别关注信道数M小于码字权重w的实用场景，通过引入特殊码字概念和使用加性组合技术，推导出一系列最优MC-CAC，并推广到AM-OPPTS MC-CAC和混合权重MC-CAC。


<details>
  <summary>Details</summary>
Motivation: 现有MC-CAC研究大多假设信道数M不小于码字权重w，但实际应用中M往往小于w。本文旨在解决这一更贴近实际应用场景的情况，填补MC-CAC理论在这一重要参数区域的研究空白。

Method: 首先引入MC-CAC中特殊码字的概念，然后运用加性组合技术，通过数学推导构建最优MC-CAC。方法包括对码字结构的理论分析和组合优化。

Result: 推导出一系列最优MC-CAC，推广了多个已知的最优CAC结果。研究成果可自然扩展到AM-OPPTS MC-CAC和混合权重MC-CAC两类相关编码。

Conclusion: 本文成功解决了M<w情况下MC-CAC的设计问题，为实际多信道确定性多址接入系统提供了理论支持，扩展了冲突避免码的应用范围。

Abstract: A conflict-avoiding code (CAC) of length L and weight w is used for deterministic multiple-access without feedback. When the number of simultaneous active users is less than or equal to w, such a code is able to provide a hard guarantee that each active user has a successful transmission within every consecutive L time slots. Recently, CACs were extended to multichannel CAcs (MC-CACs) over M orthogonal channels with the aim of increasing the number of potential users that can be supported. While most existing results on MC-CAC are derived under the assumption that M is not less than w, this paper focuses on the case that M is less than w, which is more relevant to practical application scenarios. In this paper, we first introduce the concept of exceptional codewords in MC-CACs. By employing some techniques from additive combinatorics, we derive a series of optimal MC-CACs. Along the way, several previously known optimal CAC results are generalized. Finally, our results extend naturally to AM-OPPTS MC-CACs and mixed-weight MC-CACs, two classes of relevant codes.

</details>
