<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Experimental Validation of Provably Covert Communication Using Software-Defined Radio](https://arxiv.org/abs/2508.08380)
*Rohan Bali,Trevor E. Bailey,Michael S. Bullock,Boulat A. Bash*

Main category: cs.NI

TL;DR: 论文展示了基于软件定义无线电的隐蔽射频通信，验证了理论预测并提出了进一步研究方向。


<details>
  <summary>Details</summary>
Motivation: 隐蔽通信的理论研究已有十多年，但硬件实现和实验验证仍不足，尤其是射频领域。

Method: 使用软件定义无线电（SDRs）实现隐蔽射频通信。

Result: 实验验证了平方根定律（SRL）的理论预测，并展示了隐蔽通信的可行性。

Conclusion: 研究为隐蔽通信系统的实际应用提供了基础，并提出了未来研究方向。

Abstract: The fundamental information-theoretic limits of covert, or low probability of
detection/intercept (LPD/LPI), communication have been extensively studied for
over a decade, resulting in the square root law (SRL): only $L\sqrt{n}$ covert
bits can be reliably transmitted over time-bandwidth product $n$, for constant
$L>0$. Transmitting more either results in detection or decoding errors. The
SRL imposes significant constraints on hardware realization of
mathematically-guaranteed covert communication. Indeed, they preclude using
standard link maintenance operations that are taken for granted in non-covert
communication. Thus, experimental validation of covert communication is
underexplored: to date, only two experimental studies of SRL-based covert
communication are available, both focusing on optical channels. Here, we report
a demonstration of provably-secure covert radio-frequency (RF) communication
using software-defined radios (SDRs). This validates theoretical predictions,
opens practical avenues for implementing covert communication systems, and
raises further research questions.

</details>


### [2] [LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework](https://arxiv.org/abs/2508.08535)
*Azin Sabzian,Mohammad Jalili Torkamani,Negin Mahmoudi,Kiana Kiashemshaki*

Main category: cs.NI

TL;DR: 本文综述了无线体域网（WBAN）的架构、路由策略和安全机制，提出了一种基于大语言模型的适应性WBAN框架，以解决当前在适应性、能效和量子安全方面的不足。


<details>
  <summary>Details</summary>
Motivation: 整合6G通信、后量子密码学和能量收集技术以提升WBAN性能，但现有系统缺乏统一适应性。

Method: 提出一种大语言模型驱动的适应性WBAN框架，实时协调路由、物理层选择、微能量收集和后量子安全。

Result: 综述揭示了当前基于启发式设计的局限性，并提出了面向6G医疗系统的研究议程。

Conclusion: 该框架旨在实现超可靠、安全和自优化的下一代移动健康应用WBAN。

Abstract: Wireless Body Area Networks (WBANs) enable continuous monitoring of
physiological signals for applications ranging from chronic disease management
to emergency response. Recent advances in 6G communications, post-quantum
cryptography, and energy harvesting have the potential to enhance WBAN
performance. However, integrating these technologies into a unified, adaptive
system remains a challenge. This paper surveys some of the most well-known
Wireless Body Area Network (WBAN) architectures, routing strategies, and
security mechanisms, identifying key gaps in adaptability, energy efficiency,
and quantum-resistant security. We propose a novel Large Language Model-driven
adaptive WBAN framework in which a Large Language Model acts as a cognitive
control plane, coordinating routing, physical layer selection, micro-energy
harvesting, and post-quantum security in real time. Our review highlights the
limitations of current heuristic-based designs and outlines a research agenda
for resource-constrained, 6G-ready medical systems. This approach aims to
enable ultra-reliable, secure, and self-optimizing WBANs for next-generation
mobile health applications.

</details>


### [3] [Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks](https://arxiv.org/abs/2508.08555)
*Tong Zhang,Yu Gou,Jun Liu,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度多智能体强化学习的资源管理策略（TARM），用于优化水下无线传感器网络的通信效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 水下无线传感器网络（UWSNs）面临通信环境恶劣、能源有限和信号传输受限等挑战，需要高效的资源管理策略。

Method: 通过将问题建模为分散部分可观测马尔可夫决策过程（Dec-POMDP），设计了基于深度多智能体强化学习的TARM策略，并结合流量负载感知机制和候选解空间优化算法。

Result: 仿真结果表明，TARM在不同传输需求和碰撞概率的场景中具有适应性，并验证了其在有限资源下支持高效可靠通信的有效性。

Conclusion: TARM策略通过优化通信链路调度和传输参数，显著提升了水下网络的通信效率和可靠性。

Abstract: Underwater Wireless Sensor Networks (UWSNs) represent a promising technology
that enables diverse underwater applications through acoustic communication.
However, it encounters significant challenges including harsh communication
environments, limited energy supply, and restricted signal transmission. This
paper aims to provide efficient and reliable communication in underwater
networks with limited energy and communication resources by optimizing the
scheduling of communication links and adjusting transmission parameters (e.g.,
transmit power and transmission rate). The efficient and reliable communication
multi-objective optimization problem (ERCMOP) is formulated as a decentralized
partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware
Resource Management (TARM) strategy based on deep multi-agent reinforcement
learning (MARL) is presented to address this problem. Specifically, a traffic
load-aware mechanism that leverages the overhear information from neighboring
nodes is designed to mitigate the disparity between partial observations and
global states. Moreover, by incorporating a solution space optimization
algorithm, the number of candidate solutions for the deep MARL-based
decision-making model can be effectively reduced, thereby optimizing the
computational complexity. Simulation results demonstrate the adaptability of
TARM in various scenarios with different transmission demands and collision
probabilities, while also validating the effectiveness of the proposed approach
in supporting efficient and reliable communication in underwater networks with
limited resources.

</details>


### [4] [QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach](https://arxiv.org/abs/2508.08627)
*Conghao Zhou,Lulu Sun,Xiucheng Wang,Peng Yang,Feng Lyu,Sihan Lu,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种基于代理的边缘辅助移动增强现实（MAR）通信服务方法，利用大语言模型（LLM）减少通信开销并提升体验质量（QoE）。


<details>
  <summary>Details</summary>
Motivation: 解决MAR应用中网络控制器无法获取特定信息的问题，同时应对用户数据流量的动态性和个性化需求。

Method: 1. 建立基于LLM的数字代理，连接MAR服务与网络域；2. 开发用户级QoE建模方法，实现个性化资源管理。

Result: 仿真结果显示，该方法在QoE建模准确性和通信资源效率上优于传统LLM方法。

Conclusion: 代理驱动的通信服务方法能有效提升MAR的QoE和资源效率。

Abstract: Mobile augmented reality (MAR) is envisioned as a key immersive application
in 6G, enabling virtual content rendering aligned with the physical environment
through device pose estimation. In this paper, we propose a novel agent-driven
communication service provisioning approach for edge-assisted MAR, aiming to
reduce communication overhead between MAR devices and the edge server while
ensuring the quality of experience (QoE). First, to address the inaccessibility
of MAR application-specific information to the network controller, we establish
a digital agent powered by large language models (LLMs) on behalf of the MAR
service provider, bridging the data and function gap between the MAR service
and network domains. Second, to cope with the user-dependent and dynamic nature
of data traffic patterns for individual devices, we develop a user-level QoE
modeling method that captures the relationship between communication resource
demands and perceived user QoE, enabling personalized, agent-driven
communication resource management. Trace-driven simulation results demonstrate
that the proposed approach outperforms conventional LLM-based QoE-aware service
provisioning methods in both user-level QoE modeling accuracy and communication
resource efficiency.

</details>


### [5] [Ultra Ethernet's Design Principles and Architectural Innovations](https://arxiv.org/abs/2508.08906)
*Torsten Hoefler,Karen Schramm,Eric Spada,Keith Underwood,Cedell Alexander,Bob Alverson,Paul Bottorff,Adrian Caulfield,Mark Handley,Cathy Huang,Costin Raiciu,Abdul Kabbani,Eugene Opsasnick,Rong Pan,Adee Ran,Rip Sohan*

Main category: cs.NI

TL;DR: Ultra Ethernet (UE) 1.0规范为未来AI和HPC系统定义了高性能以太网标准，其核心创新是Ultra Ethernet Transport (UET)协议。


<details>
  <summary>Details</summary>
Motivation: 利用以太网生态系统和计算效率的显著提升，为极端规模系统提供可靠、快速和高效的通信。

Method: 设计了全硬件加速的UET协议，覆盖整个以太网栈的创新。

Result: UE通过UET协议实现了高性能网络的新时代，优于20年前的InfiniBand。

Conclusion: UE 1.0规范为AI和HPC系统提供了革命性的高性能网络解决方案。

Abstract: The recently released Ultra Ethernet (UE) 1.0 specification defines a
transformative High-Performance Ethernet standard for future Artificial
Intelligence (AI) and High-Performance Computing (HPC) systems. This paper,
written by the specification's authors, provides a high-level overview of UE's
design, offering crucial motivations and scientific context to understand its
innovations. While UE introduces advancements across the entire Ethernet stack,
its standout contribution is the novel Ultra Ethernet Transport (UET), a
potentially fully hardware-accelerated protocol engineered for reliable, fast,
and efficient communication in extreme-scale systems. Unlike InfiniBand, the
last major standardization effort in high-performance networking over two
decades ago, UE leverages the expansive Ethernet ecosystem and the 1,000x gains
in computational efficiency per moved bit to deliver a new era of
high-performance networking.

</details>


### [6] [Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring](https://arxiv.org/abs/2508.09085)
*Zihan Fang,Zheng Lin,Senkang Hu,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.NI

TL;DR: 提出了一种名为DUAL-Health的不确定性感知多模态融合框架，用于动态和嘈杂环境下的户外健康监测，显著提升了检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 户外健康监测对保障人类健康和安全至关重要，但现有方法（如静态多模态深度学习框架）需要大量数据训练且难以捕捉细微变化，而基于多模态大语言模型（MLLMs）的方法虽高效但面临噪声干扰和多模态融合不鲁棒的挑战。

Method: 通过量化模态不确定性、基于不确定性定制融合权重，并在共同语义空间中对齐模态分布，解决了噪声干扰和多模态融合问题。

Result: 实验表明，DUAL-Health在检测准确性和鲁棒性上优于现有基线方法。

Conclusion: DUAL-Health框架有效解决了户外健康监测中的噪声和多模态融合问题，为动态环境下的健康监测提供了可靠解决方案。

Abstract: Outdoor health monitoring is essential to detect early abnormal health status
for safeguarding human health and safety. Conventional outdoor monitoring
relies on static multimodal deep learning frameworks, which requires extensive
data training from scratch and fails to capture subtle health status changes.
Multimodal large language models (MLLMs) emerge as a promising alternative,
utilizing only small datasets to fine-tune pre-trained information-rich models
for enabling powerful health status monitoring. Unfortunately, MLLM-based
outdoor health monitoring also faces significant challenges: I) sensor data
contains input noise stemming from sensor data acquisition and fluctuation
noise caused by sudden changes in physiological signals due to dynamic outdoor
environments, thus degrading the training performance; ii) current transformer
based MLLMs struggle to achieve robust multimodal fusion, as they lack a design
for fusing the noisy modality; iii) modalities with varying noise levels hinder
accurate recovery of missing data from fluctuating distributions. To combat
these challenges, we propose an uncertainty-aware multimodal fusion framework,
named DUAL-Health, for outdoor health monitoring in dynamic and noisy
environments. First, to assess the impact of noise, we accurately quantify
modality uncertainty caused by input and fluctuation noise with current and
temporal features. Second, to empower efficient muitimodal fusion with
low-quality modalities,we customize the fusion weight for each modality based
on quantified and calibrated uncertainty. Third, to enhance data recovery from
fluctuating noisy modalities, we align modality distributions within a common
semantic space. Extensive experiments demonstrate that our DUAL-Health
outperforms state-of-the-art baselines in detection accuracy and robustness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Topos Theory for Generative AI and LLMs](https://arxiv.org/abs/2508.08293)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了一种基于拓扑理论的新型分类生成AI架构（GAIAs），利用范畴论中的通用构造设计新的LLM架构。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究多关注线性架构或专家混合模型，而本文探索LLM范畴的拓扑性质，以构建更丰富的组合结构。

Method: 利用范畴论中的通用性质（如拉回、推出、指数对象等）设计新的LLM架构，并通过理论验证LLM范畴的完备性和拓扑性质。

Result: 证明了LLM范畴是完备的，并形成一个拓扑（类似集合的范畴），为新型架构提供了理论基础。

Conclusion: 通过范畴论的工具，为LLM设计提供了新的理论框架和实现可能性。

Abstract: We propose the design of novel categorical generative AI architectures
(GAIAs) using topos theory, a type of category that is ``set-like": a topos has
all (co)limits, is Cartesian closed, and has a subobject classifier. Previous
theoretical results on the Transformer model have shown that it is a universal
sequence-to-sequence function approximator, and dense in the space of all
continuous functions with compact support on the Euclidean space of embeddings
of tokens. Building on this theoretical result, we explore novel architectures
for LLMs that exploit the property that the category of LLMs, viewed as
functions, forms a topos. Previous studies of large language models (LLMs) have
focused on daisy-chained linear architectures or mixture-of-experts. In this
paper, we use universal constructions in category theory to construct novel LLM
architectures based on new types of compositional structures. In particular,
these new compositional structures are derived from universal properties of LLM
categories, and include pullback, pushout, (co) equalizers, exponential
objects, and subobject classifiers. We theoretically validate these new
compositional structures by showing that the category of LLMs is (co)complete,
meaning that all diagrams have solutions in the form of (co)limits. Building on
this completeness result, we then show that the category of LLMs forms a topos,
a ``set-like" category, which requires showing the existence of exponential
objects as well as subobject classifiers. We use a functorial characterization
of backpropagation to define a potential implementation of an LLM topos
architecture.

</details>


### [8] [Topos Causal Models](https://arxiv.org/abs/2508.08295)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 论文提出了一种新型因果模型——拓扑因果模型（TCMs），利用拓扑范畴的关键性质（如完备性、子对象分类器和指数对象）解决因果推断中的问题。


<details>
  <summary>Details</summary>
Motivation: 旨在展示拓扑范畴的性质（如完备性和子对象分类器）在因果推断中的核心作用，例如通过子对象分类器实现因果干预的范畴化表述。

Method: 定义TCMs为一组函数，每个函数定义一个局部自治的因果机制，组合后生成从外生变量到内生变量的全局函数。证明TCMs范畴的完备性，并利用极限和余极限解决复杂因果图。

Result: 证明TCMs范畴的完备性，展示如何通过极限和余极限近似任意因果模型，并利用子对象分类器建模因果干预。

Conclusion: TCMs为因果推断提供了一种基于拓扑范畴的通用框架，支持内部逻辑和因果等价类的推理。

Abstract: We propose topos causal models (TCMs), a novel class of causal models that
exploit the key properties of a topos category: they are (co)complete, meaning
all (co)limits exist, they admit a subobject classifier, and allow exponential
objects. The main goal of this paper is to show that these properties are
central to many applications in causal inference. For example, subobject
classifiers allow a categorical formulation of causal intervention, which
creates sub-models. Limits and colimits allow causal diagrams of arbitrary
complexity to be ``solved", using a novel interpretation of causal
approximation. Exponential objects enable reasoning about equivalence classes
of operations on causal models, such as covered edge reversal and causal
homotopy. Analogous to structural causal models (SCMs), TCMs are defined by a
collection of functions, each defining a ``local autonomous" causal mechanism
that assemble to induce a unique global function from exogenous to endogenous
variables. Since the category of TCMs is (co)complete, which we prove in this
paper, every causal diagram has a ``solution" in the form of a (co)limit: this
implies that any arbitrary causal model can be ``approximated" by some global
function with respect to the morphisms going into or out of the diagram.
Natural transformations are crucial in measuring the quality of approximation.
In addition, we show that causal interventions are modeled by subobject
classifiers: any sub-model is defined by a monic arrow into its parent model.
Exponential objects permit reasoning about entire classes of causal
equivalences and interventions. Finally, as TCMs form a topos, they admit an
internal logic defined as a Mitchell-Benabou language with an associated
Kripke-Joyal semantics. We show how to reason about causal models in TCMs using
this internal logic.

</details>


### [9] [An Efficient Application of Goal Programming to Tackle Multiobjective Problems with Recurring Fitness Landscapes](https://arxiv.org/abs/2508.08297)
*Rodrigo Lankaites Pinheiro,Dario Landa-Silva,Wasakorn Laesanklang,Ademir Aparecido Constantino*

Main category: cs.AI

TL;DR: 提出一种结合多目标算法和目标规划的方法，利用问题实例间的相似性，快速获得高质量解。


<details>
  <summary>Details</summary>
Motivation: 解决高度约束多目标问题中难以获得良好近似解集的挑战，利用问题实例间相似性提高效率。

Method: 先通过计算密集型多目标算法解决一个实例，再用目标规划和高效单目标算法解决其他实例。

Result: 在多目标车辆路径问题基准测试中，该方法在短时间内获得良好结果。

Conclusion: 该方法有效结合多目标算法和目标规划的优势，适用于具有相似适应度景观的问题场景。

Abstract: Many real-world applications require decision-makers to assess the quality of
solutions while considering multiple conflicting objectives. Obtaining good
approximation sets for highly constrained many-objective problems is often a
difficult task even for modern multiobjective algorithms. In some cases,
multiple instances of the problem scenario present similarities in their
fitness landscapes. That is, there are recurring features in the fitness
landscapes when searching for solutions to different problem instances. We
propose a methodology to exploit this characteristic by solving one instance of
a given problem scenario using computationally expensive multiobjective
algorithms to obtain a good approximation set and then using Goal Programming
with efficient single-objective algorithms to solve other instances of the same
problem scenario. We use three goal-based objective functions and show that on
benchmark instances of the multiobjective vehicle routing problem with time
windows, the methodology is able to produce good results in short computation
time. The methodology allows to combine the effectiveness of state-of-the-art
multiobjective algorithms with the efficiency of goal programming to find good
compromise solutions in problem scenarios where instances have similar fitness
landscapes.

</details>


### [10] [LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models](https://arxiv.org/abs/2508.08300)
*Yongchao Huang*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLM）自动化贝叶斯推断流程的可行性，提出了LLM-BI框架，并通过实验验证了LLM在指定先验分布和模型结构方面的能力。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断的先验分布和似然函数设定需要专业知识，限制了其广泛应用。研究旨在通过LLM自动化这一过程。

Method: 提出LLM-BI框架，通过两个实验（贝叶斯线性回归）验证LLM在指定先验和完整模型结构的能力。

Result: 实验表明LLM能成功从自然语言中提取先验分布，并基于问题描述指定完整模型结构。

Conclusion: LLM有潜力自动化贝叶斯建模关键步骤，为概率编程提供自动化推断流程。

Abstract: A significant barrier to the widespread adoption of Bayesian inference is the
specification of prior distributions and likelihoods, which often requires
specialized statistical expertise. This paper investigates the feasibility of
using a Large Language Model (LLM) to automate this process. We introduce
LLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline
for automating Bayesian workflows. As a proof-of-concept, we present two
experiments focused on Bayesian linear regression. In Experiment I, we
demonstrate that an LLM can successfully elicit prior distributions from
natural language. In Experiment II, we show that an LLM can specify the entire
model structure, including both priors and the likelihood, from a single
high-level problem description. Our results validate the potential of LLMs to
automate key steps in Bayesian modeling, enabling the possibility of an
automated inference pipeline for probabilistic programming.

</details>


### [11] [First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models](https://arxiv.org/abs/2508.08308)
*Chuanruo Fu,Yuncheng Du*

Main category: cs.AI

TL;DR: FATA是一种新的交互范式，通过引导LLMs主动生成多维补充问题，显著提升回答质量和相关性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在用户信息不完整或模糊时回答不准确的问题。

Method: 提出FATA范式，通过提示词引导LLMs生成补充问题，整合用户反馈后生成回答。

Result: FATA在综合指标上优于基线提示40%，且稳定性更高。

Conclusion: FATA通过用户参与和单轮策略，显著提升了LLMs的回答效果。

Abstract: Large Language Models (LLMs) often struggle to deliver accurate and
actionable answers when user-provided information is incomplete or
ill-specified. We propose a new interaction paradigm, First Ask Then Answer
(FATA), in which, through prompt words, LLMs are guided to proactively generate
multidimensional supplementary questions for users prior to response
generation. Subsequently, by integrating user-provided supplementary
information with the original query through sophisticated prompting techniques,
we achieve substantially improved response quality and relevance. In contrast
to existing clarification approaches -- such as the CLAM framework oriented to
ambiguity and the self-interrogation Self-Ask method -- FATA emphasizes
completeness (beyond mere disambiguation) and user participation (inviting
human input instead of relying solely on model-internal reasoning). It also
adopts a single-turn strategy: all clarifying questions are produced at once,
thereby reducing dialogue length and improving efficiency. Conceptually, FATA
uses the reasoning power of LLMs to scaffold user expression, enabling
non-expert users to formulate more comprehensive and contextually relevant
queries. To evaluate FATA, we constructed a multi-domain benchmark and compared
it with two controls: a baseline prompt (B-Prompt) and a context-enhanced
expert prompt (C-Prompt). Experimental results show that FATA outperforms
B-Prompt by approximately 40% in aggregate metrics and exhibits a coefficient
of variation 8% lower than C-Prompt, indicating superior stability.

</details>


### [12] [What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge](https://arxiv.org/abs/2508.08344)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Yuan He,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 论文提出了一种评估KG-RAG方法的新方法，揭示了现有方法在知识缺失时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法区分模型是推理还是直接检索答案，且评估指标不一致。

Method: 提出了一种构建基准和评估协议的方法，系统评估KG-RAG在知识缺失下的表现。

Result: 当前KG-RAG方法在知识缺失时推理能力有限，依赖内部记忆，泛化能力因设计而异。

Conclusion: 需要改进KG-RAG方法以提升其在知识缺失时的推理能力。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an
increasingly explored approach for combining the reasoning capabilities of
large language models with the structured evidence of knowledge graphs.
However, current evaluation practices fall short: existing benchmarks often
include questions that can be directly answered using existing triples in KG,
making it unclear whether models perform reasoning or simply retrieve answers
directly. Moreover, inconsistent evaluation metrics and lenient answer matching
criteria further obscure meaningful comparisons. In this work, we introduce a
general method for constructing benchmarks, together with an evaluation
protocol, to systematically assess KG-RAG methods under knowledge
incompleteness. Our empirical results show that current KG-RAG methods have
limited reasoning ability under missing knowledge, often rely on internal
memorization, and exhibit varying degrees of generalization depending on their
design.

</details>


### [13] [UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games](https://arxiv.org/abs/2508.08382)
*Timo Bertram*

Main category: cs.AI

TL;DR: UrzaGPT是一个基于大型语言模型（LLM）的AI，用于实时推荐《魔法风云会》的卡牌选择决策。通过低秩适应（LoRA）微调，它在小模型上实现了66.2%的准确率，展示了LLM在卡牌游戏中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于收集式卡牌游戏（CCG）的部分可观察性、长期决策和卡组变化，现有AI在卡牌选择和游戏玩法上表现远不如人类。

Method: 从开源LLM出发，使用低秩适应（LoRA）对标注的卡牌选择日志进行微调，以快速适应游戏的不同扩展版本。

Result: UrzaGPT在小模型上实现了66.2%的准确率，而未经调优的小模型完全无法完成任务，GPT-4o的零样本性能为43%。

Conclusion: 尽管未达到领域专用模型的水平，但证明了仅使用LLM实现卡牌选择AI的可行性，未来LLM可能成为高效、通用且易于更新的卡牌选择AI的基础。

Abstract: Collectible card games (CCGs) are a difficult genre for AI due to their
partial observability, long-term decision-making, and evolving card sets. Due
to this, current AI models perform vastly worse than human players at CCG tasks
such as deckbuilding and gameplay. In this work, we introduce UrzaGPT, a
domain-adapted large language model that recommends real-time drafting
decisions in Magic: The Gathering. Starting from an open-weight LLM, we use
Low-Rank Adaptation fine-tuning on a dataset of annotated draft logs. With
this, we leverage the language modeling capabilities of LLM, and can quickly
adapt to different expansions of the game. We benchmark UrzaGPT in comparison
to zero-shot LLMs and the state-of-the-art domain-specific model. Untuned,
small LLMs like Llama-3-8B are completely unable to draft, but the larger
GPT-4o achieves a zero-shot performance of 43%. Using UrzaGPT to fine-tune
smaller models, we achieve an accuracy of 66.2% using only 10,000 steps.
Despite this not reaching the capability of domain-specific models, we show
that solely using LLMs to draft is possible and conclude that using LLMs can
enable performant, general, and update-friendly drafting AIs in the future.

</details>


### [14] [Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning](https://arxiv.org/abs/2508.08385)
*Masataro Asai*

Main category: cs.AI

TL;DR: 提出了一种改进的MCTS方法，通过双层修改和树折叠技术，显著降低了节点选择的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: MCTS在经典规划中节点选择的时间复杂度较高，影响了搜索效率，而传统游戏树搜索中这一问题不显著。

Method: 采用双层MCTS，从每个选定的叶节点运行最佳优先搜索，并结合树折叠技术减少动作选择步骤。

Result: 实现了节点选择的O(1)时间复杂度，性能显著提升。

Conclusion: 该方法有效解决了MCTS在经典规划中的性能瓶颈。

Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based
Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is
that it spends a significant time deciding which node to expand next. While
selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity
with traditional array-based priority-queues for dense integer keys, the
tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly
corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily
large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node
selection is significant, unlike in game tree search, where the cost is
negligible compared to the node evaluation (rollouts) because $d$ is inherently
limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we
propose a bilevel modification to MCTS that runs a best-first search from each
selected leaf node with an expansion budget proportional to $d$, which achieves
amortized $O(1)$ runtime for node selection, equivalent to the traditional
queue-based OPEN list. In addition, we introduce Tree Collapsing, an
enhancement that reduces action selection steps and further improves the
performance.

</details>


### [15] [Solver-Aided Expansion of Loops to Avoid Generate-and-Test](https://arxiv.org/abs/2508.08442)
*Niklas Dewally,Özgür Akgün*

Main category: cs.AI

TL;DR: 提出了一种避免完全枚举的方法，通过求解器仅计算生成最终约束所需的组合，显著提高了编译效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在编译约束建模语言时，会生成所有可能的组合并通过部分求值丢弃无效项，这在大多数组合无关的问题中效率低下。

Method: 使用求解器仅计算生成最终约束所需的组合，避免完全枚举。

Result: 生成的模型与传统方法相同，但编译速度显著提升。

Conclusion: 该方法提高了将高级用户模型转换为求解器就绪形式的效率，尤其适用于大域和选择性前提条件的情况。

Abstract: Constraint modelling languages like MiniZinc and Essence rely on unrolling
loops (in the form of quantified expressions and comprehensions) during
compilation. Standard approaches generate all combinations of induction
variables and use partial evaluation to discard those that simplify to identity
elements of associative-commutative operators (e.g. true for conjunction, 0 for
summation). This can be inefficient for problems where most combinations are
ultimately irrelevant. We present a method that avoids full enumeration by
using a solver to compute only the combinations required to generate the final
set of constraints. The resulting model is identical to that produced by
conventional flattening, but compilation can be significantly faster. This
improves the efficiency of translating high-level user models into solver-ready
form, particularly when induction variables range over large domains with
selective preconditions.

</details>


### [16] [OverFill: Two-Stage Models for Efficient Language Model Decoding](https://arxiv.org/abs/2508.08446)
*Woojeong Kim,Junxiong Wang,Jing Nathan Yan,Mohamed Abdelfattah,Alexander M. Rush*

Main category: cs.AI

TL;DR: OverFill通过解耦LLM推理的prefill和decode阶段，优化了准确性与效率的权衡，显著提升了生成质量并减少了训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中高成本问题，特别是decode阶段对长序列的延迟影响。

Method: 提出OverFill，将prefill和decode阶段分离，prefill使用完整模型并行处理输入，decode切换到剪枝模型顺序生成token。

Result: 3B-to-1B配置优于1B剪枝模型83.2%，8B-to-3B配置优于3B剪枝模型79.2%，且性能与从头训练的同规模模型相当。

Conclusion: OverFill在减少训练数据的同时，显著提升了LLM推理的效率和生成质量。

Abstract: Large language models (LLMs) excel across diverse tasks but face significant
deployment challenges due to high inference costs. LLM inference comprises
prefill (compute-bound) and decode (memory-bound) stages, with decode
dominating latency particularly for long sequences. Current decoder-only models
handle both stages uniformly, despite their distinct computational profiles. We
propose OverFill, which decouples these stages to optimize accuracy-efficiency
tradeoffs. OverFill begins with a full model for prefill, processing system and
user inputs in parallel. It then switches to a dense pruned model, while
generating tokens sequentially. Leveraging more compute during prefill,
OverFill improves generation quality with minimal latency overhead. Our
3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while
the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average
across standard benchmarks. OverFill matches the performance of same-sized
models trained from scratch, while using significantly less training data. Our
code is available at https://github.com/friendshipkim/overfill.

</details>


### [17] [A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search](https://arxiv.org/abs/2508.08477)
*Joan Salvà Soler,Grégoire de Lambertye*

Main category: cs.AI

TL;DR: 本文提出了一种基于GRASP的元启发式算法，用于解决动态弧成本变化的TA-TSP问题，结合了多种构造启发式和多邻域局部搜索，在MESS 2024竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: TA-TSP扩展了经典TSP，引入了动态弧成本变化，适用于仓库操作等场景，需要高效算法解决实时路由问题。

Method: 采用GRASP元启发式，结合MIP技术将TA-TSP转化为定制TSP实例，并使用2-Opt、Swap和Relocate算子进行局部搜索。

Result: 在MESS 2024竞赛实例中，平均最优性差距为0.77%和0.40%；在合成数据集上，比Gurobi求解器优11.3%。

Conclusion: 该算法在MESS 2024中排名前三，适用于具有状态依赖旅行成本的实时路由应用。

Abstract: The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP
by introducing dynamic arc costs that change when specific \textit{trigger}
arcs are traversed, modeling scenarios such as warehouse operations with
compactable storage systems. This paper introduces a GRASP-based metaheuristic
that combines multiple construction heuristics with a multi-neighborhood local
search. The construction phase uses mixed-integer programming (MIP) techniques
to transform the TA-TSP into a sequence of tailored TSP instances, while the
improvement phase applies 2-Opt, Swap, and Relocate operators. Computational
experiments on MESS 2024 competition instances achieved average optimality gaps
of 0.77\% and 0.40\% relative to the best-known solutions within a 60-second
limit. On smaller, synthetically generated datasets, the method produced
solutions 11.3\% better than the Gurobi solver under the same time constraints.
The algorithm finished in the top three at MESS 2024, demonstrating its
suitability for real-time routing applications with state-dependent travel
costs.

</details>


### [18] [Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback](https://arxiv.org/abs/2508.08486)
*Parker Whitfill,Stewy Slocum*

Main category: cs.AI

TL;DR: 论文指出基于序数比较的LLM对齐方法存在根本性限制，无法系统性地恢复最优模型。提出通过基数反馈（如支付意愿）收集数据，实验证明该方法优于传统序数方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐技术依赖于序数偏好数据，但这类数据无法解决模型间的权衡问题，限制了最优模型的恢复。

Method: 通过收集25,000条基数判断数据（支付意愿法），并将其融入偏好微调中，以解决序数数据的局限性。

Result: 实验表明，引入基数反馈的模型能更高效地优化高影响力改进，并在下游基准（如Arena-Hard）上优于仅依赖序数的方法。

Conclusion: 基数反馈是解决LLM对齐中序数数据局限性的有效方法，显著提升了模型性能。

Abstract: Alignment techniques for LLMs rely on optimizing preference-based objectives
-- where these preferences are typically elicited as ordinal, binary choices
between responses. Recent work has focused on improving label quality or
mitigating particular biases, but we identify a more fundamental limitation:
these methods collect the wrong kind of data. We prove an impossibility result:
no algorithm relying solely on ordinal comparisons can systematically recover
the most preferred model. Intuitively, ordinal data lacks the information
needed to resolve tradeoffs -- e.g., fixing a factual error on one prompt
versus improving style on another. We show that selecting the optimal model
requires recovering preferences over \emph{models} (rather than just
responses), which can only be identified given cardinal feedback about response
quality. To address this, we collect and publicly release a dataset of 25,000
cardinal judgments using willingness-to-pay elicitations, a well-established
tool from experimental economics. Empirically, we find that incorporating
cardinal feedback into preference fine-tuning allows models to prioritize
high-impact improvements and outperform ordinal-only methods on downstream
benchmarks, such as Arena-Hard.

</details>


### [19] [POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2508.08493)
*Szymon Jakubicz,Karol Kuźniak,Jan Wawszczak,Paweł Gora*

Main category: cs.AI

TL;DR: 本文改进了POMO方法，提出POMO+，通过更智能地利用初始节点，在组合优化问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管POMO在组合优化问题上表现良好，但仍有改进空间。

Method: 改进POMO为POMO+，利用初始节点更智能地寻找解。

Result: 在CVRPLIB数据集上验证，POMO+收敛更快且结果更优，适用于最多100个客户的问题实例。

Conclusion: POMO+为组合优化领域提供了进一步改进的方向。

Abstract: In recent years, reinforcement learning (RL) methods have emerged as a
promising approach for solving combinatorial problems. Among RL-based models,
POMO has demonstrated strong performance on a variety of tasks, including
variants of the Vehicle Routing Problem (VRP). However, there is room for
improvement for these tasks. In this work, we improved POMO, creating a method
(\textbf{POMO+}) that leverages the initial nodes to find a solution in a more
informed way. We ran experiments on our new model and observed that our
solution converges faster and achieves better results. We validated our models
on the CVRPLIB dataset and noticed improvements in problem instances with up to
100 customers. We hope that our research in this project can lead to further
advancements in the field.

</details>


### [20] [Large Language Models as Oracles for Ontology Alignment](https://arxiv.org/abs/2508.08500)
*Sviatoslav Lushnei,Dmytro Shumskyi,Severyn Shykula,Ernesto Jimenez-Ruiz,Artur d'Avila Garcez*

Main category: cs.AI

TL;DR: 探讨使用大型语言模型（LLM）替代领域专家验证本体对齐中的不确定对应关系，并通过实验评估其在多个任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 本体对齐在跨领域数据整合中至关重要，但高质量对应关系的生成仍具挑战性，人工参与成本高昂。

Method: 利用LLM仅验证本体对齐系统中不确定的对应关系，并通过OAEI任务评估不同LLM的性能。

Result: 实验分析了多种LLM的表现，并与模拟Oracle的误差率进行了比较。

Conclusion: LLM可作为替代方案，验证不确定的对应关系，但仍需进一步优化。

Abstract: Ontology alignment plays a crucial role in integrating diverse data sources
across domains. There is a large plethora of systems that tackle the ontology
alignment problem, yet challenges persist in producing highly quality
correspondences among a set of input ontologies. Human-in-the-loop during the
alignment process is essential in applications requiring very accurate
mappings. User involvement is, however, expensive when dealing with large
ontologies. In this paper, we explore the feasibility of using Large Language
Models (LLM) as an alternative to the domain expert. The use of the LLM focuses
only on the validation of the subset of correspondences where an ontology
alignment system is very uncertain. We have conducted an extensive evaluation
over several matching tasks of the Ontology Alignment Evaluation Initiative
(OAEI), analysing the performance of several state-of-the-art LLMs using
different ontology-driven prompt templates. The LLM results are also compared
against simulated Oracles with variable error rates.

</details>


### [21] [GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games](https://arxiv.org/abs/2508.08501)
*Yuchen Li,Cong Lin,Muhammad Umair Nasir,Philip Bontrager,Jialin Liu,Julian Togelius*

Main category: cs.AI

TL;DR: GVGAI-LLM是一个基于视频游戏的基准测试，用于评估大型语言模型（LLMs）的推理和问题解决能力。它通过多样化的游戏设计测试模型的空间推理和规划能力，揭示了LLMs的局限性，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试大多集中在语言任务上，缺乏对空间推理和规划能力的评估。GVGAI-LLM旨在填补这一空白，提供一个多样化的测试环境。

Method: 基于General Video Game AI框架，使用游戏描述语言快速生成新游戏和关卡，避免过拟合。游戏场景用ASCII字符表示，便于语言模型处理。定义了可解释的评估指标（如有效步骤比、步骤效率等）。

Result: 零样本评估显示，LLMs在空间推理和基础规划方面存在明显不足，常犯空间和逻辑错误。结构化提示和空间接地技术带来部分改进，但问题仍未解决。

Conclusion: GVGAI-LLM为语言模型能力研究提供了可复现的测试平台，特别关注代理行为和上下文推理，推动了相关领域的进展。

Abstract: We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning
and problem-solving capabilities of large language models (LLMs). Built on the
General Video Game AI framework, it features a diverse collection of
arcade-style games designed to test a model's ability to handle tasks that
differ from most existing LLM benchmarks. The benchmark leverages a game
description language that enables rapid creation of new games and levels,
helping to prevent overfitting over time. Each game scene is represented by a
compact set of ASCII characters, allowing for efficient processing by language
models. GVGAI-LLM defines interpretable metrics, including the meaningful step
ratio, step efficiency, and overall score, to assess model behavior. Through
zero-shot evaluations across a broad set of games and levels with diverse
challenges and skill depth, we reveal persistent limitations of LLMs in spatial
reasoning and basic planning. Current models consistently exhibit spatial and
logical errors, motivating structured prompting and spatial grounding
techniques. While these interventions lead to partial improvements, the
benchmark remains very far from solved. GVGAI-LLM provides a reproducible
testbed for advancing research on language model capabilities, with a
particular emphasis on agentic behavior and contextual reasoning.

</details>


### [22] [SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering](https://arxiv.org/abs/2508.08529)
*Arshia Ilaty,Hossein Shirazi,Hajar Homayouni*

Main category: cs.AI

TL;DR: SynLLM是一个模块化框架，利用20种开源LLM生成高质量合成医疗表格数据，通过结构化提示和全面评估框架解决隐私和临床有效性问题。


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据因隐私法规受限，合成数据成为替代方案，但现有方法缺乏系统提示策略和多维评估。

Method: SynLLM采用四种提示类型（如示例驱动和规则约束），结合20种LLM生成数据，并通过统计保真度、临床一致性和隐私保护评估。

Result: 在三个医疗数据集上测试显示，规则提示在隐私与质量间取得最佳平衡。

Conclusion: LLM在良好提示和评估下可生成临床合理且隐私保护的合成数据，推动医疗研究数据共享。

Abstract: Access to real-world medical data is often restricted due to privacy
regulations, posing a significant barrier to the advancement of healthcare
research. Synthetic data offers a promising alternative; however, generating
realistic, clinically valid, and privacy-conscious records remains a major
challenge. Recent advancements in Large Language Models (LLMs) offer new
opportunities for structured data generation; however, existing approaches
frequently lack systematic prompting strategies and comprehensive,
multi-dimensional evaluation frameworks.
  In this paper, we present SynLLM, a modular framework for generating
high-quality synthetic medical tabular data using 20 state-of-the-art
open-source LLMs, including LLaMA, Mistral, and GPT variants, guided by
structured prompts. We propose four distinct prompt types, ranging from
example-driven to rule-based constraints, that encode schema, metadata, and
domain knowledge to control generation without model fine-tuning. Our framework
features a comprehensive evaluation pipeline that rigorously assesses generated
data across statistical fidelity, clinical consistency, and privacy
preservation.
  We evaluate SynLLM across three public medical datasets, including Diabetes,
Cirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt
engineering significantly impacts data quality and privacy risk, with
rule-based prompts achieving the best privacy-quality balance. SynLLM
establishes that, when guided by well-designed prompts and evaluated with
robust, multi-metric criteria, LLMs can generate synthetic medical data that is
both clinically plausible and privacy-aware, paving the way for safer and more
effective data sharing in healthcare research.

</details>


### [23] [UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss](https://arxiv.org/abs/2508.08615)
*Zhichao Wang,Xinhai Chen,Qinglin Wang,Xiang Gao,Qingyang Zhang,Menghan Jia,Xiang Zhang,Jie Liu*

Main category: cs.AI

TL;DR: 提出了一种无监督且通用的网格移动网络（UGM2N），通过局部几何特征学习和物理约束损失函数，实现了高效的网格自适应，无需依赖预适应网格。


<details>
  <summary>Details</summary>
Motivation: 传统网格移动方法计算复杂度高且几何灵活性不足，现有监督学习方法难以实现零样本泛化。

Method: 引入无监督网格自适应和物理约束损失函数（M-Uniform loss），确保网格节点均匀分布。

Result: 网络在多样化的PDE和网格几何中表现出方程无关的泛化能力，计算效率高且无网格纠缠。

Conclusion: UGM2N在网格自适应中优于现有方法，具有广泛适用性和可扩展性。

Abstract: Partial differential equations (PDEs) form the mathematical foundation for
modeling physical systems in science and engineering, where numerical solutions
demand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address
this challenge by dynamically relocating mesh nodes to rapidly-varying regions,
enhancing both simulation accuracy and computational efficiency. However,
traditional approaches suffer from high computational complexity and geometric
inflexibility, limiting their applicability, and existing supervised
learning-based approaches face challenges in zero-shot generalization across
diverse PDEs and mesh topologies.In this paper, we present an Unsupervised and
Generalizable Mesh Movement Network (UGM2N). We first introduce unsupervised
mesh adaptation through localized geometric feature learning, eliminating the
dependency on pre-adapted meshes. We then develop a physics-constrained loss
function, M-Uniform loss, that enforces mesh equidistribution at the nodal
level.Experimental results demonstrate that the proposed network exhibits
equation-agnostic generalization and geometric independence in efficient mesh
adaptation. It demonstrates consistent superiority over existing methods,
including robust performance across diverse PDEs and mesh geometries,
scalability to multi-scale resolutions and guaranteed error reduction without
mesh tangling.

</details>


### [24] [AgriGPT: a Large Language Model Ecosystem for Agriculture](https://arxiv.org/abs/2508.08632)
*Bo Yang,Yu Zhang,Lanfei Feng,Yunkui Chen,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Yurui Li,Yuxuan Chen,Guijun Yang,Yong He,Runhe Huang,Shijian Li*

Main category: cs.AI

TL;DR: AgriGPT是一个专为农业设计的LLM生态系统，通过多代理数据引擎构建高质量数据集Agri-342K，并采用Tri-RAG框架提升推理可靠性。实验表明其在农业领域显著优于通用LLM。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在农业领域应用受限，缺乏领域专用模型、数据集和评估框架。

Method: 设计多代理数据引擎构建Agri-342K数据集，采用Tri-RAG框架（密集检索、稀疏检索和多跳知识图谱推理）增强推理。

Result: AgriGPT在领域适应和推理任务上显著优于通用LLM。

Conclusion: AgriGPT为农业提供了一个模块化、可扩展的LLM生态系统，并推动了科学和行业专用LLM的发展。

Abstract: Despite the rapid progress of Large Language Models (LLMs), their application
in agriculture remains limited due to the lack of domain-specific models,
curated datasets, and robust evaluation frameworks. To address these
challenges, we propose AgriGPT, a domain-specialized LLM ecosystem for
agricultural usage. At its core, we design a multi-agent scalable data engine
that systematically compiles credible data sources into Agri-342K, a
high-quality, standardized question-answer (QA) dataset. Trained on this
dataset, AgriGPT supports a broad range of agricultural stakeholders, from
practitioners to policy-makers. To enhance factual grounding, we employ
Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining
dense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning,
thereby improving the LLM's reasoning reliability. For comprehensive
evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks
with varying types and complexities. Experiments demonstrate that AgriGPT
significantly outperforms general-purpose LLMs on both domain adaptation and
reasoning. Beyond the model itself, AgriGPT represents a modular and extensible
LLM ecosystem for agriculture, comprising structured data construction,
retrieval-enhanced generation, and domain-specific evaluation. This work
provides a generalizable framework for developing scientific and
industry-specialized LLMs. All models, datasets, and code will be released to
empower agricultural communities, especially in underserved regions, and to
promote open, impactful research.

</details>


### [25] [Diminution: On Reducing the Size of Grounding ASP Programs](https://arxiv.org/abs/2508.08633)
*HuanYu Yang,Fengming Zhu,YangFan Wu,Jianmin Ji*

Main category: cs.AI

TL;DR: 提出了一种称为“diminution”的方法，通过选择Herbrand宇宙的子集来减少ASP中的基础程序规模，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: ASP常因基础程序规模过大而效率低下，现有方法多为启发式，缺乏通用性。

Method: 引入diminution概念，定义其形式化属性，并通过特定编码利用现有ASP求解器评估候选子集。

Result: 实验显示，该方法平均减少70%的基础时间和85%的基础文件大小。

Conclusion: diminution是一种通用且有效的方法，可显著缓解ASP中的基础瓶颈。

Abstract: Answer Set Programming (ASP) is often hindered by the grounding bottleneck:
large Herbrand universes generate ground programs so large that solving becomes
difficult. Many methods employ ad-hoc heuristics to improve grounding
performance, motivating the need for a more formal and generalizable strategy.
We introduce the notion of diminution, defined as a selected subset of the
Herbrand universe used to generate a reduced ground program before solving. We
give a formal definition of diminution, analyze its key properties, and study
the complexity of identifying it. We use a specific encoding that enables
off-the-shelf ASP solver to evaluate candidate subsets. Our approach integrates
seamlessly with existing grounders via domain predicates. In extensive
experiments on five benchmarks, applying diminutions selected by our strategy
yields significant performance improvements, reducing grounding time by up to
70% on average and decreasing the size of grounding files by up to 85%. These
results demonstrate that leveraging diminutions constitutes a robust and
general-purpose approach for alleviating the grounding bottleneck in ASP.

</details>


### [26] [P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records](https://arxiv.org/abs/2508.08646)
*Naama Kashani,Mira Cohen,Uri Shaham*

Main category: cs.AI

TL;DR: 提出了一种针对电子健康记录（EHR）数据的个性化、在线且成本感知的特征选择框架，以解决传统方法在处理稀疏和异构数据时的不足。


<details>
  <summary>Details</summary>
Motivation: EHR数据复杂且多模态，传统特征选择方法难以应对其稀疏性和异质性，尤其是在考虑患者个体差异和特征成本时。

Method: 开发了一种在线获取特征的框架，结合预算约束和特征可变性成本，为个体患者个性化选择特征。

Result: 该框架能有效管理稀疏和多模态数据，在多样化医疗场景中表现出稳健且可扩展的性能。

Conclusion: 该方法支持医生在预算约束下逐步获取最有信息量的特征，提升诊断信心并优化资源利用。

Abstract: Electronic Health Records (EHR) have revolutionized healthcare by digitizing
patient data, improving accessibility, and streamlining clinical workflows.
However, extracting meaningful insights from these complex and multimodal
datasets remains a significant challenge for researchers. Traditional feature
selection methods often struggle with the inherent sparsity and heterogeneity
of EHR data, especially when accounting for patient-specific variations and
feature costs in clinical applications. To address these challenges, we propose
a novel personalized, online and cost-aware feature selection framework
tailored specifically for EHR datasets. The features are aquired in an online
fashion for individual patients, incorporating budgetary constraints and
feature variability costs. The framework is designed to effectively manage
sparse and multimodal data, ensuring robust and scalable performance in diverse
healthcare contexts. A primary application of our proposed method is to support
physicians' decision making in patient screening scenarios. By guiding
physicians toward incremental acquisition of the most informative features
within budget constraints, our approach aims to increase diagnostic confidence
while optimizing resource utilization.

</details>


### [27] [Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training](https://arxiv.org/abs/2508.08652)
*Vishakha Lall,Yisi Liu*

Main category: cs.AI

TL;DR: 本文提出了一种基于提示推理的轻量级方法（Prompt-and-Check），利用开源大语言模型（LLMs）评估程序性通信合规性，无需任务特定训练，适用于模拟训练环境。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域的模拟训练中，准确评估程序性通信合规性至关重要，但传统方法可能复杂且资源密集。

Method: 使用上下文丰富的提示，结合开源LLMs（如LLaMA 2 7B、LLaMA 3 8B和Mistral 7B），基于转录的语音交换评估协议清单的合规性。

Result: 实验表明，提示方法能够实现有效的上下文感知推理，分类准确性和一致性评分接近专家标注的真实数据。

Conclusion: LLMs在增强训练环境中的反馈和自动化评估方面具有实际应用价值。

Abstract: Accurate evaluation of procedural communication compliance is essential in
simulation-based training, particularly in safety-critical domains where
adherence to compliance checklists reflects operational competence. This paper
explores a lightweight, deployable approach using prompt-based inference with
open-source large language models (LLMs) that can run efficiently on
consumer-grade GPUs. We present Prompt-and-Check, a method that uses
context-rich prompts to evaluate whether each checklist item in a protocol has
been fulfilled, solely based on transcribed verbal exchanges. We perform a case
study in the maritime domain with participants performing an identical
simulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and
Mistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a
prompt incorporating relevant transcript excerpts is fed into the model, which
outputs a compliance judgment. We assess model outputs against expert-annotated
ground truth using classification accuracy and agreement scores. Our findings
demonstrate that prompting enables effective context-aware reasoning without
task-specific training. This study highlights the practical utility of LLMs in
augmenting debriefing, performance feedback, and automated assessment in
training environments.

</details>


### [28] [Hybrid Node-Destroyer Model with Large Neighborhood Search for Solving the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2508.08659)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.AI

TL;DR: 提出了一种迭代学习混合优化求解器，结合Node-Destroyer模型和GNNs，提升元启发式算法在CVRP中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决元启发式算法在CVRP中的性能不足问题，通过机器学习模型优化节点选择。

Method: 结合Node-Destroyer模型（基于GNNs）和LNS算子，利用图结构特性指导节点移除策略。

Result: 提升了基准算法的性能，适用于大规模实例（30,000节点），无需针对不同规模问题重新训练。

Conclusion: 混合机制显著提高了CVRP的求解质量和可扩展性。

Abstract: In this research, we propose an iterative learning hybrid optimization solver
developed to strengthen the performance of metaheuristic algorithms in solving
the Capacitated Vehicle Routing Problem (CVRP). The iterative hybrid mechanism
integrates the proposed Node-Destroyer Model, a machine learning hybrid model
that utilized Graph Neural Networks (GNNs) such identifies and selects customer
nodes to guide the Large Neighborhood Search (LNS) operator within the
metaheuristic optimization frameworks. This model leverages the structural
properties of the problem and solution that can be represented as a graph, to
guide strategic selections concerning node removal. The proposed approach
reduces operational complexity and scales down the search space involved in the
optimization process. The hybrid approach is applied specifically to the CVRP
and does not require retraining across problem instances of different sizes.
The proposed hybrid mechanism is able to improve the performance of baseline
metaheuristic algorithms. Our approach not only enhances the solution quality
for standard CVRP benchmarks but also proves scalability on very large-scale
instances with up to 30,000 customer nodes. Experimental evaluations on
benchmark datasets show that the proposed hybrid mechanism is capable of
improving different baseline algorithms, achieving better quality of solutions
under similar settings.

</details>


### [29] [Aryabhata: An exam-focused language model for JEE Math](https://arxiv.org/abs/2508.08665)
*Ritvik Rastogi,Sachin Dharashivkar,Sandeep Varma*

Main category: cs.AI

TL;DR: Aryabhata 1.0是一个7B参数的数学推理模型，专为印度JEE考试优化，结合了监督微调和强化学习，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育领域适用性不足，Aryabhata旨在提供更适合教育用途的小型开源模型。

Method: 通过合并开源推理模型，进行监督微调（SFT）和强化学习（RLVR），并结合新颖的探索策略。

Result: 在JEE Main 2025和MATH、GSM8K等基准测试中表现优异，准确率和效率均领先。

Conclusion: Aryabhata 1.0作为开源基础模型，旨在推动教育领域的小型语言模型发展。

Abstract: We present Aryabhata 1.0, a compact 7B parameter math reasoning model
optimized for the Indian academic exam, the Joint Entrance Examination (JEE).
Despite rapid progress in large language models (LLMs), current models often
remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong
open-weight reasoning models, followed by supervised fine-tuning (SFT) with
curriculum learning on verified chain-of-thought (CoT) traces curated through
best-of-$n$ rejection sampling. To further boost performance, we apply
reinforcement learning with verifiable rewards (RLVR) using A2C objective with
group-relative advantage estimation along with novel exploration strategies
such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both
in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K)
benchmarks, Aryabhata outperforms existing models in accuracy and efficiency,
while offering pedagogically useful step-by-step reasoning. We release
Aryabhata as a foundation model to advance exam-centric, open-source small
language models. This marks our first open release for community feedback
(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0); PW is actively training
future models to further improve learning outcomes for students.

</details>


### [30] [STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision](https://arxiv.org/abs/2508.08688)
*Chen Li,Han Zhang,Zhantao Yang,Fangyi Chen,Zihan Wang,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: STELAR-Vision框架通过TopoAug数据增强和Frugal Learning方法，提升了多模态任务的准确性和效率，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂多模态任务中表现不佳，且输出冗长，依赖链式推理（CoT），而其他拓扑结构可能更有效。

Method: 提出STELAR-Vision框架，结合TopoAug数据增强和Frugal Learning方法，通过监督微调和强化学习优化模型。

Result: 在MATH-V和VLM-S2H上准确率提升9.7%，优于Qwen2VL-72B-Instruct 7.3%，在OOD基准测试中表现优异。

Conclusion: STELAR-Vision通过拓扑感知推理显著提升模型性能，具有强泛化能力，代码和数据集已开源。

Abstract: Vision-language models (VLMs) have made significant strides in reasoning, yet
they often struggle with complex multimodal tasks and tend to generate overly
verbose outputs. A key limitation is their reliance on chain-of-thought (CoT)
reasoning, despite many tasks benefiting from alternative topologies like trees
or graphs. To address this, we introduce STELAR-Vision, a training framework
for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline
that enriches training with diverse topological structures. Using supervised
fine-tuning and reinforcement learning, we post-train Qwen2VL models with both
accuracy and efficiency in mind. Additionally, we propose Frugal Learning,
which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H,
STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the
larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it
outperforms Phi-4-Multimodal-Instruct by up to 28.4% and
LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong
generalization. Compared to Chain-Only training, our approach achieves 4.3%
higher overall accuracy on in-distribution datasets and consistently
outperforms across all OOD benchmarks. We have released datasets, and code will
be available.

</details>


### [31] [Simulating Generative Social Agents via Theory-Informed Workflow Design](https://arxiv.org/abs/2508.08726)
*Yuwei Yan,Jinghua Piao,Xiaochong Lan,Chenyang Shao,Pan Hui,Yong Li*

Main category: cs.AI

TL;DR: 提出了一种基于社会认知理论的统一框架，用于设计LLM社交代理，包含动机、行动规划和学习模块，显著提升了行为的真实性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有社交代理多为场景定制，缺乏通用框架，限制了其在不同社交环境中的泛化能力和行为一致性。

Method: 基于社会认知理论，提出包含动机、行动规划和学习三个模块的系统化设计框架。

Result: 实验表明，该框架代理在复杂条件下能重现真实人类行为模式，偏离真实行为数据的误差比基线低75%。

Conclusion: 动机、规划和学习模块对生成真实且一致的社交行为至关重要，移除任一模块会显著增加误差。

Abstract: Recent advances in large language models have demonstrated strong reasoning
and role-playing capabilities, opening new opportunities for agent-based social
simulations. However, most existing agents' implementations are
scenario-tailored, without a unified framework to guide the design. This lack
of a general social agent limits their ability to generalize across different
social contexts and to produce consistent, realistic behaviors. To address this
challenge, we propose a theory-informed framework that provides a systematic
design process for LLM-based social agents. Our framework is grounded in
principles from Social Cognition Theory and introduces three key modules:
motivation, action planning, and learning. These modules jointly enable agents
to reason about their goals, plan coherent actions, and adapt their behavior
over time, leading to more flexible and contextually appropriate responses.
Comprehensive experiments demonstrate that our theory-driven agents reproduce
realistic human behavior patterns under complex conditions, achieving up to 75%
lower deviation from real-world behavioral data across multiple fidelity
metrics compared to classical generative baselines. Ablation studies further
show that removing motivation, planning, or learning modules increases errors
by 1.5 to 3.2 times, confirming their distinct and essential contributions to
generating realistic and coherent social behaviors.

</details>


### [32] [Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance](https://arxiv.org/abs/2508.08774)
*Dongwook Choi,Taeyoon Kwon,Dongil Yang,Hyojun Kim,Jinyoung Yeo*

Main category: cs.AI

TL;DR: 论文提出了一种基于记忆增强的AR代理框架，旨在解决现有AR系统在复杂多步骤场景中无法利用用户长期经验和偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AR代理在支持即时任务方面表现良好，但在需要理解和利用用户长期经验和偏好的复杂多步骤场景中存在局限性。

Method: 提出了一个包含四个模块的概念框架：感知模块、记忆模块、时空推理模块和执行模块。

Result: 框架通过学习和适应用户特定经验，提供个性化任务支持，并展示了实际应用的路线图和用例。

Conclusion: 该工作旨在推动未来研究，开发更智能的AR系统，将用户交互历史与自适应、上下文感知的任务支持有效结合。

Abstract: Augmented Reality (AR) systems are increasingly integrating foundation
models, such as Multimodal Large Language Models (MLLMs), to provide more
context-aware and adaptive user experiences. This integration has led to the
development of AR agents to support intelligent, goal-directed interactions in
real-world environments. While current AR agents effectively support immediate
tasks, they struggle with complex multi-step scenarios that require
understanding and leveraging user's long-term experiences and preferences. This
limitation stems from their inability to capture, retain, and reason over
historical user interactions in spatiotemporal contexts. To address these
challenges, we propose a conceptual framework for memory-augmented AR agents
that can provide personalized task assistance by learning from and adapting to
user-specific experiences over time. Our framework consists of four
interconnected modules: (1) Perception Module for multimodal sensor processing,
(2) Memory Module for persistent spatiotemporal experience storage, (3)
Spatiotemporal Reasoning Module for synthesizing past and present contexts, and
(4) Actuator Module for effective AR communication. We further present an
implementation roadmap, a future evaluation strategy, a potential target
application and use cases to demonstrate the practical applicability of our
framework across diverse domains. We aim for this work to motivate future
research toward developing more intelligent AR systems that can effectively
bridge user's interaction history with adaptive, context-aware task assistance.

</details>


### [33] [A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions](https://arxiv.org/abs/2508.08795)
*Amir Mohammad Salehoof,Ali Ramezani,Yadollah Yaghoobzadeh,Majid Nili Ahmadabadi*

Main category: cs.AI

TL;DR: 该论文提出了一种基于功能的知识编辑分类法，补充现有机制导向的研究，探讨不同知识类型的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的知识可能过时或不准确，而重新训练成本高昂，知识编辑成为一种高效替代方案。现有研究多关注编辑机制，忽略了知识功能的作用。

Method: 引入功能导向的分类法，结合知识类型（如事实性、时间性、概念性等）和编辑机制，系统分析现有方法。

Result: 通过双轴分类法（功能与机制）梳理了当前研究，总结了方法的优缺点，并提出了评估任务和数据集。

Conclusion: 知识编辑领域仍需解决开放挑战，未来需进一步探索功能与机制的协同作用。

Abstract: Large language models (LLMs) acquire vast knowledge from large text corpora,
but this information can become outdated or inaccurate. Since retraining is
computationally expensive, knowledge editing offers an efficient alternative --
modifying internal knowledge without full retraining. These methods aim to
update facts precisely while preserving the model's overall capabilities. While
existing surveys focus on the mechanism of editing (e.g., parameter changes vs.
external memory), they often overlook the function of the knowledge being
edited. This survey introduces a novel, complementary function-based taxonomy
to provide a more holistic view. We examine how different mechanisms apply to
various knowledge types -- factual, temporal, conceptual, commonsense, and
social -- highlighting how editing effectiveness depends on the nature of the
target knowledge. By organizing our review along these two axes, we map the
current landscape, outline the strengths and limitations of existing methods,
define the problem formally, survey evaluation tasks and datasets, and conclude
with open challenges and future directions.

</details>


### [34] [GRainsaCK: a Comprehensive Software Library for Benchmarking Explanations of Link Prediction Tasks on Knowledge Graphs](https://arxiv.org/abs/2508.08815)
*Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: GRainsaCK是一个可复用的软件资源，用于标准化评估知识图谱中链接预测的解释方法。


<details>
  <summary>Details</summary>
Motivation: 知识图谱常不完整，现有链接预测方法缺乏可解释性，且缺乏标准评估协议。

Method: 提出GRainsaCK，通过模块化设计实现从模型训练到解释评估的全流程标准化。

Result: GRainsaCK提供了可扩展的模块化功能和详细文档。

Conclusion: GRainsaCK填补了评估解释方法的空白，促进了可解释性研究的标准化和复用。

Abstract: Since Knowledge Graphs are often incomplete, link prediction methods are
adopted for predicting missing facts. Scalable embedding based solutions are
mostly adopted for this purpose, however, they lack comprehensibility, which
may be crucial in several domains. Explanation methods tackle this issue by
identifying supporting knowledge explaining the predicted facts. Regretfully,
evaluating/comparing quantitatively the resulting explanations is challenging
as there is no standard evaluation protocol and overall benchmarking resource.
We fill this important gap by proposing GRainsaCK, a reusable software resource
that fully streamlines all the tasks involved in benchmarking explanations,
i.e., from model training to evaluation of explanations along the same
evaluation protocol. Moreover, GRainsaCK furthers modularity/extensibility by
implementing the main components as functions that can be easily replaced.
Finally, fostering its reuse, we provide extensive documentation including a
tutorial.

</details>


### [35] [Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2508.08816)
*Yuechen Wang,Yuming Qiao,Dan Meng,Jun Yang,Haonan Lu,Zhenyu Yang,Xudong Zhang*

Main category: cs.AI

TL;DR: E-Agent框架通过动态规划多模态工具和优化执行序列，显著提升多模态检索增强生成（mRAG）性能，减少冗余搜索。


<details>
  <summary>Details</summary>
Motivation: 解决现有mRAG方法在动态规划和视觉信息利用上的不足，以适应实时场景需求。

Method: 提出E-Agent框架，包含动态规划的多模态工具协调器（mRAG planner）和任务执行器（task executor），并引入RemPlan基准测试。

Result: E-Agent在RemPlan和三个基准测试中表现优异，准确率提升13%，冗余搜索减少37%。

Conclusion: E-Agent通过动态规划和优化执行序列，显著提升了mRAG系统的效率和准确性。

Abstract: Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising
solution to address the temporal limitations of Multimodal Large Language
Models (MLLMs) in real-world scenarios like news analysis and trending topics.
However, existing approaches often suffer from rigid retrieval strategies and
under-utilization of visual information. To bridge this gap, we propose
E-Agent, an agent framework featuring two key innovations: a mRAG planner
trained to dynamically orchestrate multimodal tools based on contextual
reasoning, and a task executor employing tool-aware execution sequencing to
implement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning
strategy that enables efficient information retrieval while minimizing
redundant tool invocations. To rigorously assess the planning capabilities of
mRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark.
This novel benchmark contains both retrieval-dependent and
retrieval-independent question types, systematically annotated with essential
retrieval tools required for each instance. The benchmark's explicit mRAG
planning annotations and diverse question design enhance its practical
relevance by simulating real-world scenarios requiring dynamic mRAG decisions.
Experiments across RemPlan and three established benchmarks demonstrate
E-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods
while reducing redundant searches by 37%.

</details>


### [36] [Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition](https://arxiv.org/abs/2508.08830)
*Mustafa Akben,Vinayaka Gude,Haya Ajjan*

Main category: cs.AI

TL;DR: MLLMs在情感识别任务中表现优于人类个体，但人类集体智慧仍超越MLLMs。人机协作（增强智能）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在情感识别任务中的能力，并与人类表现对比，以推动情感智能AI的发展。

Method: 使用RMET和MRMET测试MLLMs和人类的情感识别能力，并进行个体与集体表现的比较。

Result: MLLMs在个体层面优于人类，但人类集体智慧更强。人机协作表现最佳。

Conclusion: MLLMs在情感识别中表现突出，但人类集体智慧和人机协作更具潜力，为情感智能AI的发展指明方向。

Abstract: The ability to discern subtle emotional cues is fundamental to human social
intelligence. As artificial intelligence (AI) becomes increasingly common, AI's
ability to recognize and respond to human emotions is crucial for effective
human-AI interactions. In particular, whether such systems can match or surpass
human experts remains to be seen. However, the emotional intelligence of AI,
particularly multimodal large language models (MLLMs), remains largely
unexplored. This study evaluates the emotion recognition abilities of MLLMs
using the Reading the Mind in the Eyes Test (RMET) and its multiracial
counterpart (MRMET), and compares their performance against human participants.
Results show that, on average, MLLMs outperform humans in accurately
identifying emotions across both tests. This trend persists even when comparing
performance across low, medium, and expert-level performing groups. Yet when we
aggregate independent human decisions to simulate collective intelligence,
human groups significantly surpass the performance of aggregated MLLM
predictions, highlighting the wisdom of the crowd. Moreover, a collaborative
approach (augmented intelligence) that combines human and MLLM predictions
achieves greater accuracy than either humans or MLLMs alone. These results
suggest that while MLLMs exhibit strong emotion recognition at the individual
level, the collective intelligence of humans and the synergistic potential of
human-AI collaboration offer the most promising path toward effective emotional
AI. We discuss the implications of these findings for the development of
emotionally intelligent AI systems and future research directions.

</details>


### [37] [Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation](https://arxiv.org/abs/2508.08882)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.AI

TL;DR: 当前工具集成的数学推理系统通常采用单智能体范式，导致认知负载干扰。提出双智能体混合框架，通过分工减少干扰并提升性能。


<details>
  <summary>Details</summary>
Motivation: 单智能体范式在推理和代码生成之间切换时产生认知负载干扰，影响性能。

Method: 提出双智能体框架：推理代理负责问题分解，代码代理处理代码生成与执行。训练结合模仿学习和强化学习。

Result: 双智能体框架显著减少了认知干扰，提升了推理与代码生成的协调稳定性。

Conclusion: 分工设计有效解决了单智能体的认知负载问题，提升了数学推理系统的性能。

Abstract: Current tool-integrated mathematical reasoning systems often adopt a
single-agent paradigm, where one large language model handles problem
reasoning, code generation, and code execution in an integrated workflow. While
this design eases coordination, we hypothesize that it imposes cognitive load
interference, as the agent must interleave long-horizon reasoning with precise
program synthesis. We validate this hypothesis through a controlled comparison
between a reasoning-only agent and a reasoning-plus-code agent, finding that
the latter produces significantly fewer correct reasoning paths despite having
tool-calling capabilities. To address this, we propose a dual-agent hybrid
framework: a Reasoning Agent performs stepwise problem decomposition, and a
Code Agent handles code generation and execution. Training combines imitation
learning and reinforcement learning: the Code Agent receives strong rewards for
matching intermediate ground-truth programs and weaker rewards for valid
execution, while the Reasoning Agent is optimized chiefly via final-answer
accuracy using advantage estimation to credit intermediate steps. This
decoupled role design reduces cognitive interference and promotes stable
reasoning-coding coordination.

</details>


### [38] [Compass-Thinker-7B Technical Report](https://arxiv.org/abs/2508.08909)
*Anxiang Zeng,Haibo Zhang,Kaixiang Mo,Long Zhang,Shuman Liu,Yanhui Huang,Yawen Liu,Yuepeng Sheng,Yuwei Huang*

Main category: cs.AI

TL;DR: Compass-Thinker-7B模型通过强化学习探索大规模语言模型的推理潜力，减少计算资源需求，并在数学推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 直接在大规模模型上进行强化学习实验成本高且风险大，因此研究如何在较少资源下探索强化学习潜力。

Method: 通过专门设计的强化学习流程训练开源模型，使用30k可验证数学问题数据集，分阶段配置数据和训练设置。

Result: Compass-Thinker-7B在数学推理任务中表现优异，尤其在AIME2024评估中达到40%准确率。

Conclusion: 该模型为大规模模型的强化学习研究提供了可行路径，展示了在有限资源下的潜力。

Abstract: Recent R1-Zero-like research further demonstrates that reasoning extension
has given large language models (LLMs) unprecedented reasoning capabilities,
and Reinforcement Learning is the core technology to elicit its complex
reasoning. However, conducting RL experiments directly on hyperscale models
involves high computational costs and resource demands, posing significant
risks. We propose the Compass-Thinker-7B model, which aims to explore the
potential of Reinforcement Learning with less computational resources and
costs, and provides insights for further research into RL recipes for larger
models. Compass-Thinker-7B is trained from an open source model through a
specially designed Reinforcement Learning Pipeline. we curate a dataset of 30k
verifiable mathematics problems for the Reinforcement Learning Pipeline. By
configuring data and training settings with different difficulty distributions
for different stages, the potential of the model is gradually released and the
training efficiency is improved. Extensive evaluations show that
Compass-Thinker-7B possesses exceptional reasoning potential, and achieves
superior performance on mathematics compared to the same-sized RL
model.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B
achieves 40% accuracy.

</details>


### [39] [Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models](https://arxiv.org/abs/2508.08926)
*Wei Cai,Jian Zhao,Yuchu Jiang,Tianle Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 论文提出隐式推理安全漏洞的概念，并通过数据集SSUI展示其危害，证明简单上下文学习可显著缓解问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态输入下存在安全隐患，尤其是隐式推理漏洞。

Method: 提出隐式推理安全概念，构建SSUI数据集，并通过上下文学习验证缓解方法。

Result: SSUI数据集有效展示了隐式推理漏洞，上下文学习显著减轻了威胁。

Conclusion: 需改进跨模态隐式推理能力以应对安全隐患。

Abstract: Large Vision-Language Models face growing safety challenges with multimodal
inputs. This paper introduces the concept of Implicit Reasoning Safety, a
vulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due
to flawed or hidden reasoning. To showcase this, we developed Safe Semantics,
Unsafe Interpretations, the first dataset for this critical issue. Our
demonstrations show that even simple In-Context Learning with SSUI
significantly mitigates these implicit multimodal threats, underscoring the
urgent need to improve cross-modal implicit reasoning.

</details>


### [40] [Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty](https://arxiv.org/abs/2508.08992)
*Rui Wang,Qihan Lin,Jiayu Liu,Qing Zong,Tianshi Zheng,Weiqi Wang,Yangqiu Song*

Main category: cs.AI

TL;DR: 研究探讨了前景理论（PT）是否适用于大型语言模型（LLMs），以及表达不确定性的认知标记（如“可能”）是否影响其决策行为。通过三阶段实验和新的评估框架，发现PT对LLMs的决策建模并不总是可靠，尤其是在语言形式多样时。


<details>
  <summary>Details</summary>
Motivation: 前景理论（PT）用于人类不确定性决策，但尚未明确是否适用于LLMs。同时，认知标记表达人类不确定性，但其对LLMs决策的影响未被充分研究。

Method: 设计基于经济问卷的三阶段实验，提出更通用的评估框架，引入认知标记及其对应概率值，分析其对LLMs决策的影响。

Result: 研究发现PT对LLMs的决策建模并不一致可靠，尤其是在语言表达不确定性形式多样时。

Conclusion: 前景理论在LLMs中的应用存在局限性，认知标记的多样性会影响其决策行为，需进一步研究。

Abstract: Prospect Theory (PT) models human decision-making under uncertainty, while
epistemic markers (e.g., maybe) serve to express uncertainty in language.
However, it remains largely unexplored whether Prospect Theory applies to
contemporary Large Language Models and whether epistemic markers, which express
human uncertainty, affect their decision-making behaviour. To address these
research gaps, we design a three-stage experiment based on economic
questionnaires. We propose a more general and precise evaluation framework to
model LLMs' decision-making behaviour under PT, introducing uncertainty through
the empirical probability values associated with commonly used epistemic
markers in comparable contexts. We then incorporate epistemic markers into the
evaluation framework based on their corresponding probability values to examine
their influence on LLM decision-making behaviours. Our findings suggest that
modelling LLMs' decision-making with PT is not consistently reliable,
particularly when uncertainty is expressed in diverse linguistic forms. Our
code is released in https://github.com/HKUST-KnowComp/MarPT.

</details>


### [41] [Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory](https://arxiv.org/abs/2508.08997)
*Sizhe Yuen,Francisco Gomez Medina,Ting Su,Yali Du,Adam J. Sobey*

Main category: cs.AI

TL;DR: 论文提出了一种名为“Intrinsic Memory Agents”的新框架，通过结构化、与代理输出共同演化的记忆模板，解决了多代理LLM系统中的记忆一致性和任务完整性挑战。


<details>
  <summary>Details</summary>
Motivation: 多代理LLM系统在复杂协作问题解决中表现出潜力，但受限于上下文窗口，导致记忆一致性、角色遵守和程序完整性等问题。

Method: 采用结构化、代理特定的记忆模板，保持角色对齐的任务相关信息。

Result: 在PDDL数据集上表现优于现有方法，提升38.6%，并在复杂数据管道设计任务中表现出更高的质量。

Conclusion: 通过结构化内在记忆方法，可以提升多代理LLM系统在结构化规划任务中的能力。

Abstract: Multi-agent systems built on Large Language Models (LLMs) show exceptional
promise for complex collaborative problem-solving, yet they face fundamental
challenges stemming from context window limitations that impair memory
consistency, role adherence, and procedural integrity. This paper introduces
Intrinsic Memory Agents, a novel framework that addresses these limitations
through structured agent-specific memories that evolve intrinsically with agent
outputs. Specifically, our method maintains role-aligned memory templates that
preserve specialized perspectives while focusing on task-relevant information.
We benchmark our approach on the PDDL dataset, comparing its performance to
existing state-of-the-art multi-agentic memory approaches and showing an
improvement of 38.6\% with the highest token efficiency. An additional
evaluation is performed on a complex data pipeline design task, we demonstrate
that our approach produces higher quality designs when comparing 5 metrics:
scalability, reliability, usability, cost-effectiveness and documentation with
additional qualitative evidence of the improvements. Our findings suggest that
addressing memory limitations through structured, intrinsic approaches can
improve the capabilities of multi-agent LLM systems on structured planning
tasks.

</details>


### [42] [Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs](https://arxiv.org/abs/2508.09019)
*Shivam Dubey*

Main category: cs.AI

TL;DR: 提出了一种端到端系统，利用机制可解释性技术直接在模型内部识别和减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能放大有害偏见，传统方法（如数据过滤或后处理）效果有限，需更直接的方法。

Method: 1. 训练线性“探针”检测模型内部激活中的偏见表征；2. 计算“导向向量”以实时调整生成内容。

Result: 实验证明探针能高精度识别偏见，导向向量有效减少偏见输出。

Conclusion: 该系统为构建更安全、可解释的语言模型提供了直接方法。

Abstract: As large language models (LLMs) become more integrated into societal systems,
the risk of them perpetuating and amplifying harmful biases becomes a critical
safety concern. Traditional methods for mitigating bias often rely on data
filtering or post-hoc output moderation, which treat the model as an opaque
black box. In this work, we introduce a complete, end-to-end system that uses
techniques from mechanistic interpretability to both identify and actively
mitigate bias directly within a model's internal workings. Our method involves
two primary stages. First, we train linear "probes" on the internal activations
of a model to detect the latent representations of various biases (e.g.,
gender, race, age). Our experiments on \texttt{gpt2-large} demonstrate that
these probes can identify biased content with near-perfect accuracy, revealing
that bias representations become most salient in the model's later layers.
Second, we leverage these findings to compute "steering vectors" by contrasting
the model's activation patterns for biased and neutral statements. By adding
these vectors during inference, we can actively steer the model's generative
process away from producing harmful, stereotypical, or biased content in
real-time. We demonstrate the efficacy of this activation steering technique,
showing that it successfully alters biased completions toward more neutral
alternatives. We present our work as a robust and reproducible system that
offers a more direct and interpretable approach to building safer and more
accountable LLMs.

</details>


### [43] [A First Look at Predictability and Explainability of Pre-request Passenger Waiting Time in Ridesharing Systems](https://arxiv.org/abs/2508.09027)
*Jie Wang,Guang Wang*

Main category: cs.AI

TL;DR: 论文提出了一种基于特征交互的XGBoost模型（FiXGBoost），用于预测乘客在提交乘车请求前的等待时间，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注乘客提交请求后的等待时间预测，而忽略了请求前的预测，这对乘客和司机体验至关重要。

Method: 通过数据驱动分析供需动态对等待时间的影响，设计FiXGBoost模型进行预测，并进行特征重要性分析。

Result: 在大规模真实数据集（3000万条记录）上验证，FiXGBoost表现优异且具有高可解释性。

Conclusion: FiXGBoost为预请求等待时间预测提供了有效且可解释的解决方案。

Abstract: Passenger waiting time prediction plays a critical role in enhancing both
ridesharing user experience and platform efficiency. While most existing
research focuses on post-request waiting time prediction with knowing the
matched driver information, pre-request waiting time prediction (i.e., before
submitting a ride request and without matching a driver) is also important, as
it enables passengers to plan their trips more effectively and enhance the
experience of both passengers and drivers. However, it has not been fully
studied by existing works. In this paper, we take the first step toward
understanding the predictability and explainability of pre-request passenger
waiting time in ridesharing systems. Particularly, we conduct an in-depth
data-driven study to investigate the impact of demand&supply dynamics on
passenger waiting time. Based on this analysis and feature engineering, we
propose FiXGBoost, a novel feature interaction-based XGBoost model designed to
predict waiting time without knowing the assigned driver information. We
further perform an importance analysis to quantify the contribution of each
factor. Experiments on a large-scale real-world ridesharing dataset including
over 30 million trip records show that our FiXGBoost can achieve a good
performance for pre-request passenger waiting time prediction with high
explainability.

</details>


### [44] [CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks](https://arxiv.org/abs/2508.09054)
*Debdeep Mukherjee,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Victor Martin,Thierry Josse,Jonathan Brown,Kenza Saiah*

Main category: cs.AI

TL;DR: 提出了一种基于深度神经网络的预测性维护框架，用于早期分类轨道电路中的异常，优于传统方法，准确率达99.31%。


<details>
  <summary>Details</summary>
Motivation: 轨道电路故障可能导致连锁中断，早期检测异常对减少停机时间和收入损失至关重要。

Method: 利用深度神经网络分类异常，并通过符合性预测提供不确定性估计。

Result: 在10个CVCM故障案例中验证，方法符合ISO-17359标准，检测准确率达99.31%。

Conclusion: 该方法可扩展并适用于其他轨道电路和铁路系统，提升运行可靠性。

Abstract: Track circuits are critical for railway operations, acting as the main
signalling sub-system to locate trains. Continuous Variable Current Modulation
(CVCM) is one such technology. Like any field-deployed, safety-critical asset,
it can fail, triggering cascading disruptions. Many failures originate as
subtle anomalies that evolve over time, often not visually apparent in
monitored signals. Conventional approaches, which rely on clear signal changes,
struggle to detect them early. Early identification of failure types is
essential to improve maintenance planning, minimising downtime and revenue
loss. Leveraging deep neural networks, we propose a predictive maintenance
framework that classifies anomalies well before they escalate into failures.
Validated on 10 CVCM failure cases across different installations, the method
is ISO-17359 compliant and outperforms conventional techniques, achieving
99.31% overall accuracy with detection within 1% of anomaly onset. Through
conformal prediction, we provide uncertainty estimates, reaching 99% confidence
with consistent coverage across classes. Given CVCMs global deployment, the
approach is scalable and adaptable to other track circuits and railway systems,
enhancing operational reliability.

</details>


### [45] [SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling](https://arxiv.org/abs/2508.09105)
*Shixuan Sun,Siyuan Liang,Ruoyu Chen,Jianjie Huang,Jingzhi Li,Xiaochun Cao*

Main category: cs.AI

TL;DR: 论文提出了一种名为SMA的方法，用于在RAG和MRAG系统中追踪生成内容的来源，解决了现有方法无法可靠区分预训练、外部检索或用户输入的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法可靠追踪生成内容的来源，导致隐私泄漏责任难以界定。

Method: 提出SMA方法，结合半黑盒审计和零阶优化，通过大规模扰动采样和岭回归建模实现来源追踪，并引入跨模态归因技术。

Result: SMA能够精确区分生成内容的来源，首次实现了对MRAG系统中图像检索痕迹的成员推断。

Conclusion: SMA为复杂生成系统中的数据来源审计提供了新视角，将成员推断的重点从‘数据是否被记忆’转向‘内容来源何处’。

Abstract: Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented
Generation (MRAG) significantly improve the knowledge coverage and contextual
understanding of Large Language Models (LLMs) by introducing external knowledge
sources. However, retrieval and multimodal fusion obscure content provenance,
rendering existing membership inference methods unable to reliably attribute
generated outputs to pre-training, external retrieval, or user input, thus
undermining privacy leakage accountability
  To address these challenges, we propose the first Source-aware Membership
Audit (SMA) that enables fine-grained source attribution of generated content
in a semi-black-box setting with retrieval control capabilities.To address the
environmental constraints of semi-black-box auditing, we further design an
attribution estimation mechanism based on zero-order optimization, which
robustly approximates the true influence of input tokens on the output through
large-scale perturbation sampling and ridge regression modeling. In addition,
SMA introduces a cross-modal attribution technique that projects image inputs
into textual descriptions via MLLMs, enabling token-level attribution in the
text modality, which for the first time facilitates membership inference on
image retrieval traces in MRAG systems. This work shifts the focus of
membership inference from 'whether the data has been memorized' to 'where the
content is sourced from', offering a novel perspective for auditing data
provenance in complex generative systems.

</details>


### [46] [OpenCUA: Open Foundations for Computer-Use Agents](https://arxiv.org/abs/2508.09123)
*Xinyuan Wang,Bowen Wang,Dunjie Lu,Junlin Yang,Tianbao Xie,Junli Wang,Jiaqi Deng,Xiaole Guo,Yiheng Xu,Chen Henry Wu,Zhennan Shen,Zhuokai Li,Ryan Li,Xiaochuan Li,Junda Chen,Boyuan Zheng,Peihang Li,Fangyu Lei,Ruisheng Cao,Yeqiao Fu,Dongchan Shin,Martin Shin,Jiarui Hu,Yuyan Wang,Jixuan Chen,Yuxiao Ye,Danyang Zhang,Dikang Du,Hao Hu,Huarong Chen,Zaida Zhou,Yipu Wang,Heng Wang,Diyi Yang,Victor Zhong,Flood Sung,Y. Charles,Zhilin Yang,Tao Yu*

Main category: cs.AI

TL;DR: OpenCUA是一个开源框架，用于扩展计算机使用代理（CUA）数据和基础模型，包括标注工具、数据集和模型，性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在计算机任务中的潜力增长，研究社区需要开源CUA框架以研究其能力、局限性和风险。

Method: OpenCUA包括标注基础设施、大规模数据集AgentNet和可扩展的演示转换管道，支持长链推理。

Result: OpenCUA-32B在OSWorld-Verified上平均成功率为34.8%，超越现有开源模型和GPT-4o。

Conclusion: OpenCUA为CUA研究提供了开放基础，其方法在跨领域泛化和计算扩展方面表现优异。

Abstract: Vision-language models have demonstrated impressive capabilities as
computer-use agents (CUAs) capable of automating diverse computer tasks. As
their commercial potential grows, critical details of the most capable CUA
systems remain closed. As these agents will increasingly mediate digital
interactions and execute consequential decisions on our behalf, the research
community needs access to open CUA frameworks to study their capabilities,
limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive
open-source framework for scaling CUA data and foundation models. Our framework
consists of: (1) an annotation infrastructure that seamlessly captures human
computer-use demonstrations; (2) AgentNet, the first large-scale computer-use
task dataset spanning 3 operating systems and 200+ applications and websites;
(3) a scalable pipeline that transforms demonstrations into state-action pairs
with reflective long Chain-of-Thought reasoning that sustain robust performance
gains as data scales. Our end-to-end agent models demonstrate strong
performance across CUA benchmarks. In particular, OpenCUA-32B achieves an
average success rate of 34.8% on OSWorld-Verified, establishing a new
state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA
(GPT-4o). Further analysis confirms that our approach generalizes well across
domains and benefits significantly from increased test-time computation. We
release our annotation tool, datasets, code, and models to build open
foundations for further CUA research.

</details>


### [47] [BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair](https://arxiv.org/abs/2508.09129)
*Xianghe Pang,Shuo Tang,Rui Ye,Yuwen Du,Yaxin Du,Siheng Chen*

Main category: cs.AI

TL;DR: BrowseMaster框架通过规划器-执行器分工解决了LLM代理在信息搜索中的广度和深度平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理在信息搜索中存在搜索广度不足和推理深度受限的问题，导致效率低下和结果不连贯。

Method: 提出BrowseMaster框架，采用规划器制定搜索策略，执行器进行高效检索，分工协作以平衡搜索广度和推理深度。

Result: 在英语和中文基准测试中，BrowseMaster表现优异，分别获得30.0和46.5的高分，超越现有基线。

Conclusion: BrowseMaster通过分工协作机制，有效解决了信息搜索中的广度和深度平衡问题，适用于复杂推理任务。

Abstract: Effective information seeking in the vast and ever-growing digital landscape
requires balancing expansive search with strategic reasoning. Current large
language model (LLM)-based agents struggle to achieve this balance due to
limitations in search breadth and reasoning depth, where slow, serial querying
restricts coverage of relevant sources and noisy raw inputs disrupt the
continuity of multi-step reasoning. To address these challenges, we propose
BrowseMaster, a scalable framework built around a programmatically augmented
planner-executor agent pair. The planner formulates and adapts search
strategies based on task constraints, while the executor conducts efficient,
targeted retrieval to supply the planner with concise, relevant evidence. This
division of labor preserves coherent, long-horizon reasoning while sustaining
broad and systematic exploration, overcoming the trade-off that limits existing
agents. Extensive experiments on challenging English and Chinese benchmarks
show that BrowseMaster consistently outperforms open-source and proprietary
baselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,
which demonstrates its strong capability in complex, reasoning-heavy
information-seeking tasks at scale.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [48] [Achievable Rates of Nanopore-based DNA Storage](https://arxiv.org/abs/2508.08567)
*Brendon McBain,Emanuele Viterbo*

Main category: cs.IT

TL;DR: 论文研究了基于纳米孔的DNA存储的可实现速率，使用不依赖碱基识别算法的通道模型解码纳米孔信号，提出了一种简化的消息传递算法，并通过公开数据集验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决纳米孔信号解码的复杂性，避免依赖碱基识别算法，同时填补DNA存储数据集中缺乏纳米孔信号的空白。

Method: 方法包括使用NNC-Scrappie通道模型生成输出信号，并推导简化的消息传递算法进行软解码。通过动态时间规整（DTW）算法计算可实现速率，并应用公开数据集验证。

Result: 结果表明，在100个碱基长度的DNA链上，解码速率可达0.64-1.18 bits/碱基（取决于纳米孔质量），平均为0.96 bits/碱基。

Conclusion: 结论是该方法在单次读取下表现良好，但速率仍偏保守，未考虑纳米孔模型的校准。

Abstract: This paper studies achievable rates of nanopore-based DNA storage when
nanopore signals are decoded using a tractable channel model that does not rely
on a basecalling algorithm. Specifically, the noisy nanopore channel (NNC) with
the Scrappie pore model generates average output levels via i.i.d. geometric
sample duplications corrupted by i.i.d. Gaussian noise (NNC-Scrappie).
Simplified message passing algorithms are derived for efficient soft decoding
of nanopore signals using NNC-Scrappie. Previously, evaluation of this channel
model was limited by the lack of DNA storage datasets with nanopore signals
included. This is solved by deriving an achievable rate based on the dynamic
time-warping (DTW) algorithm that can be applied to genomic sequencing datasets
subject to constraints that make the resulting rate applicable to DNA storage.
Using a publicly-available dataset from Oxford Nanopore Technologies (ONT), it
is demonstrated that coding over multiple DNA strands of $100$ bases in length
and decoding with the NNC-Scrappie decoder can achieve rates of at least
$0.64-1.18$ bits per base, depending on the channel quality of the nanopore
that is chosen in the sequencing device per channel-use, and $0.96$ bits per
base on average assuming uniformly chosen nanopores. These rates are
pessimistic since they only apply to single reads and do not include
calibration of the pore model to specific nanopores.

</details>


### [49] [Optimum 1-Step Majority-Logic Decoding of Binary Reed-Muller Codes](https://arxiv.org/abs/2508.08736)
*Hoang Ly,Emina Soljanin*

Main category: cs.IT

TL;DR: 论文提出了一种新的硬判决解码器，能够一步完成解码，达到最大纠错能力（d_min/4），并证明其在擦除设置下是最优的。


<details>
  <summary>Details</summary>
Motivation: 现有多数逻辑解码器需要多步处理，且性能或适用范围有限，因此需要一种更高效、通用的解码方法。

Method: 提出一种新的硬判决解码器，实现所有多数投票一步完成，适用于所有参数r和m。

Result: 新解码器在一步内达到最大纠错能力（d_min/4），并在擦除设置下恢复最多d_min-1个符号。

Conclusion: 该解码器是首个同时实现最优擦除纠正和最大一步纠错能力的RM码解码器。

Abstract: The classical majority-logic decoder proposed by Reed for Reed-Muller codes
RM(r, m) of order r and length 2^m, unfolds in r+1 sequential steps, decoding
message symbols from highest to lowest degree. Several follow-up decoding
algorithms reduced the number of steps, but for a limited set of parameters, or
at the expense of reduced performance, or relying on the existence of some
combinatorial structures. We show that any one-step majority-logic decoder-that
is, a decoder performing all majority votes in one step simultaneously without
sequential processing-can correct at most d_min/4 errors for all values of r
and m, where d_min denotes the code's minimum distance. We then introduce a new
hard-decision decoder that completes the decoding in a single step and attains
this error-correction limit. It applies to all r and m, and can be viewed as a
parallel realization of Reed's original algorithm, decoding all message symbols
simultaneously. Remarkably, we also prove that the decoder is optimum in the
erasure setting: it recovers the message from any erasure pattern of up to
d_min-1 symbols-the theoretical limit. To our knowledge, this is the first
1-step decoder for RM codes that achieves both optimal erasure correction and
the maximum one-step error correction capability.

</details>


### [50] [Optimized Arithmetic Coding for Efficient Data Compression in the Resource-Constrained Internet of Things(IoT)](https://arxiv.org/abs/2508.08840)
*Vatsala Upadhyay,J. Kokila,Abhishek Vaish*

Main category: cs.IT

TL;DR: 该论文提出了一种针对物联网环境的优化算术编码算法，通过迭代优化、PCA降维和基数减少，显著提升了压缩效率，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 物联网环境中的异构数据存储和传输面临挑战，传统算术编码计算复杂度高，需要优化以适应资源受限场景。

Method: 结合迭代优化减少冗余计算、PCA降维提取关键特征、基数减少分组相似概率，优化算术编码算法。

Result: 优化算法在图像数据集上测试，压缩时间、CPU和内存消耗显著降低，压缩比达814:1，单图压缩仅101毫秒。

Conclusion: 优化算法高效且适合实时应用，为物联网环境提供了高效的数据传输和存储解决方案。

Abstract: The Internet of Things (IoT) generates vast amounts of heterogeneous data,
ranging from sensor readings to log alerts and images, that pose challenges to
storage and data transmission in resource-constrained environments. In this
context, lossless data compression techniques, like Arithmetic Coding, offer an
effective solution owing to their high compression ratio. However, the standard
Arithmetic Coding technique is computationally intensive, leading to high
memory and processing overhead. This paper proposes an optimized version of
Arithmetic coding for the IoT environment that incorporates three improvements
using Iterative and Iteration Optimizations for minimizing redundant
computations and achieving faster convergence; Principal Component
Analysis(PCA) for dimensionality reduction and identifying key features; and
lastly, Cardinality reduction for grouping similar probabilities to improve the
compression efficiency. The proposed method was evaluated on a dataset of
images and demonstrated significant reductions in the time to compress, CPU
utilization, and memory consumption, and preserves data integrity as seen
through the low RMSE values. The optimized version of the Arithmetic Coding
algorithm achieves an impressive compression ratio of 814:1 and 101 ms to
compress a single image. This makes the optimized algorithm suitable for
real-time applications and resource-constrained environments for efficient data
transmission and storage.

</details>


### [51] [A Dual Framework for Optimized Data Storage and Retrieval using Lightweight Python Blockchain and Scalable Smart Contracts with IPFS](https://arxiv.org/abs/2508.08887)
*Vatsala Upadhyay,J. Kokila,Abhishek Vaish*

Main category: cs.IT

TL;DR: 论文提出了一种高效的物联网数据存储与检索框架，结合IPFS和区块链技术，显著提升了数据上传速度和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统云存储模型存在延迟、安全性和高成本问题，而现有的IPFS-区块链方案计算效率低且成本高。

Method: 开发了一个双向数据存储与检索系统，动态监控数据并传输至IPFS，记录CID到区块链，通过智能合约实现实时访问。进一步优化为轻量级Python框架。

Result: 优化后的框架将500MB文件上传时间从117.12秒降至7.63秒（提升93.47%），进一步优化至4.2秒（再提升45%）。

Conclusion: 该框架高效、安全，适用于实时和关键物联网应用，优于现有IPFS-智能合约方案。

Abstract: The exponential growth of IoT data demands efficient, secure, and scalable
storage solutions on one hand, and efficient data migration and retrieval on
the other hand are essential for the systems to be practical and acceptable for
different applications. The traditional cloud-based models face latency,
security, and high operational costs, while existing bi-directional data
storage and retrieval-based IPFS models are not computationally efficient and
incur high gas costs at the cost of a necessary blockchain deployment. To
overcome the challenges of efficient data migration, we initially developed a
2-way data storage and retrieval system as well as a scalable framework that
dynamically monitors and transfers device-generated data to IPFS, records the
content identifier(CID) on a blockchain, and enables secure, real-time access
via smart contracts. Experimental results demonstrate that the existing work
achieved an average data upload time of 117.12 sec for a file size of 500 MB;
our framework achieves a faster upload time of 7.63 sec, marking a 93.47%
improvement. We further optimize the proposed framework to reduce the file
upload time incurred from the smart contracts by introducing a
blockchain-inspired, lightweight, and customizable Python framework that
replicates the storage and retrieval functionalities of a traditional
blockchain, where the file upload time is 4.2 sec, further optimized by 45%
from our previous approach, thus demonstrating its efficiency, security and
suitability for deploy ment in real-time and critical IoT applications and
outperforming the existing IPFS-smart contract based solutions.

</details>


### [52] [Generalized Bicycle Codes with Low Connectivity: Minimum Distance Bounds and Hook Errors](https://arxiv.org/abs/2508.09082)
*Reza Dastbasteh,Olatz Sanz Larrarte,Arun John Moncy,Pedro M. Crespo,Josu Etxezarreta Martinez,Ruben M. Otxoa*

Main category: cs.IT

TL;DR: 论文提出了新的上下界方法，用于分析广义自行车码的最小距离，并展示了两种高度简并的GB码族，分别适用于奇数和偶数距离。此外，还分析了逻辑操作和故障容忍性，并通过数值模拟评估了其逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 研究广义自行车码的最小距离及其在量子纠错中的应用，特别是探索其与表面码的相似性和潜在优势。

Method: 通过上下界分析、逻辑操作结构研究和故障容忍性设计，结合数值模拟（BP-OSD和MWPM解码器）评估性能。

Result: 发现了两种GB码族，其最小距离分别为奇数和偶数，并展示了其逻辑操作和故障容忍性。数值模拟显示其逻辑错误率阈值约为14-16%，与旋转表面码相似。

Conclusion: 广义自行车码在最小距离和逻辑操作方面表现出色，且与表面码性能相近，为量子纠错提供了新的可能性。

Abstract: We present new upper and lower bounds on the minimum distance of certain
generalized bicycle (GB) codes beyond the reach of techniques for classical
codes capable of even capturing the true minimum distance for some cases. These
bounds are then applied to illustrate the existence and analyze two highly
degenerate GB code families with parameters $[[d^2+1,2,d]]$ for odd $d \geq 3$
and $[[d^2,2,d]]$ for even $d \geq 4$, both having the property that each check
qubit is connected to exactly four data qubits similar to surface codes. For
the odd-distance family, we analyze the structure of low-weight logical Pauli
operators and demonstrate the existence of a fault-tolerant logical CNOT gate
between the two logical qubits, achievable through a simple relabeling of data
qubits. We further construct a syndrome extraction pattern for both families
that does not imply minimum distance reduction arising from extraction circuit
faults that propagate from the check qubits to the data qubits. Finally, we
numerically evaluate their logical error rates under a code capacity
depolarizing noise model using the belief propagation ordered statistics
decoding (BP-OSD) and minimum-weight perfect-matching (MWPM) decoders, yielding
thresholds of approximately $14-16\%$ for the odd and even families, very
similar to those of rotated surface codes.

</details>
