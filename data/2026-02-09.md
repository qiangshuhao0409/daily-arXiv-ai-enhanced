<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.IT](#cs.IT) [Total: 12]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [IE-RAP: An Intelligence and Efficient Reader Anti-Collision Protocol for Dense RFID Networks](https://arxiv.org/abs/2602.06626)
*Hadiseh Rezaei,Rahim Taheri,Mohammad Shojafar*

Main category: cs.NI

TL;DR: 提出IE-RAP协议，结合TDMA和FDMA机制，通过SIFT函数和距离计算优化RFID密集阅读器环境性能，提升吞吐量26%，降低等待时间74%，减少能耗52%


<details>
  <summary>Details</summary>
Motivation: RFID系统在密集阅读器环境中存在阅读器间和阅读器与标签间的碰撞问题，导致网络性能下降，需要有效的防碰撞协议来提升系统效率

Method: 提出智能高效阅读器防碰撞协议(IE-RAP)，结合TDMA和FDMA机制，采用SIFT函数和阅读器间距离计算技术，防止标签重复读取并确保通信信道及时释放

Result: 仿真结果显示，相比现有方法，吞吐量提升26%，平均等待时间降低74%，能耗减少52%，并支持移动阅读器的无缝集成

Conclusion: IE-RAP协议能有效解决RFID密集环境中的碰撞问题，显著提升网络性能指标，是一种高效的集中式防碰撞解决方案

Abstract: An advanced technology known as a radio frequency identification (RFID) system enables seamless wireless communication between tags and readers. This system operates in what is referred to as a dense reader environment, where readers are placed close to each other to optimize coverage. However, this setup comes with its challenges, as it increases the likelihood of collisions between readers and tags (reader-to-reader and reader-to-tag), leading to reduced network performance. To address this issue, various protocols have been proposed, with centralized solutions emerging as promising options due to their ability to deliver higher throughput. In this paper, we propose the Intelligence and Efficient Reader Anti-collision Protocol (IE-RAP) that improves network performance such as throughput, average waiting time, and energy consumption, which employs a powerful combination of Time Division Multiple Access (TDMA) and Frequency Division Multiple Access (FDMA) mechanisms. IE-RAP improves the efficiency of RFID networks through techniques such as the SIFT function and distance calculation between readers. By preventing re-read tags and ensuring the on-time release of the communication channel, we effectively eliminate unnecessary collisions. Our simulations emphasize the superiority of our proposed method, it increases 26% throughput, reduces 74% the average waiting time, and lower by 52% the energy consumption compared to existing approaches. Importantly, our solution supports the seamless integration of mobile readers within the network.

</details>


### [2] [Talk Like a Packet: Rethinking Network Traffic Analysis with Transformer Foundation Models](https://arxiv.org/abs/2602.06636)
*Samara Mayhoub,Chuan Heng Foh,Mahdi Boloursaz Mashhadi,Mohammad Shojafar,Rahim Tafazolli*

Main category: cs.NI

TL;DR: 该论文探索将Transformer架构作为网络流量分析的基础模型，提出了统一的预训练-微调流程，并在多个下游任务中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受Transformer在自然语言处理中的成功启发，研究者希望探索其在网络流量分析中作为基础模型的潜力，以解决传统方法在泛化性和数据效率方面的限制。

Method: 提出了统一的预训练和微调流程，构建流量基础模型；对现有模型按架构、输入模态和预训练策略进行分类；通过微调在多个下游任务（流量分类、特征预测、生成）中评估模型性能。

Result: 流量基础模型在多个下游任务中表现出良好的泛化能力，相比非基础模型基线有性能提升；这些模型能够有效学习流量表示，并在有限标注数据集上表现良好。

Conclusion: Transformer架构的网络流量基础模型具有巨大潜力，能够为未来智能网络分析系统提供有效支持，特别是在数据有限的情况下仍能保持良好性能。

Abstract: Inspired by the success of Transformer-based models in natural language processing, this paper investigates their potential as foundation models for network traffic analysis. We propose a unified pre-training and fine-tuning pipeline for traffic foundation models. Through fine-tuning, we demonstrate the generalizability of the traffic foundation models in various downstream tasks, including traffic classification, traffic characteristic prediction, and traffic generation. We also compare against non-foundation baselines, demonstrating that the foundation-model backbones achieve improved performance. Moreover, we categorize existing models based on their architecture, input modality, and pre-training strategy. Our findings show that these models can effectively learn traffic representations and perform well with limited labeled datasets, highlighting their potential in future intelligent network analysis systems.

</details>


### [3] [Makespan Minimization in Split Learning: From Theory to Practice](https://arxiv.org/abs/2602.06693)
*Robert Ganian,Fionn Mc Inerney,Dimitra Tsigkari*

Main category: cs.NI

TL;DR: 本文研究分布式机器学习中的分割学习问题，针对异构IoT设备场景，通过客户端-助手分配和任务调度优化训练时间。论文首先研究同构任务模型，提出5-近似算法，然后扩展到异构任务模型，证明不存在多项式时间近似算法，但提出新的启发式算法并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分割学习作为异构IoT设备分布式机器学习的解决方案，允许客户端将部分训练任务卸载到计算能力更强的助手设备。核心挑战在于通过联合优化客户端-助手分配和助手任务调度来最小化训练时间。现有研究主要考虑同构任务场景，但实际应用中任务通常是异构的，需要更通用的解决方案。

Method: 论文采用两阶段方法：首先研究同构任务模型（每个助手有内存基数约束），通过复杂性理论分析证明不存在精确多项式时间算法和近似方案，但提出了非平凡的多项式时间5-近似算法。然后将研究扩展到异构任务模型（助手有内存容量约束，客户端有可变内存成本），证明不存在任何近似因子的多项式时间近似算法，但通过调整5-近似算法开发了新的启发式方法。

Result: 理论结果：1) 同构任务模型下，排除了精确多项式时间算法和近似方案，但提出了5-近似算法；2) 异构任务模型下，证明除非P=NP，否则不存在任何近似因子的多项式时间近似算法。实验结果表明，新提出的启发式算法在异构任务设置中优于现有启发式方法。

Conclusion: 本文系统研究了分割学习中的客户端-助手分配和任务调度问题。对于同构任务模型，提供了理论界限和实用算法；对于更现实的异构任务模型，虽然理论上存在计算困难，但通过启发式方法仍能获得良好性能。研究为实际分布式机器学习系统的设计提供了理论指导和实用工具。

Abstract: Split learning recently emerged as a solution for distributed machine learning with heterogeneous IoT devices, where clients can offload part of their training to computationally-powerful helpers. The core challenge in split learning is to minimize the training time by jointly devising the client-helper assignment and the schedule of tasks at the helpers. We first study the model where each helper has a memory cardinality constraint on how many clients it may be assigned, which represents the case of homogeneous tasks. Through complexity theory, we rule out exact polynomial-time algorithms and approximation schemes even for highly restricted instances of this problem. We complement these negative results with a non-trivial polynomial-time 5-approximation algorithm. Building on this, we then focus on the more general heterogeneous task setting considered by Tirana et al. [INFOCOM 2024], where helpers have memory capacity constraints and clients have variable memory costs. In this case, we prove that, unless P=NP, the problem cannot admit a polynomial-time approximation algorithm for any approximation factor. However, by adapting our aforementioned 5-approximation algorithm, we develop a novel heuristic for the heterogeneous task setting and show that it outperforms heuristics from prior works through extensive experiments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架使用最优预算拒绝采样来减少rollout模型与策略之间的分布不匹配，实现更高效的LLM强化学习


<details>
  <summary>Details</summary>
Motivation: LLM的强化学习成本高昂，特别是rollout阶段。将rollout生成与策略优化解耦可以提高效率，但这会导致严重的分布不匹配问题，破坏学习稳定性。

Method: 提出Jackpot框架，采用最优预算拒绝采样来直接减少rollout模型与演化策略之间的差异。包括原则性的OBRS程序、联合更新策略和rollout模型的统一训练目标，以及通过top-k概率估计和批次级偏差校正实现的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下持续将rollout分布移向目标分布。实证显示相比重要性采样基线，Jackpot显著提高训练稳定性，在Qwen3-8B-Base上训练300步（批次大小64）时达到与on-policy RL相当的性能。

Conclusion: 基于OBRS的对齐方法使rollout生成与策略优化的解耦更接近实用和有效，为LLM的强化学习提供了更可行的解决方案。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [5] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对LLM推理失败进行全面调查，提出了新的分类框架，将推理分为具身与非具身类型，并将推理失败分为基础性、应用特定和鲁棒性三类，为理解LLM系统性弱点提供结构化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM展现出卓越的推理能力，但在看似简单的场景中仍存在显著的推理失败。目前缺乏对这些失败的系统性理解和分类，需要统一分散的研究工作，为构建更强、更可靠、更鲁棒的推理能力提供指导。

Method: 提出了一个新颖的分类框架：将推理分为具身推理和非具身推理，非具身推理进一步细分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：1）影响下游任务的基础性失败；2）特定领域表现的应用特定限制；3）微小变化导致性能不一致的鲁棒性问题。对每种失败提供明确定义、分析现有研究、探索根本原因并提出缓解策略。

Result: 创建了首个全面的LLM推理失败调查，提供了结构化视角来理解LLM的系统性弱点。发布了GitHub存储库（https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures），收集了LLM推理失败的研究工作，为该领域提供便捷入口。

Conclusion: 该调查统一了分散的研究工作，为LLM推理失败提供了系统性的分类和理解框架，为未来研究提供了有价值的见解和指导，有助于构建更强、更可靠、更鲁棒的推理能力。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [6] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT的逻辑规范框架，用于大规模状态空间MDP中的非马尔可夫奖励，通过理论片段识别和基于奖励机器+Hindsight Experience Replay的实践方法解决表达力增强带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非结构化、异构数据域上的复杂任务时表达能力有限，需要手动谓词编码。需要一种统一、可重用的框架来指定复杂任务，同时处理无限状态空间环境。

Method: 1) 使用LTLfMT（基于理论的线性时序逻辑）作为规范语言；2) 识别理论可处理的LTLfMT片段；3) 结合奖励机器和Hindsight Experience Replay将一阶逻辑规范转化为可学习形式；4) 在非线性算术理论下的连续控制环境中评估。

Result: 实验表明，定制的HER实现对于解决复杂目标任务至关重要，提出的方法能够自然地指定复杂任务，并在连续控制环境中有效工作。

Conclusion: 提出的LTLfMT框架为大规模状态空间MDP中的非马尔可夫奖励提供了一种表达能力强、统一的规范方法，通过理论片段识别和HER技术解决了表达力增强带来的计算挑战。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [7] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: 研究LLMs在医疗诊断等高风险决策中是否作为理性效用最大化者，提出可证伪条件检验LLM报告概率是否对应真实信念


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地部署在高风险领域作为智能体，其决策逻辑难以解释，需要研究它们是否具有一致的信念和稳定的偏好，以理性效用最大化者的方式运作

Method: 通过诊断挑战问题研究模型行为，提出可证伪条件来检验报告概率是否对应任何理性智能体的真实信念，并在多个医疗诊断领域评估多个LLMs

Result: 研究结果提供了关于LLM推理与理想贝叶斯效用最大化之间关系的见解，揭示了LLM报告概率与理性智能体真实信念之间的不一致性

Conclusion: 研究为LLM在高风险决策中的使用提供了重要启示，指出了当前LLM在理性决策方面的局限性，并提出了未来发展方向

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [8] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个基于图算法问题的基准测试，用于评估大型推理模型，揭示了当前模型在长上下文推理中的准确率下降和过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准存在局限：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要更严格的测试平台来评估大型推理模型的能力。

Method: 引入GrAlgoBench基准，包含9个图算法任务。图算法问题特别适合评估推理能力：需要长上下文推理、难度可精细控制、支持标准化程序化评估。

Result: 实验发现当前大型推理模型两大弱点：1) 上下文长度增加时准确率急剧下降（超过120个节点时低于50%），主要由于执行错误、弱记忆和冗余推理；2) 存在过度思考现象，大量无效的自我验证增加了推理痕迹但不改善正确性。

Conclusion: GrAlgoBench通过揭示当前模型的局限性，确立了图算法问题作为严格、多维且实际相关的测试平台，有助于推进大型推理模型的研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [9] [Trifuse: Enhancing Attention-Based GUI Grounding via Multimodal Fusion](https://arxiv.org/abs/2602.06351)
*Longhui Ma,Di Zhao,Siwei Wang,Zhao Lv,Miao Wang*

Main category: cs.AI

TL;DR: Trifuse是一个基于注意力的GUI grounding框架，通过整合注意力机制、OCR文本线索和图标级语义，无需任务特定微调即可实现强大的界面元素定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding方法主要依赖大规模数据集微调MLLMs来预测坐标，这种方法数据密集且对未见界面泛化能力差。基于注意力的替代方法虽然无需微调，但缺乏明确的空间锚点导致可靠性低。

Method: Trifuse框架明确整合了互补的空间锚点，通过Consensus-SinglePeak融合策略将注意力机制、OCR提取的文本线索和图标级语义描述结合起来，强制跨模态一致性同时保持尖锐的定位峰值。

Result: 在四个grounding基准测试上的广泛评估显示，Trifuse无需任务特定微调就能实现强大性能，显著减少对昂贵标注数据的依赖。消融研究表明，整合OCR和语义线索能持续提升基于注意力的grounding性能。

Conclusion: Trifuse通过整合互补的空间锚点，解决了现有基于注意力方法的可靠性问题，为GUI grounding提供了一个无需大量标注数据且泛化能力强的通用框架。

Abstract: GUI grounding maps natural language instructions to the correct interface elements, serving as the perception foundation for GUI agents. Existing approaches predominantly rely on fine-tuning multimodal large language models (MLLMs) using large-scale GUI datasets to predict target element coordinates, which is data-intensive and generalizes poorly to unseen interfaces. Recent attention-based alternatives exploit localization signals in MLLMs attention mechanisms without task-specific fine-tuning, but suffer from low reliability due to the lack of explicit and complementary spatial anchors in GUI images. To address this limitation, we propose Trifuse, an attention-based grounding framework that explicitly integrates complementary spatial anchors. Trifuse integrates attention, OCR-derived textual cues, and icon-level caption semantics via a Consensus-SinglePeak (CS) fusion strategy that enforces cross-modal agreement while retaining sharp localization peaks. Extensive evaluations on four grounding benchmarks demonstrate that Trifuse achieves strong performance without task-specific fine-tuning, substantially reducing the reliance on expensive annotated data. Moreover, ablation studies reveal that incorporating OCR and caption cues consistently improves attention-based grounding performance across different backbones, highlighting its effectiveness as a general framework for GUI grounding.

</details>


### [10] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: DEPO通过动态难度评估筛选训练数据，减少低效样本的rollout计算开销，在保持性能的同时将推理模型训练成本降低2倍。


<details>
  <summary>Details</summary>
Motivation: 现有的Group Relative Policy Optimization（GRPO）在训练推理模型时面临梯度信号衰减问题，特别是在处理过于简单或过于复杂的问题时。虽然DAPO等变体尝试解决梯度消失问题，但无法缓解在低效用样本上进行大量rollout带来的巨大计算开销。

Method: 提出Difficulty-Estimated Policy Optimization（DEPO）框架，包含一个在线难度评估器，在rollout阶段前动态评估和筛选训练数据，优先将计算资源分配给具有高学习潜力的样本。

Result: 实验结果表明，DEPO在保持模型性能的同时，将rollout成本降低了最多2倍，显著降低了训练高性能推理模型的计算门槛。

Conclusion: DEPO为推理模型的规模化训练提供了更高效和可持续的路径，通过智能数据筛选优化计算资源分配，解决了现有方法在计算效率和收敛稳定性方面的局限性。

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [11] [Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization](https://arxiv.org/abs/2602.06394)
*Arvid E. Gollwitzer,Paridhi Latawa,David de Gruijl,Deepak A. Subramanian,Adrián Noriega de la Colina*

Main category: cs.AI

TL;DR: QA-Token是一种质量感知的分词方法，通过双层优化、强化学习和自适应参数学习，将数据可靠性直接融入词汇表构建，在基因组学和金融领域显著提升性能，并减少15%的token数量。


<details>
  <summary>Details</summary>
Motivation: 当前的分词方法在处理序列数据时没有考虑信号质量，限制了其在嘈杂的现实世界语料库中的有效性。需要一种能够直接结合数据可靠性的分词方法。

Method: 提出QA-Token（质量感知分词），包含三个关键贡献：1）双层优化公式，联合优化词汇表构建和下游性能；2）强化学习方法，通过质量感知奖励学习合并策略并保证收敛；3）通过Gumbel-Softmax松弛的自适应参数学习机制进行端到端优化。

Result: 实验评估显示一致改进：基因组学（变异检测F1分数比BPE提高6.7个百分点）、金融（夏普比率提高30%）。在基础模型规模上，对1.7万亿碱基对的预训练语料进行分词，实现了最先进的病原体检测（94.53 MCC），同时减少15%的token数量。

Conclusion: QA-Token解锁了嘈杂的现实世界语料库（包括petabase规模的基因组序列和terabyte规模的金融时间序列），用于基础模型训练，且推理时无额外开销。

Abstract: Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.

</details>


### [12] [Intrinsic Stability Limits of Autoregressive Reasoning: Structural Consequences for Long-Horizon Execution](https://arxiv.org/abs/2602.06413)
*Hsien-Jyh Liao*

Main category: cs.AI

TL;DR: 论文提出自回归生成存在内在稳定性极限，即使在线性无分支任务中，决策优势会随执行长度指数衰减，导致长视野推理性能急剧下降，需要离散分段和图结构执行来维持稳定。


<details>
  <summary>Details</summary>
Motivation: 传统解释将LLMs在长视野任务中的性能下降归因于任务复杂性（如组合搜索爆炸或长期信用分配），但作者认为这些解释不完整。即使在无语义模糊的线性无分支任务中，自回归执行也存在内在稳定性极限，需要从结构治理角度重新理解长视野推理问题。

Method: 提出理论分析框架，推导定理A证明单路径自回归推理中的决策优势随执行长度指数衰减。通过合成环境和真实TextWorld任务的实证研究，验证理论预测的可观测性能悬崖现象。

Result: 实证研究显示性能下降与理论预测一致，揭示了自回归架构在维持长期一致性方面的结构性限制。短视野评估协议可能掩盖结构不稳定性，表明未来推理系统需要从单纯扩展转向结构化治理。

Conclusion: 长视野推理的根本约束源于自回归生成的过程级不稳定性而非单纯的任务复杂性。稳定长视野推理需要离散分段，自然诱导出有向无环图等图结构执行。这为理解长视野推理失败提供了动力学视角，并指出了纯自回归架构在维持长期一致性方面的新限制。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities, yet their performance often deteriorates sharply in long-horizon tasks, exhibiting systematic breakdown beyond certain scales. Conventional explanations primarily attribute this phenomenon to task complexity, such as combinatorial search explosion or long-term credit assignment challenges. In this work, we argue that these explanations are incomplete: even in linear, unbranched tasks without semantic ambiguity, autoregressive execution is subject to an intrinsic stability limit.
  We propose that the fundamental constraint on long-horizon reasoning arises from process-level instability in autoregressive generation rather than solely from search or task complexity, reframing long-horizon reasoning as a problem of structural governance. We derive Theorem~A, showing that decision advantage in single-path autoregressive reasoning decays exponentially with execution length, imposing a fundamental bound on maintainable reasoning chains. This result implies a structural consequence: stable long-horizon reasoning requires discrete segmentation, naturally inducing graph-like execution structures such as directed acyclic graphs (DAGs).
  Empirical studies in both synthetic environments and real TextWorld tasks reveal observable performance cliffs consistent with theoretical predictions. Our findings provide a dynamical perspective on long-horizon reasoning failure and suggest new limitations on maintaining long-term coherence under purely autoregressive architectures. Furthermore, we highlight that short-horizon evaluation protocols may obscure structural instability, indicating a potential shift from scaling toward structured governance in future reasoning systems.

</details>


### [13] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-Explore是首个系统研究4B参数规模智能体模型的成果，通过解决SFT灾难性遗忘、RL奖励信号噪声和长上下文冗余信息三大瓶颈，在多个基准测试中超越8B模型甚至更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统过度依赖大规模模型，边缘规模模型（4B参数）的潜力未被充分探索，需要系统研究如何克服小规模模型在智能体任务中的性能瓶颈。

Method: 提出AgentCPM-Explore框架，包含参数空间模型融合、奖励信号去噪和上下文信息精炼三个关键技术，解决SFT灾难性遗忘、RL奖励信号噪声和长上下文冗余信息问题。

Result: 在4B规模模型中达到SOTA，在四个基准测试中匹配或超越8B SOTA模型，在五个基准测试中超越Claude-4.5-Sonnet和DeepSeek-v3.2等更大模型，在GAIA文本任务上达到97.09%准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈不在于固有能力上限，而在于推理稳定性。通过系统训练框架，AgentCPM-Explore成功解锁了边缘规模模型被低估的潜力，为高效智能体部署提供了新方向。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [14] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: JADE是一个两层的评估框架，通过预定义评估技能和报告特定的声明级评估，解决了开放专业任务中评估的严谨性与灵活性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 评估智能体AI在开放式专业任务时面临基本困境：静态评估标准虽然严谨可重复，但无法适应多样化的有效响应策略；而基于LLM的评估方法虽然能适应个体响应，但存在不稳定性和偏见问题。

Method: JADE采用两层评估框架：第一层将专家知识编码为预定义的评估技能，提供稳定的评估标准；第二层进行报告特定的声明级评估，灵活评估多样化的推理策略，并通过证据依赖性门控来使基于被反驳声明的结论无效。

Result: 在BizBench上的实验表明，JADE提高了评估稳定性，并揭示了整体性LLM评估器遗漏的关键智能体失败模式。同时与专家制定的评估标准高度一致，并能有效迁移到医学领域基准测试。

Conclusion: JADE框架通过结合领域基础原则和动态声明级评估，有效解决了开放专业任务评估中的严谨性与灵活性矛盾，在不同专业领域都表现出良好的验证效果。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [15] [Progress Constraints for Reinforcement Learning in Behavior Trees](https://arxiv.org/abs/2602.06525)
*Finn Rietz,Mart Kartašev,Johannes A. Stork,Petter Ögren*

Main category: cs.AI

TL;DR: 提出进度约束机制，将行为树与强化学习结合，通过可行性估计器限制动作集，防止子控制器相互抵消，提升整体性能


<details>
  <summary>Details</summary>
Motivation: 行为树（BTs）提供结构化决策框架，强化学习（RL）能学习最优控制器但面临稀疏奖励、安全探索和长期信用分配问题。两者结合有潜力相互补充：BT编码领域知识简化RL训练，RL自动学习BT内控制器。但简单结合可能导致控制器相互抵消，破坏已实现子目标，降低整体性能。

Method: 提出进度约束机制，基于行为树收敛理论结果，使用可行性估计器限制允许的动作集，防止控制器相互抵消，确保学习过程保持已取得的进展。

Result: 在2D概念验证和高保真仓库环境中进行实证评估，相比先前的BT-RL集成方法，该方法在性能、样本效率和约束满足方面均有显著提升。

Conclusion: 进度约束机制有效解决了行为树与强化学习集成中的控制器冲突问题，通过理论保证的可行性约束，实现了更好的性能、样本效率和安全性，为BT-RL集成提供了更可靠的框架。

Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.

</details>


### [16] [HyPER: Bridging Exploration and Exploitation for Scalable LLM Reasoning with Hypothesis Path Expansion and Reduction](https://arxiv.org/abs/2602.06527)
*Shengxuan Qiu,Haochen Huang,Shuzhang Zhong,Pengfei Zuo,Meng Li*

Main category: cs.AI

TL;DR: HyPER是一种用于专家混合模型的多路径解码训练免费在线控制策略，通过动态扩展-缩减控制重新分配计算预算，实现更好的探索-利用平衡，在保持或提高准确率的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法在探索-利用权衡上存在局限：树状搜索通过脆弱的扩展规则硬编码探索，干扰后训练推理；并行推理则过度探索冗余假设路径且依赖弱答案选择。研究发现最优平衡是阶段依赖的，正确与错误推理路径往往在后期才分叉。

Method: 将测试时扩展重新表述为假设池上的动态扩展-缩减控制问题。提出HyPER：1) 在线控制器根据假设池演化从探索转向利用；2) 令牌级细化机制实现无需完整路径重采样的生成时高效利用；3) 长度和置信度感知的聚合策略实现可靠的答案时利用。

Result: 在四个专家混合语言模型和多样化推理基准上的实验表明，HyPER始终实现更优的准确率-计算权衡，准确率提高8-10%，同时令牌使用量减少25-40%。

Conclusion: HyPER通过动态控制探索-利用平衡，在固定计算预算下重新分配计算资源，为多路径推理提供了更有效的测试时扩展方法，显著提升了专家混合模型的推理效率。

Abstract: Scaling test-time compute with multi-path chain-of-thought improves reasoning accuracy, but its effectiveness depends critically on the exploration-exploitation trade-off. Existing approaches address this trade-off in rigid ways: tree-structured search hard-codes exploration through brittle expansion rules that interfere with post-trained reasoning, while parallel reasoning over-explores redundant hypothesis paths and relies on weak answer selection. Motivated by the observation that the optimal balance is phase-dependent and that correct and incorrect reasoning paths often diverge only at late stages, we reformulate test-time scaling as a dynamic expand-reduce control problem over a pool of hypotheses. We propose HyPER, a training-free online control policy for multi-path decoding in mixture-of-experts models that reallocates computation under a fixed budget using lightweight path statistics. HyPER consists of an online controller that transitions from exploration to exploitation as the hypothesis pool evolves, a token-level refinement mechanism that enables efficient generation-time exploitation without full-path resampling, and a length- and confidence-aware aggregation strategy for reliable answer-time exploitation. Experiments on four mixture-of-experts language models across diverse reasoning benchmarks show that HyPER consistently achieves a superior accuracy-compute trade-off, improving accuracy by 8 to 10 percent while reducing token usage by 25 to 40 percent.

</details>


### [17] [LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models](https://arxiv.org/abs/2602.06533)
*Brian Rabern,Philipp Mondorf,Barbara Plank*

Main category: cs.AI

TL;DR: LogicSkills基准测试评估大语言模型在形式推理中的三项核心技能：形式符号化、反模型构建和有效性评估，发现模型在有效性评估上表现良好，但在符号化和反模型构建上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种逻辑推理基准测试中表现出色，但尚不清楚它们真正掌握了哪些核心逻辑技能。为了探究这一问题，需要设计一个统一的基准测试来隔离和评估形式推理中的基本技能。

Method: 引入LogicSkills基准测试，专注于评估三项核心逻辑技能：1) 形式符号化（将前提翻译为一阶逻辑）；2) 反模型构建（构建一个有限结构，使所有前提为真而结论为假）；3) 有效性评估（判断结论是否从给定前提中得出）。测试项目来自不带恒等式的两变量一阶逻辑片段，以自然英语和Carroll风格的非词语言呈现，并使用SMT求解器Z3验证正确性和非平凡性。

Result: 在领先的大语言模型中，有效性评估的表现较高，但形式符号化和反模型构建的表现显著较低。这表明模型可能依赖表面模式而非真正的符号或基于规则的推理。

Conclusion: 大语言模型在逻辑推理中更擅长模式匹配式的有效性判断，但在需要深入符号处理和反例构建的推理任务上存在明显不足，揭示了它们逻辑能力的局限性。

Abstract: Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\textit{formal symbolization}\unicode{x2014}$translating premises into first-order logic; (ii) $\textit{countermodel construction}\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\textit{validity assessment}\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.

</details>


### [18] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出了AgentCPM-Report，一个轻量级高性能的本地深度研究报告生成系统，采用8B参数模型和仿人类写作过程的框架，通过动态大纲修订和交替的证据起草与推理深化，在多个基准测试中优于闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究报告生成方法依赖计划-写作范式，其性能严重依赖初始大纲质量，而构建全面大纲需要强大推理能力，导致当前系统几乎完全依赖闭源或在线大模型，这带来了部署障碍以及用户数据的安全和隐私问题。

Method: 提出了AgentCPM-Report系统，包含：1）仿人类写作过程的框架，采用写作即推理策略（WARP），允许在报告生成过程中动态修订大纲；2）8B参数的深度研究智能体，交替进行证据起草和推理深化；3）多阶段智能体训练策略，包括冷启动、原子技能强化学习和整体管道强化学习。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym等基准测试中，AgentCPM-Report优于领先的闭源系统，在Insight指标上取得显著提升。

Conclusion: AgentCPM-Report提供了一个轻量级、高性能的本地解决方案，通过创新的写作即推理策略和多阶段训练方法，使小型模型能够生成高质量的深度研究报告，解决了对闭源大模型的依赖问题。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [19] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 提出SeeUPO算法，解决现有RL算法在多轮交互场景中缺乏收敛保证的问题，通过序列级顺序更新策略优化实现单调改进和全局最优收敛。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的AI智能体主要使用强化学习训练，但主流RL算法在多轮交互场景中缺乏验证的收敛保证，导致训练不稳定和无法收敛到最优策略。

Method: 提出SeeUPO算法：将多轮交互建模为顺序执行的多智能体赌博机问题，采用反向执行顺序的逐轮顺序策略更新，通过逆向归纳确保单调改进和全局最优收敛。

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著提升：Qwen3-14B上相对增益43.3%-54.6%，Qwen2.5-14B上24.1%-41.9%，且训练稳定性更优。

Conclusion: SeeUPO解决了多轮交互场景中RL算法的收敛保证问题，提供了一种无评论家且具有收敛保证的方法，显著提升了训练性能和稳定性。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [20] [Same Answer, Different Representations: Hidden instability in VLMs](https://arxiv.org/abs/2602.06652)
*Farooq Ahmad Wani,Alessandro Suglia,Rohit Saxena,Aryo Pradipta Gema,Wai-Chung Kwan,Fazl Barez,Maria Sofia Bucarelli,Fabrizio Silvestri,Pasquale Minervini*

Main category: cs.AI

TL;DR: 本文提出一个表示感知和频率感知的评估框架，发现VLMs在保持预测不变时内部表示会发生显著漂移，模型规模增大不提升鲁棒性，扰动对不同任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs鲁棒性评估主要基于输出层面的不变性，隐含假设稳定预测反映稳定的多模态处理。本文认为这一假设不足，需要更深入评估内部表示的变化。

Method: 提出表示感知和频率感知的评估框架，测量内部嵌入漂移、频谱敏感性和结构平滑性（视觉token的空间一致性），结合标准标签指标。在SEEDBench、MMMU和POPE数据集上测试现代VLMs。

Result: 发现三种失效模式：1) 模型保持预测答案时内部表示发生显著漂移；2) 模型规模增大不提升鲁棒性，更大模型更敏感；3) 扰动对不同任务影响不同：损害推理但减少幻觉误报。

Conclusion: 仅依赖输出不变性评估VLMs鲁棒性不足，需要结合内部表示分析。模型规模增大不必然提升鲁棒性，扰动对不同任务有复杂影响，为VLMs鲁棒性评估提供新视角。

Abstract: The robustness of Vision Language Models (VLMs) is commonly assessed through output-level invariance, implicitly assuming that stable predictions reflect stable multimodal processing. In this work, we argue that this assumption is insufficient. We introduce a representation-aware and frequency-aware evaluation framework that measures internal embedding drift, spectral sensitivity, and structural smoothness (spatial consistency of vision tokens), alongside standard label-based metrics. Applying this framework to modern VLMs across the SEEDBench, MMMU, and POPE datasets reveals three distinct failure modes. First, models frequently preserve predicted answers while undergoing substantial internal representation drift; for perturbations such as text overlays, this drift approaches the magnitude of inter-image variability, indicating that representations move to regions typically occupied by unrelated inputs despite unchanged outputs. Second, robustness does not improve with scale; larger models achieve higher accuracy but exhibit equal or greater sensitivity, consistent with sharper yet more fragile decision boundaries. Third, we find that perturbations affect tasks differently: they harm reasoning when they disrupt how models combine coarse and fine visual cues, but on the hallucination benchmarks, they can reduce false positives by making models generate more conservative answers.

</details>


### [21] [Autoregressive Models for Knowledge Graph Generation](https://arxiv.org/abs/2602.06707)
*Thiviyan Thanapalasingam,Antonis Vozikis,Peter Bloem,Paul Groth*

Main category: cs.AI

TL;DR: ARK是一种自回归知识图谱生成模型，将图谱视为三元组序列，无需显式规则监督即可学习语义约束，在IntelliGraphs基准测试中达到89.2%-100%语义有效性，并能生成未见过的图谱。


<details>
  <summary>Details</summary>
Motivation: 知识图谱生成需要模型学习三元组间的复杂语义依赖关系，同时保持领域有效性约束。与独立评分三元组的链接预测不同，生成模型必须捕捉整个子图的相互依赖关系以产生语义连贯的结构。

Method: 提出ARK（自回归知识图谱生成）模型家族，将图谱视为(head, relation, tail)三元组序列进行自回归生成。模型直接从数据中学习隐式语义约束（类型一致性、时间有效性、关系模式），无需显式规则监督。还提出SAIL，ARK的变分扩展，通过学习的潜在表示实现可控生成。

Result: 在IntelliGraphs基准测试中，模型在多样化数据集上达到89.2%到100.0%的语义有效性，同时能生成训练中未见的新图谱。分析显示模型容量（隐藏维度≥64）比架构深度更重要，循环架构在达到与基于Transformer的替代方案相当有效性的同时提供显著计算效率。

Conclusion: 自回归模型为知识图谱生成提供了有效框架，在知识库补全和查询回答中具有实际应用价值。

Abstract: Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. Unlike link prediction, which scores triples independently, generative models must capture interdependencies across entire subgraphs to produce semantically coherent structures. We present ARK (Auto-Regressive Knowledge Graph Generation), a family of autoregressive models that generate KGs by treating graphs as sequences of (head, relation, tail) triples. ARK learns implicit semantic constraints directly from data, including type consistency, temporal validity, and relational patterns, without explicit rule supervision. On the IntelliGraphs benchmark, our models achieve 89.2% to 100.0% semantic validity across diverse datasets while generating novel graphs not seen during training. We also introduce SAIL, a variational extension of ARK that enables controlled generation through learned latent representations, supporting both unconditional sampling and conditional completion from partial graphs. Our analysis reveals that model capacity (hidden dimensionality >= 64) is more critical than architectural depth for KG generation, with recurrent architectures achieving comparable validity to transformer-based alternatives while offering substantial computational efficiency. These results demonstrate that autoregressive models provide an effective framework for KG generation, with practical applications in knowledge base completion and query answering.

</details>


### [22] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出一种基于语义LTL到自动机转换的多任务强化学习方法，利用结构化任务嵌入提升策略泛化能力


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中，需要学习一个能够泛化到任意（可能未见）任务的通用策略。现有方法在处理复杂LTL规范时存在局限性，需要更有效的任务表示方法。

Method: 利用新一代语义LTL到自动机转换技术，构建语义标记的自动机，从中提取结构化任务嵌入来条件化策略，支持完整的LTL规范。

Result: 在多个领域实验中，该方法实现了最先进的性能，能够扩展到现有方法失败的复杂规范场景。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术能够有效支持多任务强化学习，处理复杂LTL规范，提升策略的泛化能力。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [23] [Towards Understanding What State Space Models Learn About Code](https://arxiv.org/abs/2602.06774)
*Jiali Wu,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.AI

TL;DR: SSM代码模型在预训练中优于Transformer捕捉代码语法语义，但在微调时会遗忘某些语法语义关系，尤其是在短距离依赖任务中。作者提出SSM-Interpret诊断框架和架构改进来提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然状态空间模型(SSMs)在代码理解任务上表现优异，但其内部机制仍不透明。需要系统分析SSM代码模型学习的内容，并与Transformer模型进行对比，以理解其优势和局限性。

Method: 1. 对SSM和Transformer代码模型进行首次系统性比较分析；2. 提出SSM-Interpret频率域框架，诊断微调过程中的频谱变化；3. 基于分析结果提出架构改进方案。

Result: SSMs在预训练中比Transformers更好地捕捉代码语法和语义，但在任务微调时会遗忘某些语法语义关系，特别是在强调短距离依赖的任务中。SSM-Interpret揭示了微调过程中频谱向短距离依赖的偏移。

Conclusion: 通过SSM-Interpret框架的诊断，提出的架构改进显著提升了SSM代码模型的性能，验证了分析能够直接指导更好的模型设计。

Abstract: State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a black box. We present the first systematic analysis of what SSM-based code models actually learn and perform the first comparative analysis of SSM and Transformer-based code models. Our analysis reveals that SSMs outperform Transformers at capturing code syntax and semantics in pretraining but forgets certain syntactic and semantic relations during fine-tuning on task, especially when the task emphasizes short-range dependencies. To diagnose this, we introduce SSM-Interpret, a frequency-domain framework that exposes a spectral shift toward short-range dependencies during fine-tuning. Guided by these findings, we propose architectural modifications that significantly improve the performance of SSM-based code model, validating that our analysis directly enables better models.

</details>


### [24] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 研究比较了理性主动学习（最大化期望信息增益）与人类正向测试策略在概念学习中的表现，发现前者在复杂规则中有效，但在简单概念上表现不佳，揭示了人类"确认偏误"可能是维持可处理推理的理性适应。


<details>
  <summary>Details</summary>
Motivation: 研究人类主动概念学习中查询选择策略的权衡：如何在查询的信息量与学习者生成和评分假设的稳定性之间取得平衡。探索人类"确认偏误"（正向测试策略）是否是一种认知错误，还是维持可处理推理的理性适应。

Method: 使用神经符号贝叶斯学习器，其中假设是由大型语言模型生成的可执行程序，通过贝叶斯更新重新加权。比较两种策略：理性主动学习（选择最大化近似期望信息增益的查询）和人类正向测试策略（查询当前最佳假设预测为正的实例）。在经典数字游戏的概念学习任务中进行实验。

Result: 期望信息增益策略在需要证伪的复杂规则（如复合规则或例外规则）中有效，但在简单概念上表现不佳。这种失败源于EIG策略与LLM提议分布之间的支持不匹配：高度诊断性的边界查询将后验推向生成器产生无效或过于具体程序的区域，导致粒子近似中的支持不匹配陷阱。正向测试策略虽然信息次优，但通过选择"安全"查询倾向于保持提议有效性，在简单规则上收敛更快。

Conclusion: 人类的"确认偏误"（正向测试策略）可能不是认知错误，而是在人类思维特有的稀疏、开放假设空间中维持可处理推理的理性适应。这种策略通过避免支持不匹配陷阱，在简单概念学习中表现更好。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [25] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但当前交互环境严重不足，现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力，并实证证明了环境多样性扩展对智能体学习鲁棒性的重要性。

Conclusion: ScaleEnv通过从零构建交互式环境解决了环境稀缺问题，证明了扩展环境多样性对智能体泛化能力的关键作用，为通用智能体训练提供了有效框架。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [26] [POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models](https://arxiv.org/abs/2602.06822)
*Yi Chen,Wonjin Shin,Shuhong Liu,Tho Mai,Jeongmo Lee,Chuanbo Hua,Kun Wang,Jun Liu,Joo-Young Kim*

Main category: cs.AI

TL;DR: POP是一种轻量级在线结构化剪枝框架，通过分区机制实现上下文条件化的动态剪枝，在自回归生成过程中减少计算开销，无需预处理或重训练。


<details>
  <summary>Details</summary>
Motivation: 当前的结构化剪枝方法在推理时采用固定的剪枝决策，忽略了自回归token生成过程中出现的稀疏模式，无法充分利用上下文信息进行动态优化。

Method: POP将模型通道划分为保留区、候选区和剪枝区：预填充阶段定义粗粒度剪枝分区，解码阶段在候选区内生成细粒度掩码，避免全通道重新评估。粗粒度分区保留持续重要的权重，细粒度掩码提供解码时的上下文条件化变化。

Result: 在多种大型基础模型（LLMs、MoEs、VLMs）上的广泛评估表明，POP比现有剪枝方法提供更高的准确性，同时产生更小的计算开销并最小化推理延迟。

Conclusion: POP是一种轻量级即插即用的在线剪枝框架，无需预处理（离线校准、重训练或学习预测器），能够在自回归生成过程中实现上下文条件化的动态剪枝，显著提升效率。

Abstract: Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.

</details>


### [27] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 提出一个基于纳什均衡分析的博弈论框架，用于预测和引导大型语言模型群体的行为，通过将代理行为建模为人类子群体的混合选择，实现可解释的系统级预测和社会期望结果的引导。


<details>
  <summary>Details</summary>
Motivation: 为了解决在开放文本空间中计算均衡的困难，并预测和引导LLM群体的行为，特别是避免政治排斥等社会病理现象，需要一种能够分析多智能体LLM动态并实现社会期望结果的方法。

Method: 将每个代理的行为建模为人类子群体的混合选择，代理主动战略性地选择与哪些群体对齐；采用标准凹效用假设推导闭式纳什均衡特征；作为现有对齐流程（如RLHF）之上的主动对齐层。

Result: 在社交媒体设置中，LLM群体（特别是基于推理的模型）可能表现出政治排斥现象，即某些子群体被所有LLM代理忽略；该方法能够避免这种病理现象，展示了在多领域调节多智能体LLM动态的潜力。

Conclusion: 该博弈论框架为预测和引导LLM群体行为提供了可解释且行为实质性的方法，能够通过明确的行动指导将对齐目标转向社会期望结果，在调节多智能体LLM动态方面具有广阔应用前景。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [28] [An Adaptive Differentially Private Federated Learning Framework with Bi-level Optimization](https://arxiv.org/abs/2602.06838)
*Jin Wang,Hui Ma,Fei Xing,Ming Yan*

Main category: cs.AI

TL;DR: 提出自适应差分隐私联邦学习框架，通过客户端轻量压缩模块、服务器自适应梯度裁剪和约束感知聚合机制，解决异构数据和隐私约束下的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际部署中面临设备异构性和非独立同分布数据导致的梯度更新不稳定和偏差问题。当强制实施差分隐私时，传统的固定梯度裁剪和高斯噪声注入会进一步放大梯度扰动，导致训练振荡和性能下降。

Method: 1. 客户端：引入轻量级本地压缩模块，正则化中间表示并约束梯度变异性，减轻本地优化期间的噪声放大
2. 服务器：基于历史更新统计动态调整裁剪阈值的自适应梯度裁剪策略，避免过度裁剪和噪声主导
3. 服务器：设计约束感知聚合机制，抑制不可靠或噪声主导的客户端更新，稳定全局优化

Result: 在CIFAR-10和SVHN数据集上的大量实验表明，该框架提高了收敛稳定性和分类准确率。

Conclusion: 提出的自适应差分隐私联邦学习框架有效解决了异构和隐私约束环境下的模型效率问题，通过客户端正则化、自适应裁剪和智能聚合机制实现了更稳定的训练和更好的性能。

Abstract: Federated learning enables collaborative model training across distributed clients while preserving data privacy. However, in practical deployments, device heterogeneity, non-independent, and identically distributed (Non-IID) data often lead to highly unstable and biased gradient updates. When differential privacy is enforced, conventional fixed gradient clipping and Gaussian noise injection may further amplify gradient perturbations, resulting in training oscillation and performance degradation and degraded model performance. To address these challenges, we propose an adaptive differentially private federated learning framework that explicitly targets model efficiency under heterogeneous and privacy-constrained settings. On the client side, a lightweight local compressed module is introduced to regularize intermediate representations and constrain gradient variability, thereby mitigating noise amplification during local optimization. On the server side, an adaptive gradient clipping strategy dynamically adjusts clipping thresholds based on historical update statistics to avoid over-clipping and noise domination. Furthermore, a constraint-aware aggregation mechanism is designed to suppress unreliable or noise-dominated client updates and stabilize global optimization. Extensive experiments on CIFAR-10 and SVHN demonstrate improved convergence stability and classification accuracy.

</details>


### [29] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态分类任务中的属性解释与智能体系统中的轨迹诊断，发现传统属性解释方法不适用于诊断智能体执行失败，而基于轨迹的评估方法能有效定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，AI系统从静态预测转向多步骤决策的智能体系统。传统可解释AI方法主要针对单次预测，不清楚这些方法如何适用于行为随时间演化的智能体设置。需要弥合静态可解释性与智能体可解释性之间的差距。

Method: 在静态分类任务中比较属性解释方法，在智能体基准测试（TAU-bench Airline和AssistantBench）中比较基于轨迹的诊断方法。使用基于轨迹的评估标准来定位行为故障。

Result: 属性方法在静态设置中特征排序稳定（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障。基于轨迹的评估能一致定位行为故障，发现状态跟踪不一致在失败运行中高出2.7倍，并将成功概率降低49%。

Conclusion: 需要从静态解释转向轨迹级可解释性来评估和诊断自主AI行为。基于轨迹的诊断方法更适合智能体系统，能有效识别行为故障的根本原因。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [30] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个包含20个任务的AI研究科学基准测试套件，用于评估LLM代理在完整科研生命周期中的能力，结果显示代理在4个任务上超过人类SOTA，但在16个任务上未能达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 为了加速LLM代理在科学研究中的应用，需要建立一个能够全面评估代理在完整科研生命周期中能力的基准测试，包括从想法生成到实验分析和迭代优化的全过程。

Method: 从顶级机器学习论文中选取20个任务构建AIRS-Bench套件，涵盖语言建模、数学、生物信息学和时间序列预测等多个领域。采用灵活的基准格式，支持新任务的集成和不同代理框架的严格比较。使用前沿模型配合顺序和并行架构建立基线。

Result: 代理在4个任务上超过了人类SOTA，但在16个任务上未能达到人类水平。即使代理在某些任务上超越了人类基准，也未能达到该任务的理论性能上限，表明AIRS-Bench远未饱和，仍有很大改进空间。

Conclusion: AIRS-Bench是一个有效的评估工具，能够推动自主科学研究的发展。研究结果表明当前LLM代理在科学研究能力上仍有显著提升空间，开源基准测试套件将促进该领域的进一步发展。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [31] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI代理在任务执行前、中、后预测成功率时表现出过度自信，成功率仅22%的代理预测77%成功率。反直觉的是，信息更少的执行前评估比执行后评估有更好的区分能力，而对抗性提示（重构为bug查找）实现了最佳校准。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理是否能够准确预测自己在任务上的成功率，探索代理性不确定性（agentic uncertainty）的表现形式，了解AI代理对自身能力的评估准确性。

Method: 在任务执行前、执行中和执行后三个阶段收集AI代理的成功概率估计；比较不同信息量下的预测准确性；使用对抗性提示方法，将评估重构为bug查找任务。

Result: 发现代理性过度自信现象：一些实际成功率仅22%的代理预测成功率为77%；反直觉地，信息更少的执行前评估比标准执行后评估有更好的区分能力（尽管差异不总是显著）；对抗性提示（bug查找方法）实现了最佳校准效果。

Conclusion: AI代理在评估自身任务成功率时存在系统性过度自信；减少信息量可能改善预测区分能力；对抗性提示策略可以显著提高校准质量，这对AI系统的自我评估能力开发具有重要意义。

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [32] [UAV-Mounted Aerial Relays in Military Communications: A Comprehensive Survey](https://arxiv.org/abs/2602.06061)
*Faisal Al-Kamali,Francois Chan,Hussein A. Ammar,James H. Bayes,Claude D'Amours*

Main category: cs.IT

TL;DR: 该论文全面综述了军事通信中的空中中继系统，比较了空中中继与地面中继的优劣，介绍了AAR和ARIS两种技术，提出了MCRES评估指标和决策算法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统地面中继在军事通信中存在位置固定、易受物理障碍限制的缺陷，而无人机搭载的空中中继系统具有动态灵活的优势，能够适应复杂战场环境，需要系统性地评估和选择最适合军事任务的中继方案。

Method: 1. 对空中中继系统进行全面综述和定性分析；2. 提出多维度评估指标MCRES（任务关键中继效能评分）；3. 开发决策算法（Algorithm 1）用于系统选择最优中继类型；4. 比较主动空中中继（AAR）和空中可重构智能表面（ARIS）两种技术。

Result: 建立了完整的空中中继评估框架，通过MCRES指标能够定量评估中继系统在机动性、抗干扰、部署速度、隐蔽性、覆盖范围和自主性等关键属性上的表现，为不同军事场景提供科学的中继选择依据。

Conclusion: 无人机搭载的空中中继系统在军事通信中具有显著优势，特别是AAR和ARIS技术。通过提出的MCRES评估框架和决策算法，能够根据具体任务需求选择最优中继方案，但实际部署仍面临挑战，需要进一步研究以提升在对抗性军事环境中的鲁棒性和弹性。

Abstract: Relays are pivotal in military communication networks, expanding coverage and ensuring reliable connectivity in challenging operational environments. While traditional terrestrial relays (TR) are constrained by fixed locations and vulnerability to physical obstructions, unmanned aerial vehicle (UAV)-mounted aerial relays (AR) offer a dynamic and flexible alternative by operating above obstacles and adapting to changing battlefield conditions. This paper provides a comprehensive survey of AR systems in military communications, presenting a detailed comparison between AR and TR paradigms and examining two specific AR technologies: active aerial relays (AAR) and aerial reconfigurable intelligent surface (ARIS) relays. The survey delves into their operation, benefits, challenges, and military applications, supported by a qualitative analysis across metrics such as coverage, flexibility, security, and cost. A novel multi-dimensional metric, the mission-critical relay effectiveness score (MCRES), is introduced as a quantitative method for evaluating relay suitability based on mission-specific weights for critical attributes like mobility, jamming resilience, deployment speed, stealth, coverage, and autonomy. Furthermore, we present Algorithm 1, a decision-making framework that leverages the MCRES to guide the systematic selection of the optimal relay type, AR or TR, and subsequently AAR or ARIS, tailored to the unique demands of a given military scenario, such as dynamic battlefield operations, electronic warfare, or covert missions. Finally, the paper addresses current implementation challenges and outlines promising future research directions to advance the deployment of robust and resilient UAV-mounted relay systems in contested military environments.

</details>


### [33] [Asymptotically Optimal Aperiodic Doppler Resilient Complementary Sequence Sets Via Generalized Quasi-Florentine Rectangles](https://arxiv.org/abs/2602.06045)
*Zheng Wang,Zhiye Yang,Yang Yang,Avik Ranjan Adhikary,Keqin Feng*

Main category: cs.IT

TL;DR: 提出基于广义准Florentine矩形和Butson型Hadamard矩阵构建非周期性多普勒弹性互补序列集的方法，证明其渐近最优性


<details>
  <summary>Details</summary>
Motivation: 多普勒弹性互补序列集在现代通信和感知系统中，特别是在高移动性环境中至关重要，需要构建性能更优的序列集

Method: 1. 将准Florentine矩形的定义精炼为更一般化的广义准Florentine矩形，并提出系统化构造方法；2. 基于广义准Florentine矩形和Butson型Hadamard矩阵构建多组非周期性多普勒弹性互补序列集

Result: 提出的非周期性多普勒弹性互补序列集相对于非周期性多普勒弹性互补序列集的下界具有渐近最优性

Conclusion: 通过推广准Florentine矩形概念并结合Butson型Hadamard矩阵，成功构建了渐近最优的非周期性多普勒弹性互补序列集，为高移动性环境下的通信和感知系统提供了有效的序列设计方案

Abstract: Doppler-resilient complementary sequence (DRCS) sets play a vital role in modern communication and sensing systems, particularly in high-mobility environments. This work makes two primary contributions. First, we refine the definition of quasi-Florentine rectangles to a more general form,termed generalized quasi-Florentine rectangles, and propose a systematic method for their construction. Second, we propose several sets of aperiodic DRCS based on generalized quasi Florentine rectangles and Butson-type Hadamard matrices. The proposed aperiodic DRCS sets are shown to be asymptotically optimal with respect to the lower bound of aperiodic DRCS sets.

</details>


### [34] [Deep Unfolded Fractional Optimization for Maximizing Robust Throughput in 6G Networks](https://arxiv.org/abs/2602.06062)
*Anh Thi Bui,Robert-Jeron Reifert,Hayssam Dahrouj,Aydin Sezgin*

Main category: cs.IT

TL;DR: 提出UI-DUFP框架，通过深度展开分式规划和不确定性注入训练，实现6G网络下行波束赋形中加权和速率的最大化，在信道不完美条件下提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要利用AI工具进行高效鲁棒的优化。传统优化方法计算复杂度高，而现有深度学习方法在信道不完美条件下鲁棒性不足，需要开发能处理信道不确定性的鲁棒优化框架。

Method: 提出不确定性注入深度展开分式规划(UI-DUFP)框架：1)将分式规划迭代展开为可训练的神经网络层；2)用投影梯度下降步骤进行精炼；3)训练时注入采样的信道不确定性；4)优化基于分位数的目标函数。

Result: 仿真结果表明，UI-DUFP相比传统加权最小均方误差、分式规划和深度学习基线方法，实现了更高的加权和速率和更好的鲁棒性，同时保持低推理时间和良好的可扩展性。

Conclusion: 深度展开与不确定性感知训练相结合，是6G网络中鲁棒优化的有效方法，为AI赋能的无线网络优化提供了有前景的解决方案。

Abstract: The sixth-generation (6G) of wireless communication networks aims to leverage artificial intelligence tools for efficient and robust network optimization. This is especially the case since traditional optimization methods often face high computational complexity, motivating the use of deep learning (DL)-based optimization frameworks. In this context, this paper considers a multi-antenna base station (BS) serving multiple users simultaneously through transmit beamforming in downlink mode. To account for robustness, this work proposes an uncertainty-injected deep unfolded fractional programming (UI-DUFP) framework for weighted sum rate (WSR) maximization under imperfect channel conditions. The proposed method unfolds fractional programming (FP) iterations into trainable neural network layers refined by projected gradient descent (PGD) steps, while robustness is introduced by injecting sampled channel uncertainties during training and optimizing a quantile-based objective. Simulation results show that the proposed UI-DUFP achieves higher WSR and improved robustness compared to classical weighted minimum mean square error, FP, and DL baselines, while maintaining low inference time and good scalability. These findings highlight the potential of deep unfolding combined with uncertainty-aware training as a powerful approach for robust optimization in 6G networks.

</details>


### [35] [UAV-Enabled Short-Packet Communication via Fluid Antenna Systems](https://arxiv.org/abs/2602.06206)
*Xusheng Zhu,Kai-Kit Wong,Hanjiang Hong,Han Xiao,Hao Xu,Tuo Wu,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文开发了一个用于分析无人机使能的短包通信框架，利用流体天线系统辅助中继网络，在短包机制下推导了块错误率的闭式表达式，并提出了考虑FAS端口选择开销的能效最大化问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决城市环境中无人机短包通信的挑战，特别是如何利用流体天线系统提升性能，同时考虑实际系统中FAS端口选择的非理想开销问题。

Method: 采用基于特征值的可处理方法来建模空间相关的Nakagami-m衰落链路，推导BLER的闭式表达式，进行高信噪比渐近分析，并设计分层算法联合优化系统参数。

Result: 数值结果验证了分析的正确性，显示FAS能提供显著功率增益，但操作开销带来了关键权衡，这决定了FAS端口的最优数量和无人机部署的最佳高度。

Conclusion: 本研究为FAS辅助的无人机通信提供了关键见解，揭示了在功率增益和操作开销之间的权衡，以及由遮挡和路径损耗平衡决定的最优部署策略。

Abstract: This paper develops a framework for analyzing UAV-enabled short-packet communication, leveraging fluid antenna system (FAS)-assisted relaying networks. Operating in the short-packet regime and focusing on challenging urban environments, we derive novel, closed-form expressions for the block error rate (BLER). This is achieved by modeling the spatially correlated Nakagami-$m$ fading link via a tractable eigenvalue-based approach. A high-signal-to-noise ratio (SNR) asymptotic analysis is also presented, revealing the system's fundamental diversity order. Building on this analysis, we formulate a novel energy efficiency (EE) maximization problem that, unlike idealized models, uniquely incorporates the non-trivial time and energy overheads of FAS port selection. An efficient hierarchical algorithm is proposed to jointly optimize key system parameters. Numerical results validate our analysis, demonstrating that while FAS provides substantial power gains, the operational overhead creates a critical trade-off. This trade-off dictates an optimal number of FAS ports and a non-trivial optimal UAV deployment altitude, governed by the balance between blockage and path loss. This work provides key insights for FAS-aided UAV communications.

</details>


### [36] [Private Sum Computation: Trade-Offs between Communication, Randomness, and Privacy](https://arxiv.org/abs/2602.06238)
*Remi A. Chou,Joerg Kliewer,Aylin Yener*

Main category: cs.IT

TL;DR: 论文研究了多用户与融合中心之间的隐私保护求和计算问题，在保证隐私约束下最小化通信和随机性开销，并建立了与秘密共享的联系。


<details>
  <summary>Details</summary>
Motivation: 在多用户系统中，融合中心需要计算所有用户比特序列的和，同时要保护用户数据的隐私，防止用户与融合中心合谋获取其他用户的敏感信息。这需要在隐私保护和通信效率之间找到平衡。

Method: 通过信息论方法分析隐私约束下的通信和随机性需求，建立隐私求和计算与秘密共享之间的联系，证明秘密共享是生成本地随机性的必要条件。

Result: 1. 表征了用户与融合中心之间所需的最小通信量以及用户所需的最小随机性量；2. 证明了对于任意δ≥0，秘密共享是生成隐私求和计算所需本地随机性的必要条件。

Conclusion: 隐私求和计算问题可以通过信息论框架进行分析，秘密共享是实现隐私保护的关键机制，这一联系为设计高效的隐私保护分布式计算协议提供了理论基础。

Abstract: Consider multiple users and a fusion center. Each user possesses a sequence of bits and can communicate with the fusion center through a one-way public channel. The fusion center's task is to compute the sum of all the sequences under the privacy requirement that a set of colluding users, along with the fusion center, cannot gain more than a predetermined amount $δ$ of information, measured through mutual information, about the sequences of other users. Our first contribution is to characterize the minimum amount of necessary communication between the users and the fusion center, as well as the minimum amount of necessary randomness at the users. Our second contribution is to establish a connection between private sum computation and secret sharing by showing that secret sharing is necessary to generate the local randomness needed for private sum computation, and prove that it holds true for any $δ\geq 0$.

</details>


### [37] [AI-Limited Fluid Antenna-Aided Integrated Sensing and Communication Systems](https://arxiv.org/abs/2602.06247)
*Farshad Rostami Ghadi,Kai-Kit Wong,F. Javier Lopez-Martinez,Zhentian Zhang,Hyundong Shin,Christos Masouros*

Main category: cs.IT

TL;DR: 论文研究了集成感知与通信(ISAC)在AI表示瓶颈和流体天线系统(FAS)下的基本极限，揭示了AI瓶颈等效于加性表示噪声，并推导了ISAC容量-失真区域，证明了FAS长度决定了端口选择增益的多样性阶数。


<details>
  <summary>Details</summary>
Motivation: 研究在AI表示瓶颈约束下，结合流体天线系统(FAS)的集成感知与通信(ISAC)系统的性能极限。AI编码器将理想高斯波形映射为有限容量的潜在表示，而FAS接收器选择信道条件最佳的端口，需要分析这种架构下的基本性能边界。

Method: 首先将AI瓶颈建模为加性表示噪声，降低通信和感知的信噪比。然后推导ISAC容量-失真区域，建立紧密的逆界和可达界。利用Jakes相关信道的空间自由度特性，分析FAS长度对端口选择增益的影响，证明有效多样性阶数等于Jakes相关矩阵的数值秩。

Result: AI瓶颈等效于加性表示噪声，降低通信和感知SNR。端口选择增益受FAS物理长度限制，有效多样性阶数等于Jakes相关矩阵的数值秩。增大FAS长度可使选择端口SNR接近AI施加的天花板，使通信速率和感知MSE趋近AI限制的基本边界。

Conclusion: 在AI表示瓶颈和FAS的ISAC系统中，FAS长度是决定性能的关键因素。增大FAS长度可提升端口选择增益，使系统性能接近AI限制的基本边界，为实际系统设计提供了理论指导。

Abstract: This paper characterizes the fundamental limits of integrated sensing and communication (ISAC) when the transmitter is subject to an artificial intelligence (AI) representation bottleneck and the receiver employs a fluid antenna system (FAS). Specifically, the message is first encoded into an ideal Gaussian waveform and mapped by an AI encoder into a finite-capacity latent representation that constitutes the physical channel input, while the FAS receiver selects the port experiencing the most favorable channel conditions. We reveal that the AI bottleneck is equivalent to an additive representation noise, which reduces both the communication and sensing signal-to-noise ratios (SNRs) at the selected port. We then derive the resulting ISAC capacitydistortion region and establish tight converse and achievability bounds under general fading models, including Jakes-correlated channels. Leveraging the spatial degrees of freedom (DoF) characterization of the Jakes' model, we furthermore prove that the port-selection gain is fundamentally constrained by the physical length of the FAS region: the effective diversity order equals the numerical rank of the Jakes' correlation matrix and increases only with the FAS length. Consequently, enlarging the FAS length allows the selected-port SNR to approach the AI-imposed ceiling, driving the achievable communication rate and sensing mean square error (MSE) toward their AI-limited fundamental bounds. Numerical results corroborate the analysis and scaling laws.

</details>


### [38] [Hermitian Self-dual Generalized Reed-Solomon Codes](https://arxiv.org/abs/2602.06377)
*Chun'e Zhao,Wenping Ma*

Main category: cs.IT

TL;DR: 本文完全解决了Hermitian自对偶GRS码的存在性和构造问题，证明了当n≤q+1时只有两类Hermitian自对偶GRS码，并给出了两种显式构造方法。


<details>
  <summary>Details</summary>
Motivation: MDS自对偶码具有重要的理论和实践意义，GRS码是最重要的MDS码。虽然欧几里得自对偶MDS码的研究很多，但Hermitian自对偶GRS码的研究相对有限。由于Hermitian自对偶GRS码在n>q+1时不存在，本文致力于研究n≤q+1情况下的GRS码。

Method: 首先证明了当n≤q+1时，只存在两类Hermitian自对偶GRS码，证实了文献[13]中的猜想并同时提供了证明。其次提出了两种显式构造方法。

Result: 完全解决了Hermitian自对偶GRS码的存在性和构造问题，确定了在n≤q+1条件下只有两类这样的码存在，并给出了具体的构造方法。

Conclusion: 本文对Hermitian自对偶GRS码的研究做出了完整贡献，解决了存在性问题和构造问题，为相关领域提供了理论基础和实用工具。

Abstract: Maximum Distance Separable (MDS) self-dual codes are of significant theoretical and practical importance. Generalized Reed-Solomon (GRS) codes are the most prominent MDS codes. Correspondingly there have been many research on constructions of Euclidean self-dual MDS codes by using GRS codes. However, the study on Hermitian self-dual GRS codes is relatively limited. Since Hermitian self-dual GRS codes do not exist for $n>q+1$, this paper is devoted to an investigation of GRS codes in the case where $n\le q+1$. First, we prove that when $n\leq q+1$, there are only two classes of Hermitian self-dual GRS codes, confirming the conjecture in [13] and providing its proof simultaneously. Second, we present two explicit construction methods. Thus, the existence and construction of Hermitian self-dual GRS codes are fully solved.

</details>


### [39] [Joint Lossy Compression for a Vector Gaussian Source under Individual Distortion Criteria](https://arxiv.org/abs/2602.06464)
*Shuao Chen,Junyuan Gao,Yuxuan Shi,Yongpeng Wu,Giuseppe Caire,H. Vincent Poor,Wenjun Zhang*

Main category: cs.IT

TL;DR: 该论文研究了向量高斯源的联合压缩问题，改进了半定条件满足时的结果，并推导了半定条件不满足时的新理论界限，量化了相关性对压缩效率的提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往忽略半定条件不满足的情况，且即使半定条件满足时，也缺乏对相关性如何实现更高效压缩的定量刻画。需要建立适用于实际相关源压缩的理论界限。

Method: 分析最优源重构的性质并给出其维度的上界；在可扩展的两类相关性协方差框架下，证明满足半定条件的概率随源长度指数衰减；确定向量源应具备的组件间相关性以实现Hadamard压缩率；推导包含相关性的显式率失真函数。

Result: 当半定条件不满足时，低维重构对高效压缩至关重要；满足半定条件的概率随源长度指数衰减；确定了实现Hadamard压缩率所需的相关性权衡；定量刻画了充分利用源相关性带来的压缩效率增益。

Conclusion: 该工作完善了半定条件满足时的结果，并建立了半定条件不满足时的理论界限，定量揭示了相关性在压缩中的重要作用，为实际相关源压缩提供了理论基础。

Abstract: This paper investigates the joint compression problem of a vector Gaussian source, where an individual distortion constraint is imposed on each source component. It is known that the rate-distortion function (RDF) is lower-bounded by the rate derived from the Hadamard inequality, which becomes exact when the semidefinite condition (SDC) holds. However, existing works often overlook the case where the SDC is not satisfied. Moreover, even when the SDC holds, a quantitative characterization of how correlations enable more efficient compression is lacking. In this work, we refine the results when the SDC is satisfied and derive new theoretical results when the SDC is not satisfied, thereby establishing theoretical limits for practical source compression with correlations. Specifically, we examine the properties of optimal source reconstruction and provide upper bounds on its dimension, showing that lower-dimensional reconstructions are essential for efficient compression when the SDC does not hold. Within a scalable two-type correlation (2TC) covariance framework, we prove that the probability of satisfying the SDC decays exponentially with source length, emphasizing the importance of exploring theoretical limits when the SDC is not met. Additional, we determine the component-wise correlations that a vector source should possess to achieve the Hadamard compression rate, revealing the trade-off between distortion constraints and correlations. More importantly, by deriving an explicit RDF with correlations incorporated, we quantitatively characterize the gain in compression efficiency achieved by fully leveraging source correlations.

</details>


### [40] [Codes for Metastability-Containing Addition](https://arxiv.org/abs/2602.06467)
*Johannes Bund,Christoph Lenzen,Moti Medina*

Main category: cs.IT

TL;DR: 研究在不确定性下的加法问题，其中加数以区间而非精确值表示，提出可恢复编码概念以控制不确定性放大，设计渐进最优编码并讨论相应加法器实现。


<details>
  <summary>Details</summary>
Motivation: 当模拟值转换为数字测量时可能出现亚稳态，导致比特不稳定，进而影响电路级计算。使用二进制编码进行加法会放大不确定性，需要设计不放大不确定性的编码方案。

Method: 提出可恢复编码概念，证明在给定加数组合不确定性边界下可保持和可恢复编码的速率上界，设计渐进最优的保持加数组合不确定性的编码，并讨论如何实现该编码的加法器。

Result: 证明了可保持和可恢复编码的速率上界，设计了渐进最优的编码方案，该方案能够保持加数的组合不确定性而不放大。

Conclusion: 提出的编码方法能够有效控制不确定性加法中的误差放大问题，结合现有技术可实现显著的延迟降低，适用于包含输入亚稳态的各种已知或未来构造。

Abstract: We investigate the fundamental task of addition under uncertainty, namely, addends that are represented as intervals of numbers rather than single values. One potential source of such uncertainty can occur when obtaining discrete-valued measurements of analog values, which are prone to metastability. Naturally, unstable bits impact gate-level and, consequently, circuit-level computations. Using Binary encoding for such an addition produces a sum with an amplified imprecision. Hence, the challenge is to devise an encoding that does not amplify the imprecision caused by unstable bits. We call such codes recoverable. While this challenge is easily met for unary encoding, no suitable codes of high rates are known. In this work, we prove an upper bound on the rate of preserving and recoverable codes for a given bound on the addends' combined uncertainty. We then design an asymptotically optimal code that preserves the addends' combined uncertainty. We then discuss how to obtain adders for our code. The approach can be used with any known or future construction for containing metastability of the inputs. We conjecture that careful design based on existing techniques can lead to significant latency reduction.

</details>


### [41] [FDD CSI Feedback under Finite Downlink Training: A Rate-Distortion Perspective](https://arxiv.org/abs/2602.06479)
*Shuao Chen,Junyuan Gao,Yuxuan Shi,Yongpeng Wu,Giuseppe Caire,H. Vincent Poor,Wenjun Zhang*

Main category: cs.IT

TL;DR: 该论文建立了FDD多天线OFDM系统中CSI反馈的理论极限，推导了在有限长度训练下的整体速率失真函数，并分析了其收敛特性。


<details>
  <summary>Details</summary>
Motivation: 在FDD多天线OFDM系统中，用户需要通过上行链路反馈CSI，但实际系统中训练长度有限且存在估计误差。现有研究缺乏在有限训练长度下CSI反馈的理论性能极限分析，需要建立完整的理论框架来理解系统性能边界。

Method: 采用MMSE信道估计结合渐进最优上行反馈策略，推导整体CSI反馈系统的速率失真函数。提供非渐进边界和渐进缩放分析，特别关注训练符号数超过天线维度的情况，分析不同下行SNR下的性能表现。

Result: 当训练符号足够多时，整体RDF收敛到用户完全获取下行CSI时的直接RDF。关键发现：在固定下行SNR下，收敛速率与训练长度成反比。仿真验证边界紧致，有限训练下整体RDF与直接RDF存在显著偏差。

Conclusion: 该研究建立了FDD多天线OFDM系统中CSI反馈的完整理论框架，揭示了训练长度对反馈性能的关键影响，为实际系统设计提供了理论指导。

Abstract: This paper establishes the theoretical limits of channel state information (CSI) feedback in frequency-division duplexing (FDD) multi-antenna orthogonal frequency-division multiplexing (OFDM) systems under finite-length training with Gaussian pilots. The user employs minimum mean-squared error (MMSE) channel estimation followed by asymptotically optimal uplink feedback. Specifically, we derive a general rate-distortion function (RDF) of the overall CSI feedback system. We then provide both non-asymptotic bounds and asymptotic scaling for the RDF under arbitrary downlink signal-to-noise ratio (SNR) when the number of training symbols exceeds the antenna dimension. A key observation is that, with sufficient training, the overall RDF converges to the direct RDF corresponding to the case where the user has full access to the downlink CSI. More importantly, we demonstrate that even at a fixed downlink SNR, the convergence rate is inversely proportional to the training length. The simulation results show that our bounds are tight, and under very limited training, the deviation between the overall RDF and the direct RDF is substantial.

</details>


### [42] [Type-Based Unsourced Federated Learning With Client Self-Selection](https://arxiv.org/abs/2602.06601)
*Kaan Okumus,Khac-Hoang Ngo,Unnikrishnan Kunnath Ganesan,Giuseppe Durisi,Erik G. Ström,Shashi Raj Pandey*

Main category: cs.IT

TL;DR: 提出一种基于本地训练损失与中心阈值的客户端自选择策略，结合无源多址接入框架，实现无线联邦学习中的隐私保护客户端选择


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端选择方法通常需要服务器端掌握客户端特定信息，这会损害隐私。为了解决这个问题，需要一种不依赖服务器端知识的客户端选择方法

Method: 提出客户端自选择策略：客户端仅通过比较本地计算的训练损失与中心更新的选择阈值来决定是否参与训练。将该策略集成到基于类型的无源多址接入框架中，在分布式MIMO网络上实现鲁棒聚合

Result: 模拟结果显示，所提出的自选择策略性能与最先进的服务器端选择方法相当，并且始终优于随机客户端选择

Conclusion: 该方法实现了完全无源的客户端选择，服务器无需知道客户端身份，且不需要信道状态信息，在保护隐私的同时保持了良好的性能

Abstract: We address the client-selection problem in federated learning over wireless networks under data heterogeneity. Existing client-selection methods often rely on server-side knowledge of client-specific information, thus compromising privacy. To overcome this issue, we propose a client self-selection strategy based solely on the comparison between locally computed training losses and a centrally updated selection threshold. Furthermore, to support robust aggregation of clients' updates over wireless channels, we integrate this client self-selection strategy into the recently proposed type-based unsourced multiple-access framework over distributed multiple-input multiple-output (D-MIMO) networks. The resulting scheme is completely unsourced: the server does not need to know the identity of the clients. Moreover, no channel state information is required, neither at the clients nor at the server side. Simulation results conducted over a D-MIMO wireless network show that the proposed self-selection strategy matches the performance of a comparable state-of-the-art server-side selection method and consistently outperforms random client selection.

</details>


### [43] [FaA-CAF: Modular Single-RF-Chain Near-Field mmWave Sensing via Clip-On Antenna Fabric](https://arxiv.org/abs/2602.06767)
*Pin-Han Ho,Haoran Mei,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Tong Wei,Wei Duan*

Main category: cs.IT

TL;DR: FaACAF是一种硬件高效感知架构，通过频率选择模块在共享波导上合成空间孔径，使用单射频链实现近场毫米波感知，无需射频开关和多通道前端。


<details>
  <summary>Details</summary>
Motivation: 近场毫米波感知对未来无线系统至关重要，但传统方法需要大型天线阵列、多射频链或机械扫描，导致空间可观测性与系统简单性之间存在根本矛盾。

Method: 提出FaACAF架构，采用频率作为孔径范式，在共享波导基板上附加频率选择模块，通过FMCW信号同时索引感知孔径并协调上下行信号分布和回波复用，实现全被动、全模拟的无开关操作。

Result: 实现了在线自校准机制稳定频率到孔径映射，两个案例研究验证了方法的鲁棒性并量化了模块化部署带来的可预测感知裕度权衡。

Conclusion: FaACAF证明近场空间可观测性可以通过频域架构协调而非硬件扩展来扩展，为未来无线系统中的具身感知和集成感知通信提供了可重构且硬件高效的途径。

Abstract: Near field mmWave sensing is poised to play a key role in future wireless systems, enabling environment-aware, embodied, and application adaptive operation under stringent form-factor and hardware constraints. However, achieving high spatial resolution in the near field typically requires large antenna arrays, multiple radio frequency (RF) chains, or mechanical scanning, creating a fundamental tension between spatial observability and system simplicity. This paper presents frequency as aperture clip on antenna fabric (FaACAF), a hardware efficient sensing by design architecture that synthesizes spatial aperture through the FaA paradigm using a single RF chain. FaACAF realizes a modular clip on aperture fabric, in which frequency selective clip on modules (CMs) are attached to a shared guided-wave substrate and implicitly coordinated by the instantaneous frequency modulated continuous wave (FMCW) excitation frequency. In this fabric, FMCW signaling simultaneously indexes the sensing aperture and orchestrates uplink/downlink signal distribution and echo multiplexing in a switch free, fully passive, and all analog manner, eliminating RF switching and multichannel front ends. An online self calibration mechanism stabilizes the frequency to aperture mapping under practical attachment variability without requiring full matrix calibration. Two case studies illustrate the robustness of the proposed approach and quantify the predictable sensing margin tradeoffs introduced by modular deployment. Overall, FaACAF demonstrates that near field spatial observability can be scaled through architectural coordination in the frequency domain rather than hardware expansion, providing a reconfigurable and hardware efficient pathway toward embodied sensing and integrated sensing and communication (ISAC) in future wireless systems.

</details>
