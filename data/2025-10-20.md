<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Targeted Attacks and Defenses for Distributed Federated Learning in Vehicular Networks](https://arxiv.org/abs/2510.15109)
*Utku Demir,Tugba Erpek,Yalin E. Sagduyu,Sastry Kompella,Mengran Xue*

Main category: cs.NI

TL;DR: 本文分析了分布式联邦学习在车联网中的安全漏洞，设计了针对性的数据投毒和后门攻击，并提出了相应的防御机制来增强DFL对抗新兴网络威胁的能力。


<details>
  <summary>Details</summary>
Motivation: 在移动边缘设备（如地面车辆和无人机群）构成的网络中，传统联邦学习依赖中心服务器的协调存在计算负担和连接限制问题。分布式联邦学习虽然提供了可扩展性和鲁棒性，但仍面临日益复杂的网络攻击威胁。

Method: 设计了复杂的目标训练数据投毒和后门（木马）攻击，在车联网环境中分析这些新兴漏洞，并开发有效的防御机制来加强DFL的安全性。

Result: 分析了DFL相对于个体学习在面对此类攻击时的韧性表现，并展示了所提出防御机制的有效性。

Conclusion: 分布式联邦学习在对抗网络攻击方面具有优势，但仍需专门的防御机制来应对日益复杂的安全威胁，特别是在车联网等关键基础设施环境中。

Abstract: In emerging networked systems, mobile edge devices such as ground vehicles
and unmanned aerial system (UAS) swarms collectively aggregate vast amounts of
data to make machine learning decisions such as threat detection in remote,
dynamic, and infrastructure-constrained environments where power and bandwidth
are scarce. Federated learning (FL) addresses these constraints and privacy
concerns by enabling nodes to share local model weights for deep neural
networks instead of raw data, facilitating more reliable decision-making than
individual learning. However, conventional FL relies on a central server to
coordinate model updates in each learning round, which imposes significant
computational burdens on the central node and may not be feasible due to the
connectivity constraints. By eliminating dependence on a central server,
distributed federated learning (DFL) offers scalability, resilience to node
failures, learning robustness, and more effective defense strategies. Despite
these advantages, DFL remains vulnerable to increasingly advanced and stealthy
cyberattacks. In this paper, we design sophisticated targeted training data
poisoning and backdoor (Trojan) attacks, and characterize the emerging
vulnerabilities in a vehicular network. We analyze how DFL provides resilience
against such attacks compared to individual learning and present effective
defense mechanisms to further strengthen DFL against the emerging cyber
threats.

</details>


### [2] [Structural Generalization for Microservice Routing Using Graph Neural Networks](https://arxiv.org/abs/2510.15210)
*Chenrui Hu,Ziyu Cheng,Di Wu,Yuxiao Wang,Feng Liu,Zhimin Qiu*

Main category: cs.NI

TL;DR: 提出基于图神经网络的端到端智能路由优化框架，通过建模微服务调用关系图，使用多层GNN进行高阶信息聚合和结构建模，结合边感知注意力机制提升路由决策效率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂拓扑下微服务系统路由决策效率低和整体性能优化问题，传统方法难以有效处理动态并发环境中的服务调用关系。

Method: 将微服务建模为图结构，服务节点和通信链路分别作为节点和边；使用多维特征作为输入；采用多层图神经网络进行信息聚合；引入边感知注意力机制捕获服务通信不稳定性和瓶颈风险；输出候选路径评分指导动态路由。

Result: 在路由准确性、预测误差和系统稳定性等多个关键指标上优于现有主流策略，能有效处理高动态并发环境，表现出强性能、鲁棒性和结构泛化能力。

Conclusion: 基于图神经网络的端到端优化框架为微服务智能路由提供了有效解决方案，在复杂拓扑环境下具有显著优势，为分布式系统性能优化开辟了新途径。

Abstract: This paper focuses on intelligent routing in microservice systems and
proposes an end-to-end optimization framework based on graph neural networks.
The goal is to improve routing decision efficiency and overall system
performance under complex topologies. The method models invocation
relationships among microservices as a graph. In this graph, service nodes and
communication links are treated as graph nodes and edges. Multi-dimensional
features such as node states, link latency, and call frequency are used as
input. A multi-layer graph neural network is employed to perform high-order
information aggregation and structural modeling. The model outputs a score for
each candidate service path. These scores are then used to guide dynamic
routing decisions. To improve the model's ability to assess path quality, an
edge-aware attention mechanism is introduced. This mechanism helps the model
capture instability and bottleneck risks in service communications more
accurately. The paper also conducts a systematic analysis of the model's
performance under different network depths, topology densities, and service
scales. It evaluates the effectiveness of the method in terms of routing
accuracy, prediction error, and system stability. Experimental results show
that the proposed method outperforms existing mainstream strategies across
multiple key metrics. It handles highly dynamic and concurrent microservice
environments effectively and demonstrates strong performance, robustness, and
structural generalization.

</details>


### [3] [Content and Access Networks Synergies: Tradeoffs in Public and Private Investments by Content Providers](https://arxiv.org/abs/2510.15373)
*Pranay Agarwal,D. Manjunath*

Main category: cs.NI

TL;DR: 该研究分析了内容提供商在公共投资和私人投资之间的权衡，考虑了四种不同的交互模型：集中分配、合作博弈、非合作博弈和议价博弈。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机普及带来的内容消费增长，互联网服务提供商要求内容提供商分担接入网络基础设施升级成本。内容提供商需要在公共投资和私人投资之间做出决策。

Method: 研究了四种CP间交互模型：集中分配、合作博弈、非合作博弈和议价博弈，通过数值结果评估不同激励结构对CP效用的影响。

Result: 议价博弈可能导致比非合作和集中模型更高的公共投资，但如果CP被激励投资私人基础设施，这种效益会减少。

Conclusion: 不同的交互模型对CP的投资决策有显著影响，议价博弈在某些情况下能促进更多公共投资，但私人投资激励会削弱这种效果。

Abstract: The ubiquity of smartphones has fueled content consumption worldwide, leading
to an ever-increasing demand for a better Internet experience. This has
necessitated an upgrade of the capacity of the access network. The Internet
service providers (ISPs) have been demanding that the content providers (CPs)
share the cost of upgrading access network infrastructure. A \emph{public
investment} in the infrastructure of a neutral ISP will boost the profit of the
CPs, and hence, seems a rational strategy. A CP can also make a \emph{private
investment} in its infrastructure and boost its profits. In this paper, we
study the trade-off between public and private investments by a CP when the
decision is made under different types of interaction between them.
Specifically, we consider four interaction models between CPs -- centralized
allocation, cooperative game, non-cooperative game, and a bargaining game --
and determine the public and private investment for each model. Via numerical
results, we evaluate the impact of different incentive structures on the
utility of the CPs. We see that the bargaining game can result in higher public
investment than the non-cooperative and centralized models. However, this
benefit gets reduced if the CPs are incentivized to invest in private
infrastructure.

</details>


### [4] [Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity](https://arxiv.org/abs/2510.15802)
*Tommaso Bonato,Sepehr Abdous,Abdul Kabbani,Ahmad Ghalayini,Nadeen Gebara,Terry Lam,Anup Agarwal,Tiancheng Chen,Zhuolong Yu,Konstantin Taranov,Mahmoud Elhaddad,Daniele De Sensi,Soudeh Ghorbani,Torsten Hoefler*

Main category: cs.NI

TL;DR: Uno是一个统一的数据中心内外通信系统，通过集成快速拥塞反应和公平速率控制的传输协议，结合擦除编码和自适应路由的负载均衡方案，显著改善了数据中心内外流量的完成时间。


<details>
  <summary>Details</summary>
Motivation: 云计算和AI工作负载对数据中心内外通信效率提出了前所未有的需求。数据中心内外流量共存以及网络RTT差异使得拥塞管理和流量路由复杂化，特别是数据中心内流量的快速拥塞反应会导致与较慢的数据中心间流量竞争时的速率不公平问题。

Method: 提出Uno系统，集成快速拥塞反应和公平速率控制的传输协议，结合擦除编码和自适应路由的负载均衡方案，统一处理数据中心内外环境。

Result: 与最先进方法（如Gemini）相比，Uno显著改善了数据中心内外流量的完成时间。

Conclusion: Uno系统通过统一的方法有效解决了数据中心内外流量共存带来的拥塞管理和速率公平性问题。

Abstract: Cloud computing and AI workloads are driving unprecedented demand for
efficient communication within and across datacenters. However, the coexistence
of intra- and inter-datacenter traffic within datacenters plus the disparity
between the RTTs of intra- and inter-datacenter networks complicates congestion
management and traffic routing. Particularly, faster congestion responses of
intra-datacenter traffic causes rate unfairness when competing with slower
inter-datacenter flows. Additionally, inter-datacenter messages suffer from
slow loss recovery and, thus, require reliability. Existing solutions overlook
these challenges and handle inter- and intra-datacenter congestion with
separate control loops or at different granularities. We propose Uno, a unified
system for both inter- and intra-DC environments that integrates a transport
protocol for rapid congestion reaction and fair rate control with a load
balancing scheme that combines erasure coding and adaptive routing. Our
findings show that Uno significantly improves the completion times of both
inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data](https://arxiv.org/abs/2510.15096)
*Alana Renda,Jillian Ross,Michael Cafarella,Jacob Andreas*

Main category: cs.AI

TL;DR: OpenEstimate是一个用于评估语言模型在不确定性下进行数值估计任务的多领域基准测试，发现前沿语言模型生成的先验分布通常不准确且过于自信。


<details>
  <summary>Details</summary>
Motivation: 现实世界中语言模型需要在信息不完整的情况下进行不确定性推理，但现有评估主要关注有明确答案的问题，缺乏对不确定性推理能力的系统评估。

Method: 开发了OpenEstimate基准测试，通过数值估计任务评估语言模型，要求模型综合背景信息并输出概率先验分布，评估其准确性和校准度。

Result: 六个前沿语言模型生成的先验分布通常不准确且过于自信，性能在不同不确定性诱导方式下略有改善，但在采样策略、推理努力或提示设计变化下基本不受影响。

Conclusion: OpenEstimate为前沿语言模型提供了具有挑战性的评估平台，有助于开发在概率估计和不确定性推理方面表现更好的模型。

Abstract: Real-world settings where language models (LMs) are deployed -- in domains
spanning healthcare, finance, and other forms of knowledge work -- require
models to grapple with incomplete information and reason under uncertainty. Yet
most LM evaluations focus on problems with well-defined answers and success
criteria. This gap exists in part because natural problems involving
uncertainty are difficult to construct: given that LMs have access to most of
the same knowledge as humans, it is non-trivial to design questions for which
LMs will struggle to produce correct answers, but which humans can answer
reliably. As a result, LM performance on reasoning under uncertainty remains
poorly characterized. To address this gap, we introduce OpenEstimate, an
extensible, multi-domain benchmark for evaluating LMs on numerical estimation
tasks that require models to synthesize significant amounts of background
information and express predictions as probabilistic priors. We assess these
priors for accuracy and calibration, quantifying their usefulness relative to
samples from the true distribution of interest. Across six frontier LMs, we
find that LM-elicited priors are often inaccurate and overconfident.
Performance improves modestly depending on how uncertainty is elicited from the
model, but is largely unaffected by changes in sampling strategy, reasoning
effort, or prompt design. The OpenEstimate benchmark thus offers a challenging
evaluation for frontier LMs and a platform for developing models that are
better at probabilistic estimation and reasoning under uncertainty.

</details>


### [6] [Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors](https://arxiv.org/abs/2510.15547)
*Usman Ali,Ali Zia,Waqas Ali,Umer Ramzan,Abdul Rehman,Muhammad Tayyab Chaudhry,Wei Xiang*

Main category: cs.AI

TL;DR: 提出MM-HCAN多模态超图对比注意力网络，用于电机故障诊断，通过超图拓扑和对比学习实现多模态传感器融合，在三个真实基准测试中达到99.82%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉复杂多模态信号关系，局限于单模态数据或单一故障类型，且在噪声或跨域条件下性能下降，需要更鲁棒的故障诊断方案。

Method: MM-HCAN将对比学习集成到多模态传感器融合的超图拓扑中，联合建模模态内和模态间依赖关系，超越欧几里得嵌入空间，同时诊断轴承、定子和转子故障。

Result: 在三个真实基准测试中达到99.82%准确率，具有强大的跨域泛化能力和噪声鲁棒性，消融研究验证了各组件贡献。

Conclusion: MM-HCAN为全面多故障诊断提供了可扩展且鲁棒的解决方案，支持工业环境中的预测性维护和延长资产寿命。

Abstract: Reliable induction motor (IM) fault diagnosis is vital for industrial safety
and operational continuity, mitigating costly unplanned downtime. Conventional
approaches often struggle to capture complex multimodal signal relationships,
are constrained to unimodal data or single fault types, and exhibit performance
degradation under noisy or cross-domain conditions. This paper proposes the
Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified
framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is
the first to integrate contrastive learning within a hypergraph topology
specifically designed for multimodal sensor fusion, enabling the joint
modelling of intra- and inter-modal dependencies and enhancing generalisation
beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis
of bearing, stator, and rotor faults, addressing the engineering need for
consolidated di- agnostic capabilities. Evaluated on three real-world
benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain
generalisation and resilience to noise, demonstrating its suitability for
real-world deployment. An ablation study validates the contribution of each
component. MM-HCAN provides a scalable and robust solution for comprehensive
multi-fault diagnosis, supporting predictive maintenance and extended asset
longevity in industrial environments.

</details>


### [7] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习的程序化关卡设计方法，使用两个智能体（蜂鸟和浮岛）在Unity 3D环境中协同工作，实现自主生成和解决游戏内容。


<details>
  <summary>Details</summary>
Motivation: 探索程序化内容生成在游戏开发中的应用，通过AI智能体自主生成和解决游戏关卡，减少人工设计工作量，提高游戏的可重玩性和扩展性。

Method: 使用Unity ML-Agents工具包，采用近端策略优化算法训练两个智能体：蜂鸟智能体作为求解器学习导航和收集花朵，浮岛智能体根据障碍物位置和蜂鸟初始状态生成花朵布局。

Result: 系统表现出涌现行为和强大的泛化能力，能够在各种环境配置下有效工作，实现了高效的游戏关卡自主设计和解决。

Conclusion: 深度强化学习在游戏内容生成方面具有巨大潜力，能够推动AI在创意游戏开发过程中的应用边界，为自主游戏关卡设计开辟了新途径。

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [8] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: 本文认为AGI发展的瓶颈在于理论而非数据或规模，基于波普尔和德奇的关键理性主义，挑战柏拉图表示假说，提出因果力学作为机制优先的方法，强调假设空间变化作为首要操作，通过结构原则使错误发现和修正变得可行。


<details>
  <summary>Details</summary>
Motivation: 当前AGI进展受限于理论而非数据或规模，观测等价的世界在干预下可能分化，仅观测充分性无法保证干预能力，需要从观测学习转向错误中心的范式转变。

Method: 提出因果力学方法，将假设空间变化作为首要操作，使用概率结构而非预设，包括局部性和自主性原则、独立因果机制的规范不变形式、组合自主性原则等结构原则，提供可操作诊断工具。

Result: 建立了使错误发现和修正可行的理论框架，通过结构原则支持模块化干预、可分离性和类比保持，旨在构建能够将不可达错误转化为可达错误并修正的系统支架。

Conclusion: AGI发展需要理论突破，因果力学提供了一种机制优先的方法，通过假设空间变化和结构原则使错误发现和修正变得可行，为构建能够持续学习和改进的智能系统提供了理论支架。

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [9] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: HugAgent是一个用于评估AI模型能否模拟个体推理风格和信念演变的基准测试，包含合成和人类双轨设计，旨在推动机器推理与人类个体思维的对接。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要反映群体共识，缺乏对个体推理风格和信念轨迹的捕捉能力，需要开发能够适应个体推理特征的AI系统。

Method: 采用双轨设计：合成轨道用于规模化和系统性压力测试，人类轨道提供生态有效的"出声思考"推理数据，评估模型在预测个体推理和信念更新方面的表现。

Result: 实验表明最先进的LLMs在个体适应方面仍存在持续差距，HugAgent成为首个可扩展的机器推理与人类个体思维对齐基准。

Conclusion: HugAgent为机器推理与人类个体思维的对齐提供了首个可扩展基准，推动了更人性化推理AI的发展。

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [10] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: 提出了一个包含733,651个面部表情记录的大规模纵向工作场所情感数据集，覆盖38名员工30.5个月的数据，包含7种情感概率和32个扩展情感指标，验证了已知心理模式并实现了高精度的情感分类和预测。


<details>
  <summary>Details</summary>
Motivation: 解决真实工作场所环境中情感识别因缺乏大规模、纵向自然数据集而面临的挑战，捕捉COVID-19大流行期间的情感反应。

Method: 收集38名员工30.5个月的面部表情数据，使用深度学习进行情感识别，计算32个扩展情感指标，并通过随机森林和LSTM模型进行基线实验。

Result: 数据集成功复制了已知心理模式（周末效应：效价提升192%，p<0.001），员工离职预测AUC=1.0，情感分类准确率91.2%，效价预测R2=0.84。

Conclusion: 这是目前公开可用的最大、最长的纵向工作场所情感数据集，为情感识别、情感动态建模、情感传染、离职预测和情感感知系统设计等研究提供了重要资源。

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [11] [From Checklists to Clusters: A Homeostatic Account of AGI Evaluation](https://arxiv.org/abs/2510.15236)
*Brett Reynolds*

Main category: cs.AI

TL;DR: 本文提出AGI评估应使用非对称权重和持续性测试，将通用智能视为稳态属性集群，而非简单的多领域能力组合。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估存在两个问题：平等加权所有领域忽视了人类智能研究中各领域的重要性差异；快照测试无法区分持久能力和脆弱表现。

Method: 提出两种评估扩展：基于因果中心性的加权评分（导入CHC衍生权重）和集群稳定性指数系列（评估配置持久性、持久学习和错误纠正）。

Result: 这些扩展保持了多领域广度，同时减少了脆弱性和博弈行为，提供了无需架构访问的黑盒协议。

Conclusion: 通用智能应被理解为稳态属性集群，AGI评估需要衡量能力在扰动下的共现机制，而非仅关注静态能力配置。

Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they
typically assign symmetric weights and rely on snapshot scores. This creates
two problems: (i) equal weighting treats all domains as equally important when
human intelligence research suggests otherwise, and (ii) snapshot testing can't
distinguish durable capabilities from brittle performances that collapse under
delay or stress. I argue that general intelligence -- in humans and potentially
in machines -- is better understood as a homeostatic property cluster: a set of
abilities plus the mechanisms that keep those abilities co-present under
perturbation. On this view, AGI evaluation should weight domains by their
causal centrality (their contribution to cluster stability) and require
evidence of persistence across sessions. I propose two battery-compatible
extensions: a centrality-prior score that imports CHC-derived weights with
transparent sensitivity analysis, and a Cluster Stability Index family that
separates profile persistence, durable learning, and error correction. These
additions preserve multidomain breadth while reducing brittleness and gaming. I
close with testable predictions and black-box protocols labs can adopt without
architectural access.

</details>


### [12] [Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions](https://arxiv.org/abs/2510.15258)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Jun Xu,Fu Zhang,Wenbo Lei,Annie Wang,Peng Gong*

Main category: cs.AI

TL;DR: 提出了一种基于LLM代理与知识图谱交互的多维数据分析方法，构建动态协作的分析生态系统，在生态系统分析、关系挖掘和用户驱动探索分析方面具有显著优势


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从海量、异构、复杂关联的多维数据中提取深度洞察面临挑战。LLM在处理结构化知识时存在幻觉问题且难以实时更新，而知识图谱虽然能显式存储结构化知识，但其静态特性限制了动态交互和分析能力

Method: 利用LLM代理从非结构化数据中自动提取产品数据，实时构建和可视化知识图谱，并通过交互平台支持用户对图节点进行深度探索和分析

Result: 实验结果表明该方法在产品生态系统分析、关系挖掘和用户驱动探索分析方面具有显著优势

Conclusion: 该方法为多维数据分析提供了新思路和工具，构建了动态协作的分析生态系统

Abstract: In the current era of big data, extracting deep insights from massive,
heterogeneous, and complexly associated multi-dimensional data has become a
significant challenge. Large Language Models (LLMs) perform well in natural
language understanding and generation, but still suffer from "hallucination"
issues when processing structured knowledge and are difficult to update in
real-time. Although Knowledge Graphs (KGs) can explicitly store structured
knowledge, their static nature limits dynamic interaction and analytical
capabilities. Therefore, this paper proposes a multi-dimensional data analysis
method based on the interactions between LLM agents and KGs, constructing a
dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to
automatically extract product data from unstructured data, constructs and
visualizes the KG in real-time, and supports users in deep exploration and
analysis of graph nodes through an interactive platform. Experimental results
show that this method has significant advantages in product ecosystem analysis,
relationship mining, and user-driven exploratory analysis, providing new ideas
and tools for multi-dimensional data analysis.

</details>


### [13] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent是一个基于经验驱动的学习框架，通过将像素级GUI交互构建为状态-动作知识图来解决API缺失环境下的效率瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件大多缺乏可访问的API，导致LLM智能体只能通过像素级GUI操作，面临效率低下、短视决策和试错探索等问题。

Method: 构建持久的状态-动作知识图(SA-KG)，将功能相似但视觉不同的GUI状态连接起来，并设计基于图拓扑的混合内在奖励机制。

Result: 在Civilization V和Slay the Spire两个复杂GUI决策环境中，相比最先进方法显著提升了探索效率和策略深度。

Conclusion: KG-Agent通过结构化历史经验和解耦策略规划与探索发现，有效解决了API缺失环境下的长期规划问题。

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [14] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS是一个受人类记忆启发的多模态智能体系统，使用图结构的多模态上下文记忆和语义标签进行概念驱动的检索，在性能和速度上都优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统主要关注文本信息的存储，忽略了多模态信号的重要性。受人类记忆多模态特性的启发，希望开发一个更符合认知科学原理的多模态记忆系统。

Method: 系统包含四个循环阶段：编码（理解输入）、存储记忆（保存重要信息）、检索（从记忆中搜索相关上下文）和执行（完成任务）。使用语义标签和图结构的多模态上下文记忆，而非传统的向量数据库。

Result: 在ImageNet分类任务中比传统多模态RAG方法快3.5倍，在MSC基准测试中优于MemGPT。

Conclusion: AUGUSTUS系统通过模拟人类记忆的多模态特性和图结构记忆组织，在多模态任务中实现了更好的性能和效率。

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [15] [WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation](https://arxiv.org/abs/2510.15306)
*Kuang-Da Wang,Zhao Wang,Yotaro Shimose,Wei-Yao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: WebGen-V是一个用于指令到HTML生成的新基准和框架，通过无界可扩展的代理爬取框架、结构化分节数据表示和分节多模态评估协议，提升了数据质量和评估粒度。


<details>
  <summary>Details</summary>
Motivation: 利用LLM在编码和多模态理解方面的最新进展，解决现有指令到HTML生成任务中数据质量和评估粒度不足的问题。

Method: 提出三个关键创新：1)无界可扩展的代理爬取框架持续收集真实网页；2)结构化分节数据表示整合元数据、局部UI截图和JSON格式的文本图像资源；3)分节多模态评估协议对齐文本、布局和视觉组件。

Result: 通过最先进LLM的实验和消融研究验证了结构化数据和分节评估的有效性，以及各组件对性能的贡献。

Conclusion: WebGen-V是首个实现高粒度代理爬取和评估的指令到HTML生成工作，提供了从真实数据采集、网页生成到结构化多模态评估的统一流程。

Abstract: Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.

</details>


### [16] [VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data](https://arxiv.org/abs/2510.15317)
*Tingqiao Xu,Ziru Zeng,Jiayu Chen*

Main category: cs.AI

TL;DR: VERITAS是一个通过整合视觉先验和多模态模型来提升监督微调数据质量的管道，能有效减少事实错误和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型的监督微调数据质量不足，存在事实错误和幻觉问题，主要原因是视觉感知能力不足。

Method: 使用视觉识别模型和OCR提取结构化视觉先验，结合三个LMM评估原始答案，通过统计融合得到共识分数作为真实标签，训练轻量级评判模型，最后让LMM基于评判生成优化答案。

Result: 在六个多模态基准测试中，使用VERITAS处理数据的模型表现优于使用原始数据的模型，尤其在文本丰富和细粒度推理任务中。评判模型能力接近SOTA LMM但更高效。

Conclusion: VERITAS能有效提升多模态模型的监督微调数据质量，减少幻觉问题，且评判模型在保持高性能的同时更加高效。

Abstract: The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.

</details>


### [17] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: 提出了DEPO框架，通过优势解耦算法、难度感知长度惩罚和优势裁剪方法，减少大推理模型中的低效推理，在保持准确性的同时显著缩短响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法虽然提高了模型准确性，但存在响应过长和过度思考问题，导致推理延迟和计算消耗增加，特别是对于简单任务。

Method: DEPO框架包含三个核心组件：优势解耦算法指导减少低效token；难度感知长度惩罚降低整体响应长度；优势裁剪方法防止策略优化偏差。

Result: 在DeepSeek-Distill-Qwen模型上，DEPO实现了序列长度减少39%，减少了低效token中的过度推理路径，同时在整体准确性上优于基础模型。

Conclusion: DEPO框架有效解决了大推理模型中的低效推理问题，在减少计算消耗的同时保持了模型性能。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [18] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 提出基于强化学习和关系图卷积神经网络的自动布局引擎，通过提高网格分辨率、精确引脚信息集成和动态布线资源估计，在模拟环境中实现13.8%死区减少、40.6%线长减少和73.4%布线成功率提升。


<details>
  <summary>Details</summary>
Motivation: 解决模拟集成电路布局中由于电气约束、问题特定约束以及布局与布线步骤相互依赖而导致的机器学习技术应用受限问题，满足布局工程师对布线感知布局解决方案的需求。

Method: 开发基于强化学习和关系图卷积神经网络的自动布局引擎，采用高网格分辨率、精确引脚信息集成和动态布线资源估计技术，平衡布线效率和面积效率。

Result: 在模拟环境中，与过去基于学习的最先进技术相比，实现了13.8%的死区减少、40.6%的线长减少和73.4%的布线成功率提升。

Conclusion: 所提出的方法能够生成更具布线可行性的布局结果，满足工业标准，显著提升了模拟集成电路布局的质量和效率。

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


### [19] [Corrigibility Transformation: Constructing Goals That Accept Updates](https://arxiv.org/abs/2510.15395)
*Rubi Hudson*

Main category: cs.AI

TL;DR: 本文提出了可修正性（corrigibility）的概念，即AI目标不会激励其避免目标更新或关闭。作者开发了一种转换方法，可将任何目标转换为可修正版本而不牺牲性能，并通过网格世界实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: AI在训练过程中不应抵抗目标更新，但部分学习的目标往往会激励AI避免进一步更新。可修正性对于训练收敛、纠正错误和适应人类偏好变化至关重要，但目前缺乏既具可修正性又具竞争力的目标规范。

Method: 首先形式化定义可修正性，然后引入一种转换方法，通过近视地获取在无成本阻止更新条件下的奖励预测来构建任何目标的可修正版本。该方法可递归扩展到新创建的代理，并防止代理故意修改其目标。

Result: 两个网格世界实验表明，这些可修正目标可以被有效学习，并产生期望的行为。

Conclusion: 提出的转换方法能够构建既具可修正性又具竞争力的目标，为AI安全提供了重要保障。

Abstract: For an AI's training process to successfully impart a desired goal, it is
important that the AI does not attempt to resist the training. However,
partially learned goals will often incentivize an AI to avoid further goal
updates, as most goals are better achieved by an AI continuing to pursue them.
We say that a goal is corrigible if it does not incentivize taking actions that
avoid proper goal updates or shutdown. In addition to convergence in training,
corrigibility also allows for correcting mistakes and changes in human
preferences, which makes it a crucial safety property. Despite this, the
existing literature does not include specifications for goals that are both
corrigible and competitive with non-corrigible alternatives. We provide a
formal definition for corrigibility, then introduce a transformation that
constructs a corrigible version of any goal that can be made corrigible,
without sacrificing performance. This is done by myopically eliciting
predictions of reward conditional on costlessly preventing updates, which then
also determine the reward when updates are accepted. The transformation can be
modified to recursively extend corrigibility to any new agents created by
corrigible agents, and to prevent agents from deliberately modifying their
goals. Two gridworld experiments demonstrate that these corrigible goals can be
learned effectively, and that they lead to the desired behavior.

</details>


### [20] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: MARS是一个端到端的强化学习框架，通过自博弈训练LLM在多智能体系统中进行合作与竞争推理，显著提升了多智能体推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在单智能体任务中有效，但在多轮多智能体场景中的扩展仍未被充分探索，主要面临长时程信用分配和智能体特定优势估计的挑战。

Method: MARS框架包含回合级优势估计器用于信用分配，以及智能体特定优势归一化来稳定多智能体训练，通过自博弈在合作和竞争游戏中训练。

Result: 从Qwen3-4B训练的MARS智能体在保留游戏中性能提升达28.7%，在推理基准测试中集成到领先多智能体系统后，AIME提升10.0%，GPQA-Diamond提升12.5%。

Conclusion: 在战略游戏中通过自博弈进行端到端RL训练是开发LLM可泛化多智能体推理能力的有效方法。

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [21] [Adaptive Minds: Empowering Agents with LoRA-as-Tools](https://arxiv.org/abs/2510.15416)
*Pavan C Shekar,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: Adaptive Minds是一个智能代理系统，将LoRA适配器作为领域专用工具，让基础LLM能够分析查询并动态选择最相关的LoRA工具，实现按需切换领域专家。


<details>
  <summary>Details</summary>
Motivation: 解决单一微调模型或基于规则路由的局限性，通过动态选择领域专家适配器来提供更准确、专业的响应，同时保持对话能力。

Method: 使用基础LLM作为语义路由器分析查询，动态选择最相关的LoRA工具；结合LangGraph进行工作流管理，支持API和Web界面。

Result: 系统能够无缝切换不同领域专家，提供准确的专业响应，同时保持可扩展性和可扩展性。

Conclusion: Adaptive Minds提供了一个可扩展的领域自适应AI辅助基础，结合了多智能体编排的灵活性和参数高效微调的高效性。

Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as
domain-specific tools. Instead of relying on a single fine-tuned model or rigid
rule-based routing, our approach empowers the base LLM itself to act as a
semantic router analyzing each query and dynamically selecting the most
relevant LoRA tool. This enables the agent to seamlessly switch between
different domain experts on demand. By combining the flexibility of multi-agent
orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive
Minds delivers accurate, specialized responses while preserving conversational
ability. The system is built with LangGraph for workflow management, supports
both API and web interfaces, and is fully open source, providing a scalable and
extensible foundation for domain-adaptive AI assistance.

</details>


### [22] [Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning](https://arxiv.org/abs/2510.15514)
*Boyin Liu,Zhuo Zhang,Sen Huang,Lipeng Xie,Qingxu Fu,Haoran Chen,LI YU,Tianyi Hu,Zhaoyang Liu,Bolin Ding,Dongbin Zhao*

Main category: cs.AI

TL;DR: 提出了一个检测和解决强化学习中判断不一致性的框架，包含冲突检测率(CDR)指标和去冲突图奖励(DGR)方法，显著提升训练稳定性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习中面临判断不一致性问题，特别是偏好循环等逻辑一致性问题尚未得到充分解决，这会影响强化学习的稳定性。

Method: 引入冲突检测率(CDR)量化判断冲突，提出去冲突图奖励(DGR)框架，通过构建偏好图、转换为无冲突有向无环图(DAG)，生成逻辑一致的奖励信号。

Result: 实验结果表明，该框架相比强基线方法显著提升了训练稳定性和模型性能。

Conclusion: 逻辑一致性是AI反馈中关键且现在可管理的维度，该框架为强化学习中的判断一致性问题提供了有效解决方案。

Abstract: However, this method often faces judgment inconsistencies that can
destabilize reinforcement learning. While prior research has focused on the
accuracy of judgments, the critical issue of logical coherence especially
issues such as preference cycles hasn't been fully addressed. To fill this gap,
we introduce a comprehensive framework designed to systematically detect and
resolve these inconsistencies during the reinforcement learning training
process. Our framework includes two main contributions: first, the Conflict
Detection Rate (CDR), a new metric that quantifies judgment conflicts, and
second, Deconflicted Graph Rewards (DGR), a framework that purifies signals by
removing cycles before policy optimization. DGR constructs preference graphs
from the initial judgments, transforms them into conflict-free Directed Acyclic
Graphs (DAGs), and generates a logically coherent reward signal that is
compatible with any policy optimizer. Experimental results show that our
framework significantly enhances training stability and model performance
compared to strong baselines, establishing logical consistency as a crucial and
now manageable dimension of AI feedback.

</details>


### [23] [JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament](https://arxiv.org/abs/2510.15560)
*Jiayuan Bai,Xuan-guang Pan,Chongyang Tao,Shuai Ma*

Main category: cs.AI

TL;DR: JudgeSQL是一个用于Text-to-SQL任务中SQL候选选择的结构化推理框架，通过强化学习训练推理型判断模型，并结合加权共识机制来提高选择准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在测试时扩展时面临选择正确SQL查询的瓶颈，传统选择方法如自一致性和最佳N解码只能提供浅层信号，容易产生不一致评分、脆弱推理链，无法捕捉细粒度语义差异。

Method: 开发基于推理的SQL判断模型，通过强化学习在可验证奖励指导下提炼推理轨迹；构建加权共识锦标赛机制，整合显式推理偏好和隐式生成器置信度。

Result: 在BIRD基准测试上的广泛实验表明，JudgeSQL展现出优越的SQL判断能力、良好的跨尺度泛化能力以及对生成器容量的鲁棒性。

Conclusion: JudgeSQL通过结构化推理和加权共识机制重新定义了SQL候选选择，提供了更可靠和高效的解决方案。

Abstract: Text-to-SQL is a pivotal task that bridges natural language understanding and
structured data access, yet it remains fundamentally challenging due to
semantic ambiguity and complex compositional reasoning. While large language
models (LLMs) have greatly advanced SQL generation though prompting, supervised
finetuning and reinforced tuning, the shift toward test-time scaling exposes a
new bottleneck: selecting the correct query from a diverse candidate pool.
Existing selection approaches, such as self-consistency or best-of-$N$
decoding, provide only shallow signals, making them prone to inconsistent
scoring, fragile reasoning chains, and a failure to capture fine-grained
semantic distinctions between closely related SQL candidates. To this end, we
introduce JudgeSQL, a principled framework that redefines SQL candidate
selection through structured reasoning and weighted consensus tournament
mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills
reasoning traces with reinforcement learning guided by verifiable rewards,
enabling accurate and interpretable judgments. Building on this, a weighted
consensus tournament integrates explicit reasoning preferences with implicit
generator confidence, yielding selections that are both more reliable and more
efficient. Extensive experiments on the BIRD benchmark demonstrate that
JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale
generalization and robustness to generator capacity.

</details>


### [24] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: 开发了一个机器学习框架，通过整合患者历史就诊信息来改进健康监测，特别是在就诊次数有限且频率不规律的情况下。该模型首先基于最近一次就诊数据评估初始疾病风险，然后利用历史影像和临床生物标志物信息进行精炼。


<details>
  <summary>Details</summary>
Motivation: 医学中的时间背景对于评估患者健康状况随时间变化至关重要，特别是在就诊次数有限且频率可变的情况下，需要有效整合历史信息来改进健康监测。

Method: 开发了一个两阶段机器学习框架：1) 基于最近一次就诊数据估计初始疾病风险；2) 整合历史影像和临床生物标志物信息来精炼风险评估。应用于前列腺癌风险预测，使用了28,342名患者近十年的数据。

Result: 整合历史背景显著提高了特异性：对于当前就诊时的临床显著前列腺癌风险预测，假阳性率从51%降至33%（整合3次历史影像）和24%（额外整合临床数据）；对于5年内风险预测，假阳性率从64%降至9%。

Conclusion: 随时间收集的信息提供了相关背景，可显著提高医学风险预测的特异性。对于多种进展性疾病，通过背景信息充分降低假阳性率，可为低基线风险人群扩展纵向健康监测项目，实现早期检测和改善健康结果。

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>


### [25] [Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism](https://arxiv.org/abs/2510.15600)
*Haoran Sun,Yankai Jiang,Zhenyu Tang,Yaning Pan,Shuang Gu,Zekai Lin,Lilong Wang,Wenjie Lou,Lei Liu,Lei Bai,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出了SciRecipe数据集和Thoth模型，通过"Sketch-and-Fill"范式和结构化奖励机制，显著提升了科学实验协议生成的完整性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的科学实验协议往往不完整或不一致，限制了其在可重复科学研究中的实用性。

Method: 引入SciRecipe数据集（12K+结构化协议），提出"Sketch-and-Fill"范式分离分析、结构和表达，采用结构化组件奖励机制评估步骤粒度、动作顺序和语义保真度，并通过分阶段的知识到行动过程训练Thoth模型。

Result: Thoth在多个基准测试中持续超越专有和开源LLM，在步骤对齐、逻辑排序和语义准确性方面实现显著改进。

Conclusion: 该方法为构建可靠的科学助手铺平了道路，能够有效连接知识与实验执行，所有数据、代码和模型将公开发布。

Abstract: The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

</details>


### [26] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 提出了一个名为freephdlabor的开源多智能体框架，通过完全动态的工作流程和模块化架构实现科学发现的自动化，解决了现有系统工作流程僵化和上下文管理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的科学发现自动化系统存在两个基本限制：僵化的预编程工作流程无法适应中间发现，以及不充分的上下文管理阻碍了长期研究。需要一种能够动态调整工作流程并有效管理研究上下文的系统。

Method: 开发了freephdlabor多智能体框架，采用完全动态的工作流程（由实时智能体推理确定）和模块化架构，支持用户根据领域需求修改、添加或移除智能体。框架提供自动上下文压缩、基于工作空间的通信、跨会话内存持久化和非阻塞人工干预等基础设施。

Result: 该框架将自动化研究从孤立的单次尝试转变为持续的研究计划，能够系统性地基于先前探索并整合人类反馈。

Conclusion: 通过提供构建可定制合作科学家系统的架构原则和实际实现，这项工作旨在促进自动化研究在科学领域的更广泛采用，使从业者能够部署交互式多智能体系统来自主进行端到端研究。

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [27] [Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences](https://arxiv.org/abs/2510.15716)
*Keertana Chidambaram,Karthik Vinary Seetharaman,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 该论文提出了解决RLHF中两个关键限制的方法：人类评估者的多样性和成对反馈的局限性，通过引入排名反馈和异质偏好处理来改进模型对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF和DPO方法假设统一的标注者偏好并依赖二元比较，忽略了人类评估者的多样性和成对反馈的局限性，这限制了模型对齐的效果。

Method: 1) 将偏好学习与计量经济学文献联系起来，证明三元或更多响应的排名能确保可识别性；2) 开发DPO的EM变体来发现潜在标注者类型并训练混合LLM；3) 提出使用最小最大遗憾公平准则的聚合算法。

Result: 建立了生成模型对齐中公平性和个性化的理论和算法框架，解决了异质用户偏好的处理问题。

Conclusion: 通过排名反馈和异质偏好处理，为生成模型对齐中的公平性和个性化提供了有效的解决方案，改进了现有RLHF方法的局限性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become central to
aligning large language models with human values, typically by first learning a
reward model from preference data which is then used to update the model with
reinforcement learning. Recent alternatives such as Direct Preference
Optimization (DPO) simplify this pipeline by directly optimizing on
preferences. However, both approaches often assume uniform annotator
preferences and rely on binary comparisons, overlooking two key limitations:
the diversity of human evaluators and the limitations of pairwise feedback. In
this work, we address both these issues. First, we connect preference learning
in RLHF with the econometrics literature and show that binary comparisons are
insufficient for identifying latent user preferences from finite user data and
infinite users, while (even incomplete) rankings over three or more responses
ensure identifiability. Second, we introduce methods to incorporate
heterogeneous preferences into alignment algorithms. We develop an
Expectation-Maximization adaptation of DPO that discovers latent annotator
types and trains a mixture of LLMs accordingly. Then we propose an aggregation
algorithm using a min-max regret fairness criterion to produce a single
generative policy with equitable performance guarantees. Together, these
contributions establish a theoretical and algorithmic framework for fairness
and personalization for diverse users in generative model alignment.

</details>


### [28] [Invoice Information Extraction: Methods and Performance Evaluation](https://arxiv.org/abs/2510.15727)
*Sai Yashwant,Anurag Dubey,Praneeth Paikray,Gantala Thulsiram*

Main category: cs.AI

TL;DR: 提出从发票文档中提取结构化信息的方法和评估指标，用于评估提取数据与标注真值的准确性


<details>
  <summary>Details</summary>
Motivation: 需要标准化评估发票信息提取方法，比较不同提取方法的表现并识别各字段提取的优缺点

Method: 使用Docling和LlamaCloud服务预处理扫描或数字发票，提取关键字段如发票号、日期、总金额和供应商详情

Result: 建立了包含字段级精度、一致性检查失败和精确匹配准确率的稳健评估框架

Conclusion: 提出的评估指标为比较不同提取方法提供了标准化方式，并能突出字段特定性能的优缺点

Abstract: This paper presents methods for extracting structured information from
invoice documents and proposes a set of evaluation metrics (EM) to assess the
accuracy of the extracted data against annotated ground truth. The approach
involves pre-processing scanned or digital invoices, applying Docling and
LlamaCloud Services to identify and extract key fields such as invoice number,
date, total amount, and vendor details. To ensure the reliability of the
extraction process, we establish a robust evaluation framework comprising
field-level precision, consistency check failures, and exact match accuracy.
The proposed metrics provide a standardized way to compare different extraction
methods and highlight strengths and weaknesses in field-specific performance.

</details>


### [29] [AURA: An Agent Autonomy Risk Assessment Framework](https://arxiv.org/abs/2510.15739)
*Lorenzo Satta Chiris,Ayush Mishra*

Main category: cs.AI

TL;DR: AURA是一个统一的框架，用于检测、量化和减轻由智能AI代理产生的风险，采用基于gamma的风险评分方法，支持人机协作监管和自主风险评估。


<details>
  <summary>Details</summary>
Motivation: 随着自主智能AI系统在组织中的广泛应用，对齐、治理和风险管理方面的持续挑战阻碍了大规模部署，需要一种有效的风险评估框架。

Method: AURA引入基于gamma的风险评分方法，平衡评估准确性与计算效率，提供交互式流程来评分、评估和减轻AI代理风险，支持人机协作监管和代理间通信机制。

Result: 该框架能够实现稳健的风险检测和缓解，同时平衡计算资源，使其成为企业环境中大规模、可治理智能AI的关键推动者。

Conclusion: AURA支持负责任和透明的智能AI采用，为大规模企业级智能AI部署提供了关键的风险管理解决方案。

Abstract: As autonomous agentic AI systems see increasing adoption across
organisations, persistent challenges in alignment, governance, and risk
management threaten to impede deployment at scale. We present AURA (Agent
aUtonomy Risk Assessment), a unified framework designed to detect, quantify,
and mitigate risks arising from agentic AI. Building on recent research and
practical deployments, AURA introduces a gamma-based risk scoring methodology
that balances risk assessment accuracy with computational efficiency and
practical considerations. AURA provides an interactive process to score,
evaluate and mitigate the risks of running one or multiple AI Agents,
synchronously or asynchronously (autonomously). The framework is engineered for
Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)
communication mechanisms, allowing for seamless integration with agentic
systems for autonomous self-assessment, rendering it interoperable with
established protocols (MCP and A2A) and tools. AURA supports a responsible and
transparent adoption of agentic AI and provides robust risk detection and
mitigation while balancing computational resources, positioning it as a
critical enabler for large-scale, governable agentic AI in enterprise
environments.

</details>


### [30] [Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment](https://arxiv.org/abs/2510.15748)
*Minlin Zeng,Zhipeng Zhou,Yang Qiu,Zhiqi Shen*

Main category: cs.AI

TL;DR: 提出了首个将帕金森病评估的多模态学习建模为多目标优化问题的系统TRIP，解决了传统方法需要模态同步训练和依赖所有模态推理的限制。


<details>
  <summary>Details</summary>
Motivation: 传统多模态方法存在两个主要限制：(1)训练时需要同步所有模态，(2)推理时依赖所有模态，这阻碍了实际应用。

Method: 将多模态学习建模为多目标优化问题，引入基于边界的类别重平衡策略来处理模态内不平衡问题。

Result: 在三个公共数据集上的实验表明，TRIP在异步设置下比最佳基线分别提升16.48、6.89和11.55个百分点，在同步设置下提升4.86和2.30个百分点。

Conclusion: TRIP框架在帕金森病评估中实现了最先进的性能，展现了其有效性和适应性。

Abstract: Parkinson's disease assessment has garnered growing interest in recent years,
particularly with the advent of sensor data and machine learning techniques.
Among these, multimodal approaches have demonstrated strong performance by
effectively integrating complementary information from various data sources.
However, two major limitations hinder their practical application: (1) the need
to synchronize all modalities during training, and (2) the dependence on all
modalities during inference. To address these issues, we propose the first
Parkinson's assessment system that formulates multimodal learning as a
multi-objective optimization (MOO) problem. This not only allows for more
flexible modality requirements during both training and inference, but also
handles modality collapse issue during multimodal information fusion. In
addition, to mitigate the imbalance within individual modalities, we introduce
a margin-based class rebalancing strategy to enhance category learning. We
conduct extensive experiments on three public datasets under both synchronous
and asynchronous settings. The results show that our framework-Towards Relaxed
InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best
baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous
setting, and by 4.86 and 2.30 percentage points in the synchronous setting,
highlighting its effectiveness and adaptability.

</details>


### [31] [Preliminary Quantitative Study on Explainability and Trust in AI Systems](https://arxiv.org/abs/2510.15769)
*Allen Daniel Sunny*

Main category: cs.AI

TL;DR: 本文通过定量实验研究AI系统的可解释性与用户信任之间的关系，发现交互式解释能增强用户参与度和信任度，解释的清晰度和相关性是信任的关键决定因素。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4等大规模AI模型在关键领域（如法律、医疗、金融）的部署，关于AI系统的信任和透明度问题日益紧迫，需要研究可解释性如何影响用户信任。

Method: 采用定量实验设计，通过基于网络的交互式贷款审批模拟，比较不同类型解释（从基本特征重要性到交互式反事实解释）对用户信任感知的影响。

Result: 结果显示交互性增强了用户参与度和信心，解释的清晰度和相关性是信任的关键决定因素。

Conclusion: 研究为人本可解释AI领域提供了实证证据，强调了可解释性设计对用户感知的可测量影响。

Abstract: Large-scale AI models such as GPT-4 have accelerated the deployment of
artificial intelligence across critical domains including law, healthcare, and
finance, raising urgent questions about trust and transparency. This study
investigates the relationship between explainability and user trust in AI
systems through a quantitative experimental design. Using an interactive,
web-based loan approval simulation, we compare how different types of
explanations, ranging from basic feature importance to interactive
counterfactuals influence perceived trust. Results suggest that interactivity
enhances both user engagement and confidence, and that the clarity and
relevance of explanations are key determinants of trust. These findings
contribute empirical evidence to the growing field of human-centered
explainable AI, highlighting measurable effects of explainability design on
user perception

</details>


### [32] [Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL](https://arxiv.org/abs/2510.15772)
*Richard M. Bailey*

Main category: cs.AI

TL;DR: 提出了Dialectica框架，通过结构化对话、记忆、自我反思和政策约束的上下文编辑，让AI代理在复杂问题中发展专业知识。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂多维度问题中缺乏通过经验发展专业知识的内生机制的问题。

Method: 使用Dialectica框架，让代理在定义的主题上进行结构化对话，结合记忆、自我反思和政策约束的上下文编辑，将讨论视为隐式元强化学习过程。

Result: 在两种模型架构上的实验表明，启用基于反思的上下文编辑的代理在Elo评分、标准化Bradley-Terry-Davidson能力和AlphaRank质量上均优于基线。

Conclusion: 对话驱动的上下文演化是在开放不可验证领域中实现目标专业知识放大的可行路径。

Abstract: So-called `wicked problems', those involving complex multi-dimensional
settings, non-verifiable outcomes, heterogeneous impacts and a lack of single
objectively correct answers, have plagued humans throughout history. Modern
examples include decisions over justice frameworks, solving environmental
pollution, planning for pandemic resilience and food security. The use of
state-of-the-art artificial intelligence systems (notably Large Language
Model-based agents) collaborating with humans on solving such problems is being
actively explored. While the abilities of LLMs can be improved by, for example,
fine-tuning, hand-crafted system prompts and scaffolding with external tools,
LLMs lack endogenous mechanisms to develop expertise through experience in such
settings. This work address this gap with Dialectica, a framework where agents
engage in structured dialogue on defined topics, augmented by memory,
self-reflection, and policy-constrained context editing. Formally, discussion
is viewed as an implicit meta-reinforcement learning process. The
`dialogue-trained' agents are evaluated post-hoc using judged pairwise
comparisons of elicited responses. Across two model architectures (locally run
Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based
context editing during discussion produces agents which dominate their baseline
counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and
AlphaRank mass. The predicted signatures of learning are observed qualitatively
in statement and reflection logs, where reflections identify weaknesses and
reliably shape subsequent statements. Agreement between quantitative and
qualitative evidence supports dialogue-driven context evolution as a practical
path to targeted expertise amplification in open non-verifiable domains.

</details>


### [33] [Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID](https://arxiv.org/abs/2510.15782)
*Philip DiGiacomo,Haoyang Wang,Jinrui Fang,Yan Leng,W Michael Brode,Ying Ding*

Main category: cs.AI

TL;DR: 开发了六种RAG语料配置用于长新冠临床问答，结合临床指南和高质量系统评价的配置表现最佳，提出了Guide-RAG系统来有效回答长新冠临床问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在临床医学中的应用增加，为复杂新兴疾病开发有效框架面临挑战，特别是针对长新冠这样的新兴疾病。

Method: 开发并评估了六种RAG语料配置，从专家精选来源到大规模文献数据库，使用LLM作为评判框架在忠实性、相关性和全面性指标上进行评估。

Result: 结合临床指南和高质量系统评价的RAG语料配置在性能上始终优于单一指南方法和大规模文献数据库。

Conclusion: 对于新兴疾病，基于精选二次评价的检索在狭窄共识文档和未过滤原始文献之间提供了最佳平衡，支持临床决策同时避免信息过载和过度简化的指导。

Abstract: As AI chatbots gain adoption in clinical medicine, developing effective
frameworks for complex, emerging diseases presents significant challenges. We
developed and evaluated six Retrieval-Augmented Generation (RAG) corpus
configurations for Long COVID (LC) clinical question answering, ranging from
expert-curated sources to large-scale literature databases. Our evaluation
employed an LLM-as-a-judge framework across faithfulness, relevance, and
comprehensiveness metrics using LongCOVID-CQ, a novel dataset of
expert-generated clinical questions. Our RAG corpus configuration combining
clinical guidelines with high-quality systematic reviews consistently
outperformed both narrow single-guideline approaches and large-scale literature
databases. Our findings suggest that for emerging diseases, retrieval grounded
in curated secondary reviews provides an optimal balance between narrow
consensus documents and unfiltered primary literature, supporting clinical
decision-making while avoiding information overload and oversimplified
guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation
framework that integrates both curated expert knowledge and comprehensive
literature databases to effectively answer LC clinical questions.

</details>


### [34] [PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold](https://arxiv.org/abs/2510.15862)
*Yi Wan,Jiuqi Wang,Liam Li,Jinsong Liu,Ruihao Zhu,Zheqing Zhu*

Main category: cs.AI

TL;DR: PokeeResearch-7B是一个7B参数的深度研究智能体，通过统一的强化学习框架构建，在10个深度研究基准测试中达到7B规模智能体的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具增强的大型语言模型的研究智能体存在检索浅层、对齐指标弱和工具使用行为脆弱等问题，需要更鲁棒、对齐和可扩展的解决方案。

Method: 采用无标注的AI反馈强化学习框架，使用基于LLM的奖励信号优化策略，结合思维链驱动的多调用推理框架，实现自我验证和工具故障自适应恢复。

Result: 在10个流行的深度研究基准测试中，PokeeResearch-7B在7B规模深度研究智能体中达到最先进性能。

Conclusion: 精心设计的强化学习和推理框架可以产生高效、有弹性和研究级的人工智能智能体。

Abstract: Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
reinforcement learning framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through self-verification and adaptive recovery from tool
failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale deep research agents. This
highlights that careful reinforcement learning and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Adaptive Base Representation Theorem: An Alternative to Binary Number System](https://arxiv.org/abs/2510.15099)
*Ravin Kumar*

Main category: cs.IT

TL;DR: 本文提出了自适应基数表示(ABR)定理和一种新型数字系统，作为二进制系统的结构化替代方案，能够用相同比特数唯一表示十进制数，并与现有压缩算法和纠错机制兼容。


<details>
  <summary>Details</summary>
Motivation: 为数字计算机提供二进制系统的结构化替代方案，探索数字数据表示的新方法。

Method: 提出ABR定理和数字系统，通过理论基础和数学公式证明其编码能力与二进制相同，并验证与现有技术的兼容性。

Result: ABR系统能够用相同比特数唯一表示十进制数，编码范围与二进制相同，且兼容Huffman编码、算术编码和汉明码等现有技术。

Conclusion: ABR数字系统可作为二进制系统的可行替代方案，在信息理论和数字编码领域具有应用潜力，可能推动数字数据表示和计算设计的新方法。

Abstract: This paper introduces the Adaptive Base Representation (ABR) Theorem and
proposes a novel number system that offers a structured alternative to the
binary number system for digital computers. The ABR number system enables each
decimal number to be represented uniquely and using the same number of bits,
$n$, as the binary encoding. Theoretical foundations and mathematical
formulations demonstrate that ABR can encode the same integer range as binary,
validating its potential as a viable alternative. Additionally, the ABR number
system is compatible with existing data compression algorithms like Huffman
coding and arithmetic coding, as well as error detection and correction
mechanisms such as Hamming codes. We further explore practical applications,
including digital steganography, to illustrate the utility of ABR in
information theory and digital encoding, suggesting that the ABR number system
could inspire new approaches in digital data representation and computational
design.

</details>


### [36] [Outage-Aware Sum Rate Maximization in Movable Antennas-Enabled Systems](https://arxiv.org/abs/2510.15292)
*Guojie Hu,Qingqing Wu,Ming-Min Zhao,Wen Chen,Zhenyu Xiao,Kui Xu,Jiangbo Si*

Main category: cs.IT

TL;DR: 本文研究了可移动天线(MAs)支持的多输入单输出(MISO)系统，在延迟敏感场景下，通过联合优化天线位置和发射波束成形来最大化中断感知和速率。


<details>
  <summary>Details</summary>
Motivation: 在延迟敏感场景中，用户避免发送训练信号以避免额外延迟，基站只能依赖统计信道状态信息(CSI)进行数据传输。传统固定位置天线(FPA)系统性能受限，而可移动天线可以通过优化位置来提升系统性能。

Method: 采用基于统计CSI的迫零波束成形设计，通过重要引理推导SINR的紧致均值和方差，利用Laguerre级数近似得到SINR的闭合形式CDF，并开发投影梯度上升(PGA)方法迭代优化天线位置。

Result: 数值结果表明，所提方案相比传统固定位置天线和其他竞争基准方案具有更好的性能表现。

Conclusion: 可移动天线系统在统计CSI场景下能够显著提升中断感知和速率，所提出的优化算法有效解决了天线位置和波束成形的联合优化问题。

Abstract: In this paper, we investigate the movable antennas (MAs)-enabled
multiple-input-single-output (MISO) systems, where the base station (BS)
equipped with multiple MAs serves multiple single-antenna user. The
delay-sensitive scenario is considered, where users refrain from periodically
sending training signals to the BS for channel estimations to avoid additional
latency. As a result, the BS relies solely on the statistical channel state
information (CSI) to transmit data with a fixed rate. Under this setup, we aim
to maximize the outage-aware sum rate of all users, by jointly optimizing
antenna positions and the transmit beamforming at the BS, while satisfying the
given target outage probability requirement at each user. The problem is highly
non-convex, primarily because the exact cumulative distribution function (CDF)
of the received signal-to-interference-plus-noise ratio (SINR) of each user is
difficult to derive. To simplify analysis and without comprising performance,
we adopt the statistical CSI based zero-forcing beamforming design. We then
introduce one important lemma to derive the tight mean and variance of the
SINR. Leveraging these results, we further exploit the Laguerre series
approximation to successfully derive the closedform and tight CDF of the SINR.
Subsequently, the outageaware sum rate expression is presented but still
includes complex structure with respect to antenna positions. Facing this
challenge, the projected gradient ascent (PGA) method is developed to
iteratively update antenna positions until convergence. Numerical results
demonstrate the effectiveness of our proposed schemes compared to conventional
fixed-position antenna (FPA) and other competitive benchmarks.

</details>


### [37] [Rotatable Antenna Meets UAV: Towards Dual-Level Channel Reconfiguration Paradigm for ISAC](https://arxiv.org/abs/2510.15295)
*Shiying Chen,Guangji Chen,Long Shi,Qingqing Wu,Kang Wei*

Main category: cs.IT

TL;DR: 提出了一种用于集成感知与通信(ISAC)的双层信道重构框架，通过部署可旋转天线来控制感知与通信信道的路径损耗和相关性，实现两者性能的灵活权衡。


<details>
  <summary>Details</summary>
Motivation: ISAC需要在共享的无线资源上实现感知与通信功能的平衡，但由于资源共享，难以在这两个集成功能之间达到关键权衡。

Method: 在无人机上部署可旋转天线，通过联合优化RA旋转、发射波束成形和无人机轨迹来最大化通信速率，同时满足感知性能要求。针对静态和移动无人机场景分别提出了优化方案。

Result: 仿真结果表明，所提出的设计相比基准方案显著提高了可实现的感知与通信权衡区域。

Conclusion: 该框架通过主动控制感知与通信信道的相关性，实现了两者性能的灵活权衡，为ISAC系统提供了有效的解决方案。

Abstract: Integrated sensing and communication (ISAC) is viewed as a key enabler for
future wireless networks by sharing the hardware and wireless resources between
the functionalities of sensing and communication (S&C). Due to the shared
wireless resources for both S&C, it is challenging to achieve a critical
trade-off between these two integrated functionalities. To address this issue,
this paper proposes a novel dual-level channel reconfiguration framework for
ISAC by deploying rotatable antennas at an unmanned aerial vehicle (UAV), where
both the large-scale path loss and the correlation of S&C channels can be
proactively controlled, thereby allowing a flexible trade-off between S&C
performance. To characterize the S&C tradeoff, we aim to maximize the
communication rate by jointly optimizing the RA rotation, the transmit
beamforming, and the UAV trajectory, subject to the given requirement of
sensing performance. For the typical scenario of static UAV deployment, we
introduce the concept of subspace correlation coefficient to derive closed-form
solutions for the optimal RA rotation, transmit beamforming, and UAV hovering
location. For the scenario of a fully mobile UAV, we prove that the optimal
trajectory of a UAV follows a hover-fly-hover (HFH) structure, thereby
obtaining its global optimal solution. Simulation results show that the
proposed design significantly improves the achievable S&C trade-off region
compared to benchmark schemes.

</details>


### [38] [Subverting Flexible Multiuser Communications via Movable Antenna-Enabled Jammer](https://arxiv.org/abs/2510.15298)
*Guojie Hu,Qingqing Wu,Lipeng Zhu,Kui Xu,Guoxin Li,Jiangbo Si,Jian Ouyang,Tong-Xing Zheng*

Main category: cs.IT

TL;DR: 本文研究利用可移动天线(MA)技术增强合法干扰器(MAJ)的性能，通过联合优化天线位置和干扰波束成形来最小化可疑多用户下行通信的效益。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术能够通过自适应调整天线位置来重构无线信道，为提升系统性能提供额外的空间自由度。从安全角度出发，利用MA技术增强合法干扰器来破坏可疑通信系统。

Method: 采用交替优化算法，首先确定可疑发射机在给定MAJ行动下的最优行为，然后基于此简化问题并迭代求解天线位置和干扰波束成形。针对两个可疑接收机的特殊情况，分析了天线部署规则。

Result: 数值结果表明，所提出的方案相比传统固定位置天线和其他竞争基准具有更好的性能。

Conclusion: 可移动天线技术能够有效增强合法干扰器的性能，通过智能天线部署和波束成形优化可以显著降低可疑通信系统的效益。

Abstract: Movable antenna (MA) is an emerging technology which can reconfigure wireless
channels via adaptive antenna position adjustments at transceivers, thereby
bringing additional spatial degrees of freedom for improving system
performance. In this paper, from a security perspective, we exploit the
MAenabled legitimate jammer (MAJ) to subvert suspicious multiuser downlink
communications consisting of one suspicious transmitter (ST) and multiple
suspicious receivers (SRs). Specifically, our objective is to minimize the
benefit (the sum rate of all SRs or the minimum rate among all SRs) of such
suspicious communications, by jointly optimizing antenna positions and the
jamming beamforming at the MAJ. However, the key challenge lies in that given
the MAJ's actions, the ST can reactively adjust its power allocations to
instead maximize its benefit for mitigating the unfavorable interference. Such
flexible behavior of the ST confuses the optimization design of the MAJ to a
certain extent. Facing this difficulty, corresponding to the above two
different benefits: i) we respectively determine the optimal behavior of the ST
given the MAJ's actions; ii) armed with these, we arrive at two simplified
problems and then develop effective alternating optimization based algorithms
to iteratively solve them. In addition to these, we also focus on the special
case of two SRs, and reveal insightful conclusions about the deployment rule of
antenna positions at the MAJ. Furthermore, we analyze the ideal antenna
deployment scheme at the MAJ for achieving the globally performance lower
bound. Numerical results demonstrate the effectiveness of our proposed schemes
compared to conventional fixed-position antenna (FPA) and other competitive
benchmarks.

</details>


### [39] [ProxySelect: Frequency Selectivity-Aware Scheduling for Joint OFDMA and MU-MIMO in 802.11ax WiFi](https://arxiv.org/abs/2510.15452)
*Xiang Zhang,Michail Palaiologos,Christian Bluemm,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了ProxySelect算法，用于802.11ax中OFDMA和MU-MIMO联合使用的用户调度，通过代理速率和采样候选组生成方案，在保证性能的同时降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着WiFi带宽增长，信道频率选择性增强，传统窄带用户选择算法在802.11ax中效率低下，需要新的调度算法来应对频率选择性和复杂子带分配模式。

Method: 将调度任务建模为整数线性规划问题，引入代理速率来近似ZFBF速率而无需复杂矩阵求逆，并开发基于采样的候选组生成方案限制问题规模。

Result: 仿真显示ProxySelect在接近最优速率性能的同时显著降低了复杂度。

Conclusion: ProxySelect算法有效解决了802.11ax中频率选择性带来的用户调度挑战，实现了性能与复杂度的良好平衡。

Abstract: IEEE 802.11ax introduces orthogonal frequency division multiple access
(OFDMA) to WiFi to support concurrent transmissions to a larger number of
users. As bandwidth continues to grow, WiFi channels exhibit increased
frequency selectivity, which poses new challenges for MU-MIMO user selection:
the optimal user set varies across frequency and is interleaved over subbands
(called resource units, or RUs). This frequency selectivity, coupled with the
complex subband allocation pattern, renders conventional narrowband user
selection algorithms inefficient for 802.11ax. In this paper, we propose
\emph{ProxySelect}, a scalable and frequency selectivity-aware user scheduling
algorithm for joint OFDMA and MU-MIMO usage in 802.11ax under zero-forcing
beamforming (ZFBF). The scheduling task is formulated as an integer linear
program (ILP) with binary variables indicating user (group)-RU associations,
and linear constraints ensuring standard compatibility. To reduce complexity,
we introduce a novel proxy rate--a function of individual channel strengths and
their correlations--that approximates the ZFBF rate without requiring
cubic-complexity matrix inversion. Additionally, we develop a sampling-based
candidate group generation scheme that selects up to $T$ near-orthogonal user
groups for each RU, thereby bounding the ILP size and ensuring scalability.
Simulations using realistic ray-tracing-based channel models show that
ProxySelect achieves near-optimal rate performance with significantly lower
complexity.

</details>


### [40] [Near-Field Imaging by Exploiting Frequency Correlation in Wireless Communication Networks](https://arxiv.org/abs/2510.15459)
*Tianyu Yang,Kangda Zhi,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出一种基于稀疏贝叶斯学习的近场成像方法，利用宽带无线通信网络中的多频段信息和图像相关性，通过设计两种不同的照明模式来提升成像性能。


<details>
  <summary>Details</summary>
Motivation: 解决在宽带无线通信网络下的近场成像问题，利用均匀线性阵列的近场信道特性和频域图像相关性来提升成像质量。

Method: 将图像恢复建模为特殊的多测量向量压缩感知问题，采用稀疏贝叶斯学习方法同时估计所有频段的图像系数及其相关性，并设计了两种照明模式：一种最小化感知矩阵的总相干性，另一种最大化成像区域的照明功率。

Result: 数值结果表明所提出的SBL方法有效，且两种照明设计均表现出优越性能。

Conclusion: 该方法能够有效解决多频段感知矩阵不同且系数相关的近场成像问题，通过优化照明设计进一步提升了成像性能。

Abstract: In this work, we address the near-field imaging under a wideband wireless
communication network by exploiting both the near-field channel of a uniform
linear array (ULA) and the image correlation in the frequency domain. We first
formulate the image recovery as a special multiple measurement vector (MMV)
compressed sensing (CS) problem, where at various frequencies the sensing
matrices can be different, and the image coefficients are correlated. To solve
such an MMV problem with various sensing matrices and correlated coefficients,
we propose a sparse Bayesian learning (SBL)-based solution to simultaneously
estimate all image coefficients and their correlation on multiple frequencies.
Moreover, to enhance estimation performance, we design two illumination
patterns following two different criteria. From the CS perspective, the first
design minimizes the total coherence of the sensing matrix to increase the
mutual orthogonality of the basis vectors. Alternatively, to improve SNR, the
second design maximizes the illumination power of the imaging area. Numerical
results demonstrate the effectiveness of the proposed SBL-based method and the
superiority of the illumination designs.

</details>


### [41] [New generalizations of circular complex fuzzy sets and Gaussian weighted aggregation operators](https://arxiv.org/abs/2510.15605)
*Yelda Gülfırat,Mehmet Ünver*

Main category: cs.IT

TL;DR: 提出了圆形复q阶正交对模糊集(CCq-ROFS)作为统一现有圆形复直觉模糊集和复q阶正交对模糊集的新泛化框架，并基于高斯函数构建了新的聚合算子。


<details>
  <summary>Details</summary>
Motivation: 统一现有的圆形复直觉模糊集和复q阶正交对模糊集框架，通过高斯函数实现更平滑和统计上有意义的模糊建模。

Method: 扩展高斯框架到CCq-ROFS，使用高斯三角范数和余范构建高斯加权算术和几何聚合算子。

Result: 开发了新的高斯基聚合算子，能够一致地整合隶属度和非隶属度信息用于模糊建模和决策。

Conclusion: CCq-ROFS提供了一个统一的模糊集框架，高斯基方法为模糊建模和决策提供了更有效的工具。

Abstract: In this paper, we introduce the concept of the circular complex $q$-rung
orthopair fuzzy set (CC$q$-ROFS) as a novel generalization that unifies the
existing frameworks of circular complex intuitionistic fuzzy sets (CCIFSs) and
complex $q$-rung orthopair fuzzy sets. If $q = 2$, the structure is referred to
as a circular complex Pythagorean fuzzy set, and if $q = 3$, it is called a
circular complex Fermatean fuzzy set. The proposed approach extends the
Gaussian-based framework to the CC$q$-ROFSs, aiming to achieve a smoother and
statistically meaningful representation of uncertainty. Within this setting,
new Gaussian-based aggregation operators for CC$q$-ROFSs are constructed by
employing the Gaussian triangular norm and conorm. Furthermore,
Gaussian-weighted arithmetic and Gaussian-weighted geometric aggregation
operators are formulated to enable consistent integration of membership and
non-membership information for fuzzy modeling and decision-making.

</details>


### [42] [Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization](https://arxiv.org/abs/2510.15701)
*Binggui Zhou,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种基于学习的两层架构发现框架(LTTADF)，用于在考虑电路复杂度的前提下，发现非理想BD-RIS的最优架构，解决非理想性和电路复杂度对性能的联合影响问题。


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS设计在非理想条件下的架构发现尚未研究，非理想性和电路复杂度如何共同影响BD-RIS性能尚不明确，难以在存在非理想性的情况下实现性能-电路复杂度的权衡。

Method: 提出了LTTADF框架，包含架构生成器和性能优化器，能够在大规模架构空间中有效探索，避免陷入局部最优，实现性能优化的近最优解。

Result: 数值结果为考虑性能-电路复杂度权衡的非理想BD-RIS部署提供了有价值的见解。

Conclusion: 该框架成功解决了非理想BD-RIS架构发现的计算复杂度和全局优化难题，为实现性能-电路复杂度权衡提供了有效解决方案。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been
introduced to enable advanced control over electromagnetic waves to further
increase the benefits of traditional RIS in enhancing signal quality and
improving spectral and energy efficiency for next-generation wireless networks.
A significant issue in designing and deploying BD-RIS is the tradeoff between
its performance and circuit complexity. Despite some efforts in exploring
optimal architectures with the lowest circuit complexities for ideal BD-RIS,
architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore,
how non-idealities and circuit complexity jointly affect the performance of
BD-RIS remains unclear, making it difficult to achieve the performance -
circuit complexity tradeoff in the presence of non-idealities. Essentially,
architecture discovery for non-ideal BD-RIS faces challenges from both the
computational complexity of global architecture search and the difficulty in
achieving global optima. To tackle these challenges, we propose a
learning-based two-tier architecture discovery framework (LTTADF) consisting of
an architecture generator and a performance optimizer to jointly discover
optimal architectures of non-ideal BD-RIS given specific circuit complexities,
which can effectively explore over a large architecture space while avoiding
getting trapped in poor local optima and thus achieving near-optimal solutions
for the performance optimization. Numerical results provide valuable insights
for deploying non-ideal BD-RIS considering the performance - circuit complexity
tradeoff.

</details>
