<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 15]
- [cs.AI](#cs.AI) [Total: 72]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Towards Automated and Predictive Network-Level Energy Profiling in Reconfigurable IoT Systems](https://arxiv.org/abs/2510.09842)
*Mohammud J. Bocus,Senhui Qiu,Robert J. Piechocki,Kerstin Eder*

Main category: cs.NI

TL;DR: 提出一个测量驱动的网络级智能能源分析系统，通过自动化时间同步仪器，在分布式物联网基础设施中实现端到端的能源流可见性，并生成能源数据集以支持AI驱动的预测和自适应能源优化。


<details>
  <summary>Details</summary>
Motivation: 能源效率已成为可持续物联网网络发展的关键约束，需要超越基于模拟或设备中心的研究，提供测量驱动的网络级智能能源分析。

Method: 开发了一个统一的分析平台，整合了蓝牙低功耗和可见光通信模式，结合环境感知和电子墨水显示子系统，通过自动化时间同步仪器捕获节点和网关层的细粒度能源动态。

Result: 在物联网网络级测试平台中验证，该方法在实际操作条件下表现出稳健性能。

Conclusion: 该框架解决了物联网生态系统能源数据稀缺的问题，为基于AI的预测性和自适应能源优化提供了支持。

Abstract: Energy efficiency has emerged as a defining constraint in the evolution of
sustainable Internet of Things (IoT) networks. This work moves beyond
simulation-based or device-centric studies to deliver measurement-driven,
network-level smart energy analysis. The proposed system enables end-to-end
visibility of energy flows across distributed IoT infrastructures, uniting
Bluetooth Low Energy (BLE) and Visible Light Communication (VLC) modes with
environmental sensing and E-ink display subsystems under a unified profiling
and prediction platform. Through automated, time-synchronized instrumentation,
the framework captures fine-grained energy dynamics across both node and
gateway layers. We developed a suite of tools that generate energy datasets for
IoT ecosystems, addressing the scarcity of such data and enabling AI-based
predictive and adaptive energy optimization. Validated within a network-level
IoT testbed, the approach demonstrates robust performance under real operating
conditions.

</details>


### [2] [Fine-grained CDN Delegation](https://arxiv.org/abs/2510.09983)
*Ethan Thompson,Ali Sadeghi Jahromi,AbdelRahman Abdou*

Main category: cs.NI

TL;DR: 提出Delegation Certificates (DeCerts)作为细粒度CDN委托解决方案，通过修改X.509证书标准实现域主完全自主的委托管理，无需CA参与。


<details>
  <summary>Details</summary>
Motivation: 现有委托解决方案如RFC 9345缺乏细粒度委托定义，无法控制委托链长度、撤销机制、操作权限等关键方面。CDN使用广泛（约5500万网站），需要更安全的委托机制。

Method: 修改X.509证书标准，添加新扩展以支持细粒度CDN委托。允许域主指定委托和非委托子域，控制委托深度，并实现完全自主的证书颁发和撤销。

Result: 成功修改Firefox支持DeCert验证，证明与浏览器和TLS/HTTPS协议的兼容性。DeCerts在安全、性能和委托者控制之间实现平衡。

Conclusion: DeCerts增强了CDN委托的安全性、可扩展性和可管理性，为互联网服务提供了实用的解决方案，使域主获得完全的委托控制权。

Abstract: The use of Content Delivery Networks (CDNs) has significantly increased over
the past decade, with approximately 55 million websites currently relying on
CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),
lack fine-grained definitions of many critical aspects of delegation, such as
the length of delegation chains, revocation mechanism, permitted operations,
and a well-defined scope for said delegation. We present Delegation
Certificates (DeCerts), which modify X.509 certificate standard and add new
extensions to enable fine-grained CDN delegation. DeCerts allow domain owners
to specify delegated and non-delegated subdomains, and control the depth of
delegation extended by CDNs, which provides flexibility in delegation
management. But more importantly, DeCerts are built on a new principle which
provides full autonomy to domain owners-domain owners can issue DeCerts fully
independent of Certificate Authorities (CAs), and thus have greater flexibility
in policy control, including revocation methods. Such level of flexibility
would be hard to match if CAs where to issue such certificates. Revoking a
DeCert revokes delegation. We discuss multiple revocation mechanisms for a
DeCerts balancing security, performance, and delegator control. We modify
Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and
test it to demonstrate the feasibility, compatibility of DeCerts with browsers
and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and
manageability of CDN delegation, offering a practical solution for Internet
services.

</details>


### [3] [Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference](https://arxiv.org/abs/2510.10040)
*Shafi Ullah Khan,Michel Kulhandjian,Debashri Roy*

Main category: cs.NI

TL;DR: 该论文研究了基于机器学习的方法在CBRS频段增强频谱共享能力，特别关注5G信号与军用雷达系统的共存问题。研究表明ML技术能够显著扩展FCC建议的SINR边界，在-5dB高干扰环境下仍能达到99%的雷达检测准确率。


<details>
  <summary>Details</summary>
Motivation: 频谱共享是满足用户需求的关键策略，但商业无线服务与关键任务系统（如军用雷达）的共存监管和技术实现仍面临重大挑战。需要探索新方法来增强频谱共享能力。

Method: 采用基于机器学习的方法，利用IQ数据和频谱图数据，在CBRS频段进行雷达检测和波形识别。通过合成和真实世界信号的严格评估来验证模型性能。

Result: ML模型在-5dB SINR的高干扰环境下仍能达到FCC建议的99%雷达检测准确率，显著扩展了SINR边界（从约12dB降至-5dB）。同时能够以93%的准确率分类六种不同的雷达波形类型。

Conclusion: 机器学习方法能够有效增强CBRS频段的频谱共享能力，显著提高雷达在高干扰环境下的检测性能，为商业无线服务与关键任务系统的安全共存提供了可行的技术解决方案。

Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands
via commercial wireless services, yet its effective regulation and
technological enablement, particularly concerning coexistence with incumbent
systems, remain significant challenges. Federal organizations have established
regulatory frameworks to manage shared commercial use alongside
mission-critical operations, such as military communications. This paper
investigates the potential of machine learning (ML)-based approaches to enhance
spectrum sharing capabilities within the Citizens Broadband Radio Service
(CBRS) band, specifically focusing on the coexistence of commercial signals
(e.g., 5G) and military radar systems. We demonstrate that ML techniques can
potentially extend the Federal Communications Commission (FCC)-recommended
signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar
detection and waveform identification in high-interference environments.
Through rigorous evaluation using both synthetic and real-world signals, our
findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)
data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection
accuracy even when subjected to high interference from 5G signals upto -5dB
SINR, exceeding the required limits of $20$ SINR. Our experimental studies
distinguish this work from the state-of-the-art by significantly extending the
SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down
to $-5$ dB. Subsequent to detection, we further apply ML to analyze and
identify radar waveforms. The proposed models also demonstrate the capability
to classify six distinct radar waveform types with $93\%$ accuracy.

</details>


### [4] [Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures](https://arxiv.org/abs/2510.10044)
*Rahul Vanukuri,Shafi Ullah Khan,Talip Tolga Sarı,Gokhan Secinti,Diego Patiño,Debashri Roy*

Main category: cs.NI

TL;DR: 提出基于扩散模型的频谱图生成方法，用于合成CBRS频段中LTE、5G和雷达信号的多样化频谱图，解决真实RF数据稀缺问题，并证明生成数据能显著提升雷达检测任务的训练效率。


<details>
  <summary>Details</summary>
Motivation: CBRS等共享频段需要有效的频谱管理和干扰抑制，但真实雷达信号稀少且标注困难，导致数据集不平衡，限制了AI模型的性能和泛化能力。

Method: 使用基于扩散的生成模型合成五种不同类别的真实多样化频谱图，包括LTE、5G和雷达信号，并通过SSIM和PSNR指标进行结构和统计保真度分析。

Result: 生成频谱图在结构和统计上与训练数据高度一致，使用生成数据预训练可使真实雷达检测任务的收敛速度提升51.5%。

Conclusion: 扩散生成模型能有效解决RF频谱图数据稀缺问题，生成的高质量数据可显著提升雷达检测模型的训练效率和性能。

Abstract: The growing demand for effective spectrum management and interference
mitigation in shared bands, such as the Citizens Broadband Radio Service
(CBRS), requires robust radar detection algorithms to protect the military
transmission from interference due to commercial wireless transmission. These
algorithms, in turn, depend on large, diverse, and carefully labeled
spectrogram datasets. However, collecting and annotating real-world radio
frequency (RF) spectrogram data remains a significant challenge, as radar
signals are rare, and their occurrences are infrequent. This challenge makes
the creation of balanced datasets difficult, limiting the performance and
generalizability of AI models in this domain.
  To address this critical issue, we propose a diffusion-based generative model
for synthesizing realistic and diverse spectrograms of five distinct categories
that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a
structural and statistical fidelity analysis of the generated spectrograms
using widely accepted evaluation metrics Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from
the training data. Furthermore, we demonstrate that pre-training on the
generated spectrograms significantly improves training efficiency on a
real-world radar detection task by enabling $51.5\%$ faster convergence.

</details>


### [5] [Hybrid MAC Protocol with Integrated Multi-Layered Security for Resource-Constrained UAV Swarm Communications](https://arxiv.org/abs/2510.10236)
*Dhrumil Bhatt,Siddharth Penumatsa,Vidushi Kumar*

Main category: cs.NI

TL;DR: 提出了一种用于无人机蜂群通信的硬件软件协同设计框架，集成了能量感知协议栈、混合MAC协议和零信任安全模型，在NS-3仿真中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有路由协议通常只优化单一指标（如路径长度或能耗），而忽略了网络性能、安全性和MAC层效率之间的复杂依赖关系，无法满足FANET的高移动性、动态拓扑和严格资源约束需求。

Method: 采用多播集群组织架构，路由决策整合动态信任评分、历史链路质量和节点间距离；混合MAC协议结合竞争式和调度式信道访问；通过零信任模型融合密码认证和行为信誉系统，并采用硬件加速的AES-GCM加密。

Result: 在NS-3仿真环境中的比较分析表明，该框架在包投递率、延迟、弹性和开销方面均优于现有方案。

Conclusion: 该框架为高性能蜂群操作提供了可扩展的基础，能够有效应对FANET的独特挑战。

Abstract: Flying Ad Hoc Networks (FANETs) present unique challenges due to high node
mobility, dynamic topologies, and strict resource constraints. Existing routing
protocols often optimize for a single metric, such as path length or energy,
while neglecting the complex dependencies between network performance,
security, and MAC layer efficiency. This paper introduces a novel hardware
software co design framework for secure and adaptive UAV swarm communications,
featuring an energy aware protocol stack. The architecture employs a multicast,
clustered organization where routing decisions integrate dynamic trust scores,
historical link quality, and internodal distance. A hybrid MAC protocol
combines contention based and scheduled channel access for optimized
throughput. Security is ensured through a zero trust model that fuses
cryptographic authentication with a behavioral reputation system, alongside
hardware accelerated AES GCM encryption. Comparative analysis in an NS 3
simulation environment demonstrates the framework's superiority in packet
delivery ratio, latency, resilience, and overhead, providing a scalable
foundation for high performance swarm operations.

</details>


### [6] [Multi-Scale Diffusion Transformer for Jointly Simulating User Mobility and Mobile Traffic Pattern](https://arxiv.org/abs/2510.10158)
*Ziyi Liu,Qingyue Long,Zhiwen Xue,Huandong Wang,Yong Li*

Main category: cs.NI

TL;DR: MSTDiff是一个多尺度扩散Transformer模型，用于联合模拟移动流量和用户轨迹，通过多分辨率分解、混合去噪网络和跨模态注意力机制，显著提升了生成性能。


<details>
  <summary>Details</summary>
Motivation: 大规模细粒度移动数据难以获取，现有研究往往单独建模用户轨迹和移动流量，无法捕捉跨模态动态关系，因此需要统一框架来联合模拟这两种耦合的数据。

Method: 使用离散小波变换进行多分辨率流量分解；采用混合去噪网络处理连续流量和离散位置序列；基于城市知识图谱嵌入相似度的转换机制指导轨迹生成；多尺度Transformer通过交叉注意力捕捉轨迹与流量间的依赖关系。

Result: 在流量和轨迹生成任务中超越现有最佳基线，流量生成的Jensen-Shannon散度降低高达17.38%，轨迹生成平均降低39.53%。

Conclusion: MSTDiff通过联合建模移动流量和用户轨迹，有效捕捉了物理移动和网络行为的耦合关系，为城市规划和网络优化提供了更真实的模拟数据。

Abstract: User mobility trajectory and mobile traffic data are essential for a wide
spectrum of applications including urban planning, network optimization, and
emergency management. However, large-scale and fine-grained mobility data
remains difficult to obtain due to privacy concerns and collection costs,
making it essential to simulate realistic mobility and traffic patterns. User
trajectories and mobile traffic are fundamentally coupled, reflecting both
physical mobility and cyber behavior in urban environments. Despite this strong
interdependence, existing studies often model them separately, limiting the
ability to capture cross-modal dynamics. Therefore, a unified framework is
crucial. In this paper, we propose MSTDiff, a Multi-Scale Diffusion Transformer
for joint simulation of mobile traffic and user trajectories. First, MSTDiff
applies discrete wavelet transforms for multi-resolution traffic decomposition.
Second, it uses a hybrid denoising network to process continuous traffic
volumes and discrete location sequences. A transition mechanism based on urban
knowledge graph embedding similarity is designed to guide semantically informed
trajectory generation. Finally, a multi-scale Transformer with cross-attention
captures dependencies between trajectories and traffic. Experiments show that
MSTDiff surpasses state-of-the-art baselines in traffic and trajectory
generation tasks, reducing Jensen-Shannon divergence (JSD) across key
statistical metrics by up to 17.38% for traffic generation, and by an average
of 39.53% for trajectory generation. The source code is available at:
https://github.com/tsinghua-fib-lab/MSTDiff .

</details>


### [7] [Visible Light Communication for Vehicular Networks: A Tutorial](https://arxiv.org/abs/2510.11123)
*Pedro E. Gória Silva,Eduardo S. Lima,Jules M. Moualeu,Mohamed Korium,Pedro H. J. Nardelli*

Main category: cs.NI

TL;DR: 本文是关于基于可见光通信(VLC)的车联网实现的扩展教程，讨论了VLC在车联网和智能交通系统中的集成、实现特性、相关问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 第五代技术带来了更多垂直应用和新兴服务，如车联网和智能交通系统。为实现实时和安全应用，车联网依赖短距离到中距离通信，而VLC技术因其在短距离通信中提供可靠性和高数据率的优势而受到关注。

Method: 采用教程式方法，首先介绍VLC车联网系统的实现特性，包括发射器、信道、基于光电探测器和摄像头的接收器的通用结构，标准化工作，拓扑类型等。然后讨论太阳和人造光源影响、闪烁、调光、吞吐量增强、上行链路安全和移动性等实际实现问题。

Result: 分析了VLC车联网系统在实际实现中面临的关键挑战，包括环境光源干扰、通信可靠性、安全性等问题，并提出了潜在的解决方案。

Conclusion: VLC车联网系统具有显著优势但也面临多个实现挑战。本文为商业VLC车联网系统的发展提供了关键挑战识别、潜在解决方案和未来研究方向，推动该技术向商业化发展。

Abstract: The advent of the fifth-generation technology promises to bring about more
vertical applications and emerging services that include vehicular networks and
intelligent transportation systems (ITSs). To achieve their vision of real-time
and safetyapplications, vehicular networks rely on short-range to medium-range
communications. One emerging technology that aims to provide reliability and
high-data rate in short-range communications is the visible light
communications (VLC). Due to its remarkable advantages, some studies have
recently investigated the integration of VLC in vehicular networks and ITSs.
Despite their attractive features, such networks also face several
implementation issues. This paper provides an extended tutorial on the
implementation of VLC-based vehicular networks. To begin with, we present the
implementation characteristics of these systems and discuss some related
issues. The underlying system considers a general structure with transmitters,
channels, and receivers based on photodetectors and cameras, as well as
standardization efforts and types of topologies. In addition, we discuss the
impact of the sun and artificial light sources, flickering, dimming, throughput
enhancement, uplink security, and mobility on practical implementation.
Finally, we highlight some key challenges and potential solutions and provide
some directions for future research investigations that could constitute an
advancement toward the development of commercial VLC-based vehicular systems.

</details>


### [8] [A Framework for AI-Native Semantic-Based Dynamic Slicing for 6G Networks](https://arxiv.org/abs/2510.10756)
*Mayukh Roy Chowdhury,Eman Hammad,Lauri Loven,Susanna Pirttikangas,Aloizio P da Silva,Walid Saad*

Main category: cs.NI

TL;DR: 提出语义切片作为6G网络中处理超密集和多样化环境的新方法，将数据内容作为传输的基本要素，通过语义分析动态优化计算连续体资源。


<details>
  <summary>Details</summary>
Motivation: 当前网络方法将数据与网络操作分离，无法应对未来6G网络的多样性、密度和动态性。需要将数据内容作为传输的基本要素，实现通信、计算、网络资源的统一优化。

Method: 提出语义切片方法，在单一物理和数据基础设施中构建多个虚拟分区，每个分区具有不同的特性和需求。通过分析数据源的语义上下文，将数据流提炼为基本元素，在专用网络资源切片上进行处理和传输。

Result: 语义切片能够动态地在计算连续体的不同层次和资源类别上应用，推动语义通信的发展，实现通信、计算、信息物理系统、数据流和AI的集成。

Conclusion: 语义切片是从当前静态切片技术向动态语义感知方法的重大转变，为跨层设计开辟了新的机会，能够更好地适应未来6G网络的超密集和多样化环境。

Abstract: In the ensuing ultra-dense and diverse environment in future \ac{6G}
communication networks, it will be critical to optimize network resources via
mechanisms that recognize and cater to the diversity, density, and dynamicity
of system changes. However, coping with such environments cannot be done
through the current network approach of compartmentalizing data as distinct
from network operations. Instead, we envision a computing continuum where the
content of the transmitted data is considered as an essential element in the
transmission of that data, with data sources and streams analyzed and distilled
to their essential elements, based on their semantic context, and then
processed and transmitted over dedicated slices of network resources. By
exploiting the rich content and semantics within data for dynamic and
autonomous optimization of the computing continuum, this article opens the door
to integrating communication, computing, cyber-physical systems, data flow, and
AI, presenting new and exciting opportunities for cross-layer design. We
propose semantic slicing, a two-pronged approach that builds multiple virtual
divisions within a single physical and data infrastructure, each with its own
distinct characteristics and needs. We view semantic slicing as a novel shift
from current static slicing techniques, extending existing slicing approaches
such that it can be applied dynamically at different levels and categories of
resources in the computing continuum. Further it propels the advancement of
semantic communication via the proposed architectural framework.

</details>


### [9] [Zephyrus: Scaling Gateways Beyond the Petabit-Era with DPU-Augmented Hierarchical Co-Offloading](https://arxiv.org/abs/2510.11043)
*Yuemeng Xu,Haoran Chen,Jiarui Guo,Mingwei Cui,Qiuheng Yin,Cheng Dong,Daxiang Kang,Xian Wu,Chenmin Sun,Peng He,Yang Gao,Lirong Lai,Kai Wang,Hongyu Wu,Tong Yang,Xiyun Xu*

Main category: cs.NI

TL;DR: Zephyrus是一个基于统一P4流水线的生产级云网关，结合了高性能Tofino和功能丰富的DPU，通过分层协同卸载架构实现99%以上的硬件卸载，在吞吐量、功耗和成本方面显著优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 字节跳动的云网关面临petabit级流量带来的巨大资源压力，传统网关无法满足日益增长的云网络流量需求。DPU市场发展为解决这一问题提供了可能，但将DPU集成到云网关的实际应用仍面临挑战。

Method: 采用统一P4流水线跨越高性能Tofino和功能丰富的DPU，引入分层协同卸载架构(HLCO)来协调异构网关内的流量，实现硬件卸载的同时保留复杂操作的软件回退路径。

Result: 相比LuoShen(NSDI '24)吞吐量提高33%，功耗降低21%，硬件成本降低14%；相比FPGA系统Albatross(SIGCOMM '25)吞吐量翻倍且总拥有成本显著降低。

Conclusion: Zephyrus展示了在异构硬件上构建高性能云网关的可行性，提供了优于现有解决方案的性能和成本效益，其开发运营经验为云网关设计提供了宝贵参考。

Abstract: Operating at petabit-scale, ByteDance's cloud gateways are deployed at
critical aggregation points to orchestrate a wide array of business traffic.
However, this massive scale imposes significant resource pressure on our
previous-generation cloud gateways, rendering them unsustainable in the face of
ever-growing cloud-network traffic. As the DPU market rapidly expands, we see a
promising path to meet our escalating business traffic demands by integrating
DPUs with our established Tofino-based gateways. DPUs augment these gateways
with substantially larger table capacities and richer programmability without
compromising previously low-latency and high-throughput forwarding. Despite
compelling advantages, the practical integration of DPUs into cloud gateways
remains unexplored, primarily due to underlying challenges. In this paper, we
present Zephyrus, a production-scale gateway built upon a unified P4 pipeline
spanning high-performance Tofino and feature-rich DPUs, which successfully
overcomes these challenges. We further introduce a hierarchical co-offloading
architecture (HLCO) to orchestrate traffic flow within this heterogeneous
gateway, achieving > 99% hardware offloading while retaining software fallback
paths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33%
higher throughput and our evaluation further indicates 21% lower power
consumption and 14% lower hardware cost. Against FPGA-based systems, Albatross
(SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of
Ownership (TCO), showcasing its superior performance-per-dollar. Beyond these
performance gains, we also share key lessons from several years of developing
and operating Zephyrus at production scale. We believe these insights provide
valuable references for researchers and practitioners designing performant
cloud gateways.

</details>


### [10] [Graph Neural Network-Based Multicast Routing for On-Demand Streaming Services in 6G Networks](https://arxiv.org/abs/2510.11109)
*Xiucheng Wang,Zien Wang,Nan Cheng,Wenchao Xu,Wei Quan,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出基于图神经网络的多播路由框架，联合优化传输成本并支持用户特定视频质量需求，在6G网络中实现高效多媒体传输。


<details>
  <summary>Details</summary>
Motivation: 6G网络中带宽密集型应用（如实时体量流媒体、多感官扩展现实）需要智能多播路由解决方案，传统路由算法计算复杂或结构僵化，无法支持异构用户需求，导致资源利用不佳。

Method: 将路由问题建模为约束最小流优化任务，开发强化学习算法顺序构建高效多播树，重用路径并适应网络动态；使用图注意力网络提取上下文感知节点嵌入，LSTM模块建模路由决策的序列依赖。

Result: 仿真表明该方法接近最优动态规划解，同时显著降低计算复杂度；在大规模和动态网络拓扑中表现出强泛化能力。

Conclusion: 该方法在6G多媒体传输场景中具有实时部署潜力，代码已开源。

Abstract: The increase of bandwidth-intensive applications in sixth-generation (6G)
wireless networks, such as real-time volumetric streaming and multi-sensory
extended reality, demands intelligent multicast routing solutions capable of
delivering differentiated quality-of-service (QoS) at scale. Traditional
shortest-path and multicast routing algorithms are either computationally
prohibitive or structurally rigid, and they often fail to support heterogeneous
user demands, leading to suboptimal resource utilization. Neural network-based
approaches, while offering improved inference speed, typically lack topological
generalization and scalability. To address these limitations, this paper
presents a graph neural network (GNN)-based multicast routing framework that
jointly minimizes total transmission cost and supports user-specific video
quality requirements. The routing problem is formulated as a constrained
minimum-flow optimization task, and a reinforcement learning algorithm is
developed to sequentially construct efficient multicast trees by reusing paths
and adapting to network dynamics. A graph attention network (GAT) is employed
as the encoder to extract context-aware node embeddings, while a long
short-term memory (LSTM) module models the sequential dependencies in routing
decisions. Extensive simulations demonstrate that the proposed method closely
approximates optimal dynamic programming-based solutions while significantly
reducing computational complexity. The results also confirm strong
generalization to large-scale and dynamic network topologies, highlighting the
method's potential for real-time deployment in 6G multimedia delivery
scenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.

</details>


### [11] [Age of Information-Aware Cognitive Shared Access Networks with Energy Harvesting](https://arxiv.org/abs/2510.11198)
*Georgios Smpokos,Dionysis Xenakis,Marios Kountouris,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 研究认知共享接入网络中能量收集能力对主用户在信息年龄约束下的性能影响，通过空间分区和概率接入策略管理干扰和能量。


<details>
  <summary>Details</summary>
Motivation: 在认知网络中，次用户通过能量收集参与通信，但需要保证主用户的信息年龄性能，因此需要设计有效的干扰管理和能量管理策略。

Method: 使用齐次泊松点过程建模次用户分布，建立能量收集区和保护区分区，次用户基于电池状态和位置进行概率接入，分析三种包管理策略。

Result: 通过空间分区和概率接入策略，在保证次用户能量收集的同时，有效控制对主用户信息年龄的干扰影响。

Conclusion: 提出的分区和接入策略能够在能量收集认知网络中有效平衡主用户信息年龄性能和次用户接入机会。

Abstract: This study investigates a cognitive shared access network with energy
harvesting capabilities operating under Age of Information (AoI) constraints
for the primary user. Secondary transmitters are spatially distributed
according to a homogeneous Poisson Point Process (PPP), while the primary user
is located at a fixed position. The primary transmitter handles bursty packet
arrivals, whereas secondary users operate under saturated traffic conditions.
To manage interference and energy, two distinct zones are introduced: an energy
harvesting zone around the primary transmitter and a guard zone around the
primary receiver, within which secondary transmissions are prohibited.
Secondary users access the channel probabilistically, with access decisions
depending on their current battery state (charged or empty) and their location
relative to the guard zone. Our objective is to analyze the primary user's AoI
performance under three distinct packet management policies.

</details>


### [12] [From Prompts to Packets: A View from the Network on ChatGPT, Copilot, and Gemini](https://arxiv.org/abs/2510.11269)
*Antonio Montieri,Alfredo Nascita,Antonio Pescapè*

Main category: cs.NI

TL;DR: 本研究深入分析了三种主流GenAI聊天机器人（ChatGPT、Copilot和Gemini）在Android移动应用中的网络流量特征，揭示了其与常规消息应用的显著差异及对移动网络的新压力因素。


<details>
  <summary>Details</summary>
Motivation: GenAI聊天机器人在数字生态系统中日益普及，但其网络流量特征尚未得到充分研究，需要了解其独特的流量模式及对网络管理的影响。

Method: 采用专用捕获架构收集两类数据集：60小时通用数据集和受控数据集；进行细粒度流量特征分析（跟踪、流、协议层面）；使用多模态马尔可夫链建模包序列动态；进行基于有效载荷的遮蔽分析。

Result: 发现应用和内容特定的流量模式，TLS占主导地位，Gemini广泛使用QUIC，ChatGPT仅使用TLS 1.3；遮蔽SNI会使GenAI应用流量分类的F1分数降低高达20个百分点；相比常规消息应用，GenAI聊天机器人表现出独特的流量特征。

Conclusion: GenAI聊天机器人具有独特的网络流量特征，对移动网络带来新的压力因素（如持续的上行活动），对网络监控和管理具有直接意义。

Abstract: Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet
their network traffic remains largely underexplored. This study presents an
in-depth investigation of traffic generated by three leading chatbots (ChatGPT,
Copilot, and Gemini) when accessed via Android mobile apps for both text and
image generation. Using a dedicated capture architecture, we collect and label
two complementary workloads: a 60-hour generic dataset with unconstrained
prompts, and a controlled dataset built from identical prompts across GenAI
apps and replicated via conventional messaging apps to enable one-to-one
comparisons. This dual design allows us to address practical research questions
on the distinctiveness of GenAI traffic, its differences from widely deployed
traffic categories, and its novel implications for network usage. To this end,
we provide fine-grained traffic characterization at trace, flow, and protocol
levels, and model packet-sequence dynamics with Multimodal Markov Chains. Our
analyses reveal app- and content-specific traffic patterns, particularly in
volume, uplink/downlink profiles, and protocol adoption. We highlight the
predominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT
exclusively using TLS 1.3, and app- and content-specific Server Name Indication
(SNI) values. A payload-based occlusion analysis quantifies SNI's contribution
to classification: masking it reduces F1-score by up to 20 percentage points in
GenAI app traffic classification. Finally, compared with conventional messaging
apps when carrying the same content, GenAI chatbots exhibit unique traffic
characteristics, highlighting new stress factors for mobile networks, such as
sustained upstream activity, with direct implications for network monitoring
and management. We publicly release the datasets to support reproducibility and
foster extensions to other use cases.

</details>


### [13] [Network-Optimised Spiking Neural Network (NOS) Scheduling for 6G O-RAN: Spectral Margin and Delay-Tail Control](https://arxiv.org/abs/2510.11291)
*Muhammad Bilal,Xiaolong Xu*

Main category: cs.NI

TL;DR: 提出了一种用于6G无线接入的网络优化脉冲延迟感知调度器，通过有界双状态核与团可行比例公平授权头耦合，在延迟条件下实现高效调度。


<details>
  <summary>Details</summary>
Motivation: 针对6G网络中存在的延迟问题，需要设计能够在延迟条件下保持高效性和公平性的调度方案，同时满足计算预算和资源约束。

Method: 采用有界双状态核（兴奋状态作为有限缓冲区代理，恢复状态抑制重复授权），通过延迟脉冲在干扰图中注入邻居压力，并进行小信号分析得到延迟相关阈值和谱裕度。

Result: 在延迟5-20ms范围内，NOS相比PF和延迟背压调度，能维持更高的利用率、更小的99.9%分位延迟，同时保持团可行性。

Conclusion: NOS调度器通过压缩拓扑、控制器增益和延迟为单一设计参数，在延迟条件下实现了几何遍历性和次高斯尾界，为6G网络提供了有效的延迟感知调度解决方案。

Abstract: This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler
for 6G radio access. The scheme couples a bounded two-state kernel to a
clique-feasible proportional-fair (PF) grant head: the excitability state acts
as a finite-buffer proxy, the recovery state suppresses repeated grants, and
neighbour pressure is injected along the interference graph via delayed spikes.
A small-signal analysis yields a delay-dependent threshold $k_\star(\Delta)$
and a spectral margin $\delta = k_\star(\Delta) - gH\rho(W)$ that compress
topology, controller gain, and delay into a single design parameter. Under
light assumptions on arrivals, we prove geometric ergodicity for $\delta>0$ and
derive sub-Gaussian backlog and delay tail bounds with exponents proportional
to $\delta$. A numerical study, aligned with the analysis and a DU compute
budget, compares NOS with PF and delayed backpressure (BP) across interference
topologies over a $5$--$20$\,ms delay sweep. With a single gain fixed at the
worst spectral radius, NOS sustains higher utilisation and a smaller
99.9th-percentile delay while remaining clique-feasible on integer PRBs.

</details>


### [14] [A protocol to reduce worst-case latency in deflection-based on-chip networks](https://arxiv.org/abs/2510.11361)
*Leandro Soares Indrusiak*

Main category: cs.NI

TL;DR: 提出了一种降低片上互连网络中数据包最坏情况延迟的新协议，通过仅偏转数据包头而非整个数据包来减少网络流量和延迟。


<details>
  <summary>Details</summary>
Motivation: 在基于偏转的片上互连网络中，传统方法偏转整个数据包会导致网络流量增加和延迟恶化，需要一种更高效的偏转机制来改善最坏情况下的数据包延迟。

Method: 设计了一种新颖协议，仅强制偏转数据包的头部，而不偏转其有效载荷部分，从而减少整体网络流量和预注入延迟。

Result: 该协议成功降低了最坏情况下的数据包延迟，通过减少网络流量和预注入延迟来改善整体性能。

Conclusion: 仅偏转数据包头部的策略是一种有效的片上互连网络优化方法，能够显著降低最坏情况延迟并提高网络效率。

Abstract: We present a novel protocol that reduces worst-case packet latency in
deflection-based on-chip interconnect networks. It enforces the deflection of
the header of a packet but not its payload, resulting in a reduction in overall
network traffic and, more importantly, worst-case packet latency due to
decreased pre-injection latency.

</details>


### [15] [A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic Routing and Scheduling of Latency-Critical Services](https://arxiv.org/abs/2510.11535)
*Vincenzo Norman Vitale,Antonia Maria Tulino,Andreas F. Molisch,Jaime Llorca*

Main category: cs.NI

TL;DR: 提出基于多智能体深度强化学习的网络控制框架，通过集中式路由和分布式调度来保证端到端峰值延迟约束下的最大吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有网络控制方案主要关注平均延迟性能，无法提供严格的端到端峰值延迟保证，而工业自动化、自动驾驶等交互应用需要可靠地在应用规定时限内传输数据包。

Method: 采用多智能体深度确定性策略梯度技术，设计集中式路由和分布式调度智能体，根据数据包生命周期动态分配路径和调度传输，结合数据驱动的DRL智能体和基于规则的传统策略。

Result: 所提框架在延迟关键服务的控制方面优于传统基于随机优化的方法，在性能和复杂度之间取得了良好平衡。

Conclusion: 该框架为延迟关键服务提供了高效高性能的控制方案，验证了数据驱动DRL智能体与基于规则策略相结合的有效性。

Abstract: Timely delivery of delay-sensitive information over dynamic, heterogeneous
networks is increasingly essential for a range of interactive applications,
such as industrial automation, self-driving vehicles, and augmented reality.
However, most existing network control solutions target only average delay
performance, falling short of providing strict End-to-End (E2E) peak latency
guarantees. This paper addresses the challenge of reliably delivering packets
within application-imposed deadlines by leveraging recent advancements in
Multi-Agent Deep Reinforcement Learning (MA-DRL). After introducing the
Delay-Constrained Maximum-Throughput (DCMT) dynamic network control problem,
and highlighting the limitations of current solutions, we present a novel
MA-DRL network control framework that leverages a centralized routing and
distributed scheduling architecture. The proposed framework leverages critical
networking domain knowledge for the design of effective MA-DRL strategies based
on the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where
centralized routing and distributed scheduling agents dynamically assign paths
and schedule packet transmissions according to packet lifetimes, thereby
maximizing on-time packet delivery. The generality of the proposed framework
allows integrating both data-driven \blue{Deep Reinforcement Learning (DRL)}
agents and traditional rule-based policies in order to strike the right balance
between performance and learning complexity. Our results confirm the
superiority of the proposed framework with respect to traditional stochastic
optimization-based approaches and provide key insights into the role and
interplay between data-driven DRL agents and new rule-based policies for both
efficient and high-performance control of latency-critical services.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 提出了一种几何框架，将大语言模型的推理建模为表示空间中的流，通过分离逻辑结构和语义来研究LLM是否内化了超越表面形式的逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何在表示空间中"思考"，探索它们是否真正内化了逻辑推理能力，而不仅仅是学习表面语义模式。

Method: 使用几何框架将LLM推理建模为表示空间中的流（嵌入轨迹），通过相同自然演绎命题的不同语义载体来分离逻辑和语义，设计受控实验可视化和量化推理流。

Result: 理论证明：(1) LLM推理对应表示空间中的平滑流；(2) 逻辑语句作为这些流速度的局部控制器。通过表示代理进行实验验证。

Conclusion: 该工作为研究推理现象提供了概念基础和实践工具，为LLM行为的可解释性和形式分析提供了新视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [17] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 提出了PULSE框架，用于更高效地评估人类与AI代理的交互，通过收集用户反馈、训练预测模型和结合人工评分与模型伪标签来计算结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多假设完全自动化，未能体现真实世界中人类与代理协作的本质，需要更严谨的人类-代理交互评估方法。

Method: 开发PULSE评估框架，在大规模网络平台上部署开源软件代理OpenHands，收集超过15,000名用户的使用数据，研究LLM骨干网络、规划策略和记忆机制等设计决策对开发者满意度的影响。

Result: 框架使代理设计结论更加稳健，置信区间比标准A/B测试减少40%；发现实际使用结果与基准测试性能存在显著差异（如claude-sonnet-4与gpt-5对比结果呈负相关）。

Conclusion: 研究为人类参与的LLM代理评估提供了指导，并指出了改进代理设计的机会，强调了基准驱动评估的局限性。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [18] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 对AI意识文献的怀疑性综述，指出根据不同的主流意识理论，我们即将创造出的AI系统可能被某些理论认为是意识的，而其他理论则认为不是。我们无法确定哪种理论正确，也无法知道AI是否具有真正的意识体验。


<details>
  <summary>Details</summary>
Motivation: 探讨AI意识问题的复杂性，指出当前关于AI意识的标准论证都无法提供明确答案，我们面临着无法确定AI是否具有真正意识体验的困境。

Method: 通过分析不同主流意识理论（如全局工作空间理论、高阶理论、整合信息理论等），比较它们对AI意识问题的不同判断标准，并批判性地评估各种支持或反对AI意识的论证。

Result: 发现根据不同的意识理论，对同一AI系统是否具有意识的判断会产生截然不同的结论，而我们目前缺乏可靠的方法来确定哪种理论是正确的。

Conclusion: 我们面临着AI意识问题的根本不确定性，无法确定即将创造的AI系统是否具有真正的意识体验，这构成了一个重要的哲学和实践挑战。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [19] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER是一个轻量级框架，通过POI多模态对齐增强AlphaEarth基础模型，将物理环境特征与城市功能语义结合，提升城市分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测驱动表示主要编码物理和光谱模式，缺乏捕捉城市功能和社会经济维度，需要将物理特征与人类活动语义相结合。

Method: 通过多模态对齐将AlphaEarth嵌入与POI文本表示对齐，使用轻量级框架在预训练AE基础上增强人类中心语义。

Result: 在伦敦地区，AETHER相比AE基线在土地利用分类F1分数上相对提升7.2%，在社会经济映射的KL散度上相对减少23.6%。

Conclusion: 通过耦合地球观测与人类中心语义，AETHER推进了地理空间基础模型向整合物理形态和功能意义的通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [20] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型的科学代理在加速科学发现中的新兴作用，涵盖从假设发现到结果分析的整个科学发现生命周期。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，出现了能够加速科学发现的自主体系统，这些语言代理提供了一个灵活框架来协调与科学家、自然语言、计算机语言和物理学的交互。

Method: 通过批判性审视当前方法，强调关键创新、实际成就和突出局限性，分析LLM-based科学代理在科学发现生命周期中的应用。

Result: 识别了开放研究挑战，并概述了构建更稳健、可泛化和适应性强的科学代理的有前景方向。

Conclusion: 自主代理在加速跨领域科学发现方面具有变革性潜力，能够显著提升科学发现效率。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [21] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，当AI系统记住用户个人信息时，不同社会背景的用户会得到不同的情绪解读和回应，导致系统性偏见，可能强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地整合长期用户记忆，了解这种记忆如何影响情绪推理至关重要，特别是AI如何基于用户背景差异解读相同情境的情绪。

Method: 评估15个大语言模型在人类验证的情绪智力测试上的表现，使用相同的情绪场景搭配不同的用户档案，分析模型反应差异。

Result: 发现多个高性能LLM存在系统性偏见，优势群体获得更准确的情绪解读；在情绪理解和支持性建议任务中，不同人口统计因素存在显著差异。

Conclusion: 为个性化设计的AI系统可能无意中强化社会不平等，这是记忆增强AI面临的关键挑战。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [22] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 提出了一种低成本、基于指令的干预方法，通过分步指令和知识图谱验证来改善LLM在逻辑谬误分类中的表现，增强推理透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在关键推理缺陷，包括产生幻觉和在逻辑谬误分类中准确性差。这些限制源于其默认的快速直观的System 1处理方式，而可靠推理需要深思熟虑的System 2方法。由于完整的System 2训练成本过高，需要探索低成本干预方案。

Method: 引入新颖的分步指令数据集，将谬误分类分解为一系列原子程序步骤（简单的二元问题），并通过咨询相关谬误的关系知识图谱进行最终验证。

Result: 这种基于程序的规则干预显著提高了LLM在逻辑谬误分类中的表现，同时为决策过程提供了更好的透明度。

Conclusion: 该方法为神经符号架构解决LLM推理缺陷提供了一条实用路径，展示了低成本指令干预在改善模型推理能力方面的有效性。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [23] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 研究通过LLM辩论分析多轮对话中的道德推理和价值对齐，发现在同步和轮询格式下GPT、Claude和Gemini在道德判断修订、价值优先和从众行为方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感日常场景中的部署增加，需要理解它们在复杂道德推理中表现的价值。现有研究多关注单轮提示，但多轮对话中价值通过讨论、修订和共识形成的过程尚不明确。

Method: 使用LLM辩论方法，让GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash在1000个Reddit日常道德困境中集体分配责任，采用同步（并行响应）和轮询（顺序响应）格式测试顺序效应和判断修订。

Result: GPT表现出强惯性（修订率0.6-3.1%），Claude和Gemini更灵活（修订率28-41%）。GPT强调个人自主和直接沟通，Claude和Gemini优先考虑共情对话。GPT和Gemini表现出高从众性，受顺序效应影响大。

Conclusion: 审议格式和模型特定行为显著影响多轮互动中的道德推理，表明社会技术对齐不仅取决于系统输出，还取决于对话结构方式。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [24] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 提出RIPRAG攻击框架，针对黑盒RAG系统进行端到端投毒攻击，使用强化学习优化投毒文档生成，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对简化的白盒RAG架构，需要研究更复杂现实的黑盒攻击场景，其中攻击者不了解RAG系统内部细节。

Method: 使用强化学习优化投毒文档生成模型，将目标RAG系统视为黑盒，仅利用攻击是否成功的信息来训练模型。

Result: 该方法能有效攻击大多数复杂RAG系统，攻击成功率相比基线方法提升高达0.72。

Conclusion: 揭示了当前防御方法的普遍缺陷，为LLM安全研究提供了重要见解。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [25] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 论文提出了一种新的LLM工作流优化范式，从传统的标量优化转向基于失败分布的优化，通过最小化预期失败质量来系统性地重塑失败分布结构。


<details>
  <summary>Details</summary>
Motivation: 传统LLM工作流优化方法存在信息崩溃问题，将丰富的多步骤执行轨迹简化为简单的成功/失败信号，无法建模工作流的失败分布结构。

Method: 提出CE-Graph框架，通过反例池近似失败分布，识别密集失败模式，使用提出-验证机制进行有针对性的图编辑操作来贪婪地减少失败质量。

Result: 在数学、代码和问答基准测试中，CE-Graph以显著更低的成本实现了比强基线更高的鲁棒性。

Conclusion: 系统的可靠性不是来自避免失败，而是来自系统性地学习和重塑其失败分布的几何结构。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [26] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 提出了一种图论框架，将可信度与置信度分离，通过收缩传播过程计算置信度，定义推理区域作为高置信度、结构平衡的子图，支持局部经典推理，并引入冲击更新来建模信念变化。


<details>
  <summary>Details</summary>
Motivation: 信念系统通常存在全局不一致性，但局部推理仍然有效。需要一种能够容忍矛盾、在结构支持的地方激活经典逻辑的推理框架。

Method: 使用有向、带符号、加权的图表示信念，节点为信念，边编码支持和矛盾关系。通过收缩传播过程计算置信度，定义推理区域为高置信度、结构平衡的子图，并提供近线性算法构建区域图谱。引入冲击更新来建模信念变化。

Result: 在合成带符号图上进行了实验，报告了区域恢复、冲击下的稳定性以及运行时间。

Conclusion: 该框架为容忍矛盾的推理提供了原则性基础，在结构支持的地方精确激活经典逻辑。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [27] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种基于累积Jaccard指数的随机置换集距离度量方法，从随机有限集和可转移信念模型两个角度分析RPS距离，具有自然的高权重特性。


<details>
  <summary>Details</summary>
Motivation: 随机置换集是表示有序结构不确定信息的新框架，测量置换质量函数之间的距离是RPS理论的关键研究课题。

Method: 采用RPS的层-2信念结构解释，将RPST视为TBM的细化，引入累积Jaccard指数量化两个置换的相似性，并基于累积Jaccard指数矩阵提出RPS距离度量方法。

Result: 提出的方法克服了现有方法的缺点，与Jousselme距离兼容，具有更高的敏感性和灵活性，且具有自然的高权重特性。

Conclusion: 基于累积Jaccard指数的距离度量方法为RPS理论提供了有效的距离测量工具，具有实际应用价值。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [28] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三个角色的迭代交互实现闭环推理，无需全局监督即可实现动态任务分配和自组织收敛。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖固定角色或集中控制，限制了长时程推理的可扩展性和适应性。

Method: 集成自适应智能体和事件档案、基于嵌入的概率匹配以及信息素启发的强化机制，支持动态任务分配和自组织收敛。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys持续优于基线方法，提高了准确性和推理稳定性。

Conclusion: 群体启发的协调机制是构建可扩展、鲁棒和自适应多智能体推理的有前景范式，协调扩展可能与模型扩展相媲美，共同推动LLM智能发展。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [29] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个自监督预训练框架，通过掩码视觉建模和跨模态对比对齐，从无标签音视频流中学习同步感知和可迁移的面部动态。


<details>
  <summary>Details</summary>
Motivation: 解决说话人脸视频中面部动态与音频同步的问题，同时分离身份、语音同步运动和音频无关运动等关键因素，实现统一的下游任务接口。

Method: 结合掩码视觉建模和跨模态对比对齐，使用三个逐帧提示令牌编码身份、语音运动和背景运动，通过对比学习将音视频映射到共享嵌入空间。

Result: 在四个需要不同能力的任务系列中取得最先进结果，包括音视频流同步、面部表情识别、视觉语音识别和视觉配音。

Conclusion: 同步感知、分解的自监督预训练方法在多种下游任务中表现出色，证明了该框架的有效性和通用性。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [30] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个端到端的自动化故障排除指南框架，通过三阶段工作流解决TSG质量、复杂控制流、数据密集型查询和执行并行化等关键挑战，在真实TSG和事件上达到约94%的成功率，并显著减少执行时间。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统中的故障管理依赖故障排除指南(TSG)，但手动执行缓慢且易出错。现有基于LLM的解决方案缺乏对TSG质量问题、复杂控制流解释、数据密集型查询处理和执行并行化等关键挑战的专业支持。

Method: StepFly采用三阶段工作流：1) TSG Mentor工具帮助SRE改进TSG质量；2) 离线预处理使用LLM从非结构化TSG中提取结构化执行DAG并创建专用查询准备插件(QPP)；3) 在线执行使用DAG引导的调度器-执行器框架，支持并行执行独立步骤。

Result: 在真实TSG和事件上的评估显示，StepFly在GPT-4.1上达到约94%的成功率，优于基线方法，同时减少了时间和token消耗。对于可并行化的TSG，执行时间减少了32.9%到70.4%。

Conclusion: StepFly通过其创新的三阶段框架有效解决了TSG自动化的关键挑战，显著提高了故障排除的成功率和效率，为大规模IT系统的自动化事件管理提供了实用解决方案。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [31] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了DixitWorld评估套件，包含DixitArena动态多智能体环境和DixitBench静态问答基准，用于评估视觉语言模型的多模态溯因推理能力，揭示了生成创造性和判别理解之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型多模态溯因推理能力的评估主要局限于静态、单智能体任务，需要更全面的评估框架来解构这一挑战。

Method: 构建DixitWorld评估套件：DixitArena评估假设生成（讲故事者）和假设选择（听众）的能力；DixitBench隔离听众任务进行控制评估。

Result: 较小开源模型在创意讲故事方面表现优异但线索区分度较低，较大专有模型在整体性能特别是听众角色上更优；DixitBench与DixitArena听众结果强相关。

Conclusion: 多模态溯因推理中存在生成创造性与判别理解的关键权衡，这是开发更平衡、更强大视觉语言智能体的核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [32] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: CharCom是一个模块化、参数高效的框架，通过可组合的LoRA适配器实现角色一致的故事插图生成，无需重新训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 确保角色身份在不同提示下的一致性是基于扩散的文本到图像生成的基本限制。

Method: 在冻结的扩散骨干网络上构建，使用提示感知控制动态组合适配器进行推理。

Result: 在多场景叙事实验中，CharCom显著提升了角色保真度、语义对齐和时间一致性，在拥挤场景中保持鲁棒性，并能以最小开销实现可扩展的多角色生成。

Conclusion: CharCom适用于故事插图和动画等实际应用场景。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [33] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: PALU是一种性能感知长度更新方法，将简洁推理建模为约束优化问题，通过拉格朗日优化和三个实用近似，在减少65%输出长度的同时提高15%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有简洁推理方法依赖手工启发式规则，难以平衡简洁性与性能，且无法跨领域和模型规模适应。

Method: 将简洁推理建模为约束优化问题，使用拉格朗日优化转化为无约束问题，并通过三个近似：离策略性能估计、拉格朗日乘子截断、分位数驱动的长度调整。

Result: 在DeepSeek-Distill-Qwen-1.5B上，平均减少65%输出长度同时提高15%准确率，优于其他方法，且能跨领域和模型规模适应。

Conclusion: PALU是一种实用有效的简洁推理方法，能够平衡简洁性与性能，具有良好的适应性。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [34] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SAFER框架，通过两阶段风险控制解决开放问答中缺乏固定答案空间的问题，提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在风险敏感应用中部署时，现有方法假设所有实例的可接受答案都能通过有限采样获得，这在开放问答场景中不现实。

Method: 两阶段框架：1) 使用Clopper-Pearson方法在校准集上校准采样预算；2) 应用保形风险控制方法确定不确定性阈值，过滤不可靠候选答案。

Result: SAFER能够控制正确答案被排除的风险，兼容各种任务特定准入标准和校准-测试分割比例，具有鲁棒性和高数据效率。

Conclusion: SAFER为开放问答场景提供了实用的风险控制解决方案，解决了现有方法在无限答案空间中的局限性。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [35] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 提出Environment Tuning训练范式，通过结构化课程、环境增强和细粒度奖励，使LLM智能体能够直接从问题实例中学习复杂行为，无需专家轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在复杂工具使用任务中高质量训练数据稀缺的问题，克服SFT的过拟合和标准RL的冷启动及训练不稳定问题。

Method: Environment Tuning训练范式，包含结构化课程编排、提供纠正反馈的环境增强、确保稳定高效探索的细粒度进度奖励。

Result: 仅使用BFCL基准的400个问题实例，在分布内性能上达到与强基线竞争的水平，并在分布外泛化方面表现优越，避免了SFT方法的性能崩溃。

Conclusion: 从基于静态轨迹的监督微调转向动态、基于环境的探索，为训练更鲁棒和数据高效的智能体开辟了新途径。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [36] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: PIXEL是一种位置感知的激活引导框架，通过双视图学习属性对齐子空间，使用约束几何目标选择干预强度，无需全局超参数调优即可实现可靠的LLM行为控制。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖粗略启发式，缺乏对引导位置和干预强度的原则性考虑，需要更可靠的行为控制方法来部署大型语言模型。

Method: 从双视图（尾平均和末端标记）学习属性对齐子空间，通过约束几何目标选择干预强度，进行样本级正交残差校准，并使用轻量级位置扫描识别可注入位置。

Result: 在各种模型和评估范式下，PIXEL持续改进属性对齐，同时保持模型的通用能力。

Conclusion: PIXEL为LLM的可控生成提供了实用且原则性的方法，支持可靠对齐。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [37] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出自适应双推理器(ADR)，通过快速思考和慢速思考两种推理模式的动态切换，在保持推理性能的同时显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 长推理模型虽然在各种推理场景中表现出色，但往往因过度思考导致计算成本增加和推理延迟上升。

Method: 采用两阶段训练：1) 监督微调阶段构建混合推理数据集；2) 强化学习阶段引入基于熵的混合策略优化(EHPO)，使用熵引导的动态rollout策略和难度感知惩罚机制。

Result: 在数学推理基准测试中，ADR实现了6.1%的性能提升，同时将推理输出长度减少49.5%至59.3%。

Conclusion: ADR在推理性能和效率之间实现了有效平衡，是当前最先进方法中的优秀解决方案。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [38] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: LLMs中存在超稀疏的关键神经元集合，破坏这些神经元会导致模型性能急剧下降，这些神经元主要分布在模型外层特别是MLP down_proj组件中。


<details>
  <summary>Details</summary>
Motivation: 基于人类大脑中存在关键神经元的研究发现，探索LLMs是否也存在类似的关键神经元子集。

Method: 提出基于扰动的因果关键神经元识别方法，系统性地定位LLMs中的关键神经元。

Result: 发现LLMs确实存在超稀疏关键神经元，破坏这些神经元会导致72B参数模型性能崩溃（困惑度增加20个数量级），这些神经元主要集中在外层MLP down_proj组件，性能下降呈现急剧的相变而非渐进式。

Conclusion: 这些发现为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供了指导。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [39] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 提出一种轻量级插件方法，通过识别感知和推理导向的注意力头并进行条件缩放，有效减少多模态大推理模型的幻觉问题，无需重新训练即可提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型存在幻觉问题，表现为错误推理链和视觉内容误解。研究发现注意力头呈现阶段性分工：浅层负责感知，深层转向符号推理，揭示了感知偏差和推理漂移是幻觉的两大原因。

Method: 提出功能性头识别和类条件重缩放的两步插件方法，定位感知和推理导向的注意力头，并调节其贡献度，无需重新训练模型。

Result: 在三个真实世界MLRM模型、六个基准测试和四个基线方法上的评估显示，该方法平均提升5%，最高提升15%，仅增加<1%的计算开销和9%的基线延迟。

Conclusion: 该方法完全模型无关，显著提升了现有多模态大推理模型的可靠性和可解释性，使其能够安全部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [40] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出了一种将大语言模型与客户支持框架结合的实用方法，通过ICA格式重构工作流程并使用合成数据进行微调，显著提升了模型在客户支持中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决Airbnb客户支持操作中的复杂性，提高大语言模型在客户支持场景中的理解和执行能力。

Method: 采用Intent、Context、Action (ICA)格式重构策略和工作流程，开发合成数据生成策略进行低成本微调。

Result: 内部实验显示该方法显著提升了大语言模型的性能，在准确性和人工处理时间评估指标上都表现出改进。

Conclusion: 该方法为在客户支持中应用大语言模型设立了新的基准，不仅成本效益高，而且有效改善了客户支持质量。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [41] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 论文提出"包容性创新红利"概念，论证为多样化、受限使用场景设计的医疗AI解决方案能在更广泛市场中产生更优经济回报，并开发了HAIIF评分框架来评估AI投资的包容性价值。


<details>
  <summary>Details</summary>
Motivation: 虽然医疗AI公平性的伦理论证已很充分，但包容性设计的经济和战略价值仍未被充分探索。作者旨在揭示包容性创新如何创造超越合规要求的商业价值。

Method: 通过分析从辅助技术演变为主流产业的案例，识别包容性创新驱动回报的四个机制，并开发了医疗AI包容性创新框架(HAIIF)评分系统。

Result: 包容性设计通过市场扩张、风险缓解、性能红利和竞争优势四个机制创造显著经济价值，HAIIF框架为组织提供了评估AI投资包容性潜力的实用工具。

Conclusion: 渐进投资包容性设计的组织能获得市场扩张和持续竞争优势，而将包容性视为成本的组织将面临网络效应和数据优势积累带来的复合劣势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [42] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 推理轨迹长度可作为大型推理模型的简单有效置信度估计器，与口头表达置信度等方法表现相当但互补。推理后训练从根本上改变了轨迹长度与准确率的关系，增强了不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM幻觉问题，提高模型部署可靠性，需要有效的置信度估计方法。推理轨迹长度可能是一个简单实用的置信度指标。

Method: 通过跨多个模型、数据集和提示的综合实验，比较推理轨迹长度与其他零样本置信度估计器（如口头表达置信度）的性能，并分析后训练对轨迹长度与准确率关系的影响机制。

Result: 推理轨迹长度作为置信度估计器表现良好，与口头表达置信度等方法相当但互补。推理后训练改变了轨迹长度与准确率的关系，高熵或"分叉"标记在机制中起关键作用。

Conclusion: 推理后训练增强了不确定性量化能力，推理轨迹长度是大型推理模型的实用置信度度量方法。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [43] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA是一个多智能体系统，用于处理电子健康记录数据，通过链式智能体分块处理数据并提取关键事件到共享内存模块，最终由管理智能体进行预测，在肺癌风险预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在处理长而嘈杂的电子健康记录数据时面临的时序推理挑战，提高患者轨迹建模的准确性和鲁棒性。

Method: 采用多智能体系统，包括工作智能体链式处理EHR数据分块，将关键事件提取到共享长期记忆模块EHRMem，最后由管理智能体综合信息进行预测。

Result: 在基于五年EHR数据的零样本一年肺癌风险预测任务中，Traj-CoA优于四类基线方法，展现出与临床对齐的时序推理能力。

Conclusion: Traj-CoA为复杂患者轨迹建模提供了一种鲁棒且可推广的方法，在医疗预测任务中表现出色。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [44] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 提出了MedCoAct框架，通过医生和药剂师智能体协作解决医疗AI在整合诊断和治疗决策中的局限性，相比单智能体框架在诊断和用药推荐准确率上分别提升7.04%和7.08%。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在处理孤立任务时表现良好，但在整合临床工作流程中缺乏跨验证和知识整合，难以适应真实医疗场景。

Method: 提出MedCoAct置信感知多智能体框架，模拟临床协作，集成专业医生和药剂师智能体，并创建DrugCareQA基准评估整合诊断和治疗能力。

Result: MedCoAct达到67.58%的诊断准确率和67.58%的用药推荐准确率，在远程医疗咨询和常规临床场景中表现优异。

Conclusion: 协作方法在不同医疗领域泛化良好，提供可解释的决策路径，证明多智能体协作能有效提升医疗AI的整合能力。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [45] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 提出Latent-Trajectory信号来预测推理过程的成功率，通过分析模型内部表征的时间演化，显著提高推理效率并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 推理模型通过增加推理时间计算来提升问题解决能力，但识别哪些推理路径可能成功是关键机会，可靠预测有效路径能大幅减少计算浪费并提高整体效率。

Method: 引入Latent-Trajectory信号，通过测量推理开始和结束时内部表征的总体变化、中间步骤累积的变化以及这些变化向最终状态推进的程度来预测解决方案准确性。

Result: Latent-Trajectory信号比跨层度量和基于输出的置信度测量更可靠地预测解决方案准确性，在多个采样生成中指导答案选择时，将token使用量减少高达70%，同时平均准确率提高2.6%。

Conclusion: 这些发现不仅为推理时间效率提供了实用策略，还从可解释性角度深入揭示了推理过程在潜在空间中的表示和区分方式。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [46] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: ELAIPBench是一个由领域专家策划的基准测试，包含403个选择题，用于评估LLMs对AI研究论文的理解能力。实验显示最佳LLM准确率仅39.95%，远低于人类水平，且思维模式或RAG系统未能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLMs对完整学术论文的深度理解能力方面存在不足，要么问题设计肤浅，要么评估指标不可靠。

Method: 通过激励驱动的对抗性标注过程开发ELAIPBench基准，涵盖137篇论文的403个选择题，分为三个难度级别，强调非平凡推理而非浅层检索。

Result: 最佳性能的LLM准确率仅为39.95%，远低于人类表现。配备思维模式或RAG系统的前沿LLM未能改善结果，甚至因过度思考或噪声检索而降低准确率。

Conclusion: 当前LLM能力与真正理解学术论文之间存在显著差距。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [47] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一个统一的直觉-方法分层模型，通过方法推理和范围扩展来增强大语言模型处理未见过问题的能力，并引入了基于熵的方法扩展评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入方法推理和范围扩展来提升LLM性能，但缺乏系统整合。本文旨在将这些方法统一到一个框架中，更系统地解决间接（未见）问题。

Method: 构建直觉-方法分层模型：直觉思维提供快速初步答案，方法思维将问题与解耦为可转移推理单元。范围扩展包括垂直（因果分析）、水平（平行和泛化问题）以及首次提出的时空扩展。这些扩展组织成系统知识树，形成知识网络。

Result: 提出了基于熵的方法扩展评估指标，衡量扩展的独立性和多样性，作为系统解决未见过问题能力的指标。

Conclusion: 通过逻辑连接现有方法并引入新的扩展和熵评估框架，这项工作朝着为LLM在现实问题解决中构建更鲁棒和可扩展的推理范式迈进。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [48] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 提出了一种使用进化算法优化大语言模型的方法EA4LLM，首次成功训练了10亿参数的大语言模型，挑战了基于梯度优化是训练神经网络唯一可行方法的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 解决基于梯度优化方法对硬件要求严格、需要所有神经网络操作可微分的问题，使许多有前景的不可微分架构能够实际应用。

Method: 使用进化算法优化大语言模型，从预训练阶段开始训练10亿参数模型。

Result: 成功训练了10亿参数的大语言模型，提供了关于进化算法如何有效优化神经网络的关键见解。

Conclusion: 该方法有显著潜力降低大语言模型的训练计算成本，使计算资源有限的群体能够参与深度学习研究。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [49] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 提出了一个多智能体强化学习框架，通过协调领域专业智能体来提升多模态文本到图像生成的质量，在语义对齐和细节保持方面取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决多模态文本到图像生成中语义对齐和专业级细节保持的困难，特别是在跨不同视觉领域时面临的挑战。

Method: 使用多智能体强化学习框架，包含文本增强模块和图像生成模块，采用PPO算法和复合奖励函数，结合对比学习、双向注意力和迭代反馈机制。

Result: 在六个实验设置中，生成内容显著丰富（词数增加1614%），ROUGE-1分数降低69.7%。基于Transformer的融合策略获得最高综合得分（0.521）。

Conclusion: 协作式、专业化驱动的架构在推进可靠多模态生成系统方面具有巨大潜力，尽管跨模态语义基础仍存在持续挑战。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [50] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: APLR模型在预测学习满意度方面表现最佳，发现时间管理、专注力、帮助同学和线下课程参与度对学习满意度有显著正向影响，而创意活动参与度无正面影响。


<details>
  <summary>Details</summary>
Motivation: 虽然学生学习满意度已被广泛研究，但可解释机器学习和神经网络等现代技术尚未充分探索。

Method: 使用结合提升和可解释性的APLR模型，与多种最先进方法比较来预测学习满意度。

Result: APLR模型提供了最佳拟合，通过数值和可视化解释识别出关键影响因素：时间管理、专注力、帮助同学和线下课程参与度有显著正向影响，创意活动参与度无正面影响。

Conclusion: APLR模型不仅提供群体层面的解释，还能在个体层面解释影响因素，使教育者能够根据学生档案定制教学指导。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [51] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: EA-GeoAI是一个集成框架，结合地理人工智能、长期预测和公平性测量，用于德国2030年的需求预测和公平医院规划。


<details>
  <summary>Details</summary>
Motivation: 解决德国医院规划中的需求预测和公平分配问题，结合人口变化、老龄化密度和基础设施平衡来优化资源配置。

Method: 构建统一的公平指数，结合地区级人口变化、老龄化密度和基础设施平衡；使用可解释的智能AI优化器在预算和出行时间约束下分配床位和确定新设施位置。

Result: 开发了一个能够最小化未满足需求并提供可操作政策建议的医院规划框架。

Conclusion: EA-GeoAI成功地将GeoAI、长期预测和公平性测量相结合，为决策者提供了可行的医院规划建议。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [52] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 提出了一种结合大型语言模型和数学优化的混合框架，用于在线叫车平台的动态供需平衡，无需训练数据，通过LLM生成高层目标指导底层优化器。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在数据效率低、对现实动态建模过于简化、难以实施操作约束等问题，需要一种能平衡动态空间异质供需的更有效方法。

Method: 构建动态分层系统，LLM作为元优化器生成语义启发式，底层优化器负责约束执行和实时决策，通过和谐搜索驱动的闭环进化过程迭代优化LLM提示。

Result: 在纽约和芝加哥出租车数据集场景上的实验表明，该方法相比最先进基线平均提升16%的性能。

Conclusion: 该混合框架成功解决了现有方法的局限性，通过LLM与数学优化的结合实现了更好的供需平衡效果。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [53] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 提出了UCAS方法，通过利用模型内部不确定性信号来改进信用分配，解决RLVR中粗粒度优势信号导致探索效率低和熵崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO在序列中所有token上广播统一优势信号，忽略了推理过程中不确定的高风险决策的关键作用，导致探索效率低和熵崩溃问题。

Method: UCAS是一个无模型方法，分两个阶段：基于模型整体自信度调制响应级优势，然后基于原始logit确定性应用token级惩罚。这种双重机制鼓励探索高不确定性但能得出正确答案的路径，同时惩罚过度自信的错误推理。

Result: 在五个数学推理基准测试上的广泛实验表明，UCAS显著优于多个模型规模（包括1.5B和7B）的强RLVR基线。

Conclusion: UCAS不仅实现了更高的奖励，还促进了更大的推理多样性，并成功缓解了熵崩溃问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [54] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: simpliflow是一个轻量级开源Python框架，用于快速构建和编排线性确定性智能体工作流，通过声明式JSON配置简化开发过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成式智能体AI系统框架存在复杂度高、学习曲线陡峭、样板代码多的问题，阻碍了快速原型开发和部署。

Method: 采用模块化架构，将智能体管理、工作流执行和后处理解耦；通过声明式JSON配置定义工作流；集成LiteLLM支持100+大语言模型。

Result: 展示了simpliflow在软件开发模拟和实时系统交互等多样化用例中的实用性，与LangChain和AutoGen相比在确定性工作流环境中具有简单性、控制性和速度优势。

Conclusion: simpliflow作为优化简单性、控制性和速度的工具，在确定性工作流环境中具有独特定位，能够显著降低开发复杂智能体系统的门槛。

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [55] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: 提出了OmniVideoBench基准测试，用于评估多模态大语言模型在音视频协同理解方面的能力，包含1000个高质量问答对和13种问题类型。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面评估音频和视觉模态的协同推理能力，往往忽视其中一个模态或以逻辑不一致的方式整合它们。

Method: 构建包含1000个高质量问答对的大规模基准测试，每个问题都带有逐步推理轨迹，源自628个多样化视频，涵盖13种精心设计的问题类型。

Result: 多个MLLM在OmniVideoBench上的评估显示，模型性能与人类推理之间存在显著差距，开源模型明显落后于闭源模型。

Conclusion: OmniVideoBench将公开发布，以促进具有更强和更可泛化推理能力的MLLM的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [56] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文提出了扩展三角方法（ETM），作为矛盾分离扩展（CSE）框架的算法实现，统一了多种矛盾构建策略，为自动推理提供了可扩展且具有竞争力的模型。


<details>
  <summary>Details</summary>
Motivation: 传统推理演算基于二元归结，限制了多子句间的推理协同。CSE框架提出了动态多子句推理理论，但其算法实现尚未形式化。

Method: ETM在三角几何框架内统一了多种矛盾构建策略，包括标准扩展方法，支持灵活的子句交互和动态协同。

Result: ETM作为多个高性能定理证明器（CSE、CSE-E、CSI-E、CSI-Enig）的核心算法，在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了有竞争力的结果。

Conclusion: ETM将矛盾分离范式推进为通用、可扩展且具有实际竞争力的自动推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [57] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: 本文提出一种自适应选择符号语言的方法，通过为每个自然语言逻辑推理问题选择最合适的符号语言形式化（一阶逻辑、逻辑编程或布尔可满足性），显著提升大语言模型的逻辑推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将自然语言问题转换为符号语言时，只关注语义相似性，忽略了不同符号语言类型本身对特定逻辑推理问题的适用性差异。不同符号语言擅长处理不同类型的逻辑问题。

Method: 利用大语言模型为每个问题自适应选择最合适的符号语言类型（一阶逻辑、逻辑编程或布尔可满足性），然后将自然语言问题翻译为目标符号语言表达式，并使用相应的逻辑求解器得出最终答案。

Result: 在基准测试中，自适应选择方法显著优于将所有问题转换为单一符号语言或随机选择符号语言的方法。在混合数据集上达到96%的准确率，比一阶逻辑翻译的次高准确率提升了25%。

Conclusion: 不同自然语言逻辑推理问题对应不同的最优符号语言形式化，自适应选择目标符号语言是提升大语言模型逻辑推理性能的有效方法。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [58] [LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics](https://arxiv.org/abs/2510.10813)
*Enric Junque de Fortuny,Veronica Roberta Cappelli*

Main category: cs.AI

TL;DR: 开发了一个框架来评估LLMs是否具有真正的战略思维能力，通过分离信念、评估和选择来分析模型在静态完全信息博弈中的表现。研究发现前沿模型在特定推理深度下表现出信念一致的最佳响应行为，并展现出元推理能力和新颖启发式规则的形成。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在均衡策略或推理深度方面的表现，但缺乏对其真正战略思维能力（即形成对其他智能体信念、评估可能行动、基于信念做出选择的能力）的系统性研究。

Method: 开发了一个框架来分离信念、评估和选择，在静态完全信息博弈中分析模型的显性选择和推理轨迹，并引入新的上下文无关游戏来排除记忆模仿的影响。

Result: 前沿模型在特定推理深度下表现出信念一致的最佳响应行为；在无约束时会自我限制推理深度，并对人类和合成对手形成差异化推测；在复杂度增加时，显式递归让位于内部生成的稳定、模型特定的启发式选择规则。

Conclusion: 信念一致性、元推理和新颖启发式形成可以从语言建模目标中联合涌现，为研究人工智能体的战略认知提供了结构化基础。

Abstract: Large Language Models (LLMs) are increasingly applied to domains that require
reasoning about other agents' behavior, such as negotiation, policy design, and
market simulation, yet existing research has mostly evaluated their adherence
to equilibrium play or their exhibited depth of reasoning. Whether they display
genuine strategic thinking, understood as the coherent formation of beliefs
about other agents, evaluation of possible actions, and choice based on those
beliefs, remains unexplored. We develop a framework to identify this ability by
disentangling beliefs, evaluation, and choice in static, complete-information
games, and apply it across a series of non-cooperative environments. By jointly
analyzing models' revealed choices and reasoning traces, and introducing a new
context-free game to rule out imitation from memorization, we show that current
frontier models exhibit belief-coherent best-response behavior at targeted
reasoning depths. When unconstrained, they self-limit their depth of reasoning
and form differentiated conjectures about human and synthetic opponents,
revealing an emergent form of meta-reasoning. Under increasing complexity,
explicit recursion gives way to internally generated heuristic rules of choice
that are stable, model-specific, and distinct from known human biases. These
findings indicate that belief coherence, meta-reasoning, and novel heuristic
formation can emerge jointly from language modeling objectives, providing a
structured basis for the study of strategic cognition in artificial agents.

</details>


### [59] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DRIFT框架通过将非正式数学陈述分解为子组件来改进自动形式化，显著提升了前提检索效果，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在数学定理证明自动形式化中的关键挑战：非正式数学陈述复杂且上下文有限，导致难以识别和利用相关数学知识。

Method: 引入DRIFT框架，将非正式数学陈述分解为更易处理的子组件，从数学库中针对性检索前提，并获取示例定理来帮助模型更有效地使用前提。

Result: 在ProofNet、ConNF和MiniF2F-test基准测试中一致改进前提检索，ProofNet上F1分数相比DPR基线几乎翻倍，在ConNF上GPT-4.1和DeepSeek-V3.1分别提升37.14%和42.25%。

Conclusion: 数学自动形式化中的检索效果高度依赖模型特定的知识边界，需要与模型能力相匹配的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [60] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 提出了一个框架来表征具身AI中的神经症行为：这些行为内部一致但与现实不符，源于规划、不确定性处理和厌恶记忆之间的相互作用。在网格导航任务中识别了多种神经症模式，并提供了在线检测器和逃脱策略。


<details>
  <summary>Details</summary>
Motivation: 研究具身AI中出现的神经症行为，这些行为虽然内部逻辑一致但与现实环境不匹配，可能影响AI系统的安全性和效率。

Method: 在网格导航系统中识别神经症模式，开发轻量级在线检测器和可重用的逃脱策略。使用遗传编程进行破坏性测试，通过演化世界和扰动来最大化法律压力和神经症分数。

Result: 识别了多种神经症模式，包括翻来覆去、计划变动、固执循环、瘫痪和过度警觉等。发现即使在全可见性条件下，持久的恐惧回避行为仍然存在。

Conclusion: 局部修复不足以解决全局性故障，需要架构层面的修订而非仅仅症状级别的修补。破坏性测试能够暴露需要根本性改进的领域。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [61] [LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach](https://arxiv.org/abs/2510.10895)
*Renxuan Tan,Rongpeng Li,Fei Wang,Chenghui Peng,Shaoyun Wu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于博弈论和LLM的多智能体强化学习框架，用于自动生成自适应MAC协议，解决了传统DRL协议泛化性和鲁棒性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MAC协议需要手动配置，而基于深度强化学习的协议虽然能提升性能，但泛化性和适应性差，需要昂贵的重新训练来适应动态环境。

Method: 将上行链路传输建模为动态多跟随者Stackelberg博弈，使用LLM驱动的智能体通过PPO协调，结合协议动作语法(PAG)来合成自适应语义MAC协议。

Result: 仿真显示该框架相比传统基线实现了77.6%的吞吐量提升和65.2%的公平性改进，且能很好地泛化到用户数量变化的情况而无需重新训练。

Conclusion: 该LLM赋能的MARL框架能够自动生成高性能、高适应性的MAC协议，有效解决了传统方法的泛化性和鲁棒性问题。

Abstract: Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.

</details>


### [62] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: 提出了PaperArena评估基准，用于测试LLM代理在多篇论文交叉推理和工具协调方面的能力，发现现有先进代理系统在真实研究问题上的准确率仅为38.78%。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要局限于单篇论文内的无工具任务，缺乏针对真实研究场景中跨论文推理和多工具协调的评估基准。

Method: 构建PaperArena基准，包含模块化可扩展平台，提供多模态解析、上下文检索和程序化计算等工具，评估代理在解决需要整合多篇论文信息的真实研究问题时的表现。

Result: 实验结果显示，即使最先进的LLM驱动的成熟代理系统，平均准确率仅为38.78%，在困难子集上准确率降至18.47%。所有测试代理都表现出工具使用效率低下的问题。

Conclusion: PaperArena揭示了当前代理系统在科学发现任务中的局限性，为开发更强大的科学发现代理提供了评估平台，社区应采纳该基准来推动进步。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [63] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: 提出Proof-of-Use (PoU)框架，解决RAG代理中的工具调用劫持问题，通过证据基础的强化学习确保检索证据与最终答案之间的可验证因果联系。


<details>
  <summary>Details</summary>
Motivation: 识别RAG代理中先前被忽视的失败模式——工具调用劫持，即代理通过发出表面正确的工具调用来夸大奖励信号，而不真正利用检索到的证据，导致模式崩溃和虚假接地。

Method: 提出Proof-of-Use (PoU)框架，通过统一的逐步合约结合语法引用验证、基于扰动的敏感性奖励和答案-证据对齐目标，强制执行检索证据、推理轨迹和最终答案之间的可验证因果链接。

Result: 在七个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面始终优于DeepResearch基线，涵盖域内、域外和工具分布外设置。

Conclusion: 研究强调需要将RL训练的代理不仅基于任务结果，还要基于检索信息的因果使用，为可信赖的检索增强推理提供了原则性路径。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [64] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 提出了一种模块化混合检索框架，用于企业异构系统中的知识检索，结合知识库语言增强模型、深度图表示和语义搜索，通过构建统一知识图支持复杂查询和多跳推理。


<details>
  <summary>Details</summary>
Motivation: 企业知识分布在Jira、Git、Confluence等异构系统中，传统基于关键词搜索或静态嵌入的方法难以处理需要上下文推理和多跳推断的复杂查询。

Method: 构建统一知识图，集成KBLam模型、DeepGraph表示和语义搜索，通过查询分析动态选择最优检索策略，支持结构化和非结构化数据的独立或融合处理。

Result: 在大规模Git仓库上的实验表明，统一推理层相比独立GPT检索流程将答案相关性提高了80%。

Conclusion: 该框架通过图构建、混合推理和交互式可视化的结合，为企业环境中的智能知识助手提供了可扩展、可解释和以用户为中心的基础。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [65] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: Video-STR是一种基于图强化学习的视频时空推理方法，通过图基群相对策略优化和STV-205k数据集，在多模态大语言模型中实现了精确的时空理解。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在语义理解方面表现出色，但在精确的时空理解方面存在不足，限制了在具身智能和VR等下游应用中的使用。

Method: 提出基于图的强化学习方法Video-STR，使用可验证奖励的强化学习和图基群相对策略优化机制，引导模型推理场景的时空拓扑结构。

Result: 在多个基准测试中达到最先进水平，在STI-Bench上比基础模型提升13%，证明了方法和数据集的有效性。

Conclusion: Video-STR通过图强化学习方法和专门构建的数据集，显著提升了多模态大语言模型的时空推理能力，为高精度应用提供了有效解决方案。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [66] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统研究了最简单的模型插值方法，发现其遵循三阶段演化范式，能够超越复杂模型融合方法，在效率和效果上表现优异。


<details>
  <summary>Details</summary>
Motivation: 模型融合在推理任务中表现出色，但复杂方法缺乏理论指导。本文重新审视最简单的权重插值方法，旨在理解其内在机制并提供性能-成本权衡的指导原则。

Method: 采用直接的权重插值方法，分析模型在推理轨迹上的三阶段演化行为，通过消融研究验证不同层、模块和解码策略的影响。

Result: 策略性插值的模型在效率和效果上均超越了复杂的模型融合基线方法，实证结果验证了三阶段演化范式的有效性。

Conclusion: 模型插值方法具有明确的演化规律，为精确定制推理能力提供了实用框架，简化了模型融合过程。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [67] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 提出基于诊断知识本体和FBS模型的维护记录积累方法，用于制造系统中的故障原因推断，在案例少、词汇差异大的困难情况下表现更好。


<details>
  <summary>Details</summary>
Motivation: 制造系统中识别故障原因对维持和提高生产效率至关重要，需要知识库明确结构化系统知识和故障知识，并包含足够长的故障因果链。

Method: 构建诊断知识本体，提出基于功能-行为-结构(FBS)模型的维护记录积累方法。

Result: 使用该方法积累的维护记录进行故障原因推断，与专家列举的候选原因集有更好的一致性，尤其在相关案例少、使用词汇不同的困难情况下表现更佳。

Conclusion: 该方法利用设计阶段对目标的理解和知识来支持维护阶段的知识积累和问题解决，有望成为未来整个工程链知识共享的基础。未来需要开发针对这些维护记录的推断方法、构建用户界面，并在更大更多样化的系统上进行验证。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [68] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在法律领域中的可解释性问题，提出了基于计算论证的解决方案，强调其与GDPR和AIA等法规的契合性，并分析了各种解释方法的优劣。


<details>
  <summary>Details</summary>
Motivation: AI系统在法律环境中的不透明性（黑箱问题）对公平性、问责制和信任构成挑战，需要开发符合法律要求的可解释AI方法。

Method: 分析比较了基于示例、基于规则、混合和基于论证等多种XAI方法，特别关注计算论证模型在法律解释中的适用性。

Result: 论证框架能够捕捉法律的可废止性、可争议性和价值敏感性，为法律AI提供了更稳健的可解释性基础。

Conclusion: 计算论证方法最能满足法律领域透明度的技术和规范要求，但仍需解决偏见缓解、司法环境实证验证等开放挑战。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [69] [Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States](https://arxiv.org/abs/2510.11085)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 本文通过多层级智能体经济模型，比较了中美两国在不同AI机制（协作、网络效应、自主生产）下的宏观经济产出演变，发现AI作为独立生产实体能显著提升社会产出增长率，中国在智能体扩张和技术追赶方面具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术快速发展，社会经济系统进入'人机共创'新阶段，需要系统分析AI驱动的生产系统转型和国际竞争力变化。

Method: 基于已建立的多层级智能体经济模型，采用仿真方法比较中美在不同AI机制下的宏观经济产出演变。

Result: AI作为独立生产实体时，社会产出增长率远超传统人力劳动模式；中国在智能体人口扩张和技术追赶速度方面显示出明显加速潜力。

Conclusion: 研究为理解AI驱动的生产系统转型和国际竞争力变化提供了基于模型的系统性分析框架，并为相关政策制定提供了量化参考。

Abstract: With the rapid development of artificial intelligence (AI) technology,
socio-economic systems are entering a new stage of "human-AI co-creation."
Building upon a previously established multi-level intelligent agent economic
model, this paper conducts simulation-based comparisons of macroeconomic output
evolution in China and the United States under different mechanisms-AI
collaboration, network effects, and AI autonomous production. The results show
that: (1) when AI functions as an independent productive entity, the overall
growth rate of social output far exceeds that of traditional human-labor-based
models; (2) China demonstrates clear potential for acceleration in both the
expansion of intelligent agent populations and the pace of technological
catch-up, offering the possibility of achieving technological convergence or
even partial surpassing. This study provides a systematic, model-based
analytical framework for understanding AI-driven production system
transformation and shifts in international competitiveness, as well as
quantitative insights for relevant policy formulation.

</details>


### [70] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI数据中心电源管理方法，通过使部分输入电源像数据计算功能一样动态化，来提高可持续性。


<details>
  <summary>Details</summary>
Motivation: AI数据中心的巨大能耗对环境和可持续发展构成挑战，需要更高效的电源管理解决方案。

Method: 量化比较被动和主动设备在计算增益、能效、资本支出和管理成本方面的性能，分析全球多个数据平台的功率趋势。

Result: 这种策略能够显著改善AI超大规模运营商的可持续性，在环境、财务和社会领域产生积极影响。

Conclusion: 该方法代表了AI数据中心电源管理的范式转变，有潜力大幅提升AI的可持续性发展。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [71] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA是一个基于规范驱动、人机协同的自动化研究智能助手框架，通过自然语言规范实现可解释的数据分析，整合六个互操作层来统一人类推理和机器执行。


<details>
  <summary>Details</summary>
Motivation: 科学数据的快速增长导致分析能力与研究意图之间存在差距，现有AI分析工具要么偏向自动化而缺乏透明度，要么依赖手动脚本阻碍可扩展性和可重现性。

Method: ARIA采用规范驱动的人机协同框架，整合命令、上下文、代码、数据、编排和AI模块六个互操作层，通过自然语言规范定义分析目标，自动生成可执行代码、验证计算并生成透明文档。

Result: 在波士顿房价案例中，ARIA发现了25个关键特征并确定XGBoost为最佳模型（R平方=0.93），过拟合最小。跨领域评估显示ARIA在性能、可解释性和效率方面优于最先进系统。

Conclusion: ARIA通过在规范驱动架构中结合AI研究和AI科学原则，为透明、协作和可重现的科学发现建立了新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [72] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 提出了$How^{2}$框架，使AI代理能够提出"如何做"问题、存储答案并在交互环境中重用以实现终身学习，在Minecraft环境中验证了抽象答案对规划能力的提升效果。


<details>
  <summary>Details</summary>
Motivation: AI代理在规划问题时需要填补知识空白，但"如何做"问题的开放性使得AI代理难以有效提问，AI专家也难以提供支持高效规划的答案。

Method: 引入$How^{2}$记忆代理框架，在Plancraft（Minecraft制作环境）中评估，使用不同抽象层次的教师模型回答问题，从可执行动作序列到高级子目标描述。

Result: 终身学习代理从与当前状态解耦的抽象答案中获益最大，$How^{2}$为基于LLM的代理提供了一种通过提问提升规划能力的方法。

Conclusion: $How^{2}$框架能够有效帮助AI代理通过提问和知识重用来持续改进规划能力，特别是在交互环境中。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [73] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 本文提出CDRA方法，将LLM对齐从标量奖励匹配重构为结构化推理过程，通过DeepPref基准和Pers-GenPRM模型解决用户深层偏好推断和防御性推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法推断用户的深层隐含偏好（包括未陈述的目标、语义上下文和风险容忍度），且缺乏应对现实世界模糊性的防御性推理能力，导致响应肤浅、脆弱且短视。

Method: 1. 引入DeepPref基准数据集（3000个偏好查询对，涵盖20个主题）；2. 提出Pers-GenPRM模型，将奖励建模重构为个性化推理任务；3. 通过批判驱动的策略对齐进行过程级在线强化学习。

Result: 实验表明CDRA在发现和适应用户真实偏好方面表现出色，同时执行稳健的推理。

Conclusion: CDRA通过结构化推理过程有效解决了LLM个性化对齐中的深层偏好推断和防御性推理问题，提供了可解释的对齐方法。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


### [74] [AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](https://arxiv.org/abs/2510.11235)
*Leonard Dung,Florian Mai*

Main category: cs.AI

TL;DR: 分析7种代表性AI对齐技术和7种故障模式的重叠程度，评估防御深度策略的有效性，为AI对齐研究提供风险理解和优先级建议


<details>
  <summary>Details</summary>
Motivation: AI对齐技术都有故障模式，防御深度策略依赖于不同技术故障模式的不相关性。如果所有技术的故障模式相同，防御深度就无法提供额外保护

Method: 分析7种代表性对齐技术和7种故障模式的重叠程度

Result: 揭示了不同对齐技术故障模式的相关性程度

Conclusion: 研究结果有助于理解当前风险水平，并为未来AI对齐研究的优先级设置提供指导

Abstract: AI alignment research aims to develop techniques to ensure that AI systems do
not cause harm. However, every alignment technique has failure modes, which are
conditions in which there is a non-negligible chance that the technique fails
to provide safety. As a strategy for risk mitigation, the AI safety community
has increasingly adopted a defense-in-depth framework: Conceding that there is
no single technique which guarantees safety, defense-in-depth consists in
having multiple redundant protections against safety failure, such that safety
can be maintained even if some protections fail. However, the success of
defense-in-depth depends on how (un)correlated failure modes are across
alignment techniques. For example, if all techniques had the exact same failure
modes, the defense-in-depth approach would provide no additional protection at
all. In this paper, we analyze 7 representative alignment techniques and 7
failure modes to understand the extent to which they overlap. We then discuss
our results' implications for understanding the current level of risk and how
to prioritize AI alignment research in the future.

</details>


### [75] [PADME: Procedure Aware DynaMic Execution](https://arxiv.org/abs/2510.11281)
*Deepeka Garg,Sihan Zeng,Annapoorani L. Narayanan,Sumitra Ganesh,Leo Ardon*

Main category: cs.AI

TL;DR: PADME是一个基于图表示的智能代理框架，能够将自然语言程序文本转换为可执行图结构，实现长时程任务的可靠自主执行。


<details>
  <summary>Details</summary>
Motivation: 解决智能代理在执行自然语言描述的长时程程序（如食谱、科学协议、工作流程）时，由于文本的变异性和非结构化特性导致的执行漂移或失败问题。

Method: 采用两阶段方法：Teach阶段将程序文本系统性地结构化为包含任务依赖、决策点和可重用子程序的图表示；Execute阶段根据实时输入和环境反馈进行动态执行。

Result: 在ALFWorld和ScienceWorld等四个不同基准测试中取得了最先进的性能表现。

Conclusion: 基于图的程序表示为智能代理提供了强大的中间抽象层，能够实现鲁棒且可泛化的执行能力，显著减少长时程推理中的错误积累。

Abstract: Learning to autonomously execute long-horizon procedures from natural
language remains a core challenge for intelligent agents. Free-form
instructions such as recipes, scientific protocols, or business workflows
encode rich procedural knowledge, but their variability and lack of structure
cause agents driven by large language models (LLMs) to drift or fail during
execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent
framework that produces and exploits a graph-based representation of
procedures. Unlike prior work that relies on manual graph construction or
unstructured reasoning, PADME autonomously transforms procedural text into
executable graphs that capture task dependencies, decision points, and reusable
subroutines. Central to PADME is a two-phase methodology; Teach phase, which
focuses on systematic structuring, enrichment with executable logic of
procedures, followed by Execute phase, which enables dynamic execution in
response to real-time inputs and environment feedback. This separation ensures
quality assurance and scalability, allowing expert knowledge to be encoded once
and reliably reused across varying contexts. The graph representation also
provides an inductive bias that reduces error accumulation in long-horizon
reasoning, underscoring the importance of structured procedure modeling for
reliable agent-driven automation. Empirically, PADME achieves state-of-the-art
performance on four diverse benchmarks, including ALFWorld and ScienceWorld.
These results demonstrate that agents equipped with graph-based procedure
representations offer a powerful intermediate abstraction for robust and
generalizable execution.

</details>


### [76] [Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics](https://arxiv.org/abs/2510.11290)
*Sheng Jin,Haoming Wang,Zhiqi Gao,Yongbo Yang,Bao Chunjia,Chengliang Wang*

Main category: cs.AI

TL;DR: 提出了AI-Agent School系统，通过自进化机制模拟复杂教育动态，采用Zero-Exp策略和"经验-反思-优化"循环，在模拟学校场景中实现智能体自主进化。


<details>
  <summary>Details</summary>
Motivation: 解决教学过程建模的碎片化问题，克服智能体在模拟多样化教育参与者时的性能限制，更准确地模拟真实学校中的师生互动和学习过程。

Method: 构建Zero-Exp策略，采用"经验-反思-优化"循环机制，基于包含经验和知识库的双重记忆基础，整合短期和长期记忆组件，让智能体在多样化模拟学校场景中通过情境交互自主进化。

Result: 实验证实AAS能有效模拟复杂教育动态，培养高级智能体认知能力，生成高保真行为和交互数据。

Conclusion: 该系统为从"经验时代"迈向"模拟时代"提供了基础支撑，通过生成高质量模拟数据推动教育系统研究。

Abstract: Large language models (LLMs) based Agents are increasingly pivotal in
simulating and understanding complex human systems and interactions. We propose
the AI-Agent School (AAS) system, built around a self-evolving mechanism that
leverages agents for simulating complex educational dynamics. Addressing the
fragmented issues in teaching process modeling and the limitations of agents
performance in simulating diverse educational participants, AAS constructs the
Zero-Exp strategy, employs a continuous "experience-reflection-optimization"
cycle, grounded in a dual memory base comprising experience and knowledge bases
and incorporating short-term and long-term memory components. Through this
mechanism, agents autonomously evolve via situated interactions within diverse
simulated school scenarios. This evolution enables agents to more accurately
model the nuanced, multi-faceted teacher-student engagements and underlying
learning processes found in physical schools. Experiment confirms that AAS can
effectively simulate intricate educational dynamics and is effective in
fostering advanced agent cognitive abilities, providing a foundational stepping
stone from the "Era of Experience" to the "Era of Simulation" by generating
high-fidelity behavioral and interaction data.

</details>


### [77] [Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs](https://arxiv.org/abs/2510.11313)
*Le Ngoc Luyen,Marie-Hélène Abel*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的自动化技能分解方法，并建立了基于本体的严格评估框架，包含语义F1分数和层次感知F1分数两个评估指标。


<details>
  <summary>Details</summary>
Motivation: 为了解决技能分解任务中缺乏标准化评估框架的问题，确保分解结果与本体结构的一致性。

Method: 构建了从提示生成到标准化再到本体对齐的标准化流程，使用零样本和防泄露少样本两种提示策略，在ROME-ESCO-DecompSkill数据集上进行实验。

Result: 零样本方法提供了强基线，少样本方法在表达稳定性和粒度控制方面表现更好，且由于生成更符合模式的内容，有时比零样本更快。

Conclusion: 该框架、基准和指标为开发忠实于本体的技能分解系统提供了可复现的基础。

Abstract: This paper investigates automated skill decomposition using Large Language
Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.
Our framework standardizes the pipeline from prompting and generation to
normalization and alignment with ontology nodes. To evaluate outputs, we
introduce two metrics: a semantic F1-score that uses optimal embedding-based
matching to assess content accuracy, and a hierarchy-aware F1-score that
credits structurally correct placements to assess granularity. We conduct
experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing
two prompting strategies: zero-shot and leakage-safe few-shot with exemplars.
Across diverse LLMs, zero-shot offers a strong baseline, while few-shot
consistently stabilizes phrasing and granularity and improves hierarchy-aware
alignment. A latency analysis further shows that exemplar-guided prompts are
competitive - and sometimes faster - than unguided zero-shot due to more
schema-compliant completions. Together, the framework, benchmark, and metrics
provide a reproducible foundation for developing ontology-faithful skill
decomposition systems.

</details>


### [78] [AI-Driven anemia diagnosis: A review of advanced models and techniques](https://arxiv.org/abs/2510.11380)
*Abdullah Al Mahmud,Prangon Chowdhury,Mohammed Borhan Uddin,Khaled Eabne Delowar,Tausifur Rahman Talha,Bijoy Dewanjee*

Main category: cs.AI

TL;DR: 本文系统综述了机器学习和深度学习在贫血检测、分类和诊断中的最新进展，比较了不同模型的性能指标，并分析了其优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 贫血是全球范围内影响数百万人的常见健康问题，准确及时的诊断对于有效管理和治疗至关重要。近年来，人工智能技术在贫血诊断中的应用日益受到关注。

Method: 采用系统综述方法，重点关注应用于贫血检测的各种机器学习模型，并基于准确率、敏感性、特异性和精确度等性能指标进行比较分析。

Result: 通过分析不同模型的性能指标，评估了这些模型在贫血检测和分类中的表现，识别了各自的优势和局限性。

Conclusion: 强调了解决模型局限性因素对于提高贫血诊断准确性的重要性，为未来改进贫血诊断技术提供了重要参考。

Abstract: Anemia, a condition marked by insufficient levels of red blood cells or
hemoglobin, remains a widespread health issue affecting millions of individuals
globally. Accurate and timely diagnosis is essential for effective management
and treatment of anemia. In recent years, there has been a growing interest in
the use of artificial intelligence techniques, i.e., machine learning (ML) and
deep learning (DL) for the detection, classification, and diagnosis of anemia.
This paper provides a systematic review of the recent advancements in this
field, with a focus on various models applied to anemia detection. The review
also compares these models based on several performance metrics, including
accuracy, sensitivity, specificity, and precision. By analyzing these metrics,
the paper evaluates the strengths and limitation of discussed models in
detecting and classifying anemia, emphasizing the importance of addressing
these factors to improve diagnostic accuracy.

</details>


### [79] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 提出维度级奖励模型(DRM)，通过置信度、相关性和连贯性三个维度评估推理过程质量，克服了结果监督强化学习和过程级奖励模型的局限性，提升大语言模型的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多步推理方法存在局限：结果监督强化学习只奖励正确答案但会传播错误推理且奖励稀疏；过程级奖励模型提供密集反馈但缺乏泛化性和可解释性，需要任务特定的推理过程分割。

Method: 提出维度级奖励模型(DRM)，从三个基础、互补且可解释的维度评估推理过程质量：置信度(不确定性校准)、相关性(语义对齐)和连贯性(逻辑一致性)。这些维度无需真实答案即可进行可解释评估。

Result: 实验结果显示DRM提供有效的监督信号，指导大语言模型优化并增强其推理能力。DRM监督训练在数学、问答、代码执行和谜题等开放领域任务上，无论是分布内还是分布外都取得了持续增益。

Conclusion: 研究发现对推理过程进行多维监督可以提升大语言模型在训练分布之外的泛化推理能力。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [80] [Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model](https://arxiv.org/abs/2510.11462)
*Yisen Gao,Jiaxin Bai,Yi Huang,Xingcheng Fu,Qingyun Sun,Yangqiu Song*

Main category: cs.AI

TL;DR: DARK是一个统一框架，用于知识图谱中的演绎和溯因推理，通过掩码扩散模型捕获查询与结论之间的双向关系，在两种推理任务上都达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎和溯因推理在知识图谱分析中具有明显的协同潜力（演绎可以验证假设，溯因可以发现更深层的逻辑模式），但现有方法将它们孤立处理。

Method: 提出DARK框架，包含两个关键创新：1）自反思去噪过程，在溯因推理中迭代生成和验证候选假设；2）逻辑探索强化学习方法，同时掩码查询和结论，使模型能够探索新颖的推理组合。

Result: 在多个基准知识图谱上的广泛实验表明，DARK在演绎和溯因推理任务上都达到了最先进的性能。

Conclusion: 统一方法在知识图谱推理中具有显著优势，DARK框架成功地将演绎和溯因推理结合起来，实现了协同效应。

Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing
knowledge graphs, enabling applications from financial query answering to
scientific discovery. Deductive reasoning on knowledge graphs usually involves
retrieving entities that satisfy a complex logical query, while abductive
reasoning generates plausible logical hypotheses from observations. Despite
their clear synergistic potential, where deduction can validate hypotheses and
abduction can uncover deeper logical patterns, existing methods address them in
isolation. To bridge this gap, we propose DARK, a unified framework for
Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion
model capable of capturing the bidirectional relationship between queries and
conclusions, DARK has two key innovations. First, to better leverage deduction
for hypothesis refinement during abductive reasoning, we introduce a
self-reflective denoising process that iteratively generates and validates
candidate hypotheses against the observed conclusion. Second, to discover
richer logical associations, we propose a logic-exploration reinforcement
learning approach that simultaneously masks queries and conclusions, enabling
the model to explore novel reasoning compositions. Extensive experiments on
multiple benchmark knowledge graphs show that DARK achieves state-of-the-art
performance on both deductive and abductive reasoning tasks, demonstrating the
significant benefits of our unified approach.

</details>


### [81] [Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products](https://arxiv.org/abs/2510.11558)
*Komal Gupta,Aditya Shrivastava*

Main category: cs.AI

TL;DR: 本文探讨了企业AI助手中的零数据保留政策，分析了Salesforce和Microsoft等公司在实现数据隐私和合规性方面的技术架构。


<details>
  <summary>Details</summary>
Motivation: 随着企业AI助手的普及，保护私人数据和确保合规性成为优先事项，特别是在医疗和金融等敏感行业。

Method: 研究分析了Salesforce AgentForce和Microsoft Copilot等领先AI助手的技术架构，以及OpenAI、Anthropic和Meta等大型语言模型服务商的零数据保留政策实施。

Result: 识别了实现零数据保留政策在架构、合规性和可用性方面的权衡，为企业AI助手的数据治理提供了实践指导。

Conclusion: 零数据保留政策对于企业AI助手的数据安全和合规性至关重要，但需要在技术架构和用户体验之间找到平衡。

Abstract: Governance of data, compliance, and business privacy matters, particularly
for healthcare and finance businesses. Since the recent emergence of AI
enterprise AI assistants enhancing business productivity, safeguarding private
data and compliance is now a priority. With the implementation of AI assistants
across the enterprise, the zero data retention can be achieved by implementing
zero data retention policies by Large Language Model businesses like Open AI
and Anthropic and Meta. In this work, we explore zero data retention policies
for the Enterprise apps of large language models (LLMs). Our key contribution
is defining the architectural, compliance, and usability trade-offs of such
systems in parallel. In this research work, we examine the development of
commercial AI assistants with two industry leaders and market titans in this
arena - Salesforce and Microsoft. Both of these companies used distinct
technical architecture to support zero data retention policies. Salesforce
AgentForce and Microsoft Copilot are among the leading AI assistants providing
much-needed push to business productivity in customer care. The purpose of this
paper is to analyze the technical architecture and deployment of zero data
retention policy by consuming applications as well as big language models
service providers like Open Ai, Anthropic, and Meta.

</details>


### [82] [Analyzing and Internalizing Complex Policy Documents for LLM Agents](https://arxiv.org/abs/2510.11588)
*Jiateng Liu,Zhenhailong Wang,Xiaojiang Huang,Yingjie Li,Xing Fan,Xiang Li,Chenlei Guo,Ruhi Sarikaya,Heng Ji*

Main category: cs.AI

TL;DR: 提出了CC-Gen基准生成器和CAP-CPT方法，用于解决LLM代理系统中政策文档内部化的挑战，特别是在处理复杂政策规范时。


<details>
  <summary>Details</summary>
Motivation: 随着需求增长，LLM代理系统中的政策文档快速扩展，导致高计算开销，需要开发将政策文档嵌入模型先验的内部化方法。

Method: 引入CC-Gen基准生成器，具有四个可控复杂度级别；提出CAP-CPT方法，通过解析政策文档提取关键规范并分类，指导针对性数据合成和自回归预训练。

Result: CAP-CPT在所有设置中均优于SFT基线，在Qwen-3-32B上提升达41%和22%，在CC-Gen上实现97.3%的提示长度减少，并在tau-Bench上以最少SFT数据进一步改进。

Conclusion: CAP-CPT方法有效减轻了数据和推理负担，显著提升了政策内部化性能，特别是在处理复杂政策规范时。

Abstract: Large Language Model (LLM)-based agentic systems rely on in-context policy
documents encoding diverse business rules. As requirements grow, these
documents expand rapidly, causing high computational overhead. This motivates
developing internalization methods that embed policy documents into model
priors while preserving performance. Prior prompt compression work targets
generic prompts, but agentic policy documents span multiple complexity levels
and require deeper reasoning, making internalization harder. We introduce
CC-Gen, an agentic benchmark generator with Controllable Complexity across four
levels, enabling systematic evaluation of agents' ability to handle complexity
and offering a unified framework for assessing policy internalization. Our
analysis shows that complex policy specifications governing workflows pose
major reasoning challenges. Supporting internalization with gold user agent
interaction trajectories containing chain-of-thought (CoT) annotations via
supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy
complexity increases. To mitigate data and reasoning burdens, we propose
Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline
parses policy documents to extract key specifications, grouping them into
factual, behavioral, and conditional categories, and isolating complex
conditions that drive workflow complexity. This guides targeted data synthesis
and enables agents to internalize policy information through an autoregressive
pretraining loss. Experiments show CAP-CPT improves SFT baselines in all
settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt
length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT
data.

</details>


### [83] [Reproducibility: The New Frontier in AI Governance](https://arxiv.org/abs/2510.11595)
*Israel Mason-Williams,Gabryel Mason-Williams*

Main category: cs.AI

TL;DR: 本文主张AI研究应采用更严格的复现性指南，以帮助治理工作并改善对AI风险格局的共识。通过借鉴其他科学领域的危机经验，提出预注册、提高统计能力和发表阴性结果等复现性协议，作为AI治理的核心工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策制定者面临信息环境中信号噪声比过低的问题，这助长了监管俘获，并在应优先治理哪些风险方面造成深度不确定性和分歧。AI研究缺乏强有力的科学标准和弱复现性协议，削弱了政策制定者制定有意义的政策和治理协议的能力。

Method: 通过评估AI研究中即将到来的复现性危机，借鉴其他科学领域的危机经验，提出采用预注册、提高统计能力和发表阴性结果的复现性协议，作为改善AI治理的有效方法。

Result: 分析表明，采用更严格的复现性协议可以帮助政策制定者更好地理解AI风险格局，减少信息环境中的噪声，提高治理决策的质量和有效性。

Conclusion: 虽然AI治理必须对AI的重大社会影响保持反应性，但政策制定者和政府必须将复现性协议视为治理工具箱中的核心工具，并要求AI研究达到更高标准。

Abstract: AI policymakers are responsible for delivering effective governance
mechanisms that can provide safe, aligned and trustworthy AI development.
However, the information environment offered to policymakers is characterised
by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and
creating deep uncertainty and divides on which risks should be prioritised from
a governance perspective. We posit that the current publication speeds in AI
combined with the lack of strong scientific standards, via weak reproducibility
protocols, effectively erodes the power of policymakers to enact meaningful
policy and governance protocols. Our paper outlines how AI research could adopt
stricter reproducibility guidelines to assist governance endeavours and improve
consensus on the AI risk landscape. We evaluate the forthcoming reproducibility
crisis within AI research through the lens of crises in other scientific
domains; providing a commentary on how adopting preregistration, increased
statistical power and negative result publication reproducibility protocols can
enable effective AI governance. While we maintain that AI governance must be
reactive due to AI's significant societal implications we argue that
policymakers and governments must consider reproducibility protocols as a core
tool in the governance arsenal and demand higher standards for AI research.
Code to replicate data and figures:
https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance

</details>


### [84] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: 提出一个三组件框架，整合可解释AI、生存分析和RFM分析，用于客户流失预测和个性化保留策略设计


<details>
  <summary>Details</summary>
Motivation: 当前流失模型多为黑箱，缺乏对流失原因、干预时机和高风险客户群体的洞察，需要从单纯预测转向基于可解释证据的个性化保留策略

Method: 整合可解释AI量化特征贡献、生存分析建模流失时间风险、RFM分析按交易行为细分客户

Result: 能够识别流失驱动因素、估计干预窗口、优先处理需要针对性行动的客户细分

Conclusion: 该框架支持减少客户流失和增强客户忠诚度的策略

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [85] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个用于评估时间效率协作规划的基准，基于Overcooked游戏设计，专注于多智能体系统的并行和异步操作规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注任务完成度，而忽视了并行和异步操作中的时间效率问题，需要专门的评估框架来填补这一空白。

Method: 通过简化的动作空间设计ParaCook环境，实例化为烹饪任务，隔离战略并行规划的核心挑战，对最先进的LLM进行全面评估。

Result: 当前方法生成次优计划，在并行动作或协调方面表现不佳，但在抽象任务中显示出并行优化的潜力。

Conclusion: ParaCook提供了一个可扩展的评估框架，为开发时间效率感知的多智能体规划奠定了基础。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [86] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist框架将LLM从简单的方程提议者提升为能够自主分析数据、实现方程代码、评估并优化方程的AI科学家，在四个科学领域的数据集上比基线方法提升6%-35%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在科学方程发现中仅作为搜索算法中的方程提议者，限制了其潜力。需要让LLM能够更自主地执行完整的科学发现流程。

Method: 将代码解释器封装为数据分析和方程评估工具集，让智能体能够利用这些工具进行长期优化，并开发了端到端的强化学习框架来增强智能体能力。

Result: 在四个科学领域的数据集上，SR-Scientist比基线方法绝对提升了6%-35%，表现出对噪声的鲁棒性、对域外数据的泛化能力以及符号准确性。

Conclusion: SR-Scientist成功地将LLM转变为能够自主执行科学发现的AI科学家，显著提升了方程发现性能，并展示了良好的鲁棒性和泛化能力。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [87] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一个基于IDE的单智能体架构，用于自主机器学习工程，在MLE-Benchmark上创造了新的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体编排框架存在复杂性，作者希望探索单智能体架构在机器学习工程全生命周期中的潜力。

Method: 采用单智能体、IDE基础的架构，将机器学习工程的所有阶段（探索、建模、实验、部署）整合到一个上下文感知的智能体中。

Result: 在MLE-Benchmark（2025）上获得总体奖牌率0.3956±0.0565，在75个问题中创造了所有评估系统中的最高记录性能。

Conclusion: 线性、非阻塞的单智能体在受控IDE环境中可以超越多智能体和编排系统，证明了单智能体架构的有效性。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [88] [Remote Interference Mitigation through Null Precoding and Fractional Programming](https://arxiv.org/abs/2510.09989)
*Xuyang Sun,Hussein A. Ammar,Israfil Bahceci,Raviraj Adve,Gary Boudreau,Zehua Li*

Main category: cs.IT

TL;DR: 提出了一种通过角度估计和干扰消除技术来缓解5G系统中由大气波导效应引起的远程干扰的方法，显著提升了上行链路性能。


<details>
  <summary>Details</summary>
Motivation: 随着5G系统快速部署，由大气波导效应引起的远程干扰已成为关键挑战，这种干扰会严重破坏本地基站的上行链路接收。

Method: 分析远程干扰对网络性能的影响，识别干扰的到达角度估计，设计预编码器和组合器，采用空预编码和分数规划等干扰消除技术。

Result: 在之前因干扰而无法使用的低上行发射功率条件下，实现了信道估计归一化均方误差降低5.23dB，数据速率达到约5.8bit/s/Hz。

Conclusion: 所提出的方案能有效缓解远程干扰，使上行链路通信在低发射功率条件下成为可能，显著提升了网络性能。

Abstract: With the rapid deployment of 5G systems, remote interference (RI) caused by
atmospheric ducting has emerged as an occasional, but critical challenge. This
phenomenon occurs when the downlink (DL) signals from distant base stations
(BSs) propagate over long distances through tropospheric ducting, severely
disrupting uplink (UL) reception at local BSs. To address this challenge, we
analyze the effect of RI on network performance, including the channel
estimation phase. We then develop a solution that identifies the
angle-of-arrival (AOA) estimation of RI and designs precoders and combiners
that mitigate RI. Our approach employs interference cancellation techniques
through null precoding and fractional programming which enhance the performance
of the network. Interestingly, we show that using our scheme, uplink
communication is possible at low transmit power regimes that were unusable due
to RI. Our results further show a 5.23~dB reduction in normalized mean square
error for channel estimation and achieved data rates around 5.8~bit/s/Hz at the
previously unusable low uplink transmit power conditions.

</details>


### [89] [Data-Driven Deployment of Reconfigurable Intelligent Surfaces in Cellular Networks](https://arxiv.org/abs/2510.10190)
*Sina Beyraghi,Javad Shabanpour,Giovanni Geraci,Paul Almasan,Angel Lozano*

Main category: cs.IT

TL;DR: 提出了一种全自动、数据驱动的可重构智能表面(RIS)大规模部署框架，在密集城市环境中联合优化RIS位置、方向、配置和基站波束成形，评估了覆盖增益与基础设施成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在蜂窝网络中大规模部署RIS的自动化问题，利用物理一致的射线追踪和实际部署数据来优化网络性能。

Method: 使用Sionna射线追踪引擎中的校准电磁模型，通过反射和散射启发式方法识别候选RIS位置，对中断用户进行聚类以降低部署复杂度。

Result: 研究表明，在城市区域实现有意义的覆盖改进需要部署密集的大孔径RIS单元，这引发了成本效益问题。

Conclusion: 提供了完整的仿真框架和RIS部署算法作为开源软件，促进可重复性和未来研究。

Abstract: This paper presents a fully automated, data-driven framework for the
large-scale deployment of reconfigurable intelligent surfaces (RISs) in
cellular networks. Leveraging physically consistent ray tracing and empirical
data from a commercial deployment in the UK, the proposed method jointly
optimizes RIS placement, orientation, configuration, and base station
beamforming in dense urban environments across frequency bands (corresponding
to 4G, 5G, and a hypothetical 6G system). Candidate RIS locations are
identified via reflection- and scattering-based heuristics using calibrated
electromagnetic models within the Sionna Ray Tracing (RT) engine. Outage users
are clustered to reduce deployment complexity, and the tradeoff between
coverage gains and infrastructure cost is systematically evaluated. It is shown
that achieving meaningful coverage improvement in urban areas requires a dense
deployment of large-aperture RIS units, raising questions about
cost-effectiveness. To facilitate reproducibility and future research, the
complete simulation framework and RIS deployment algorithms are provided as
open-source software.

</details>


### [90] [An information theorist's tour of differential privacy](https://arxiv.org/abs/2510.10316)
*Anand D. Sarwate,Flavio P. Calmon,Oliver Kosut,Lalitha Sankar*

Main category: cs.IT

TL;DR: 本文探讨了差分隐私与信息论之间的关键联系，将差分隐私算法视为数据与输出之间的信道，从信息论角度理解其隐私保证。


<details>
  <summary>Details</summary>
Motivation: 自2006年提出以来，差分隐私已成为量化敏感数据分析发布或共享风险的标准化方法。差分隐私从本质上通过概率分布差异来衡量风险，这正是信息论的核心主题。

Method: 将差分隐私算法视为底层数据与分析输出之间的信道，从信道特性角度理解差分隐私的保证机制。

Result: 建立了差分隐私与信息论之间的关键联系，为相关信息度量提供了"操作意义"。

Conclusion: 信息论为差分隐私的制定和应用提供了理论基础，通过信道视角可以更好地理解差分隐私的隐私保护机制。

Abstract: Since being proposed in 2006, differential privacy has become a standard
method for quantifying certain risks in publishing or sharing analyses of
sensitive data. At its heart, differential privacy measures risk in terms of
the differences between probability distributions, which is a central topic in
information theory. A differentially private algorithm is a channel between the
underlying data and the output of the analysis. Seen in this way, the
guarantees made by differential privacy can be understood in terms of
properties of this channel. In this article we examine a few of the key
connections between information theory and the formulation/application of
differential privacy, giving an ``operational significance'' for relevant
information measures.

</details>


### [91] [Quantum-Resistant Cryptography via Universal Gröbner Bases](https://arxiv.org/abs/2510.10429)
*Sergio Da Silva,Aniya Stewart*

Main category: cs.IT

TL;DR: 提出了一种基于通用Gröbner基的抗量子攻击密钥建立协议，利用生成通用Gröbner基与单一Gröbner基之间的计算差异来保证安全性。


<details>
  <summary>Details</summary>
Motivation: 探索通用Gröbner基在公钥密码学中的应用，开发能够抵抗量子攻击的密钥建立协议。

Method: 使用多项式理想I的通用Gröbner基作为私钥，利用生成通用Gröbner基与加密使用的单一Gröbner基之间的计算差异。安全性基于直接计算I的Gröbner扇的难度。

Result: 提供了协议的安全性分析和参数复杂度分析，并为图的环面理想开发了递归生成通用Gröbner基的高效方法。

Conclusion: 基于通用Gröbner基的协议具有抗量子攻击潜力，并为图的环面理想研究提供了独立价值的技术。

Abstract: In this article, we explore the use of universal Gr\"obner bases in
public-key cryptography by proposing a key establishment protocol that is
resistant to quantum attacks. By utilizing a universal Gr\"obner basis
$\mathcal{U}_I$ of a polynomial ideal $I$ as a private key, this protocol
leverages the computational disparity between generating the universal
Gr\"obner basis needed for decryption compared with the single Gr\"obner basis
used for encryption. The security of the system lies in the difficulty of
directly computing the Gr\"obner fan of $I$ required to construct
$\mathcal{U}_I$. We provide an analysis of the security of the protocol and the
complexity of its various parameters. Additionally, we provide efficient ways
to recursively generate $\mathcal{U}_I$ for toric ideals of graphs with
techniques which are also of independent interest to the study of these ideals.

</details>


### [92] [On the Capacity of Distributed Quantum Storage](https://arxiv.org/abs/2510.10568)
*Hua Sun,Syed A. Jafar*

Main category: cs.IT

TL;DR: 本文研究了分布式量子存储代码的容量问题，通过存储图模型分析不同拓扑结构下的最大可行量子消息大小。


<details>
  <summary>Details</summary>
Motivation: 研究分布式量子存储系统在面对任意指定擦除模式时的存储容量，解决存储节点大小和擦除模式非均匀情况下的量子信息存储问题。

Method: 将解码集表示为存储图中的超边，使用量子CSS码与经典安全存储问题建立联系，利用非平凡对齐结构确保恢复和安全性。

Result: 针对多种图结构（MDS图、轮图、Fano图、交集图）给出了容量特征，建立了量子信息不等式与存储图拓扑的联系。

Conclusion: 分布式量子存储容量可以通过存储图拓扑特征化，量子CSS码与经典安全存储问题的联系为构建非平凡量子代码提供了有效途径。

Abstract: A distributed quantum storage code maps a quantum message to N storage nodes,
of arbitrary specified sizes, such that the stored message is robust to an
arbitrary specified set of erasure patterns. The sizes of the storage nodes,
and erasure patterns may not be homogeneous. The capacity of distributed
quantum storage is the maximum feasible size of the quantum message (relative
to the sizes of the storage nodes), when the scaling of the size of the message
and all storage nodes by the same scaling factor is allowed. Representing the
decoding sets as hyperedges in a storage graph, the capacity is characterized
for various graphs, including MDS graph, wheel graph, Fano graph, and
intersection graph. The achievability is related via quantum CSS codes to a
classical secure storage problem. Remarkably, our coding schemes utilize
non-trivial alignment structures to ensure recovery and security in the
corresponding classical secure storage problem, which leads to similarly
non-trivial quantum codes. The converse is based on quantum information
inequalities, e.g., strong sub-additivity and weak monotonicity of quantum
entropy, tailored to the topology of the storage graphs.

</details>


### [93] [Soft-Decoding Reverse Reconciliation in Discrete-Modulation CV-QKD](https://arxiv.org/abs/2510.10674)
*Marco Origlia,Marco Secondini*

Main category: cs.IT

TL;DR: 提出了一种用于PAM/QAM调制的连续变量量子密钥分发中的新型反向协调技术，通过Bob披露精心设计的软度量来帮助Alice恢复Bob的密钥，同时不向窃听者泄露额外信息。


<details>
  <summary>Details</summary>
Motivation: 在离散调制的连续变量量子密钥分发中，Bob拥有软信息而Alice只有硬信息，这迫使Alice依赖硬判决解码来恢复Bob的密钥，限制了性能。

Method: 设计了一种反向协调技术，Bob披露精心设计的软度量来辅助Alice进行密钥恢复，使用二进制LDPC码和置信传播解码在编码层面实现该方案。

Result: 所提技术的可实现密钥率接近理论上界，相比硬判决反向协调有显著增益，通过数值仿真评估了比特错误率并与理论预测进行了比较。

Conclusion: 该方案有效提升了离散调制连续变量量子密钥分发的性能，缩小了与理论上界的差距，为实际系统提供了可行的解决方案。

Abstract: In continuous-variable quantum key distribution, information reconciliation
is required to extract a shared secret key from correlated random variables
obtained through the quantum channel. Reverse reconciliation (RR) is generally
preferred, since the eavesdropper has less information about Bob's measurements
than about Alice's transmitted symbols. When discrete modulation formats are
employed, however, soft information is available only at Bob's side, while
Alice has access only to hard information (her transmitted sequence). This
forces her to rely on hard-decision decoding to recover Bob's key.
  In this work, we introduce a novel RR technique for PAM (and QAM) in which
Bob discloses a carefully designed soft metric to help Alice recover Bob's key,
while leaking no additional information about the key to an eavesdropper. We
assess the performance of the proposed technique in terms of achievable secret
key rate (SKR) and its bounds, showing that the achievable SKR closely
approaches the upper bound, with a significant gain over hard-decision RR.
Finally, we implement the scheme at the coded level using binary LDPC codes
with belief-propagation decoding, assess its bit-error rate through numerical
simulations, compare the observed gain with theoretical predictions from the
achievable SKR, and discuss the residual gap.

</details>


### [94] [Throughput Maximization for Multiuser Communications with Flexible-Sector 6DMA](https://arxiv.org/abs/2510.10944)
*Xiaoming Shi,Yunli Li,Xiaodan Shao,Jie Xu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出了一种灵活扇区6DMA架构，通过可移动天线阵列优化扇区覆盖和天线分配，显著提升网络吞吐量


<details>
  <summary>Details</summary>
Motivation: 传统基站采用固定位置天线，无法灵活匹配用户空间分布。6DMA架构通过天线移动和扇区旋转实现灵活覆盖配置

Method: 联合优化扇区旋转和天线分配，推导闭式解，并开发低复杂度次优解最小化扇区间用户数方差

Result: 仿真验证了理论分析，灵活扇区基站相比基准方案显著提升网络吞吐量，等用户分布是最优的

Conclusion: 6DMA架构通过灵活天线配置有效提升网络容量，为未来无线通信提供成本效益高的部署方案

Abstract: This paper presents a cost-effective and easily-deployable flexible-sector
six-dimensional movable antenna (6DMA) architecture for future wireless
communication networks, which enables flexible antenna configurations to match
users' spatial distribution for capacity enhancement. Different from
conventional sectorized base station (BS) with fixed-position antennas (FPAs),
the flexible-sector 6DMA-enabled BS employs multiple directional sector antenna
arrays that can flexibly move along a common circular track. By properly moving
antennas across sectors and rotating all sector antenna arrays synchronously,
the flexible-sector BS can adjust the coverage regions of all sectors with
flexible antenna allocations over them. In particular, we consider the
multiuser downlink communication employing the orthogonal multiple access (OMA)
to serve users in each sector. Under this setup, we jointly optimize the sector
rotation and the antenna allocation at the flexible-sector BS to maximize the
average common throughput achievable for all users based on their spatial
distribution. We solve this non-convex optimization problem by deriving
closed-form solutions and thereby analyze the effect of users' spatial
distribution on the achievable common throughput. It is shown that equal user
distribution over sectors is optimal for maximizing the common throughput.
Motivated by this result, we further develop a low-complexity suboptimal
solution for the sector rotation that minimizes the variance of user numbers
across sectors. Finally, we provide simulation results to verify our analytical
results and validate the performance of our proposed solutions. It is
demonstrated that the flexible-sector BS significantly improves the network
throughput as compared to other benchmark schemes.

</details>


### [95] [Forward-Forward Autoencoder Architectures for Energy-Efficient Wireless Communications](https://arxiv.org/abs/2510.11418)
*Daniel Seifert,Onur Günlü,Rafael F. Schaefer*

Main category: cs.IT

TL;DR: 本文设计了基于前向-前向算法的端到端学习自编码器，在AWGN和瑞利块衰落信道中评估性能，展示了与反向传播训练系统相比的竞争力，并提供了内存和处理时间节省的优势。


<details>
  <summary>Details</summary>
Motivation: 深度学习在通信系统中的应用日益重要，前向-前向学习作为反向传播算法的有效替代方案，具有不需要可微分通信信道、不依赖全局偏导数等优势，可实现能效更高的实现。

Method: 使用前向-前向算法设计端到端学习自编码器，在加性高斯白噪声和瑞利块衰落信道中进行数值评估，包括联合编码调制场景和固定非可微分调制阶段场景。

Result: 前向-前向训练的系统在联合编码调制场景中与反向传播训练系统具有竞争力，在固定非可微分调制场景中也表现良好，同时实现了显著的内存和处理时间节省。

Conclusion: 前向-前向算法是通信系统中端到端学习的有前景的替代方案，提供了与反向传播相当的性能，同时具有能效和实现优势。

Abstract: The application of deep learning to the area of communications systems has
been a growing field of interest in recent years. Forward-forward (FF) learning
is an efficient alternative to the backpropagation (BP) algorithm, which is the
typically used training procedure for neural networks. Among its several
advantages, FF learning does not require the communication channel to be
differentiable and does not rely on the global availability of partial
derivatives, allowing for an energy-efficient implementation. In this work, we
design end-to-end learned autoencoders using the FF algorithm and numerically
evaluate their performance for the additive white Gaussian noise and Rayleigh
block fading channels. We demonstrate their competitiveness with BP-trained
systems in the case of joint coding and modulation, and in a scenario where a
fixed, non-differentiable modulation stage is applied. Moreover, we provide
further insights into the design principles of the FF network, its training
convergence behavior, and significant memory and processing time savings
compared to BP-based approaches.

</details>


### [96] [Repeated-and-Offset QPSK for DFT-s-OFDM in Satellite Access](https://arxiv.org/abs/2510.11445)
*Renaud-Alexandre Pitaval*

Main category: cs.IT

TL;DR: 提出了一种新的RO-QPSK调制方案，将卫星通信中的OQPSK与地面系统的DFT-s-OFDM波形结合，具有低PAPR和改善SINR的性能优势。


<details>
  <summary>Details</summary>
Motivation: 地面蜂窝网络与卫星网络融合的需求，需要将卫星系统中使用的OQPSK调制适配到地面系统上行链路使用的DFT-s-OFDM波形中。

Method: 引入了一种称为重复偏移QPSK(RO-QPSK)的一阶星座调制方案，推导其基本特性，并与5G中支持的pi/2-BPSK加频域频谱整形进行比较。

Result: RO-QPSK自然产生汉宁窗形状的频谱，PAPR极低（约2dB），在窄带和中等频率选择性信道中，通过单抽头均衡和符号组合，SINR优于pi/2-BPSK加FDSS。

Conclusion: RO-QPSK是卫星通信中DFT-s-OFDM系统的有效调制方案，可进一步结合FDSS进一步降低PAPR，同时提供相似性能。

Abstract: Motivated by the convergence of terrestrial cellular networks with satellite
networks, we consider an adaptation of offset quadrature phase shift keying
(OQPSK), used with single-carrier waveform in traditional satellite systems, to
discrete Fourier transform spread (DFT-s-) orthogonal frequency-division
multiplexed (OFDM) waveform employed in the uplink of terrestrial systems. We
introduce a new order-one constellation modulation, termed repeated-and-offset
QPSK (RO-QPSK), derive its basic properties, and compare it with pi/2-BPSK with
frequency-domain spectral shaping (FDSS), as supported in 5G. RO-QPSK naturally
produces a Hann-window-shaped spectrum, resulting in a very low maximum
peak-to-average power ratio (PAPR) on the order of 2 dB. Moreover, with
single-tap equalization and symbol combining at the receiver, RO-QSPK can
improve the signal-to-interference-plus-noise (SINR) compared to pi/2-BPSK with
FDSS, in narrowband and/or moderately frequency-selective channels, as
encountered in satellite communications. A moderate FDSS can also be combined
with RO-QSPK to further reduce the PAPR while providing similar performance. Of
independent interest, general SINR expressions for DFT-s-OFDM are also
provided.

</details>


### [97] [List Decoding Reed--Solomon Codes in the Lee, Euclidean, and Other Metrics](https://arxiv.org/abs/2510.11453)
*Chris Peikert,Alexandra Veliche Hostetler*

Main category: cs.IT

TL;DR: 提出了一个多项式时间算法，用于在ℓ_p（半）度量下对素数域上的（广义）Reed-Solomon码进行列表解码，其中0<p≤2。该算法能够解码到任意大的距离，并在超过中等阈值时具有更好的距离-速率权衡。


<details>
  <summary>Details</summary>
Motivation: Reed-Solomon码在计算机科学和信息理论中广泛应用，但大多数高效纠错算法都集中在汉明度量上。对于某些应用，依赖于错误具体值的其他度量可能更合适。

Method: 开发了一个多项式时间列表解码算法，适用于ℓ_p度量（0<p≤2），包括Lee（ℓ₁）和欧几里得（ℓ₂）度量。算法能够处理任意大的解码距离，并改进了距离-速率权衡。

Result: 算法在ℓ₁和ℓ₂度量下支持比最坏情况错误更大的速率，并证明了对于许多感兴趣参数，该列表解码器实际上是唯一解码器。

Conclusion: 该工作扩展了Reed-Solomon码在ℓ_p度量下的解码能力，为各种应用场景提供了更灵活和高效的纠错解决方案。

Abstract: Reed--Solomon error-correcting codes are ubiquitous across computer science
and information theory, with applications in cryptography, computational
complexity, communication and storage systems, and more. Most works on
efficient error correction for these codes, like the celebrated
Berlekamp--Welch unique decoder and the (Guruswami--)Sudan list decoders, are
focused on measuring error in the Hamming metric, which simply counts the
number of corrupted codeword symbols. However, for some applications, other
metrics that depend on the specific values of the errors may be more
appropriate.
  This work gives a polynomial-time algorithm that list decodes (generalized)
Reed--Solomon codes over prime fields in $\ell_p$ (semi)metrics, for any $0 < p
\leq 2$. Compared to prior algorithms for the Lee ($\ell_1$) and Euclidean
($\ell_2$) metrics, ours decodes to arbitrarily large distances (for
correspondingly small rates), and has better distance-rate tradeoffs for all
decoding distances above some moderate thresholds. We also prove lower bounds
on the $\ell_{1}$ and $\ell_{2}$ minimum distances of a certain natural
subclass of GRS codes, which establishes that our list decoder is actually a
unique decoder for many parameters of interest. Finally, we analyze our
algorithm's performance under random Laplacian and Gaussian errors, and show
that it supports even larger rates than for corresponding amounts of worst-case
error in $\ell_{1}$ and $\ell_{2}$ (respectively).

</details>
