<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning](https://arxiv.org/abs/2512.19696)
*Sebastian Racedo,Brigitte Jaumard,Oscar Delgado,Meysam Masoudi*

Main category: cs.NI

TL;DR: 本文提出GRLDyP方法，使用图神经网络辅助的深度强化学习，在O-RAN中动态选择O-CU位置和服务功能链路由，以最小化能耗同时满足服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 传统RAN部署中逻辑功能与物理位置的绑定是静态的，在时变流量和资源条件下会导致效率低下。O-RAN解耦了传统RAN组件，但需要解决动态资源分配问题。

Method: 将问题建模为马尔可夫决策过程，使用图神经网络辅助的深度强化学习（GRLDyP）。GNN编码瞬时网络拓扑和资源利用率，DRL策略学习平衡服务等级、延迟和能耗。为每个传入服务流联合选择路由和O-CU位置。

Result: 在蒙特利尔市24小时流量跟踪数据集上的评估显示，与静态映射基线相比，动态O-CU选择和路由显著降低了能耗，且未违反QoS约束。

Conclusion: 基于DRL的SFC供应是能量感知、资源自适应O-RAN部署的实用控制原语，动态资源分配能有效提升能效。

Abstract: Open Radio Access Network (O RAN) disaggregates conventional RAN into interoperable components, enabling flexible resource allocation, energy savings, and agile architectural design. In legacy deployments, the binding between logical functions and physical locations is static, which leads to inefficiencies under time varying traffic and resource conditions. We address this limitation by relaxing the fixed mapping and performing dynamic service function chain (SFC) provisioning with on the fly O CU selection. We formulate the problem as a Markov decision process and solve it using GRLDyP, i.e., a graph neural network (GNN) assisted deep reinforcement learning (DRL). The proposed agent jointly selects routes and the O-CU location (from candidate sites) for each incoming service flow to minimize network energy consumption while satisfying quality of service (QoS) constraints. The GNN encodes the instantaneous network topology and resource utilization (e.g., CPU and bandwidth), and the DRL policy learns to balance grade of service, latency, and energy. We perform the evaluation of GRLDyP on a data set with 24-hour traffic traces from the city of Montreal, showing that dynamic O CU selection and routing significantly reduce energy consumption compared to a static mapping baseline, without violating QoS. The results highlight DRL based SFC provisioning as a practical control primitive for energy-aware, resource-adaptive O-RAN deployments.

</details>


### [2] [Automated Fault Detection in 5G Core Networks Using Large Language Models](https://arxiv.org/abs/2512.19697)
*Parsa Hatami,Ahmadreza Majlesara,Ali Majlesi,Babak Hossein Khalaj*

Main category: cs.NI

TL;DR: 利用大语言模型（LLM）自动化网络故障检测与分类，通过在Kubernetes测试网络中注入多种故障类型，收集数据并微调GPT-4.1 nano模型，显著提升故障检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现代电信网络数据量快速增长、规模不断扩大，对高可靠性要求日益迫切。网络需要支持包括高度敏感和关键任务在内的多种应用服务，要求快速准确地检测和解决网络错误。传统故障诊断方法在复杂环境中效率低下。

Method: 在基于Kubernetes的测试网络中故意注入多种网络错误类型（pod故障、pod终止、网络延迟、网络丢包、磁盘I/O故障），收集健康与故障状态下的数据。数据集包含不同网络组件（pod）的日志，以及系统描述、事件、往返时间测试和pod状态信息等补充数据。通过API对GPT-4.1 nano模型在该数据集上进行微调。

Result: 微调后的GPT-4.1 nano模型在故障检测准确率上相比基础模型有显著提升。研究结果表明LLM方法在实现闭环、无需操作员的故障管理方面具有潜力。

Conclusion: 基于LLM的方法能够增强网络可靠性，减少服务提供商因停机时间产生的运营成本，为实现自动化网络故障管理提供了有效途径。

Abstract: With the rapid growth of data volume in modern telecommunication networks and the continuous expansion of their scale, maintaining high reliability has become a critical requirement. These networks support a wide range of applications and services, including highly sensitive and mission-critical ones, which demand rapid and accurate detection and resolution of network errors. Traditional fault-diagnosis methods are no longer efficient for such complex environments.\cite{b1} In this study, we leverage Large Language Models (LLMs) to automate network fault detection and classification. Various types of network errors were intentionally injected into a Kubernetes-based test network, and data were collected under both healthy and faulty conditions. The dataset includes logs from different network components (pods), along with complementary data such as system descriptions, events, Round Trip Time (RTT) tests, and pod status information. The dataset covers common fault types such as pod failure, pod kill, network delay, network loss, and disk I/O failures. We fine-tuned the GPT-4.1 nano model via its API on this dataset, resulting in a significant improvement in fault-detection accuracy compared to the base model. These findings highlight the potential of LLM-based approaches for achieving closed-loop, and operator-free fault management, which can enhance network reliability and reduce downtime-related operational costs for service providers.

</details>


### [3] [Smoothing Rough Edges of IPv6 in VPNs](https://arxiv.org/abs/2512.19698)
*Yejin Cho,John Heidemann*

Main category: cs.NI

TL;DR: 研究发现商业VPN在IPv6处理上存在两大问题：IPv4-only VPN会泄露用户原生IPv6地址（5%-57%用户受影响），而双栈VPN中IPv6地址常被系统降级优先级导致用户实际使用IPv4连接。


<details>
  <summary>Details</summary>
Motivation: 研究商业VPN与IPv6的交互问题，因为用户使用VPN部分原因是为了隐藏本地IP地址，但IPv6泄露会损害用户隐私。同时，双栈VPN中IPv6连接优先级问题影响IPv6的实际采用。

Method: 使用WhatIsMyIPAddress.com的12.9万日访客数据集分析VPN泄露情况；测试6个Android VPN应用验证IPv6优先级问题；提出新的IPv6地址范围解决方案并在Linux上实现原型。

Result: 发现12个之前被认为安全的VPN仍对至少5%用户存在IPv6泄露；IPv4-only VPN中5%-57%用户会暴露原生IPv6地址；6个Android VPN中有5个持续降低IPv6优先级；提出的新IPv6地址范围方案能解决优先级问题。

Conclusion: 商业VPN在IPv6支持上存在隐私泄露和连接优先级两大问题，需要VPN提供商和系统开发者共同解决。提出的新IPv6地址范围方案是可行的技术解决方案。

Abstract: How do commercial VPNs interact with IPv6? We show two "rough edges" in how commercial VPNs handle IPv6. First, we show that many IPv4-only VPNs leak IPv6 traffic to the ISP. Individual use VPNs in part to conceal their local IP addresses, so such leaks reduce user privacy. While prior work has studied VPNs in testbeds, we use a new dataset of 129k VPN-using daily visitors to WhatIsMyIPAddress.com that quantifies these leaks and show 12 VPNs previously considered safe still leak for at least 5% of their users. We show native IPv6 addresses leak most commonly in VPNs that claim only IPv4 support, with 5% to 57% of visitors of v4-only VPNs having their native IPv6 address exposed. Second, we show that most dual-stack VPNs users actually select IPv4 instead of IPv6. We observe this problem in our visitor data, and we identify the root cause arises because when user's computer follows standard address-selection rules, VPN-assigned addresses are often de-preferenced. Testing six VPNs on Android, we show that five consistently de-prioritize IPv6. Finally, we suggest a solution to IPv6 de-preferencing: we define a new IPv6 address range for VPNs that is not de-preferenced by address selection. We prototype this solution on Linux. Our findings help identify and address rough edges in the addition of IPv6 support to VPNs.

</details>


### [4] [Holographic MIMO Empowered NOMA-ISAC for 6G: Rate-Splitting Enhanced Near-Field Modeling, Multi-Objective Optimization, and Statistical Performance Validation](https://arxiv.org/abs/2512.19699)
*Sumita Majhi*

Main category: cs.NI

TL;DR: 提出了一种用于6G全息MIMO ISAC的综合框架，集成了近场传播建模、RS-NOMA架构和多目标优化方法，在统计验证中显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有NOMA ISAC研究未能充分解决全息近场传播、RSMA集成和实际损伤下的统计验证问题，需要为6G全息MIMO ISAC部署建立严谨基础。

Method: 1) 统一近场建模包含空间相关瑞利衰落、球面波前传播和亚波长天线耦合效应；2) 新颖的RS-NOMA架构实现感知与通信间的灵活干扰管理；3) 多目标优化套件比较HAO-SCA、WMMSE、SDR、FP和DRL方法；4) 5000次蒙特卡洛运行的严格统计验证。

Result: RS-NOMA相比传统NOMA实现11.7%的总速率提升，相比WMMSE在匹配感知效用下提升18.8%；感知CRLB改善2.4dB，具有99%统计置信度。

Conclusion: 该框架为实际6G全息MIMO ISAC部署建立了严谨基础，展示了RS-NOMA在集成感知与通信中的显著优势。

Abstract: Holographic multiple-input multiple-output (MIMO) systems with extremely large apertures enable transformational capabilities for sixth-generation (6G) integrated sensing and communications (ISAC). However, existing non-orthogonal multiple access (NOMA) ISAC works inadequately address: (i) holographic near-field propagation with sub-wavelength antenna spacing; (ii) rate-splitting multiple access (RSMA) integration for interference management; (iii) statistical validation under realistic impairments. This paper presents a comprehensive holographic MIMO NOMA-ISAC framework featuring: \textbf{(1)} Unified near-field modeling incorporating spatially-correlated Rayleigh fading, spherical wavefront propagation, and sub-wavelength antenna coupling effects; \textbf{(2)} Novel rate-splitting enhanced NOMA (RS-NOMA) architecture enabling flexible interference management between sensing and communication; \textbf{(3)} Multi-objective optimization suite comparing hybrid alternating optimization with successive convex approximation (HAO-SCA), weighted minimum mean square error (WMMSE), semidefinite relaxation (SDR), fractional programming (FP), and deep reinforcement learning (DRL); \textbf{(4)} Rigorous statistical validation over 5000 Monte Carlo runs with significance testing across massive MIMO scenarios (up to 1024 antennas). Results demonstrate that RS-NOMA achieves \SI{11.7}{\percent} higher sum-rate than conventional NOMA and \SI{18.8}{\percent} over WMMSE at matched sensing utility. Sensing CRLB improvements of \SI{2.4}{\decibel} are confirmed with 99\% statistical confidence. The framework establishes rigorous foundations for practical 6G holographic MIMO ISAC deployment.

</details>


### [5] [VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization](https://arxiv.org/abs/2512.19964)
*Bruno E. Farias,José Flauzino,Elias P. Duarte*

Main category: cs.NI

TL;DR: VNF-Cache是一个用于地理分布式键值数据库的缓存服务，通过NFV-COIN技术在网络中实现，显著减少响应时间并提高每秒处理请求数。


<details>
  <summary>Details</summary>
Motivation: 随着互联网数据量的指数级增长，优化数据访问的响应时间和资源使用变得至关重要。缓存是有效的解决方案，可以将数据更靠近客户端，减少对服务器的重复请求。

Method: VNF-Cache是一个NFV-COIN（网络功能虚拟化-网络内计算）服务，通过拦截网络数据包、处理、存储并在可能时直接向客户端发送值来实现缓存功能。该技术正在由IETF进行标准化。

Result: 通过在巴西、美国和日本的分布式服务器上进行概念验证实现和实验，观察到响应时间显著减少，每秒处理的请求数量显著增加。

Conclusion: VNF-Cache作为NFV-COIN缓存服务，能够有效优化地理分布式键值数据库的访问性能，减少响应时间并提高系统吞吐量。

Abstract: With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed.

</details>


### [6] [CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks](https://arxiv.org/abs/2512.20080)
*Dianxuan Fu,Xiaomin Liu,Yihao Zhang,Shikui Shen,Weisheng Hu,Qunbi Zhuge*

Main category: cs.NI

TL;DR: 提出通信感知的跨域资源分配框架，用于多数据中心光网络上的流水线并行分布式训练，相比基线降低31.25%的迭代时间，减少13.20%的阻塞请求


<details>
  <summary>Details</summary>
Motivation: 在多数据中心光网络上进行流水线并行分布式训练时，通信瓶颈成为主要性能限制因素，需要优化跨域资源分配以降低训练迭代时间

Method: 提出通信感知的跨域资源分配框架，考虑通信约束进行资源调度，优化流水线并行训练在多数据中心光网络上的性能

Result: 相比基线方法，该框架降低了31.25%的训练迭代时间，减少了13.20%的阻塞请求

Conclusion: 通信感知的跨域资源分配框架能有效优化多数据中心光网络上的流水线并行分布式训练性能，显著降低训练时间和资源阻塞

Abstract: We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines.

</details>


### [7] [Edge-Served Congestion Control for Wireless Multipath Transmission with a Transformer Agent](https://arxiv.org/abs/2512.20186)
*Liang Wang*

Main category: cs.NI

TL;DR: Jazz：一种解耦架构的多路径TCP拥塞控制系统，将决策"大脑"与内核数据路径分离，使用基于Transformer的智能体处理历史观测序列以克服部分可观测性问题，在Wi-Fi测试中提升带宽效率至少2.85%


<details>
  <summary>Details</summary>
Motivation: 多路径TCP虽能提升连接质量，但其拥塞控制演进受限于OS内核的单体设计（开发成本高、资源灵活性不足），且网络统计中的固有噪声导致部分可观测性问题，会误导数据驱动方法如深度强化学习

Method: 提出Jazz系统，采用解耦架构将决策"大脑"与内核数据路径分离，使其能在外部（边缘）实体运行；核心使用基于Transformer的智能体处理历史观测序列，克服单步强化学习的部分可观测性，学习掌握链路波动和跨路径依赖

Result: 在双频（5GHz/6GHz）Wi-Fi测试平台上，Jazz相比传统方法提升带宽效率至少2.85%，在1%丢包率下保持96.2%性能，验证了该设计作为敏捷网络智能的实用蓝图

Conclusion: Jazz通过解耦架构和基于Transformer的序列处理，有效解决了多路径TCP拥塞控制的开发障碍和部分可观测性问题，为网络智能提供了实用且高效的实现方案

Abstract: Multipath TCP is widely adopted to enhance connection quality-of-service by leveraging multiple network pathways on modern devices. However, the evolution of its core congestion control is hindered by the OS kernel, whose monolithic design imposes high development overhead and lacks the resource flexibility required for data-driven methods. Furthermore, inherent noise in network statistics induces a partial observability problem, which can mislead data-driven methods like Deep Reinforcement Learning. To bridge this gap, we propose Jazz, a system that re-architects multipath congestion control through a decoupled architecture that separates the decision-making ``brain'' from the in-kernel datapath, enabling it to operate on an external (edge) entity. At its core, Jazz employs a Transformer-based agent that processes sequences of historical observations to overcome the partial observability of single-step reinforcement learning. This allows it to learn and master fluctuating link conditions and intricate cross-path dependencies. Tested on a dual-band (5GHz/6GHz) Wi-Fi testbed, our implementation improves bandwidth efficiency by at least 2.85\% over conventional methods and maintains 96.2\% performance under 1\% packet loss, validating this design as a practical blueprint for agile network intelligence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research](https://arxiv.org/abs/2512.19799)
*Tingjia Miao,Jiawen Dai,Jingkun Liu,Jinxin Tan,Muhua Zhang,Wenkai Jin,Yuwen Du,Tian Jin,Xianghe Pang,Zexi Liu,Tu Guo,Zhengliang Zhang,Yunjie Huang,Shuo Chen,Rui Ye,Yuzhi Zhang,Linfeng Zhang,Kun Chen,Wei Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: PhysMaster是一个基于LLM的自主理论物理学家代理，通过结合抽象推理与数值计算，利用LANDAU知识库和自适应探索策略，在物理研究中实现加速、自动化和自主发现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统主要在定义明确的基准测试或文献检索等通用任务上评估，缺乏在开放科学场景中的端到端问题解决能力，特别是在抽象、数学密集、需要结合分析推理与代码计算的物理学领域。

Method: PhysMaster将抽象推理与数值计算相结合，利用LANDAU（分层学术数据宇宙）保存检索的文献、整理的先验知识和验证的方法轨迹，采用自适应探索策略平衡效率与开放探索，支持超长视野任务。

Result: 在高能理论、凝聚态理论和天体物理学问题上评估显示：(1) 加速：将劳动密集型研究从数月压缩到数小时；(2) 自动化：自主执行假设驱动循环；(3) 自主发现：独立探索开放问题。

Conclusion: PhysMaster作为自主理论物理学家代理，通过结合推理与计算、利用知识库和自适应策略，在物理研究中实现了显著加速、自动化和自主发现能力，展示了LLM在复杂科学问题解决中的潜力。

Abstract: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.

</details>


### [9] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 该研究提出了一种双目标优化方法，用于灾后救援物资分配和车辆路径规划，平衡效率与公平性，通过分支定价算法显著提升求解性能。


<details>
  <summary>Details</summary>
Motivation: 重大灾害中预置物资往往无法满足所有需求，需要在有限物资下规划车辆路径并分配救援物资，同时平衡配送效率与分配公平性。

Method: 建立混合整数规划模型，采用ε-约束法处理双目标优化（最小化基尼系数衡量的需求不满足不公平性和总旅行时间），推导最优解数学性质，引入有效不等式，设计给定可行车辆路径下的最优分配算法，开发分支定价算法。

Result: 在土耳其Van地震实际数据和伊斯坦布尔Kartal地区预测数据上的计算测试表明，分支定价算法显著优于商业MIP求解器，双目标方法在不牺牲效率的情况下将援助分配不公平性降低了34%。

Conclusion: 当时间约束非常宽松或紧张时，优先考虑需求覆盖的词典序优化有效；对于中等限制性时间约束，平衡方法对于避免不公平结果至关重要。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [10] [Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs](https://arxiv.org/abs/2512.19937)
*Eric Yeh,John Cadigan,Ran Chen,Dick Crouch,Melinda Gervasio,Dayne Freitag*

Main category: cs.AI

TL;DR: 使用插值解码技术让大语言模型模拟人类人格特质，在经济学游戏中复现人类决策行为，并探索将个体人类玩家"孪生化"到模型中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能模拟人类行为，但为每个人格特征创建独立提示词会增加实验负担并降低可复现性。需要一种更高效、可复现的方法来模拟人格特质对决策的影响。

Method: 采用插值解码技术，将人格的每个维度表示为一对相反的提示词，通过插值参数在维度上模拟行为。使用大五人格维度，通过系统搜索插值空间来匹配人类玩家的行为。

Result: 插值解码能可靠地调节大五人格各维度的得分，使LLMs在经济学游戏中模仿人类决策行为，复现心理学研究结果。初步实现了将个体人类玩家在协作游戏中"孪生化"。

Conclusion: 插值解码为使用LLMs模拟人类人格特质提供了一种高效、可复现的方法，能够可靠地调节人格维度并复现人类决策模式，为行为经济学研究提供了新工具。

Abstract: Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.

</details>


### [11] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 提出一种基于类别原型指导的视觉Transformer分割方法，用于PlantCLEF 2025细粒度多标签物种识别挑战，在私有排行榜上获得第五名。


<details>
  <summary>Details</summary>
Motivation: 解决PlantCLEF 2025挑战中的细粒度多标签物种识别问题，需要从高分辨率植被图像中同时识别多个物种。传统方法难以处理这种复杂的多标签分类任务，需要一种能够适应从单物种分类到多标签分类的域适应方法。

Method: 1. 从训练数据集中提取特征，使用K-Means聚类（K等于类别数）创建类别原型；2. 构建定制化的窄视觉Transformer，用冻结的DinoV2替换patch embedding层；3. 训练分割模型从测试图像中重建训练数据集的类别原型；4. 利用模型的注意力分数识别和定位感兴趣区域，指导分类过程。

Result: 在PlantCLEF 2025挑战的私有排行榜上获得第五名，F1分数为0.33331。与最佳提交结果仅相差0.03，表明该方法在基准任务中具有竞争力。

Conclusion: 提出的基于类别原型指导的视觉Transformer分割方法能够有效实现从单物种分类到多标签植被图像分类的域适应，在PlantCLEF 2025挑战中取得了有竞争力的性能。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [12] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 提出一种通过类内聚类生成伪标签进行分层分类的方法，以缓解细粒度视觉分类中的类内变异问题，在PlantNet300k数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务中，类内变异（同一类别内图像差异）会阻碍深度学习模型的学习，特别是当这些类别样本不足时。需要缓解类内变异问题以提升分类性能。

Method: 对每个类别单独进行聚类，发现编码图像间相似度的伪标签，然后利用这些伪标签进行分层分类，学习更细粒度的视觉特征。

Result: 在PlantNet300k数据集上取得了state-of-the-art性能，尽管部分组件尚未完全优化。初步实验揭示了未来工作需要关注的关键点。

Conclusion: 通过类内聚类生成伪标签进行分层分类的方法能够缓解类内变异问题，提升细粒度视觉分类性能，但需要进一步优化以获得更确凿的证据。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [13] [S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test](https://arxiv.org/abs/2512.19992)
*Zhe Sun,Xueyuan Yang,Yujie Lu,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 提出了S³IT基准测试，用于评估具身社交智能，通过座位排序任务测试智能体在3D环境中整合社交规范与物理约束的能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么局限于非具身的社交推理（如文本），要么局限于无社交意识的物理任务，无法评估智能体在真实具身环境中整合和权衡物理与社交约束的能力。

Method: 设计了S³IT基准测试，核心是座位排序任务：智能体需要在3D环境中为具有不同身份、偏好和复杂人际关系的LLM驱动NPC安排座位。采用可扩展的程序化框架生成多样化场景，要求智能体通过主动对话获取偏好、自主探索感知环境，并在复杂约束网络中进行多目标优化。

Result: 评估了最先进的LLM在S³IT上的表现，发现它们仍难以解决此问题，与人类基线存在明显差距。结果表明LLM在空间智能方面存在不足，但在处理具有明确文本线索的冲突时能接近人类水平。

Conclusion: S³IT基准测试填补了现有评估的空白，能够有效评估具身社交智能。当前LLM在该任务上表现不佳，揭示了其在空间智能方面的缺陷，同时也展示了其在文本线索明确的冲突解决方面的潜力。

Abstract: The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.

</details>


### [14] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 提出LieFlow方法，通过流匹配在李群上直接从数据中学习对称性，能够发现更广泛的群类型，并在2D和3D点云上成功发现了包括反射在内的离散群。


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提高机器学习性能都很重要，但需要知道数据中的底层对称性。现有方法在群类型发现方面有限制且需要较多假设。

Method: 提出LieFlow方法，将对称性发现定义为在更大假设群上学习分布，使学习到的分布与数据中观察到的对称性匹配。使用李群上的流匹配，并针对"最后一刻收敛"问题引入新的插值方案。

Result: 在2D和3D点云上的实验表明，该方法成功发现了离散群，包括通过复数域上的流匹配发现反射对称性。解决了目标模式对称排列导致的"最后一刻收敛"问题。

Conclusion: LieFlow方法比先前工作更灵活，能发现更多类型的群，且需要更少的假设。通过流匹配在数据中直接学习对称性是有效的，为对称性发现提供了新方法。

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [15] [Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks](https://arxiv.org/abs/2512.20275)
*Divya Vijay,Vignesh Ethiraj*

Main category: cs.AI

TL;DR: G-SPEC是一个神经符号框架，通过确定性验证约束概率规划，解决5G/6G网络编排中LLM代理的随机风险问题，实现零安全违规和94.1%的修复成功率。


<details>
  <summary>Details</summary>
Motivation: 随着网络向5G独立组网和6G演进，运营商面临超出静态自动化和深度强化学习能力的编排挑战。虽然大型语言模型代理为实现意图驱动网络提供了路径，但引入了拓扑幻觉和政策不合规等随机风险。

Method: 提出Graph-Symbolic Policy Enforcement and Control (G-SPEC)神经符号框架，包含治理三元组：电信适配代理(TSLAM-4B)、网络知识图谱(NKG)和SHACL约束，通过确定性验证约束概率规划。

Result: 在模拟的450节点5G核心网上评估，实现零安全违规和94.1%的修复成功率，显著优于82.4%的基线。NKG验证贡献68%的安全增益，SHACL策略贡献24%。在10K到100K节点拓扑上验证延迟按O(k^1.2)扩展，处理开销142ms。

Conclusion: G-SPEC通过神经符号方法有效缓解LLM代理的随机风险，验证延迟可扩展，142ms处理开销使其适用于SMO层操作，为5G/6G网络编排提供了可行的安全解决方案。

Abstract: As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.

</details>


### [16] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF框架从无动作视频中学习基于光流的潜在技能，实现高层规划并提升多任务和长时域任务性能


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以转化为低级动作，而潜在动作模型缺乏高层规划能力，需要一种能从视频中学习技能并支持规划的方法

Method: 提出SOF框架，通过光流中间表示学习潜在技能空间，该空间同时捕捉视频动态和机器人动作信息，实现视频技能的高层规划

Result: 实验表明SOF在多任务和长时域设置中持续提升性能，能够直接从原始视觉数据中获取和组合技能

Conclusion: SOF成功桥接了视频生成模型和潜在动作模型之间的差距，实现了从视频中学习技能并支持高层规划的能力

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [17] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 提出ProbGLC概率交叉视角地理定位方法，结合概率性和确定性模型，提升灾害响应中的定位准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致灾害事件频发，快速准确的灾害位置识别对应急响应和资源分配至关重要，需要改进现有地理定位方法。

Method: 提出ProbGLC统一框架，结合概率性和确定性地理定位模型，通过不确定性量化和局部化评分增强可解释性，支持多灾害类型交叉视角定位。

Result: 在两个灾害数据集上验证，达到0.86的Acc@1km和0.97的Acc@25km准确率，同时提供概率分布和局部化评分增强模型可解释性。

Conclusion: ProbGLC在灾害地理定位中表现出优越性能，生成式交叉视角方法有助于提升灾害响应的位置感知能力，促进更快更好的应急决策。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [18] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 本文通过强化学习将通用语言模型转化为专业内容审核分类器，在数据稀缺场景下实现比监督微调高100倍的数据效率，并揭示了RL的S型扩展规律。


<details>
  <summary>Details</summary>
Motivation: 大规模内容审核是数字生态系统的紧迫挑战，需要持续评估数十亿用户和AI生成内容。虽然大语言模型在政策导向审核方面有潜力，但在标签稀疏、政策演变、需要复杂推理的现实场景中，如何训练达到专家级准确度的系统仍待探索。

Method: 采用强化学习训练内容分类系统，系统评估多种RL训练方案和奖励塑造策略，包括可验证奖励和LLM作为评判框架，将通用语言模型转化为专业政策对齐的分类器，在三个真实内容审核任务上进行实验。

Result: RL表现出S型扩展行为：性能随训练数据、rollouts和优化步骤增加而平滑提升后逐渐饱和。在需要复杂政策推理的任务上，RL显著提升性能，数据效率比监督微调高100倍，特别适用于专家标注稀缺或昂贵的领域。

Conclusion: 强化学习是构建大规模内容审核系统的有效方法，在数据稀缺场景下具有显著优势，为工业级审核系统提供了可行的技术路径和可操作的见解。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [19] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: Reason2Decide是一个两阶段训练框架，通过解决自解释中的暴露偏差和任务分离问题，在保持高预测准确性的同时生成与预测一致的解释，适用于临床决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策支持系统面临关键挑战：在实现高预测准确性的同时生成与预测一致的解释。现有方法存在暴露偏差问题，导致解释与预测不一致。

Method: 提出两阶段训练框架：第一阶段训练模型生成解释；第二阶段联合训练标签预测和解释生成，应用计划采样逐步从基于真实标签转换到基于模型预测。

Result: 在三个医疗数据集上评估，包括专有分诊数据集和公共生物医学QA数据集。Reason2Decide在预测(F1)和解释保真度(BERTScore、BLEU、LLM-as-a-Judge)方面优于其他微调基线。在分诊任务中，对LLM生成、护士撰写和护士后处理的解释都表现出鲁棒性。

Conclusion: Reason2Decide仅使用LLM生成的解释进行第一阶段训练就能超越其他变体，表明LLM生成的解释适合预训练模型，减少对人类标注的依赖。该框架使用比当代基础模型小40倍的模型实现这些优势，使临床推理在资源受限环境中更易部署，同时提供可解释的决策支持。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [20] [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)
*Chaithra,Kamesh Kadimisetty,Biju R Mohan*

Main category: cs.AI

TL;DR: 提出一个结合LLM、RAG和市场反馈的自适应金融情感分析框架，通过强化学习优化信息源权重，在印度股市数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析工作未考虑股价或市场反馈的影响，需要开发能动态适应市场行为的系统。

Method: 1) 在SentiFin数据集上使用指令学习微调LLaMA 3.2 3B模型；2) 基于余弦相似度的RAG管道动态选择多源上下文信息；3) 引入反馈驱动模块，通过比较预测情感与实际次日股票收益调整信息源可靠性；4) 使用PPO强化学习代理学习优化信息源权重策略。

Result: 在2024-2025年NIFTY 50新闻标题上的实验结果表明，该系统在分类准确率、F1分数和市场对齐度方面显著优于基线模型和静态检索方法。

Conclusion: 结合指令调优LLM、动态反馈和强化学习的方法能够实现稳健、市场感知的金融情感建模，验证了该自适应框架的潜力。

Abstract: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.

</details>


### [21] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: MolAct是一个基于智能体强化学习的分子设计框架，通过两阶段训练实现分子编辑和优化，将分子设计形式化为多步骤的工具增强决策过程。


<details>
  <summary>Details</summary>
Motivation: 分子编辑和优化是多步骤问题，需要迭代改进分子性质同时保持化学有效性和结构相似性。现有方法缺乏将分子设计形式化为智能体强化学习问题的框架，无法有效结合推理、工具使用和分子优化。

Method: 提出MolAct框架，采用两阶段训练范式：第一阶段建立编辑能力，第二阶段重用学习到的编辑行为来优化性质。框架将LLM智能体训练为能够交替进行推理、工具使用和分子优化，通过多轮交互调用化学工具进行有效性检查、性质评估和相似性控制。

Result: 在分子编辑任务中，MolEditAgent-7B在添加、删除和替换编辑上分别达到100、95和98的有效性，优于DeepSeek-R1等基线；在分子优化任务中，MolOptAgent-7B在LogP指标上超越Claude 3.7等基线，在溶解度上保持竞争力，并在其他目标上保持平衡性能。

Conclusion: 将分子设计视为多步骤、工具增强的过程是实现可靠且可解释改进的关键。MolAct框架首次将分子设计形式化为智能体强化学习问题，展示了通过智能体学习结合推理和工具使用进行分子优化的有效性。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [22] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 通过向原始时间序列注入噪声来提升冻结大语言模型在零样本时间序列预测中的性能，无需微调即可增强模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖微调专用模块来桥接时间序列数据与LLMs预训练知识之间的差距，但完全冻结的LLMs性能对输入数据的文本表示极其敏感。本文旨在探索无需任何微调、仅通过策略性标记化就能有效利用现成LLMs进行时间序列预测的方法。

Method: 提出一种简单而有效的策略：在标记化之前向原始时间序列注入噪声。这种非侵入性干预作为一种推理时增强手段，迫使冻结的LLM基于鲁棒的时间模式而非表面数值伪影进行外推。还引入了两个新的时间序列数据集，完全排除LLM预训练数据污染的可能偏差。

Result: 通过理论分析和实证验证，该方法在多样化基准测试中表现出有效性。在新引入的数据集上，噪声注入策略一致地提升了冻结LLMs在时间序列预测中的性能。

Conclusion: 噪声注入策略为直接利用现成大语言模型进行时间序列预测提供了进一步的技术路径，展示了无需微调即可增强模型鲁棒性的可行性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [23] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 开发基于双向门控循环单元（BiGRU）的数据中心PUE预测模型，通过特征选择和超参数优化提升预测精度，并与GRU模型进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗巨大且碳足迹显著，边缘计算和AI发展推动存储容量增长。提高能效是应对气候变化、降低能源成本、提升竞争力的关键。PUE作为数据中心运营效率指标，其预测有助于理解各特征对能耗的影响，从而针对性改进能效。

Method: 1. 使用EnergyPlus模拟新加坡数据中心，获得52,560个样本和117个特征的数据集；2. 采用递归特征消除与交叉验证（RFECV）算法选择最相关特征集；3. 基于BiGRU构建PUE预测模型，并进行超参数优化；4. 与GRU模型在MSE、MAE和R-squared指标上进行性能比较。

Result: 开发了优化的BiGRU-based PUE预测模型，通过特征选择和超参数调优获得了比GRU更好的预测性能。具体比较结果未在摘要中详细说明，但通过MSE、MAE和R-squared三个指标进行了评估。

Conclusion: BiGRU模型在PUE预测任务中表现出优于传统GRU的性能，为数据中心能效管理提供了有效的预测工具，有助于通过针对性改进关键特征来提升能源效率。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [24] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 比较人类与LLM在数字游戏概念推理任务中的泛化能力，发现人类更灵活地结合规则与相似性推理，而LLM更依赖数学规则，且人类具有更强的少样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究人类与大型语言模型在概念推理任务中的泛化能力差异，探索两者在归纳偏置和推理策略上的根本区别

Method: 使用数字游戏作为概念推理任务，以贝叶斯模型为分析框架，比较人类和LLM的归纳偏置与推理策略

Result: 贝叶斯模型能更好地捕捉人类行为：人类灵活推断基于规则和基于相似性的概念，而LLM更依赖数学规则；人类能从单个示例进行少样本泛化，LLM则需要更多样本

Conclusion: 人类与LLM在数学概念推理和泛化方面存在根本性差异，突显了当前LLM与人类认知能力之间的差距

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [25] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 提出PreSa方法，通过直接学习策略而非间接学习奖励/成本模型，结合偏好学习和安全对齐，在离线偏好强化学习中实现安全策略学习。


<details>
  <summary>Details</summary>
Motivation: 离线偏好强化学习（PbRL）虽然避免了奖励工程和直接人工标注，但在许多领域和任务中确保安全性仍是关键挑战。现有基于人类反馈的安全RL方法先学习奖励和成本模型，再使用约束RL优化安全策略，但在长视野连续控制任务中，奖励和成本误差会累积，导致性能下降。

Method: 提出PreSa（偏好与安全对齐）方法：1）直接基于奖励相关的行为偏好对和轨迹段安全性的二元标签学习策略；2）将偏好学习模块与安全对齐结合到约束优化问题中；3）在拉格朗日框架内求解，直接学习奖励最大化且安全的策略，无需显式学习奖励和成本模型，也无需约束RL。

Result: 在连续控制任务上使用合成和真实人类反馈进行评估，该方法成功学习了高奖励的安全策略，优于最先进的基线方法，甚至优于使用真实奖励和成本的离线安全RL方法。

Conclusion: PreSa方法通过直接学习策略而非间接学习奖励/成本模型，有效解决了离线偏好强化学习中的安全挑战，在连续控制任务中实现了更好的安全策略学习性能。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [26] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: TongSIM是一个高保真、通用型平台，用于训练和评估具身智能体，提供100+多样化室内场景和开放式户外城镇模拟，支持从低级导航到高级复合活动的广泛研究需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI向多模态和具身智能发展，现有仿真平台大多针对特定任务设计，缺乏能够支持从低级导航到高级复合活动（如多智能体社交模拟和人机协作）的通用训练环境。

Method: 开发TongSIM平台，提供100+多样化多房间室内场景和开放式交互丰富的户外城镇模拟，包含定制化场景、任务自适应保真度、多样化智能体类型和动态环境模拟等特性。

Result: TongSIM提供了全面的评估框架和基准测试，能够精确评估智能体的感知、认知、决策、人机协作以及空间和社会推理能力，为研究人员提供灵活可扩展的统一平台。

Conclusion: TongSIM作为一个通用平台，通过加速训练、评估和推进通用具身智能的发展，填补了现有仿真平台的不足，为具身智能研究提供了统一的基础设施。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [27] [MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents](https://arxiv.org/abs/2512.20237)
*Xingbo Du,Loka Li,Duzhen Zhang,Le Song*

Main category: cs.AI

TL;DR: MemR³：一个具有闭环控制机制的记忆检索系统，通过路由器和全局证据缺口跟踪器优化LLM代理的答案质量


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统主要优化压缩和存储，缺乏对记忆检索的显式闭环控制，需要更自主、准确和兼容的记忆检索机制

Method: MemR³系统包含两个核心机制：1）路由器在检索、反思和回答三个动作中选择以优化答案质量；2）全局证据缺口跟踪器使回答过程透明化并跟踪证据收集过程

Result: 在LoCoMo基准测试中，MemR³超越了强基线，在LLM-as-a-Judge评分上表现优异，显著改进了现有检索器（RAG +7.29%，Zep +1.94%）

Conclusion: MemR³为现有记忆存储提供了即插即用的控制器，通过闭环控制机制实现了更自主、准确和透明的记忆检索系统

Abstract: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.

</details>


### [28] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow是一个针对边缘设备优化的VLA模型推理框架，通过跨请求流水线调度和内存优化技术，将推理速度提升2.55倍，实现实时动态操作


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在边缘设备上的推理延迟较高（3-5Hz），无法满足机器人实时交互需求（20-30Hz），现有优化方法需要大量重训练或牺牲模型精度

Method: 提出ActionFlow系统级推理框架，核心是跨请求流水线策略，将VLA推理重新定义为微请求的宏流水线，智能批处理内存绑定的解码阶段和计算绑定的预填充阶段，并设计了跨请求状态打包前向算子和统一KV环形缓冲区

Result: 在OpenVLA-7B模型上实现了2.55倍的FPS提升，无需重训练即可在边缘硬件上实现实时动态操作

Conclusion: ActionFlow通过系统级优化解决了VLA模型在边缘设备上的实时性瓶颈，为动态真实世界环境中的机器人部署提供了可行的解决方案

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [29] [Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation](https://arxiv.org/abs/2512.20278)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.AI

TL;DR: 论文提出通过科学方法论（假设、探测、编码）让LLM从被动工具使用者转变为主动工作流架构师，自主生成生产级代码技能，解决了技能生成中的四个结构性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 虽然CodeMem确立了可执行代码作为代理程序记忆的最佳表示形式，但从零开始自主合成这种记忆的机制尚未充分探索。需要将大语言模型从被动工具使用者转变为主动工作流架构师。

Method: 通过Outlook和OneDrive跨服务编排任务的高保真案例研究，识别并解决技能自动生成中的四个结构性瓶颈：发现差距（导航大型工具注册表）、验证差距（基础工具响应结构）、分解差距（用线性状态锚定替代低效搜索）和扩展差距（并发性和持久性）。强制执行假设、探测和编码的科学方法论。

Result: 通过强制执行科学方法论，代理能够自主编写稳健的生产级代码技能，成功解决了技能生成中的四个关键瓶颈问题。

Conclusion: 通过科学方法论（假设、探测、编码）的强制执行，可以使代理自主生成生产级代码技能，实现从被动工具使用者到主动工作流架构师的转变，解决了技能自动生成中的关键结构瓶颈。

Abstract: While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.

</details>


### [30] [SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization](https://arxiv.org/abs/2512.20333)
*Junren Li,Luhua Lai*

Main category: cs.AI

TL;DR: SynCraft是一个基于推理的框架，将合成可行性优化重新定义为精确的结构编辑问题，利用大语言模型进行原子级编辑预测，显著提升生成分子的可合成性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在化学空间探索中存在瓶颈：大量生成的分子合成不可行。现有解决方案（如后过滤或基于模板的方法）往往牺牲结构新颖性或破坏关键药效团。

Method: 将合成可行性优化重构为精确的结构编辑问题，而非序列翻译任务。利用大语言模型的新兴推理能力，预测可执行的原子级编辑序列，避免直接生成SMILES字符串的语法脆弱性。

Result: 在广泛基准测试中，SynCraft在生成具有高结构保真度的可合成类似物方面优于最先进的基线方法。成功复制了专家药物化学直觉，编辑了PLK1抑制剂并拯救了先前分子生成文献中丢弃的高评分RIPK1候选物。

Conclusion: SynCraft通过推理引导的结构编辑有效解决了生成化学中的"合成悬崖"问题，在保持结构新颖性和药效团完整性的同时显著提升分子可合成性，为AI驱动的药物发现提供了新范式。

Abstract: Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the "synthesis cliff" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.

</details>


### [31] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Janus-Pro-CXR是一个基于DeepSeek Janus-Pro的胸部X光解读系统，通过多中心前瞻性临床试验验证，在报告生成质量、关键放射学发现检测和工作流程效率方面优于现有模型，特别适合资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺，特别是基层医疗中胸部X光工作量巨大。现有多模态大语言模型评估主要依赖自动化指标或回顾性分析，缺乏严格的前瞻性临床验证。

Method: 基于DeepSeek Janus-Pro模型开发Janus-Pro-CXR系统，通过多中心前瞻性临床试验（NCT07117266）进行严格验证。采用轻量级架构和领域特定优化，并与ChatGPT 4o等最先进模型进行比较。

Result: 系统在自动化报告生成方面优于现有模型，包括参数规模更大的ChatGPT 4o（200B参数），能可靠检测六种临床关键放射学发现。前瞻性临床部署中，AI辅助显著提高报告质量评分，减少18.3%的解读时间，54.3%的病例中专家更偏好AI辅助结果。

Conclusion: Janus-Pro-CXR通过轻量级架构和领域特定优化，提高了诊断可靠性和工作流程效率，特别适合资源有限环境。模型架构和实施框架将开源，促进AI辅助放射学解决方案的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [32] [Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems](https://arxiv.org/abs/2512.20387)
*YuChe Hsu,AnJui Wang,TsaiChing Ni,YuanFu Yang*

Main category: cs.AI

TL;DR: 提出VLSM模型，通过布局草图和自然语言提示生成可执行的FlexScript代码，实现工业仿真系统的跨模态推理


<details>
  <summary>Details</summary>
Motivation: 工业仿真系统需要将视觉布局和自然语言描述转化为可执行代码，传统方法缺乏跨模态推理能力，需要统一视觉和文本理解的新范式

Method: 提出Vision-Language Simulation Model (VLSM)，构建包含12万+提示-草图-代码三元组的大规模数据集，使用视觉编码器、连接器和代码预训练语言骨干进行系统消融实验

Result: 模型达到近乎完美的结构准确性和高执行鲁棒性，提出SVR、PMR、ESR三个专门评估指标，全面评估结构完整性、参数保真度和模拟器可执行性

Conclusion: 为生成式数字孪生奠定基础，将视觉推理和语言理解集成到可执行的工业仿真系统中

Abstract: We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.

</details>


### [33] [Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale](https://arxiv.org/abs/2512.20469)
*Linfeng Zhang,Siheng Chen,Yuzhu Cai,Jingyi Chai,Junhan Chang,Kun Chen,Zhi X. Chen,Zhaohan Ding,Yuwen Du,Yuanpeng Gao,Yuan Gao,Jing Gao,Zhifeng Gao,Qiangqiang Gu,Yanhui Hong,Yuan Huang,Xi Fang,Xiaohong Ji,Guolin Ke,Zixing Lei,Xinyu Li,Yongge Li,Ruoxue Liao,Hang Lin,Xiaolu Lin,Yuxiang Liu,Xinzijian Liu,Zexi Liu,Jintan Lu,Tingjia Miao,Haohui Que,Weijie Sun,Yanfeng Wang,Bingyang Wu,Tianju Xue,Rui Ye,Jinzhe Zeng,Duo Zhang,Jiahui Zhang,Linfeng Zhang,Tianhan Zhang,Wenchang Zhang,Yuzhi Zhang,Zezhong Zhang,Hang Zheng,Hui Zhou,Tong Zhu,Xinyu Zhu,Qingguo Zhou,Weinan E*

Main category: cs.AI

TL;DR: 论文提出Bohrium+SciMaster架构，通过基础设施和生态系统方法解决规模化智能体科学面临的挑战，实现科学工作流的可追溯、可复现和高效执行。


<details>
  <summary>Details</summary>
Motivation: AI智能体正在成为运行多步骤科学工作流的实用方式，但规模化智能体科学面临诸多挑战：工作流难以观察和复现、许多工具和实验室系统未准备好支持智能体、执行难以追溯和治理、原型系统往往是定制化的限制了重用和系统性改进。

Method: 提出Bohrium+SciMaster架构：Bohrium作为AI4S资产的托管可追溯中心，将科学数据、软件、计算和实验室系统转化为智能体就绪能力；SciMaster将这些能力编排为长视野科学工作流；在基础设施和编排之间，科学智能基板将可重用模型、知识和组件组织为可执行构建块。

Result: 在11个代表性主智能体的真实工作流中演示该架构，实现了端到端科学周期时间的数量级减少，并从真实工作负载中生成了数百万规模的执行基础信号。

Conclusion: 规模化智能体科学需要基础设施和生态系统方法，Bohrium+SciMaster架构通过提供可追溯、可组合、可审计的工作流执行环境，为智能体科学的规模化应用提供了可行路径。

Abstract: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.
  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.
  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.
  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.

</details>


### [34] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLMs）与小型语言模型（SLMs）在预测危重患者休克方面的表现，发现尽管GatorTron-Base获得最高加权召回率80.5%，但整体性能相当，表明LLMs在预测临床事件方面并不天然优于SLMs。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在自然语言处理领域的应用日益广泛，但在预测性任务方面的研究相对不足。本研究旨在探索LLMs在临床预测任务（如休克预测）中的表现，并与传统SLMs进行比较，以评估LLMs在医疗预测领域的实际价值。

Method: 研究使用MIMIC III数据库中17,294个ICU住院记录，筛选出住院时间>24小时且休克指数>0.7的患者，最终得到355名正常SI指数和87名异常SI指数患者。比较了GatorTron-Base（临床数据训练）、Llama 8B、Mistral 7B等LLMs与BioBERT、DocBERT、BioClinicalBERT、Word2Vec、Doc2Vec等SLMs。采用焦点损失和交叉熵损失处理类别不平衡问题。

Result: GatorTron-Base获得了最高的加权召回率80.5%，但整体性能指标显示LLMs和SLMs之间表现相当。这表明尽管LLMs在文本任务上表现出色，但在预测未来临床事件方面并不天然优于SLMs。

Conclusion: LLMs在临床预测任务中并不比SLMs具有固有优势。未来LLMs训练应重点关注开发能够预测临床轨迹的模型，而不是仅仅专注于命名实体识别或表型分类等简单任务，以实现有意义的临床结果。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [35] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 该论文构建了首个大规模教师多模态情感分析数据集T-MED，并提出基于非对称注意力的多模态教师情感分析模型AAM-TSA，显著提升了教师情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 教师情感状态对教学效果、学生参与度和学习成就具有重要影响，但现有研究未能准确捕捉教师情感，主要因为忽略了教学表演性和教学信息对情感表达的关键影响。

Method: 1) 构建T-MED数据集：采用人机协作标注流程，收集250个真实课堂的14,938个教师情感实例，涵盖11个学科，整合文本、音频、视频和教学信息；2) 提出AAM-TSA模型：引入非对称注意力机制和分层门控单元，实现差异化跨模态特征融合和精确情感分类。

Result: 实验结果表明，AAM-TSA模型在T-MED数据集上的准确性和可解释性显著优于现有最先进方法。

Conclusion: 该研究通过构建大规模多模态数据集和创新性模型，为教师情感分析提供了系统解决方案，能够更准确地识别教师情感状态，对提升教学质量具有重要意义。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


### [36] [Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent](https://arxiv.org/abs/2512.20586)
*Humza Nusrat,Luke Francisco,Bing Luo,Hassan Bagher-Ebadian,Joshua Kim,Karen Chin-Snyder,Salim Siddiqui,Mira Shah,Eric Mellon,Mohammad Ghassemi,Anthony Doemer,Benjamin Movsas,Kundan Thind*

Main category: cs.AI

TL;DR: 研究开发了基于大语言模型的SAGE系统用于脑转移瘤立体定向放射外科自动计划，发现推理模型在保持主要剂量指标与人工计划相当的同时，能降低耳蜗剂量，并通过思维链推理提供可审计的计划过程。


<details>
  <summary>Details</summary>
Motivation: 立体定向放射外科需要精确的剂量分布，但黑盒AI系统由于透明度问题在临床应用中受限。研究旨在探索思维链推理是否能改善基于大语言模型的自动计划系统的性能和透明度。

Method: 开发了SAGE系统，在41例接受18Gy单次分割SRS治疗的脑转移瘤患者回顾性队列中进行测试。比较了推理模型和非推理模型两个变体生成的计划，评估主要终点指标（靶区覆盖率、最大剂量、适形指数、梯度指数）和耳蜗剂量。

Result: 推理模型在主要剂量指标上与人工计划相当（所有p>0.21），同时显著降低了耳蜗剂量（p=0.022）。当要求改善适形性时，推理模型表现出前瞻性约束验证（457次）和权衡考量（609次）等系统性计划行为，而非推理模型几乎没有这些思考过程。

Conclusion: 思维链推理不仅改善了基于大语言模型的自动计划系统的剂量学性能，还通过提供优化轨迹作为可审计日志，为实现透明化自动计划提供了路径。

Abstract: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.

</details>


### [37] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu,Ziyi Liu,Jiaqi Tang,Yue Ma,Renjie Pi,Jipeng Zhang,Qifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，通过主LLM协调定位代理和视觉代理，在长视频QA任务中实现更好的时间定位和细粒度推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM和工具系统在处理小时级长视频QA时，通常将内容压缩为有损摘要或依赖有限工具集，导致时间定位能力弱且错过细粒度线索。

Method: 提出多智能体框架：主LLM协调定位代理（定位问题相关片段）和视觉代理（提取针对性文本观察）。主智能体有步骤限制，通过强化学习训练以鼓励简洁、正确、高效的多智能体协作。

Result: 在提出的LongTVQA和LongTVQA+数据集上，多智能体系统显著优于强非智能体基线。实验显示强化学习进一步增强了训练智能体的推理和规划能力。

Conclusion: 该多智能体设计通过定位帮助主智能体聚焦相关片段，用视觉细节补充字幕，并产生可解释的轨迹，在长视频QA任务中表现出色。

Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [38] [Visual Event Detection over AI-Edge LEO Satellites with AoI Awareness](https://arxiv.org/abs/2512.19764)
*Chathuranga M. Wijerathna Basnayaka,Haeyoung Lee,Pandelis Kourtessis,John M. Senior,Vishalya P. Sooriarachchi,Dushantha Nalin K. Jayakody,Marko Beko,Seokjoo Shin*

Main category: cs.IT

TL;DR: 提出基于深度联合信源信道编码的AI原生LEO卫星下行链路方案，用于目标导向的视觉推理，相比传统分离编码方案具有更高推理精度、更低误分类信息年龄和更好的时效性


<details>
  <summary>Details</summary>
Motivation: 非地面网络（特别是LEO卫星系统）对支持未来关键任务应用（如灾害救援）至关重要。AI原生通信使LEO卫星能够作为智能边缘节点进行机上学习和任务导向推理，但有限的链路预算、严重的路径损耗和衰落显著限制了可靠的下行传输。

Method: 提出基于深度联合信源信道编码的下行链路方案，仅提取和传输语义相关特征，而传统分离信源信道编码传输原始图像数据。引入误分类信息年龄指标和基于阈值的AoI分析来评估信息新鲜度和视觉事件检测性能。

Result: 仿真结果表明，提出的DJSCC方案相比传统SSCC基线具有更高的推理精度、更低的平均AoMI和更好的阈值符合率，能够在AI原生LEO卫星网络中实现语义通信。

Conclusion: 该方案为6G及以后的AI原生LEO卫星网络实现了语义通信，通过深度联合信源信道编码优化目标导向的视觉推理，显著提升了系统性能。

Abstract: Non terrestrial networks (NTNs), particularly low Earth orbit (LEO) satellite systems, play a vital role in supporting future mission critical applications such as disaster relief. Recent advances in artificial intelligence (AI)-native communications enable LEO satellites to act as intelligent edge nodes capable of on board learning and task oriented inference. However, the limited link budget, coupled with severe path loss and fading, significantly constrains reliable downlink transmission. This paper proposes a deep joint source-channel coding (DJSCC)-based downlink scheme for AI-native LEO networks, optimized for goal-oriented visual inference. In the DJSCC approach, only semantically meaningful features are extracted and transmitted, whereas conventional separate source-channel coding (SSCC) transmits the original image data. To evaluate information freshness and visual event detection performance, this work introduces the age of misclassified information (AoMI) metric and a threshold based AoI analysis that measures the proportion of users meeting application specific timeliness requirements. Simulation results show that the proposed DJSCC scheme provides higher inference accuracy, lower average AoMI, and greater threshold compliance than the conventional SSCC baseline, enabling semantic communication in AI native LEO satellite networks for 6G and beyond.

</details>


### [39] [Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning](https://arxiv.org/abs/2512.19777)
*Antonio Tarizzo,Mohammad Kazemi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出一种基于学习的数字空中计算框架，通过端到端训练的解码器和码本设计，在联邦边缘学习中显著提升低信噪比下的恢复精度和鲁棒性，同时支持更广泛的对称函数聚合。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习中模型更新的重复上行传输导致通信成为主要瓶颈。现有的数字空中计算方案在低信噪比条件下性能受限，需要一种更鲁棒的设计来应对挑战性的无线环境。

Method: 提出学习型数字空中计算框架，结合无源随机接入码本、矢量量化和AMP-DA-Net解码器。该解码器基于展开的近似消息传递架构，与数字码本和参数服务器本地训练统计信息进行端到端训练。

Result: 实验表明，该设计将可靠数字空中计算操作扩展到低信噪比区域超过10dB，在全信噪比范围内匹配或改进性能。学习型解码器在消息损坏和非线性聚合下仍保持有效。

Conclusion: 该学习型数字空中计算框架显著提升了联邦边缘学习中的通信效率和鲁棒性，展示了端到端学习设计在数字空中通信中的广泛潜力，支持包括修剪均值和多数规则在内的多种对称函数聚合。

Abstract: Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.

</details>


### [40] [Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models](https://arxiv.org/abs/2512.20108)
*Yuntong Gu,Xiangming meng,Zhiyuan Lin,Sheng Wu,Linling Kuang*

Main category: cs.IT

TL;DR: 提出统一的扩散贝叶斯框架，联合解决频谱重构和主动感知问题，通过条件生成过程和不确定性感知的主动采样策略，显著提升重构精度和采样效率。


<details>
  <summary>Details</summary>
Motivation: 高保真频谱制图对频谱管理和无线态势感知至关重要，但由于观测数据稀疏且不规则，这是一个具有挑战性的不适定逆问题。现有方法通常将重构与感知解耦，缺乏信息采样的原则性机制。

Method: 提出统一的扩散贝叶斯框架，将重构任务建模为由学习扩散先验驱动的条件生成过程。推导了反向扩散过程的可处理闭式后验转移核，同时兼容线性高斯和非线性量化测量。利用扩散模型固有的概率特性，开发了不确定性感知的主动采样策略。

Result: 大量实验表明，该框架在重构精度、采样效率和低比特量化鲁棒性方面显著优于最先进的插值、基于稀疏性和深度学习的基线方法。

Conclusion: 该论文提出的统一扩散贝叶斯框架成功解决了频谱重构和主动感知的联合问题，通过条件生成和不确定性感知采样实现了高效的频谱制图，为频谱管理和无线态势感知提供了有效的解决方案。

Abstract: High-fidelity spectrum cartography is pivotal for spectrum management and wireless situational awareness, yet it remains a challenging ill-posed inverse problem due to the sparsity and irregularity of observations. Furthermore, existing approaches often decouple reconstruction from sensing, lacking a principled mechanism for informative sampling. To address these limitations, this paper proposes a unified diffusion-based Bayesian framework that jointly addresses spectrum reconstruction and active sensing. We formulate the reconstruction task as a conditional generation process driven by a learned diffusion prior. Specifically, we derive tractable, closed-form posterior transition kernels for the reverse diffusion process, which enforce consistency with both linear Gaussian and non-linear quantized measurements. Leveraging the intrinsic probabilistic nature of diffusion models, we further develop an uncertainty-aware active sampling strategy. This strategy quantifies reconstruction uncertainty to adaptively guide sensing agents toward the most informative locations, thereby maximizing spectral efficiency. Extensive experiments demonstrate that the proposed framework significantly outperforms state-of-the-art interpolation, sparsity-based, and deep learning baselines in terms of reconstruction accuracy, sampling efficiency, and robustness to low-bit quantization.

</details>


### [41] [RIS-Empowered OTFS Modulation With Faster-than-Nyquist Signaling in High-Mobility Wireless Communications](https://arxiv.org/abs/2512.20332)
*Chaorong Zhang,Benjamin K. Ng,Hui Xu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 提出了一种结合可重构智能表面、正交时频空间调制和超奈奎斯特信号的新型无线通信方案，用于高移动性环境，提升可靠性和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 高移动性无线通信系统面临严重的多普勒扩展和多径延迟，传统调制方案可靠性和频谱效率下降。OTFS调制在延迟-多普勒域具有强鲁棒性，FTN信号可进一步提升频谱效率，RIS可通过无源波束成形改善链路质量。

Method: 提出RIS-OTFS-FTN方案，建立统一的DD域输入输出关系模型，综合考虑RIS无源波束成形、FTN引起的符号间干扰和DD域信道特性。设计实用的RIS相位调整策略，采用量化相位选择最大化有效信道增益。

Result: 在标准化EVA信道模型下进行大量蒙特卡洛仿真，验证了理论结果，揭示了频谱效率、PAPR、输入回退和误码性能之间的权衡关系。该方案在可靠性和频谱效率方面均表现出显著性能增益。

Conclusion: RIS-OTFS-FTN方案为未来高移动性和频谱受限的无线系统提供了可行的解决方案，在可靠性和频谱效率方面均有显著提升。

Abstract: High-mobility wireless communication systems suffer from severe Doppler spread and multi-path delay, which degrade the reliability and spectral efficiency of conventional modulation schemes. Orthogonal time frequency space (OTFS) modulation offers strong robustness in such environments by representing symbols in the delay-Doppler (DD) domain, while faster-than-Nyquist (FTN) signaling can further enhance spectral efficiency through intentional symbol packing. Meanwhile, reconfigurable intelligent surfaces (RIS) provide a promising means to improve link quality via passive beamforming. Motivated by these advantages, we propose a novel RIS-empowered OTFS modulation with FTN signaling (RIS-OTFS-FTN) scheme. First, we establish a unified DD-domain input-output relationship that jointly accounts for RIS passive beamforming, FTN-induced inter-symbol interference, and DD-domain channel characteristics. Based on this model, we provide comprehensive analytical performance for the frame error rate, spectral efficiency, and peak-to-average power ratio (PAPR), etc. Furthermore, a practical RIS phase adjustment strategy with quantized phase selection is designed to maximize the effective channel gain. Extensive Monte Carlo simulations under a standardized extended vehicular A (EVA) channel model validate the theoretical results and provide key insights into the trade-offs among spectral efficiency, PAPR, input back-off (IBO), and error performance, with some interesting insights.The proposed RIS-OTFS-FTN scheme demonstrates notable performance gains in both reliability and spectral efficiency, offering a viable solution for future high-mobility and spectrum-constrained wireless systems.

</details>


### [42] [Viterbi State Selection for Discrete Pinching Antenna Systems](https://arxiv.org/abs/2512.20389)
*Victoria E. Galanopoulou,Thrassos K. Oikonomou,Odysseas G. Karagiannidis,Sotiris A. Tegos,Panagiotis D. Diamantoulakis*

Main category: cs.IT

TL;DR: 提出基于Viterbi状态选择(VSS)算法解决波导馈电夹持天线阵列的天线子集选择问题，将计算复杂度从指数级降低到多项式级，同时保持与穷举搜索相同的性能。


<details>
  <summary>Details</summary>
Motivation: 夹持天线阵列的天线子集选择是一个组合优化问题，具有指数复杂度。在波导馈电夹持天线阵列服务地面用户的场景中，由于可达速率对激活天线相对相位对齐高度敏感，需要高效算法来解决穷举搜索的过高计算复杂度。

Method: 提出Viterbi状态选择(VSS)算法，利用组合接收信号的相位结构。将网格状态定义为累积复增益相位的量化表示，使用基于Viterbi的幸存规则在不同阶段剪枝被支配的天线子集。

Result: 数值结果表明，所提方法能够实现与穷举搜索相同的天线选择和速率性能，同时将计算复杂度从天线数量的指数级降低到多项式级。

Conclusion: VSS算法有效解决了夹持天线阵列天线子集选择的计算复杂度问题，为实际系统部署提供了可行的解决方案，在保持性能的同时显著降低了计算负担。

Abstract: Pinching antennas enable dynamic control of electromagnetic wave propagation through reconfigurable radiating structures, but selecting an optimal subset of antennas remains a combinatorial problem with exponential complexity. This letter considers antenna subset selection for a waveguide-fed pinching antenna array serving ground users under a time-division access scheme. The achievable rate depends on the coherent superposition of the effective complex channel gains and is therefore highly sensitive to the relative phase alignment of the activated antennas. To address the prohibitive complexity of exhaustive search, we propose a Viterbi state selection (VSS) algorithm that exploits the phase structure of the combined received signal. The trellis state is defined by a quantized representation of the phase of the accumulated complex gain, and a Viterbi-based survivor rule is used to prune dominated antenna subsets across stages. Numerical results demonstrate that the proposed method achieves the same antenna selection and rate as exhaustive search, while reducing the computational complexity from exponential to polynomial in the number of available antennas.

</details>


### [43] [Information-theoretic signatures of causality in Bayesian networks and hypergraphs](https://arxiv.org/abs/2512.20552)
*Sung En Chiang,Zhaolu Liu,Robert L. Peach,Mauricio Barahona*

Main category: cs.IT

TL;DR: 该论文建立了部分信息分解（PID）与因果结构之间的理论对应关系，证明PID组件可以精确表征贝叶斯网络和超图中的因果结构，为因果发现提供了局部信息论基础。


<details>
  <summary>Details</summary>
Motivation: 现有高阶信息理论工具（如PID）虽然能分解信息为冗余、独特和协同组件，但这些数学度量与因果结构之间的理论联系尚未建立，限制了其在因果发现中的应用。

Method: 通过理论分析建立PID组件与因果结构的对应关系：在贝叶斯网络中，独特信息表征直接因果邻居，协同信息识别碰撞关系；在超图中，PID特征能区分父母、子女、共头和共尾节点，揭示多尾超边特有的高阶碰撞效应。

Result: 证明了PID组件能精确表征因果结构：独特信息对应直接因果邻居，协同信息对应碰撞关系。在超图中能区分不同类型的节点关系，并发现了多尾超边特有的高阶碰撞效应。提出了基于这些结果的因果结构系统表征方法。

Conclusion: PID为推断成对和高阶因果结构提供了严谨、模型无关的基础，引入了基于局部信息论的因果发现新范式，无需全局搜索图空间，直接从变量的局部信息足迹恢复其周围结构。

Abstract: Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery.

</details>
