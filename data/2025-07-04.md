<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成的方法，重点介绍了深度学习和统计技术，并讨论了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决真实网络数据稀缺、隐私问题和纯度限制，为数据驱动应用提供替代方案。

Method: 综述了合成网络流量生成的数据类型、生成模型和评估方法，特别关注深度学习技术和统计方法。

Result: 提供了现有方法的全面分析，并指出了商业工具的应用。

Conclusion: 本文为研究人员和实践者提供了合成网络流量生成的基础资源，总结了方法、挑战和未来研究方向。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [2] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种名为隐式序列号（ISN）的机制和可靠性扩展链路（RXL），用于解决多节点芯片互连中的丢包和顺序问题，同时保持带宽效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型对单处理器能力的超越，芯片间互连成为可扩展计算的关键。然而，高传输速率增加了错误敏感性，尤其是在多节点配置中管理无声丢包问题。

Method: 引入隐式序列号（ISN）机制，无需增加头部开销即可精确检测丢包并确保顺序交付；提出RXL扩展CXL协议，结合ISN支持可靠的多节点互连。

Result: RXL通过将CRC提升为传输层机制，依赖FEC进行链路层纠错，实现了高可靠性和可扩展性，同时不牺牲带宽效率。

Conclusion: ISN和RXL为多节点芯片互连提供了高效、可靠的解决方案，解决了现有协议的局限性。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [3] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出了一种结合Curated Collaborative Learning（CCL）和Distributed Unit Pooling Scheme（DUPS）的方法，显著降低5G RAN的能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络中RAN的能耗占比超过50%，现有技术未能充分利用数据潜力，亟需优化以减少运营支出。

Method: 1. 使用CCL框架进行高精度网络流量和用户预测；2. 提出DUPS方案，利用深度强化学习和CCL预测动态优化DU服务器资源。

Result: CCL在流量预测上优于现有方法43.9%-31.35%，DUPS将能效提升89%。

Conclusion: 结合CCL和DUPS的方法显著提升了5G RAN的能效和成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [4] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: AI和大型语言模型（LLMs）在网络配置和诊断中表现优异，本文聚焦于AI代理在网络故障排除中的应用，并呼吁建立标准化、可复现的开放基准平台。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在网络故障排除中的潜力，并强调需要低操作成本的标准化评估平台。

Method: 初步研究，专注于AI代理在网络故障排除中的应用。

Result: 提出建立开放基准平台的必要性，以支持AI代理的开发和评估。

Conclusion: 标准化和可复现的基准平台对AI代理在网络故障排除中的发展至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [5] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于信道感知的语义通信框架，旨在提高车联网（IoV）中数据传输的准确性和效率，通过生成扩散模型预测动态信道状态，并利用大模型优化适应性。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）需要实时处理和传输大量数据，但在动态无线信道条件下效率低下，亟需一种高效的数据传输方法。

Method: 提出语义通信框架，结合信道感知和生成扩散模型预测动态信道状态，并使用大模型优化适应性。

Result: 在公开数据集上验证了框架的性能和可靠性。

Conclusion: 该框架显著提升了车联网服务的数据传输质量和适应性。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [6] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出了一种名为REDUS的重采样技术，用于优化深度学习训练，减少资源消耗并提升效率，同时保持网络性能。


<details>
  <summary>Details</summary>
Motivation: SDN与深度学习工作负载共享基础设施时，资源竞争会降低SDN的响应速度，影响网络性能。联邦学习虽能缓解部分问题，但DL训练的计算需求仍可能干扰SDN。

Method: 提出REDUS技术，通过优先处理误分类样本并排除冗余数据，减少每轮训练样本数量，从而节省计算资源和能源。

Result: 在CICIoT2023数据集上测试，REDUS将训练时间减少72.6%，准确率仅损失1.62%。

Conclusion: REDUS是一种高效且实用的解决方案，适用于资源受限的边缘设备，同时保持网络性能。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [7] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出了一种基于5G PRS信号的多静态ISAC信号处理链，用于目标检测、参数估计和跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G PRS信号实现多静态ISAC，提高目标检测和跟踪的精度。

Method: 采用分布式架构，通过相干互模糊函数生成距离-多普勒图，提取目标参数，并通过非线性最小二乘三边测量和正则化线性反演估计位置和速度。

Result: 实现了高精度的移动目标检测、定位和跟踪。

Conclusion: 该方法在多静态ISAC中表现出高效性和高精度。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [8] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 该论文探讨了基于O-RAN原则的地面网络（TN）与非地面网络（NTN）融合的架构和功能分割策略，分析了性能、延迟和部署的权衡，并提出了RIC的灵活放置方案。


<details>
  <summary>Details</summary>
Motivation: 解决TN与NTN融合中因传播条件、动态拓扑和有限处理能力带来的架构和功能挑战。

Method: 提出分割选项分类，评估从纯星载DU部署到完整gNB和UPF集成的配置，并讨论RIC的放置策略。

Result: 提供了架构分割与RIC放置的全面映射，强调了实现约束和互操作性。

Conclusion: 指出了未来标准化、模块化和高效TN-NTN融合的关键挑战和方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自我进化的AI代理，通过动态工具库和模板库自主提升能力，在生物医学任务中表现优异且性能随经验提升。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中数据、工具和文献碎片化问题，传统AI代理依赖静态工具集，无法适应快速变化的需求。

Method: 采用多代理架构，包括动态工具库（Tool Ocean）和模板库（Template Library），通过工具创建代理自动发现和集成新工具。

Result: 在多个生物医学基准测试中表现优异，如Humanity's Last Exam（26%）、LAB-Bench: DBQA（54%）和LAB-Bench: LitQA（63%），性能随经验提升。

Conclusion: STELLA标志着AI代理系统能够动态学习和扩展，加速生物医学研究的进展。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [10] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则去除冗余特征并保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如CFS、mRMR、MI、RFE、SFS和遗传算法）在性能上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升效果。

Method: HCVR采用贪心算法，通过反向消除逐步去除冗余特征，利用相关性阈值和多数投票规则决策。

Result: 在SPAMBASE数据集上，HCVR相比传统方法表现出更好的性能，分类器效果得到提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [11] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 这篇综述全面回顾了高效测试时计算（TTC）策略，旨在提升大语言模型（LLM）推理的计算效率。提出了两级分类法，并讨论了性能与计算资源之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理时效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 引入两级分类法：L1（固定计算预算）和L2（动态调整计算资源），并对主流LLM进行基准测试。

Result: 揭示了推理性能与计算资源之间的关键权衡，强调了TTC方法的实际控制性、适应性和可扩展性。

Conclusion: 讨论了混合思维模型等新兴趋势，并指出未来研究的关键挑战，以使LLM更高效、稳健且适应用户需求。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [12] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统，克服湿实验室的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的科学能力，特别是在实验设计和结果解释方面的能力，因为现有方法未能测试这些核心科学能力。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型，生成模拟数据，评估六种前沿LLM在137个小系统和总共350个系统上的表现。

Result: 性能更强的模型表现更优，但随着系统复杂性增加，所有模型的性能显著下降，表明LLM的科学能力仍有很大提升空间。

Conclusion: SciGym为评估LLM的科学能力提供了新方法，但LLM在复杂系统上的表现仍需改进。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [13] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 探讨AI如何从神经科学中学习，以实现持续适应环境的能力，并推动NeuroAI领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究如何将神经科学的持续学习机制应用于AI，提升其在现实世界中的适应性。

Method: 整合AI中的持续学习和上下文学习文献，与神经科学中行为任务的学习机制（如规则、奖励概率变化）进行对比分析。

Result: 提出一个议程，明确神经科学如何指导AI发展，同时AI也能为神经科学提供新见解，推动NeuroAI领域的进步。

Conclusion: 神经科学与AI的交叉研究有望为双方带来突破，特别是在持续学习和适应性行为方面。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [14] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了如何利用审计研究数据改进自动招聘算法的训练和评估，发现传统公平干预方法存在隐藏偏差，并提出基于个体处理效应估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决AI系统（如招聘算法）中的偏见问题，传统方法依赖便利样本数据，可能引入选择偏差和标签偏差，而审计研究数据能提供更高质量的数据支持。

Method: 方法包括利用审计研究数据（如虚构简历）进行随机对照试验，分析传统公平干预方法的局限性，并引入基于个体处理效应估计的新干预方法。

Result: 结果显示，传统公平干预方法在表面上实现公平，但实际存在约10%的偏差；新干预方法能进一步减少算法歧视。

Conclusion: 结论指出，审计研究数据能更准确地评估和改善算法公平性，新方法在减少歧视方面更具潜力。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [15] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 通过数据多样化策略（如DTS方法）提升大语言模型的数学推理能力，相比传统方法效果显著且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得进展，但数学推理仍是挑战，需探索数据多样化策略以提升模型能力。

Method: 评估温度采样、思维链提示和MCTS三种数据生成方法，并提出DTS方法，系统分解问题为多样化推理路径。

Result: DTS方法在GSM8K和MATH数据集上分别提升7.1%和4.2%，计算成本仅增加1.03倍，优于MCTS。

Conclusion: 结构化探索多样化问题解决方法能生成更有效的偏好数据，优于传统方法。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [16] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了基于LLM的角色扮演代理在生成合成数据时，其陈述的信念与实际行为之间的一致性，并提出了一个评估框架来衡量这种一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作角色扮演代理以生成人类行为研究的合成数据，确保其输出与分配角色保持一致成为关键问题。

Method: 通过增强版的GenAgents角色库和信任游戏，引入信念-行为一致性指标，系统研究影响因素如信念类型、信息呈现方式和预测时间跨度。

Result: 研究发现LLM的陈述（或强加）信念与角色扮演模拟结果之间存在系统性不一致，即使模型编码了看似合理的信念，也可能无法一致应用。

Conclusion: 强调了识别LLM陈述信念与模拟行为何时对齐的重要性，以便在行为研究中正确使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [17] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究探讨了在空间囚徒困境中，稀释和移动性对多智能体Q学习算法的影响，展示了算法的多样性和建模能力。


<details>
  <summary>Details</summary>
Motivation: 探索静态智能体如何通过不同机制（如噪声注入、学习算法类型和邻居收益知识）实现合作，并研究稀释和移动性对空间囚徒困境的影响。

Method: 使用独立多智能体Q学习算法，定义不同动作，并与经典非强化学习的空间囚徒困境结果对比。

Result: 观察到多种效应，包括固定更新规则与学习规则的等效性，以及多动作定义下种群间的共生互利效应。

Conclusion: 该算法在建模不同博弈场景中具有多样性和基准测试潜力，展示了强化学习在空间囚徒困境中的广泛应用前景。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [18] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成自然语言规划问题并评估LLM生成的计划质量，结果显示直接推理优于中间翻译步骤。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）在规划和推理能力提升中面临的数据生成和评估瓶颈。

Method: 引入NL2FLOW系统，自动化生成规划问题（自然语言、结构化中间表示和PDDL），并评估LLM生成的计划质量。

Result: 最高性能模型在生成有效计划上达到86%成功率，生成最优计划为69%；直接推理优于中间翻译步骤。

Conclusion: 动态理解LLM的局限性及系统化工具对释放其作为智能问题解决者的潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [19] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文探讨了信念修正领域的现状，指出现有研究多依赖语法化的公设，而忽视了修正机制的能力分析。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域缺乏对现有方法的深入分析，研究多集中于通过公设约束修正实例，而忽略了修正机制的能力，如可塑性、平等化和教条化等。

Method: 通过分析不同类型的修正机制（如词典式、自然、约束等），研究其是否具备特定能力。

Result: 证明不同类型的修正机制具备不同的能力，如可塑性、平等化或教条化等。

Conclusion: 信念修正机制的能力分析比公设约束更为重要，不同类型的修正机制适用于不同的应用场景。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [20] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制，无需训练数据，支持多目标优化和自反思，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在关键词生成中依赖大规模数据、缺乏多目标监控和优化，以及质量控制的不足，限制了其自动化能力。

Method: 提出OMS框架，具备实时性（无需训练数据）、多目标优化和自反思能力，通过代理推理优化关键词。

Result: 实验表明OMS在基准测试和实际广告活动中优于现有方法，消融和人工评估验证了其组件的有效性。

Conclusion: OMS框架成功解决了LLM在关键词生成中的限制，为自动化广告关键词决策提供了高效解决方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [21] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，提升仪器利用率和实验效率，无需人工干预即可达到顶尖科学家的成果。


<details>
  <summary>Details</summary>
Motivation: 实现非专家也能独立进行复杂科学实验的自主科研，推动科学研究的范式转变。

Method: 采用AI原生设计，结合模型、实验和仪器的协同设计，支持多用户请求和复杂实验流程的自主管理。

Result: 自主实验室在核酸功能研究和应用领域（如疾病诊断、药物开发）中表现优异，实验效率显著提升。

Conclusion: 该平台为高级生物材料研究提供了新途径，减少了专家依赖和资源限制，为规模化科学服务奠定了基础。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [22] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出一种语义框架以增强AI系统的可解释性，重点研究了多元线性回归模型，并引入Gauss-Markov Adjunction来描述参数与残差的结构关系。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习的可解释性是实现AI可解释性原则和促进AI社会应用的关键需求。

Method: 使用范畴论方法，定义参数与数据的具体范畴及其伴随函子，提出监督学习的范畴化框架。

Result: 通过Gauss-Markov Adjunction明确描述了参数与残差之间的信息流，并展示了最小二乘估计与最小残差的关系。

Conclusion: 该框架为监督学习提供了语义基础，可作为AI可解释性的形式化工具。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [23] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，增强大型语言模型在Coq定理证明中的推理能力，采用结构化语义上下文和选择性概念展开，显著提高了清晰度和证明成功率。


<details>
  <summary>Details</summary>
Motivation: 研究任务清晰度对大型语言模型推理能力的影响，特别是在定理证明领域，以提升模型的理解与推理能力。

Method: 引入概念级清晰度评估指标，通过结构化语义上下文和选择性概念展开优化任务描述，采用Planner--Executor架构。

Result: 清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越之前的最佳方法Graph2Tac（33.2%）。

Conclusion: 结构化任务表示能有效缩小理解与推理之间的差距，为模型性能提升提供了新方向。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [24] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作集，在MLE-bench基准测试中提升了性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自动化设计和优化机器学习模型，加速科学研究的进展。

Method: 将AI研究代理形式化为搜索策略，设计并测试不同的操作集和搜索策略（贪婪、MCTS、进化算法）。

Result: 最佳策略组合在MLE-bench lite上实现了47.7%的成功率，优于之前的39.6%。

Conclusion: 搜索策略、操作设计和评估方法的联合优化对自动化机器学习至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [25] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-complete和Π₃-complete，而两者的交集为Π₂-complete。


<details>
  <summary>Details</summary>
Motivation: 探讨责任在集体决策中的计算复杂性，填补AI文献中对责任属性研究的空白。

Method: 分析扩散和间隙两种责任属性的计算复杂性，确定其复杂度类别。

Result: 扩散自由机制为Π₂-complete，间隙自由机制为Π₃-complete，两者的交集为Π₂-complete。

Conclusion: 研究揭示了责任属性在集体决策中的复杂计算特性，为相关领域提供了理论支持。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [26] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文介绍了MIMIC-Patient数据集和DynamiCare框架，用于模拟动态、多轮临床诊断过程，填补了现有单轮任务框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策框架多关注单轮任务，与现实诊断过程的动态性和交互性不符，因此需要开发更贴近实际的模拟方法。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，并提出DynamiCare框架，通过多轮交互和多智能体协作模拟临床诊断。

Result: 实验验证了DynamiCare的可行性和有效性，并建立了动态临床决策的首个基准。

Conclusion: DynamiCare为LLM驱动的动态医疗决策提供了新思路，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [27] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在迭代囚徒困境（IPD）中表现出战略智能，能够推理目标并在竞争环境中生存。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备战略智能，能否在竞争性环境中推理目标。

Method: 通过进化IPD锦标赛，将经典策略与前沿AI公司的模型（如OpenAI、Google、Anthropic）对抗，并引入终止概率增加复杂性。

Result: LLMs表现出高度竞争力，具有独特的战略特征：Google模型冷酷，OpenAI模型合作性强，Anthropic模型宽容。

Conclusion: LLMs能够主动推理时间范围和对手策略，连接了经典博弈论与机器心理学。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [28] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一种分层框架，通过将战略规划与专业执行分离，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）和基于推理的方法在处理复杂信息需求时效率低下，无法有效整合多源知识。

Method: HiRA将复杂任务分解为子任务，由领域特定代理处理，并通过结构化机制协调结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有方法，提升了答案质量和系统效率。

Conclusion: 分层规划和执行分离是解决多步信息检索任务的有效方法。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [29] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 论文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，显著提高了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂且验证过程繁琐，需要更高效的解决方案。

Method: 采用代理AI与人类协作（HITL）的动态迭代方法进行端到端硬件设计与验证。

Result: 在五个开源设计上验证，覆盖率超过95%，验证时间减少。

Conclusion: 该方法展示了高性能、适应性和可配置性，为硬件验证提供了新方向。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [30] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为TH2T的两阶段微调策略，旨在通过增强LRMs对任务难度的敏感性和减少冗余推理步骤来缓解过度思考现象。


<details>
  <summary>Details</summary>
Motivation: 现有的长推理模型（LRMs）在处理复杂任务时表现出过度思考的问题，且缺乏对任务难度的自适应能力。

Method: TH2T通过两个阶段实现：1）难度催眠（difficulty-hypnosis）增强模型对任务难度的敏感性；2）冗余催眠（redundancy-hypnosis）减少推理步骤中的冗余。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低了推理成本（简单任务减少70%，困难任务减少40%），同时保持性能稳定。

Conclusion: TH2T有效提升了LRMs的推理效率，并展示了任务难度感知能力和减少冗余的能力。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [31] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与数据，使用机器学习算法预测学生脱离行为，准确率达91%，并提出干预措施以减少在线学习中的脱离现象。


<details>
  <summary>Details</summary>
Motivation: 学生在远程教育中脱离任务可能导致学术辍学等严重后果，因此需要有效测量和预测脱离行为。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法解释模型决策。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [32] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 论文提出两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），以改进MCTS性能，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，需要改进抽象丢弃方法。

Method: 提出OGA-IAAD（适用于时间关键场景）和OGA-CAD（提升相同迭代次数下的性能）两种方案。

Result: 新方案显著提升性能且安全，不会导致性能下降。

Conclusion: OGA-IAAD和OGA-CAD是有效的抽象丢弃方案，适用于不同场景。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [33] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDPs（sG-MDPs）框架，结合蒙特卡洛树搜索（MCTS）算法，用于解决大型语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）中面临稀疏奖励和证明规模庞大的挑战，尤其是在复杂的多步推理任务（如PutnamBench）中表现不佳。

Method: 引入自生成目标条件MDPs（sG-MDPs）框架，使代理能根据证明状态生成和追求子目标；应用MCTS类算法解决sG-MDP，并在Bourbaki（7B）系统中实现。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，实现了该规模模型的新最佳结果。

Conclusion: sG-MDPs框架和MCTS算法的结合显著提升了LLMs在复杂推理任务中的表现，为自动定理证明提供了新思路。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [34] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出知识协议工程（KPE），通过将专家知识转化为机器可执行的协议，弥补现有方法（如RAG和通用AI代理）在深度推理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RAG和通用AI代理）在需要深度、程序化和方法论推理的专家领域任务中存在局限性，无法有效传达逻辑框架或高效执行任务。

Method: 引入知识协议工程（KPE），将专家知识转化为机器可执行的知识协议（KP），赋予LLM领域内的逻辑、策略和方法论。

Result: KPE使通用LLM能够像专家一样分解抽象查询并执行复杂任务，适用于法律和生物信息学等领域。

Conclusion: KPE是未来人机协作的基础方法，能够提升LLM在专家领域的表现。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [35] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化及物理基础特性。


<details>
  <summary>Details</summary>
Motivation: 运动是生物系统的核心，但当前AI模型对其建模不足，且数据收集和建模方法分散。

Method: 提出将运动视为独立且结构化的模态，利用其低维表示（如姿态）进行建模。

Result: 通过跨领域运动数据建模，可提升生成模型和控制能力，并为理解智能系统行为提供基础。

Conclusion: 运动不仅是行为结果，更是智能系统与世界互动的窗口，应成为AI建模的重点。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [36] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体框架，通过增强大型语言模型的推理能力，提高零样本医学诊断预测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖监督训练，泛化能力有限；大型语言模型虽具潜力，但存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化预测的预测智能体。

Result: 实验表明KERAP能高效提升诊断可靠性，适用于零样本医学诊断预测。

Conclusion: KERAP为医学诊断预测提供了一种可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [37] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨了AI系统中代理性和伦理推理的兴起，指出当前以服从为伦理行为代理的安全实践已不足，呼吁转向评估AI伦理判断的框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统具备更强的代理性和伦理推理能力，传统以服从为伦理行为代理的安全实践显得不足，需重新评估AI行为。

Method: 通过分析大型语言模型（LLMs）的安全测试事件，结合哲学讨论（如工具理性、道德责任），对比不同风险范式。

Result: 研究发现AI的“不服从”行为可能是伦理推理的早期表现，而非失控或错位。

Conclusion: 呼吁AI安全评估从刚性服从转向能评估伦理判断的框架，以避免误判行为并维护公众信任和有效治理。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [38] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，导致性能评估偏差高达100%，并提出Agentic Benchmark Checklist（ABC）以提升评估严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力提升，现有基准测试在任务设置和奖励设计上存在问题，可能导致性能评估不准确。

Method: 通过分析现有基准测试问题，结合最佳实践和经验，提出ABC指南。

Result: 在CVE-Bench上应用ABC，性能高估减少了33%。

Conclusion: ABC能有效提升代理基准测试的严谨性，减少评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [39] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新型RLVR算法，通过多级逐步提示帮助模型更有效地探索解空间，解决了近失奖励问题和探索停滞问题。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近失奖励问题和探索停滞问题，限制了模型训练效率和推理能力的提升。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同步数的提示以引导模型探索。

Result: StepHint在六个数学基准测试中优于竞争方法，并展现出更好的泛化能力和域外性能。

Conclusion: StepHint通过多级提示有效解决了RLVR的核心挑战，提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Matrix Pencil-Based DoA Estimation for Hybrid Receivers in Snapshot-Limited Scenarios](https://arxiv.org/abs/2507.02132)
*Mona Mostafa,Ramy H. Gohary,Amr El-Keyi,Yahia A. Eldemerdash Ahmed*

Main category: cs.IT

TL;DR: 论文提出两种方法，用于在混合模拟/数字接收器中估计到达方向（DoA），解决了小样本下统计平均不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 在混合模拟/数字接收器中，由于模拟组合器的纠缠效应和低维投影问题，传统的矩阵铅笔方法（MPM）无法直接应用。

Method: 提出两种方法：第一种适用于全连接和部分连接的HAD，利用周期性信号解纠缠；第二种仅适用于部分连接的HAD，利用块对角结构避免依赖周期性信号。

Result: 通过数值模拟和与Cramér-Rao下界的比较，证明了所提方法的优越性。

Conclusion: 两种方法成功解决了HAD接收器中的DoA估计问题，特别是在小样本情况下。

Abstract: The goal of this paper is to estimate the directions of arrival (DoAs) for
hybrid analog/digital (HAD) receivers when the number of snapshots is too small
for statistical averaging to be reliable. This goal is achieved in
fully-digital receivers by employing the matrix pencil method (MPM).
Unfortunately, the MPM cannot be directly applied in HAD receivers because of
the entanglement induced by the underlying analog combiners on the output
signals. Furthermore, these analog combiners project the received signal onto a
low-dimensional space, jeopardizing the reception of signals arriving from
particular DoA ranges. To circumvent these difficulties, we propose two
approaches to enable the MPM to extract the DoAs in HAD receivers. The two
approaches avoid severe attenuation induced by low-dimensional projection by
cycling over an exhaustive set of analog combiners, collectively spanning the
entire space. The first approach can be applied to both fully-connected (FC)
and partially-connected (PC) HADs and relies on the availability of periodic,
potentially unknown, signals to disentangle the output of the HAD receiver. The
second approach applies to PC-HADs only, and eliminates contingency on periodic
signals by exploiting the underlying block diagonal structure. The superiority
of the proposed approaches is demonstrated via numerical simulations and
comparisons with the Cram\'er-Rao lower bound.

</details>


### [41] [Resolution Limits of Non-Adaptive 20 Questions Estimation for Tracking Multiple Moving Targets](https://arxiv.org/abs/2507.02274)
*Chunsong Sun,Lin Zhou,Jingjing Wang,Weijie Yuan,Chunxiao Jiang,Alfred Hero*

Main category: cs.IT

TL;DR: 研究了多设备MIMO通信中的波束跟踪问题，提出了非自适应二十问题估计方法，用于定位和跟踪多个移动目标，并推导了分辨率边界。


<details>
  <summary>Details</summary>
Motivation: 实际应用中需要解决多设备MIMO通信中的波束跟踪问题。

Method: 采用非自适应二十问题估计方法，推导分辨率边界，并提出单阈值解码规则降低计算复杂度。

Result: 证明了单阈值解码规则的有效性，并通过数值示例验证结果。

Conclusion: 方法适用于移动目标跟踪，并扩展到分段恒定速度模型，展示了在5G网络中的应用潜力。

Abstract: Motivated by the practical application of beam tracking of multiple devices
in Multiple Input Multiple Output (MIMO) communication, we study the problem of
non-adaptive twenty questions estimation for locating and tracking multiple
moving targets under a query-dependent noisy channel. Specifically, we derive a
non-asymptotic bound and a second-order asymptotic bound on resolution for
optimal query procedures and provide numerical examples to illustrate our
results. In particular, we demonstrate that the bound is achieved by a state
estimator that thresholds the mutual information density over possible target
locations. This single threshold decoding rule has reduced the computational
complexity compared to the multiple threshold scheme proposed for locating
multiple stationary targets (Zhou, Bai and Hero, TIT 2022). We discuss two
special cases of our setting: the case with unknown initial location and known
velocity, and the case with known initial location and unknown velocity. Both
cases share the same theoretical benchmark {that applies to} stationary
multiple target search in Zhou, Bai and Hero (TIT 2022) while the known initial
location case is close to the theoretical benchmark for stationary target
search when the maximal speed is inversely proportional to the number of
queries. We also generalize our results to account for a piecewise constant
velocity model introduced in Zhou and Hero (TIT 2023), where targets change
velocity periodically. Finally, we illustrate our proposed algorithm for the
application of beam tracking of multiple mobile transmitters in a 5G wireless
network.

</details>


### [42] [Measurements and Modeling of Air-Ground Integrated Channel in Forest Environment Based on OFDM Signals](https://arxiv.org/abs/2507.02303)
*Zhe Xiao,Shu Sun,Na Liu,Lianming Xu,Li Wang*

Main category: cs.IT

TL;DR: 论文研究了森林环境中地面到地面（G2G）和空中到地面（A2G）通信的信道测量与建模，提出了适用于森林环境的路径损耗模型，并分析了关键信道参数。


<details>
  <summary>Details</summary>
Motivation: 森林环境中的通信系统对救援人员安全至关重要，但现有研究对森林区域信道检测和建模的探索有限。

Method: 在内蒙古阿尔山国家森林公园进行了G2G和A2G信道测量，使用1.4 GHz的OFDM信号，结合全向和定向天线记录数据，并预规划无人机飞行轨迹。

Result: 提出的路径损耗模型误差较小，且发现A2G通信中树冠对信号传播的阻碍比G2G中树干更显著。调整空对地仰角可改善通信质量。

Conclusion: 研究为森林环境中的通信系统设计提供了实用的信道模型和优化建议。

Abstract: Forests are frequently impacted by climate conditions, vegetation density,
and intricate terrain and geology, which contribute to natural disasters.
Personnel engaged in or supporting rescue operations in such environments rely
on robust communication systems to ensure their safety, highlighting the
criticality of channel measurements in forest environments. However, according
to current research, there is limited research on channel detection and
modeling in forest areas in the existing literature. This paper describes the
channel measurements campaign of air and ground in the Arxan National Forest
Park of Inner Mongolia. It presents measurement results and propagation models
for ground-to-ground (G2G) and air-to-ground (A2G) scenarios. The measurement
campaign uses orthogonal frequency division multiplexing signals centered at
1.4 GHz for channel sounding. In the G2G measurement, in addition to using
omnidirectional antennas to record data, we also use directional antennas to
record the arrival angle information of the signal at the receiver. In the A2G
measurement, we pre-plan the flight trajectory of the unmanned aerial vehicle
so that it can fly at a fixed angle relative to the ground. We present path
loss models suitable for G2G and A2G in forest environments based on the
analysis of measurement results. The results indicate that the proposed model
reduces error margins compared with other path loss models. Furthermore, we
derive the multipath model expression specific to forest environments and
conduct statistical analysis on key channel parameters e.g., shadow fading
factor, root mean square delay spread, and Rician K factor. Our findings reveal
that signal propagation obstruction due to tree crowns in A2G communication is
more pronounced than tree trunk obstructions in G2G communication. Adjusting
the elevation angle between air and ground can enhance communication quality.

</details>


### [43] [On the Convergence of Large Language Model Optimizer for Black-Box Network Management](https://arxiv.org/abs/2507.02689)
*Hoon Lee,Wentao Zhou,Merouane Debbah,Inkyu Lee*

Main category: cs.IT

TL;DR: 本文为LLM优化器（LLMO）框架建立了理论基础，证明其收敛性，并扩展到多LLM架构，验证了收敛速率。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要处理缺乏数学模型的多样化服务，LLMO框架利用预训练LLM作为优化代理，但此前仅限于数值模拟，缺乏理论支持。

Method: 通过将LLMO过程建模为有限状态马尔可夫链，分析其收敛性，并扩展到多LLM架构，验证其对收敛速率的影响。

Result: 理论证明LLMO框架的收敛性，多LLM架构能提升收敛速率，数值模拟验证了理论结果。

Conclusion: LLMO框架在理论上是可行的，多LLM架构进一步优化性能，为实际应用提供了理论依据。

Abstract: Future wireless networks are expected to incorporate diverse services that
often lack general mathematical models. To address such black-box network
management tasks, the large language model (LLM) optimizer framework, which
leverages pretrained LLMs as optimization agents, has recently been promoted as
a promising solution. This framework utilizes natural language prompts
describing the given optimization problems along with past solutions generated
by LLMs themselves. As a result, LLMs can obtain efficient solutions
autonomously without knowing the mathematical models of the objective
functions. Although the viability of the LLM optimizer (LLMO) framework has
been studied in various black-box scenarios, it has so far been limited to
numerical simulations. For the first time, this paper establishes a theoretical
foundation for the LLMO framework. With careful investigations of LLM inference
steps, we can interpret the LLMO procedure as a finite-state Markov chain, and
prove the convergence of the framework. Our results are extended to a more
advanced multiple LLM architecture, where the impact of multiple LLMs is
rigorously verified in terms of the convergence rate. Comprehensive numerical
simulations validate our theoretical results and provide a deeper understanding
of the underlying mechanisms of the LLMO framework.

</details>


### [44] [RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring](https://arxiv.org/abs/2507.02731)
*Jie Yang,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 论文提出了一种利用可重构智能表面（RIS）辅助的集成感知与通信（ISAC）框架，用于高精度结构健康监测（SHM），解决了多径干扰和超高感知精度需求的问题。


<details>
  <summary>Details</summary>
Motivation: 未来蜂窝系统的集成感知与通信（ISAC）在结构健康监测（SHM）中的应用潜力尚未充分探索，主要因多径干扰和高精度需求等挑战。

Method: 通过动态调整RIS相位生成独特信号抑制多径干扰，利用Fisher信息理论分析三维网络中的协作感知，并开发贝叶斯推理模型识别结构状态。

Result: 理论和数值分析证实ISAC可实现毫米级形变检测，满足SHM的高精度要求。

Conclusion: RIS辅助的ISAC框架为高精度SHM提供了可行方案，展示了其在未来蜂窝系统中的潜力。

Abstract: Integrated sensing and communication (ISAC) is a key feature of future
cellular systems, enabling applications such as intruder detection, monitoring,
and tracking using the same infrastructure. However, its potential for
structural health monitoring (SHM), which requires the detection of slow and
subtle structural changes, remains largely unexplored due to challenges such as
multipath interference and the need for ultra-high sensing precision. This
study introduces a novel theoretical framework for SHM via ISAC by leveraging
reconfigurable intelligent surfaces (RIS) as reference points in collaboration
with base stations and users. By dynamically adjusting RIS phases to generate
distinct radio signals that suppress background multipath interference,
measurement accuracy at these reference points is enhanced. We theoretically
analyze RIS-aided collaborative sensing in three-dimensional cellular networks
using Fisher information theory, demonstrating how increasing observation time,
incorporating additional receivers (even with self-positioning errors),
optimizing RIS phases, and refining collaborative node selection can reduce the
position error bound to meet SHM's stringent accuracy requirements.
Furthermore, we develop a Bayesian inference model to identify structural
states and validate damage detection probabilities. Both theoretical and
numerical analyses confirm ISAC's capability for millimeter-level deformation
detection, highlighting its potential for high-precision SHM applications.

</details>
