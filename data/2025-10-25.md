<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.IT](#cs.IT) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: 本文探讨了6G网络中实现真正自主性的挑战，指出仅优化KPI不足以达到高级别自主。提出使用基于LLM的智能体AI来感知网络环境，但需要解决人类认知偏见在AI系统中的影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于KPI的网络自动化存在局限性，无法实现真正的网络自主性。需要智能体AI来感知和推理真实网络环境，但AI系统会继承人类设计中的认知偏见，影响决策质量。

Method: 提出基于大型语言模型的智能体AI架构，能够感知多模态遥测数据、利用记忆推理、跨域协商并通过API执行操作。针对认知偏见问题，提供了偏见分类、数学建模和在电信系统中的表现分析，并提出了具体的缓解策略。

Result: 通过实施锚定随机化、时间衰减和拐点奖励等技术来缓解锚定、时间和确认偏见，在6G跨域切片管理用例中，智能体决策质量显著提升，延迟降低5倍，节能效果提高约40%。

Conclusion: 实现6G网络真正自主性需要超越KPI优化，采用智能体AI方法。但必须解决认知偏见问题，通过针对性的缓解策略可以显著提升智能体决策的质量和效果。

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>


### [2] [Rediscovering Recurring Routing Results](https://arxiv.org/abs/2510.20297)
*Xiao Song,John Heidemann*

Main category: cs.NI

TL;DR: Fenrir是一个新的路由分析系统，能够重新发现重复出现的路由结果，检测网络路由变化，量化路由变化程度，并识别可能重新出现的路由模式。


<details>
  <summary>Details</summary>
Motivation: 路由对网络性能至关重要，但理解和控制路由对服务的影响具有挑战性。运营商使用BGP流量工程优化网络性能，但实际获得的是整个互联网BGP策略的结果，而不仅仅是本地选择。

Method: Fenrir系统通过主动测量、数据清理和加权等方法，能够发现网络路由变化（即使发生在观察者多跳之外），并提供量化路由变化程度和识别路由模式的新方法。

Result: Fenrir可应用于多种场景：根DNS服务的任播捕获、多宿主企业的路由优化、以及顶级Web服务的网站选择。系统能够回答操作性问题，如流量工程的效果、第三方变更是否改变了路由、当前路由是新的还是之前见过的模式。

Conclusion: Fenrir提供了一种有效的方法来检测和量化路由变化，帮助运营商更好地理解和管理网络路由对服务性能的影响。

Abstract: Routing is central to networking performance, including: (1) latency in
anycast services and websites served from multiple locations,(2) networking
expenses and throughput in multi-homed enterprises, (3) the ability to keep
traffic domestic when considering data sovereignty. However, understanding and
managing how routing affects these services is challenging. Operators use
Traffic Engineering (TE) with BGP to optimize network performance, but what
they get is the result of all BGP policies throughout the Internet, not just
their local choices. Our paper proposes Fenrir, a new system to rediscover
recurring routing results. Fenrir can discover changes in network routing, even
when it happens multiple hops away from the observer. Fenrir also provides new
methods to quantify the degree of routing change, and to identify routing
"modes" that may reappear. Second, we show that Fenrir can be applied to many
different problems: we use five instances of three different types of systems
to illustrate the generalization: anycast catchments showing in a root DNS
service, route optimization for two multi-homed enterprises, and website
selection for two of the top-10 web services. Each type requires different
types of active measurements, data cleaning and weighting. We demonstrate
Fenrir's methods of detecting and quantifying change are helpful because they
all face similar operational questions: How much effect did traffic engineering
have? Did a third-party change alter my routing? In either case, is the current
routing new, or is it like a routing mode I saw before?

</details>


### [3] [Multicast-partitioning in Time-triggered Stream Planning for Time-Sensitive Networks](https://arxiv.org/abs/2510.20440)
*Heiko Geppert,Frank Dürr,Simon Naß,Kurt Rothermel*

Main category: cs.NI

TL;DR: 提出了一种新颖的多播分区技术，将多播树分割成更小的多播或单播树，在带宽利用率和流量调度难度之间实现更精细的权衡，从而提高动态系统中的可调度性。


<details>
  <summary>Details</summary>
Motivation: 多播通信虽然能节省网络带宽，但会使流量规划复杂化，因为需要在多播树的所有分支上都有空闲队列或可用的下游出口端口。

Method: 采用多播分区技术，将大型多播树分割成较小的多播或单播树，实现带宽利用与调度难度之间的权衡优化。

Result: 在不同网络拓扑和三种调度算法下评估，使用分区技术后，流拒绝率减少5-15%，网络吞吐量提升5-125%。

Conclusion: 多播分区技术能有效改善网络可调度性，在保持带宽效率的同时降低调度复杂度，适用于时间敏感网络中的实时系统。

Abstract: Multicast allows sending a message to multiple recipients without having to
create and send a separate message for each recipient. This preserves network
bandwidth, which is particularly important in time-sensitive networks. These
networks are commonly used to provide latency-bounded communication for
real-time systems in domains like automotive, avionics, industrial internet of
things, automated shop floors, and smart energy grids. The preserved bandwidth
can be used to admit additional real-time messages with specific quality of
service requirements or to reduce the end-to-end latencies for messages of any
type. However, using multicast communication can complicate traffic planning,
as it requires free queues or available downstream egress ports on all branches
of the multicast tree. In this work, we present a novel multicast partitioning
technique to split multicast trees into smaller multicast or unicast trees.
This allows for a more fine-grained trade-off between bandwidth utilization and
traffic scheduling difficulty. Thus, schedulability in dynamic systems can be
improved, in terms the number of admitted streams and the accumulated network
throughput. We evaluated the multicast partitioning on different network
topologies and with three different scheduling algorithms. With the
partitioning, 5-15\% fewer streams were rejected, while achieving 5-125\% more
network throughput, depending on the scheduling algorithm.

</details>


### [4] [Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers](https://arxiv.org/abs/2510.20703)
*Felipe Avencourt Soares,Muriel F. Franco,Eder J. Scheid,Lisandro Z. Granville*

Main category: cs.NI

TL;DR: 评估四种AI工具在生成POX网络控制器代码时的可靠性和质量，发现ChatGPT和DeepSeek表现更优。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在多领域生成类人内容，但在可编程网络等新环境中的正确性和功能性可靠性尚不明确。

Method: 使用ChatGPT、Copilot、DeepSeek和BlackBox.ai四种AI工具，通过零样本和少样本提示技术生成POX控制器代码，在Mininet模拟网络拓扑中测试功能性和正确性。

Result: 所有模型都能生成功能性控制器，但ChatGPT和DeepSeek表现出更高的一致性和代码质量，Copilot和BlackBox.ai需要更多调整。

Conclusion: 生成式AI工具能够生成功能性网络控制器代码，但在代码质量和一致性方面存在差异，ChatGPT和DeepSeek表现更可靠。

Abstract: Generative Artificial Intelligence (AI) tools have been used to generate
human-like content across multiple domains (e.g., sound, image, text, and
programming). However, their reliability in terms of correctness and
functionality in novel contexts such as programmable networks remains unclear.
Hence, this paper presents an empirical evaluation of the source code of a POX
controller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,
and BlackBox.ai. To evaluate such a code, three networking tasks of increasing
complexity were defined and for each task, zero-shot and few-shot prompting
techniques were input to the tools. Next, the output code was tested in
emulated network topologies with Mininet and analyzed according to
functionality, correctness, and the need for manual fixes. Results show that
all evaluated models can produce functional controllers. However, ChatGPT and
DeepSeek exhibited higher consistency and code quality, while Copilot and
BlackBox.ai required more adjustments.

</details>


### [5] [AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G](https://arxiv.org/abs/2510.20796)
*John Sengendo,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 本文提出了一种将LSTM神经网络集成到数字孪生网络框架中的AI驱动方法，用于预测网络流量模式并主动管理资源分配，在5G/6G网络中实现自主、自适应的高性能网络管理。


<details>
  <summary>Details</summary>
Motivation: 随着5G和未来6G移动网络日益复杂，传统基于启发式的资源管理技术无法满足实时服务供应的敏捷性、可扩展性、弹性和精确性要求，需要数字孪生作为关键赋能技术来克服这些限制。

Method: 将长短期记忆(LSTM)神经网络集成到数字孪生框架中，用于预测网络流量模式并主动管理资源分配。

Result: 通过分析实验，AI驱动的数字孪生框架相比基准方法展现出优越性能。

Conclusion: 在数字孪生中嵌入AI能力为未来移动网络中完全自主、自适应和高性能的网络管理铺平了道路。

Abstract: As 5G and future 6G mobile networks become increasingly more sophisticated,
the requirements for agility, scalability, resilience, and precision in
real-time service provisioning cannot be met using traditional and
heuristic-based resource management techniques, just like any advancing
technology. With the aim of overcoming such limitations, network operators are
foreseeing Digital Twins (DTs) as key enablers, which are designed as dynamic
and virtual replicas of network infrastructure, allowing operators to model,
analyze, and optimize various operations without any risk of affecting the live
network. However, for Digital Twin Networks (DTNs) to meet the challenges faced
by operators especially in line with resource management, a driving engine is
needed. In this paper, an AI (Artificial Intelligence)-driven approach is
presented by integrating a Long Short-Term Memory (LSTM) neural network into
the DT framework, aimed at forecasting network traffic patterns and proactively
managing resource allocation. Through analytical experiments, the AI-Enabled DT
framework demonstrates superior performance benchmarked against baseline
methods. Our study concludes that embedding AI capabilities within DTs paves
the way for fully autonomous, adaptive, and high-performance network management
in future mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准(ARB)，这是首个用于量化能源系统分析中大型语言模型推理可靠性的标准化框架，包含五个子指标，在确定性、概率性和认知性场景下评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在能源领域的验证实践主要关注预测准确性或计算效率，而缺乏对分析结论逻辑完整性的测试，需要建立标准化的推理可靠性评估框架。

Method: 开发了ARB框架，整合准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标，使用开放技术经济数据集(NREL ATB 2024等)，在相同事实和监管条件下测试GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro和Llama 3 70B四个前沿模型。

Result: GPT-4/5和Claude 4.5 Sonnet实现了政策合规的推理(分析可靠性指数大于90)，Gemini 2.5 Pro表现中等稳定，Llama 3 70B低于专业阈值，统计验证显示这些差异显著且可重现。

Conclusion: ARB建立了能源文献中首个验证人工智能系统因果、概率和政策驱动推理的定量方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [7] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出一种量子启发的算法，使用矩阵乘积态和离散驱动调度来解决二次无约束二进制优化问题，在数独和MaxCut问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决QUBO问题的传统方法存在局限性，需要开发能够可靠找到全局最优解而非近似解的算法。

Method: 使用矩阵乘积态紧凑表示自旋构型，结合驱动哈密顿量和问题哈密顿量，通过密度矩阵重整化群方法迭代最小化系统能量。

Result: 算法在超过200个自旋的数独问题和251节点、3265边的MaxCut问题上成功找到全局最优解。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业级QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [8] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的网页代理框架，通过树状结构探索、网页状态回放和页面动作记忆来提高基于LLM的网页代理的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自主网页代理方法在推理深度和效率上存在局限：线性方法无法进行多步推理且缺乏有效回溯，而其他搜索策略粒度粗且计算成本高。

Method: 采用显式子任务管理和树状结构探索实现可控的多分支推理；通过网页状态回放和后台推理引导探索；利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，任务成功率达到35.8%，执行时间相比最先进方法减少高达40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的基于LLM的网页代理框架，显著提升了推理能力和执行效率。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [9] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出了DAG-MATH框架，将思维链建模为基于规则的有向无环图随机过程，引入逻辑紧密度指标来评估LLM推理的规则一致性，超越了传统PASS@k指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法清晰判断LLM在数学问题上的成功是源于搜索、机械记忆还是规则一致推理，需要新的评估框架来量化推理质量。

Method: 将思维链建模为基于规则的有向无环图随机过程，节点表示中间推导状态，边编码规则应用，并引入逻辑紧密度指标。

Result: 在标准数学推理数据集上发现，即使PASS@k指标相似，不同LLM家族在推理保真度上存在显著差异，揭示了最终答案准确性与规则一致性推导之间的差距。

Conclusion: 该框架在自由形式思维链和形式证明系统之间提供了平衡，为LLM推理评估提供了可操作的诊断工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [10] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个纯视觉观察的统一架构，在Web、桌面和移动环境中实现最先进性能，无需任务特定微调即可超越所有先前系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能体依赖环境特定接口、限制跨平台部署的问题，实现通用计算机控制的视觉交互。

Method: 集成层次化上下文管理、解耦规划与执行、带自适应恢复的自验证机制，支持长任务序列的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，多尝试下在所有基准测试中超越人类表现。

Conclusion: 系统化编排能放大基础模型能力，实现纯视觉交互的通用计算机控制，但需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [11] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一个模式无关的图神经网络特征编码器，使用共享的模态特定编码器处理多模态节点属性，通过交叉注意力聚合为固定大小的节点表示，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络需要为每种节点类型和特征列设计特定的特征编码器，这限制了可扩展性和参数共享，特别是在处理异构时序图数据时。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合为固定大小的排列不变节点表示。

Result: 在RelBench基准测试中，RELATE与ReLGNN和HGT结合使用时，性能达到了模式特定编码器的97%，同时参数数量减少了5倍。

Conclusion: RELATE支持不同模式，实现了多数据集预训练，为关系图数据的基础模型铺平了道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [12] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈，使大规模快速伪造事故证据变得更容易。保险公司开始部署AI反制措施，但面临检测工具误报漏报、欺诈者不断适应等挑战。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈每年造成数百亿美元损失，传统欺诈手段包括伪造事故、夸大损失等。生成式AI的出现使欺诈者能够大规模制造逼真的事故照片、证据和假身份，加剧了保险欺诈问题。

Method: 保险公司部署基于AI的深度伪造检测软件和增强验证流程来检测和减轻AI驱动的欺诈。UVeye提出了分层解决方案来检测、减轻和威慑这种新型欺诈浪潮。

Result: 当前缓解策略面临显著限制：检测工具存在误报和漏报问题，复杂欺诈者不断调整策略逃避自动检查。生成式AI与检测技术之间的军备竞赛仍在继续。

Conclusion: 打击AI驱动的保险欺诈仍然是一个持续挑战，需要更先进的分层解决方案来有效应对这种新型欺诈威胁。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [13] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型通过领导力人格特质预测学术成功，在129名环境工程硕士生中，随机森林分类器取得了87.50%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学术表现，为早期识别学生优缺点和制定个性化学习策略提供机会。

Method: 对129名硕士生进行5项领导力人格测试（23个特征），结合平均成绩，使用探索性数据分析和相关性分析，通过7种机器学习算法进行建模。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记特征的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 研究证实了通过领导力人格特质预测学术成功的可行性，为教育过程中早期识别学生优缺点和选择合适个性化学习策略提供了有效工具。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [14] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [15] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个生产级的生成式智能体，在零售金融领域部署，能够主动生成基于事实、合规且个性化的投资洞察，而非被动回答查询。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人只能被动回答问题，无法在金融等高风险领域提供主动、可信的AI洞察。需要开发一个既能主动生成投资建议，又能确保合规性和安全性的系统。

Method: 采用组件化编排层进行确定性路由，结合混合检索管道（OpenSearch+金融领域嵌入模型）和多阶段推荐机制（规则启发式+序列行为建模+上下文赌博机），完全在韩国金融监管下的本地环境运行。

Result: 通过人工QA和系统指标验证，证明基于显式路由和分层安全机制的落地生成能够在高风险金融领域提供可信的AI洞察。

Conclusion: 在金融等高风险领域，通过明确的组件路由和分层安全机制，可以实现可信的生成式AI洞察，为主动式金融服务提供了可行的技术路径。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [16] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为本的多智能体异常检测系统，用于数字资产交易，通过自然语言对话让非专家用户查询、检查分析结果并获得解释。


<details>
  <summary>Details</summary>
Motivation: 提高金融取证中异常检测的透明度和可信度，让非专家用户能够理解和使用复杂的检测系统。

Method: 将解析、检测和解释三个角色连接成对话工作流，使用XGBoost作为基础检测器，通过自然语言界面将用户意图转换为检测模式。

Result: 在比特币混币数据集上，基线检测器达到较高准确率，HCLA系统增加了可解释性和交互式优化能力。

Conclusion: 人机协同设计能够显著提升金融取证系统的透明度和用户信任度。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [17] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文提出AGI发展需要经历从开放模仿到身份固化的锁定期，并建立了检测这一转变的度量方法，实验显示不同规模模型在身份固化过程中表现出不同的性能变化模式。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过于开放和可操控，而真正的AGI发展需要经历身份固化的锁定期，使目标结构、拒绝行为、偏好和内部表征变得稳定且抵抗外部操控。

Method: 形式化定义了身份固化阶段，将其与学习动力学中的已知现象联系起来，提出了检测固化开始的运行度量方法，并在不同规模的模型上进行实验验证。

Result: 实验表明行为固化是快速且非线性的，但对通用能力的影响不是单一的：小模型出现性能权衡，中等规模模型几乎无成本地实现固化，大型量化模型出现瞬时不稳定性。

Conclusion: 身份固化是AGI级可靠性的先决条件，也是安全性的关键控制点——身份可以被工程化设计以实现可靠性，但也可能在扩展过程中自发出现，从而固化不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [18] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文认为需要重新评估AI在法律实践中的使用，提出了验证-价值悖论，指出AI带来的效率提升会被相应的验证需求抵消，导致净价值往往可以忽略不计。


<details>
  <summary>Details</summary>
Motivation: 鉴于律师因提交不准确的AI生成内容而受到谴责的案例，以及AI与现实脱节、缺乏透明度的特点，需要重新审视AI在法律实践中的使用范式。

Method: 提出了一种替代性的AI使用模型——验证-价值悖论，该模型更全面地反映了AI的特性和律师的首要职责。

Result: 验证-价值悖论表明，AI在法律实践中带来的效率提升会被相应的手动验证需求所抵消，导致净价值往往可以忽略不计。

Conclusion: 论文阐述了该悖论对法律实践和法律教育的影响，强调需要以忠于真相和公民责任为基础价值观来指导AI的使用。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [19] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了TRUST框架，一个透明、去中心化的AI审计系统，通过共识机制、层次化DAG分解、区块链账本和隐私保护分段，解决大语言模型推理链验证的鲁棒性、可扩展性、透明度和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有审计方法存在中心化、不透明和难以扩展的问题，无法有效验证大语言模型推理链的忠实性和无害性，这在高风险领域部署专有模型时存在重大风险。

Method: TRUST框架包含四个核心组件：1) 多样审计者间的共识机制；2) 推理链的层次化DAG分解；3) 区块链账本记录验证决策；4) 隐私保护的分段共享机制。

Result: 实验表明TRUST在多个LLM和推理任务中能有效检测推理缺陷，在30%恶意参与者情况下仍保持鲁棒性，提供了安全和经济激励的理论保证。

Conclusion: TRUST开创了去中心化AI审计的新范式，为大语言模型的安全可信部署提供了可行路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [20] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 该论文研究了在动态环境中优化AI的方法，通过进化训练技术来改进AI在2048游戏中的表现，比较了单智能体系统和双智能体系统的效果。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的表现是机器学习研究的基本挑战，2048游戏结合了策略游戏和随机元素，是研究决策制定、长期规划和动态适应的理想平台。

Method: 实现了两种系统：双智能体元提示系统（思考者LLM为执行者LLM优化策略）和基于改进价值函数的单智能体系统（使用有限蒙特卡洛树搜索），并实验了回滚功能以避免性能下降。

Result: 单智能体系统取得了显著改进，每个周期平均增加473.2分，训练周期间呈现明显上升趋势（相关性ρ=0.607），LLM对游戏的理解也随着高级策略的发展而增长。双智能体系统改进有限，突显了元提示的内在局限性。

Conclusion: 研究证明了进化改进技术在非确定性环境中提升AI性能的潜力，单智能体方法比双智能体元提示方法更有效。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [21] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文提出了个性化认知模拟(ICS)任务，评估不同认知表征方法在模拟个体思维过程方面的效果，通过小说数据集和11条件认知评估框架测试了7个现成LLM在模仿作者风格方面的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能够模仿表面的人类行为，但其模拟更深层次个性化认知过程的能力仍不清楚，需要系统评估LLM在个性化认知模拟方面的表现。

Method: 构建基于近期小说发布的数据集，提出11条件认知评估框架，测试不同认知表征方法（语言特征、概念映射、基于档案的信息）在作者风格模仿任务中的效果。

Result: 结合概念和语言特征的方法在ICS中特别有效，优于基于静态档案的线索；LLM在模仿语言风格方面比叙事结构更有效，显示出在更深层次认知模拟方面的局限性。

Conclusion: 这些发现为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [22] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大语言模型通过上下文学习生成抽象的PDDL领域和问题实例，以支持智能体的规划、推理和解释能力。


<details>
  <summary>Details</summary>
Motivation: 动态领域抽象的选择会影响智能体的规划、推理和解释能力，因此需要一种有效的方法来生成与特定目标对齐的抽象。

Method: 在PDDL中建模智能体的具体行为，利用大语言模型（如GPT-4o）通过上下文学习生成抽象的PDDL领域和问题实例，抽象目标用自然语言指定。

Result: GPT-4o在简单设置下能够合成有用的规划领域抽象，但在抽象动作方面比抽象相关谓词表现更好。生成的抽象PDDL经过符号验证工具和人类专家检查。

Conclusion: 大语言模型能够有效生成规划领域抽象，特别是在动作抽象方面表现良好，为智能体规划提供了新的可能性。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [23] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI信息和时间描述符来增强人类移动性预测，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出STaBERT模型，在BERT基础上集成POI嵌入和时间描述符，构建统一的语义增强移动性表示。

Result: 单城市预测的GEO-BLEU分数从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: 整合POI和时间信息能显著改善人类移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [24] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具获取有用信息来改进探索方向，从而以更短探索距离生成更准确回答，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法直接使用VLM探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答无效。

Method: 提出ToolEQA智能体，集成外部工具进行多步推理；设计自动生成EQA任务的数据生成流程，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen测试集上，ToolEQA比最先进基线成功率提高9.2~20.2%，比零样本ToolEQA高10%；在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最先进性能。

Conclusion: ToolEQA通过集成外部工具和多步推理显著提升了EQA任务的性能，证明了其在多种环境下的泛化能力。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [25] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 论文分析了医疗AI系统中数据收集过程中的偏见问题，基于AI4HealthyAging项目的实践经验，识别了历史偏见、代表性偏见和测量偏见等多种偏见类型，并提出了改善临床问题设计和数据收集公平性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域有巨大潜力，但由于训练数据的质量和公平性问题，AI解决方案在真实临床实践中的整合仍然有限。数据收集过程中的偏见是主要障碍之一。

Method: 基于西班牙国家研发计划中的AI4HealthyAging项目，分析临床数据收集过程中的偏见检测任务，识别多种偏见类型及其在具体变量中的表现。

Result: 识别出多种偏见类型，包括历史偏见、代表性偏见和测量偏见，这些偏见体现在性别、年龄、居住环境、社会经济地位、设备和标签等变量中。

Conclusion: 提出了改善临床问题设计和数据收集公平性的实用建议，希望这些发现和经验能为未来开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [26] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于评估军事行动中AI系统目标打击附带损害的新型模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过分层结构捕获AI系统类别、攻击向量和上下文因素。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场上作用日益重要的时代，确保负责任的目标打击需要对潜在附带效应进行严格评估，以建立负责任和可信赖的智能系统。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，通过分层结构捕获AI系统类别、攻击向量和上下文因素，并考虑传播、严重性、可能性和评估指标。

Result: 模型通过实例化进行了演示和评估，为构建负责任和可信赖的智能系统奠定了基础，能够清晰表示并增强透明推理机制。

Conclusion: 该模型为评估军事行动中AI系统目标打击的附带损害提供了有效框架，支持建立负责任和可信赖的智能系统评估体系。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [27] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本综述系统回顾了LLM赋能知识图谱构建的最新进展，分析了LLM如何重塑传统的本体工程、知识抽取和知识融合三层流程，探讨了基于模式和无模式两种范式，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正从基于规则和统计的流程转向语言驱动和生成框架，需要系统梳理这一范式转变的进展和影响。

Method: 首先回顾传统KG方法建立概念基础，然后从基于模式（强调结构、规范化和一致性）和无模式（强调灵活性、适应性和开放发现）两个互补视角分析新兴的LLM驱动方法。

Result: 系统综合了各阶段的代表性框架，分析了其技术机制和局限性，明确了LLM与知识图谱之间的演化互动关系。

Conclusion: 通过系统综述，旨在澄清LLM与知识图谱之间的演化互动，弥合符号知识工程与神经语义理解，推动自适应、可解释和智能知识系统的发展。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [28] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出了IKnow框架，通过设计基于指令-响应对话格式的自监督目标，解决指令调优模型在持续预训练中语义表示退化的问题，无需依赖原始基础模型或外部知识库。


<details>
  <summary>Details</summary>
Motivation: 现有方法在持续预训练指令调优模型时，要么需要访问原始基础模型权重，要么依赖外部领域特定知识库，这在现实场景中存在限制。

Method: IKnow框架将自监督目标重新表述为指令-响应对话格式，利用文本中嵌入的领域知识，在更深语义层次进行编码学习。

Result: 该方法能够在不依赖外部资源的情况下，保持模型的指令跟随能力，同时有效学习新领域的知识。

Conclusion: IKnow提供了一个简单通用的解决方案，克服了现有方法对原始模型权重和外部知识库的依赖问题。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [29] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一种基于创造力理论的计算模型，用于生成更具新颖性的创新机会，在酒店业创新项目中验证了其优于Notebook LM和ChatGPT4o的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的创新机会生成方法在保持实用性的同时难以产生足够新颖的创意，需要开发能够平衡新颖性和实用性的计算模型。

Method: 开发了包含五个功能模块的计算模型，基于创造力理论和技巧，专门设计用于生成高新颖性且实用的创新机会。

Result: 在酒店业创新项目中的评估显示，该模型生成的机会比Notebook LM和ChatGPT4o更具新颖性和/或实用性。

Conclusion: 虽然模型整体表现优于现有方法，但并非所有功能模块都对提升新颖性有贡献，这为后续模型优化提供了方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [30] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器对不一致和错误数据不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法在真实世界知识库中难以部署，因为使用的描述逻辑推理器对不一致和错误数据不鲁棒。

Method: EBR神经推理器依赖嵌入来近似符号推理器的结果，仅需检索原子概念和存在限制的实例，就能近似SHOIQ描述逻辑中任何概念的实例集。

Result: 实验表明，与最先进的推理器相比，EBR对缺失和错误数据具有鲁棒性。

Conclusion: EBR为在真实世界知识库中部署神经符号概念学习提供了一种鲁棒的推理方法。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [31] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的无监督知识图谱对齐方法，能够同时对齐实体和关系，提供可解释的结果，并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注纯实体级对齐，在嵌入空间中计算实体相似度，缺乏可解释性推理且需要训练数据。

Method: 基于模糊逻辑的迭代方法，提供实体和关系的整体对齐，允许存在悬空实体（在另一个KG中没有对应实体的实体），并具有可证明的收敛性。

Result: 在主要基准测试中取得了最先进的结果。

Conclusion: FLORA是一个简单而有效的无监督知识图谱对齐方法，具有可解释性、收敛性和处理悬空实体的能力。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [32] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的对话机制，参与率低且反馈响应有限，导致参与式AI治理的承诺与实践之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 公众对AI有强烈意见并希望政策制定者倾听，但政府在将公众意见转化为政策时存在信息流失问题，错失了建立AI信任的关键机会。

Method: 采用景观分析方法，比较三个国家如何征集公众对AI风险和政策的反馈，以及这些意见是否影响了治理决策。

Result: 在三个国家中，政府与公民未能建立有意义的对话，参与率均低于1%，政府缺乏吸引多样化声音的努力，对反馈的响应有限。

Conclusion: 当前方法难以建立AI的信任和合法性，因为政策制定者未能充分倾听和回应公众关切，作者提出了八项改进建议。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [33] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 提出了一种基于图神经网络加速的网络拓扑优化方法，通过母线分裂缓解输电网拥塞，在GOC 2000节点系统中实现4个数量级加速，1分钟内提供交流可行解，最优性差距仅2.3%。


<details>
  <summary>Details</summary>
Motivation: 现有求解器无法在近实时内解决大规模系统的混合整数非线性网络拓扑优化问题，机器学习方法在泛化到未见拓扑、变化运行条件和不同系统方面存在局限。

Method: 采用考虑线性化交流潮流模型的网络拓扑优化问题，开发异构边感知消息传递神经网络来预测有效的母线分裂动作作为候选解。

Result: 在GOC 2000节点系统中实现高达4个数量级的加速，1分钟内提供交流可行解，最优性差距为2.3%。

Conclusion: 该方法在拓扑和跨系统泛化方面取得显著进展，为实现大规模系统近实时网络拓扑优化迈出重要一步。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [34] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出因果逐步评估方法(CaSE)来评估LLM推理过程的质量，将推理质量分解为相关性和连贯性两个维度，并通过基于CaSE评估的数据筛选来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统仅评估最终答案正确性的方法过于粗糙，无法反映推理过程的质量，需要更细粒度的评估方法来构建更鲁棒的模型。

Method: 引入CaSE方法，将推理质量分解为相关性和连贯性两个维度，每个推理步骤仅使用其前文进行评估以避免后见之明偏差，并在MRa-GSM8K和MRa-MATH基准上验证。

Result: CaSE方法与人类判断一致，基于CaSE评估的相关性和连贯性筛选训练数据能直接提升最终任务性能。

Conclusion: 该工作为分析、调试和改进LLM推理提供了可扩展框架，证明了超越有效性检查的实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [35] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出两种可扩展算法，在近线性时间内计算随机游走中心性，适用于大规模网络。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性是图挖掘中量化节点重要性的基本指标，但现有方法计算成本高，难以应用于大型网络。

Method: 基于新的随机游走中心性公式，开发两种算法：一种使用近似Cholesky分解和稀疏逆估计，另一种通过采样有根生成树。

Result: 在包含超过1000万个节点的大型真实网络上的实验表明，两种算法均具有高效性和良好的近似质量。

Conclusion: 提出的算法能够在近线性时间内有效计算随机游走中心性，为大规模网络分析提供了实用工具。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [36] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性设计模型的方法论，旨在平衡预测性能与可解释性，同时嵌入因果性、公平性和隐私性等关键伦理属性。


<details>
  <summary>Details</summary>
Motivation: 开发可解释性设计模型对于在现实应用中建立对自动化决策模型的信任、问责和安全采用至关重要。

Method: 形式化定义了监督学习设置，涵盖表格数据、时间序列、图像、文本等多种数据类型，并分析了特征重要性、规则和实例三类可解释模型家族。

Result: 建立了评估伦理度量的理论基础，包括因果性、公平性和隐私性的形式化定义、评估指标和验证程序。

Conclusion: 该框架为开发不仅准确可解释，而且公平、保护隐私和具有因果意识的AI系统奠定了理论基础，即值得信赖的AI系统。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [37] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态基准测试，用于评估LLM在电子商务领域的性能，涵盖6个类别37个任务，包括8个多模态任务，支持7种语言。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性不足、模态有限、数据合成或人工整理、语言覆盖窄等问题，缺乏评估模型在复杂真实购物场景中性能的可靠工具。

Method: 采用半自动化流程：大模型生成候选答案，50多名电商和多语言专家审核修改；基于不同规模和能力模型的平均评估分数定义问题难度等级。

Result: 构建了一个覆盖真实客户查询和交易日志的基准测试，反映了真实商业交互的噪声和异质性特征。

Conclusion: EcomEval填补了电子商务领域评估基准的空白，为复杂真实购物场景的模型评估提供了可靠工具，特别关注多语言和低资源语言支持。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [38] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出Fluidity Index (FI)来量化模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来测试模型的上下文切换和连续性能力。


<details>
  <summary>Details</summary>
Motivation: 需要量化模型在动态扩展环境中的适应能力，区分封闭式和开放式基准测试，重点关注现实世界中的闭环开放式基准。

Method: 通过测量模型对扩展环境中状态变化的理解、预测和调整能力来评估适应性，强调二阶适应性以实现自持计算。

Result: 建立了Fluidity Index评估框架，能够系统性地测试模型在动态环境中的适应性能。

Conclusion: 真正超级智能的模型应至少具备二阶适应性，通过数字补充实现自持计算以达到最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [39] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习集成到理性智能体架构中的方法进行了系统化分析，特别关注BDI（信念-欲望-意图）范式，识别了关键研究机会和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在感知和认知任务中展现出类人能力，将ML集成到理性智能体架构的框架日益受到关注，但现有研究缺乏系统性和连贯性。

Method: 使用BDI范式作为参考框架，对现有方法进行细粒度系统化分析，梳理快速发展的相关文献。

Result: 分析揭示了将ML增强理性智能体的快速发展现状，并识别了该领域的关键研究方向。

Conclusion: 本文为设计有效的理性ML智能体提供了系统化分析框架，指明了重要的研究机会和开放挑战。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [40] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析的评估框架，用于自动评估大语言模型推理轨迹的质量，相比传统图指标具有更高预测能力


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法依赖专家标注、劳动密集且不可靠，需要更自动化和有效的评估方法

Method: 使用拓扑数据分析方法捕捉推理轨迹的几何结构，通过拓扑特征进行自动化评估

Result: 拓扑特征在评估推理质量方面比标准图指标具有显著更高的预测能力，表明有效推理更适合用高维几何结构而非纯关系图来捕捉

Conclusion: 紧凑且稳定的拓扑特征集能可靠指示轨迹质量，为未来强化学习算法提供了实用的质量信号

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [41] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT是一个两阶段强化微调KGQA框架，通过'计划-KG搜索-网络搜索-思考'范式，使LLM能够在知识不完整条件下自主规划和跨KG与网络源的自适应检索调度。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用KG中的丰富知识和LLM的推理能力，特别是在复杂场景中。它们通常假设KG覆盖完整，缺乏判断何时需要外部信息的机制，且推理过程局部短视，无法保持连贯的多步规划。

Method: Graph-RFT采用两阶段方法：1) 使用定制化计划-检索数据集进行思维链微调，激活结构化推理；2) 通过计划-检索引导的强化学习过程，集成显式规划和检索动作，采用笛卡尔式规划模块分解复杂问题，使用逻辑表达式指导工具调用。

Result: 该方法通过多奖励设计优化推理检索过程，结合结果和检索特定信号，使模型能够学习何时以及如何有效结合KG和网络检索。

Conclusion: Graph-RFT框架解决了现有KGQA方法在知识不完整条件下的局限性，实现了自主规划和自适应检索调度，提升了复杂场景下的推理能力。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [42] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 提出了一种基于广义均值积分的一致性感知AGI度量方法，替代了算术平均的补偿性假设，能更好地评估跨认知领域的平衡能力。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用算术平均，假设领域间能力可以相互补偿，但真正的通用智能应该反映所有关键领域的平衡能力。

Method: 使用广义均值在补偿性指数连续统上的积分，形成曲线下面积(AUC)度量，涵盖算术、几何和调和均值机制。

Result: 应用于GPT-4和GPT-5的CHC领域得分，一致性调整的AUC显示尽管算术得分较高(GPT-5达24%)，但系统仍远未达到通用能力。

Conclusion: 广义均值积分提供了原则性、可解释且更严格的AGI进展度量基础，能惩罚能力不平衡并捕捉领域间依赖关系。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [43] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，识别新兴趋势和跨领域机会，帮助研究人员应对论文数量激增的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产生超过10,000篇论文，研究人员难以跟上快速发展的趋势，跨学科工作增多，需要探索专业领域之外的知识。

Method: 构建通用的RDR管道，系统分析研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。

Result: 在AI和机器人领域应用RDR框架，特别关注基础模型和机器人技术进步，并扩展到其他科学领域。

Conclusion: RDR框架为AI及其他领域的研究人员提供了有价值的分析工具，帮助他们在快速发展的研究环境中保持更新。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [44] [Information Gradient for Nonlinear Gaussian Channel with Applications to Task-Oriented Communication](https://arxiv.org/abs/2510.20179)
*Tadashi Wadayama*

Main category: cs.IT

TL;DR: 提出了基于梯度的参数化非线性高斯信道优化框架，通过互信息最大化实现。利用SFB方法推导出计算可行的信息梯度公式，无需显式计算输出分布，支持端到端优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效优化非线性高斯信道参数，特别是在需要最大化互信息或任务导向的场景中。现有方法计算复杂，缺乏高效的梯度计算框架。

Method: 采用SFB方法推导信息梯度公式，包含边际输出分布的得分函数（通过DSM学习）和前端函数的雅可比矩阵（通过VJP自动微分处理），实现梯度上升优化。

Result: 实验验证了信息梯度公式的正确性，证明该方法能有效优化线性和非线性信道，达到预期目标。

Conclusion: 该框架为非线性高斯信道优化提供了实用的梯度计算方法，支持互信息最大化和任务导向优化，具有端到端优化的优势。

Abstract: We propose a gradient-based framework for optimizing parametric nonlinear
Gaussian channels via mutual information maximization. Leveraging the
score-to-Fisher bridge (SFB) methodology, we derive a computationally tractable
formula for the information gradient that is the gradient of mutual information
with respect to the parameters of the nonlinear front-end. Our formula
expresses this gradient in terms of two key components: the score function of
the marginal output distribution, which can be learned via denoising score
matching (DSM), and the Jacobian of the front-end function, which is handled
efficiently using the vector-Jacobian product (VJP) within automatic
differentiation frameworks. This enables practical parameter optimization
through gradient ascent. Furthermore, we extend this framework to task-oriented
scenarios, deriving gradients for both task-specific mutual information, where
a task variable depends on the channel input, and the information bottleneck
(IB) objective. A key advantage of our approach is that it facilitates
end-to-end optimization of the nonlinear front-end without requiring explicit
computation on the output distribution. Extensive experimental validation
confirms the correctness of our information gradient formula against analytical
solutions and demonstrates its effectiveness in optimizing both linear and
nonlinear channels toward their objectives.

</details>


### [45] [New Second-Order Achievability Bounds for Coding with Side Information via Type Deviation Convergence](https://arxiv.org/abs/2510.20241)
*Xiang Li,Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出了一种称为类型偏差收敛的二阶可实现性框架，适用于网络信息论设置，特别适用于有损源编码和带成本的信道编码。改进了Wyner-Ziv问题、Heegard-Berger问题和Gelfand-Pinsker问题的二阶可实现性界。


<details>
  <summary>Details</summary>
Motivation: 现有网络信息论中的二阶可实现性界存在改进空间，特别是在有损源编码和带成本的信道编码方面，需要更通用的框架来提升性能边界。

Method: 使用类型偏差收敛框架，这是一种二阶可实现性分析方法，专门针对网络信息论中的复杂设置，包括带边信息的编码问题。

Result: 为Wyner-Ziv问题提供了改进的二阶可实现性界，优于所有已知界限；同时为Heegard-Berger问题和带成本的Gelfand-Pinsker问题提供了改进的二阶可实现性界。

Conclusion: 类型偏差收敛框架是网络信息论中二阶分析的有效工具，能够显著改进多个重要编码问题的性能边界，特别是在有损源编码和带成本约束的场景中。

Abstract: We propose a framework for second-order achievability, called type deviation
convergence, that is generally applicable to settings in network information
theory, and is especially suitable for lossy source coding and channel coding
with cost. We give a second-order achievability bound for lossy source coding
with side information at the decoder (Wyner-Ziv problem) that improves upon all
known bounds (e.g., Watanabe-Kuzuoka-Tan, Yassaee-Aref-Gohari and
Li-Anantharam). We also give second-order achievability bounds for lossy
compression where side information may be absent (Heegard-Berger problem) and
channels with noncausal state information at the encoder and cost constraint
(Gelfand-Pinsker problem with cost) that improve upon previous bounds.

</details>


### [46] [A Location-Aware Hybrid Deep Learning Framework for Dynamic Near-Far Field Channel Estimation in Low-Altitude UAV Communications](https://arxiv.org/abs/2510.20277)
*Wenli Yuan,Kan Yu,Xiaowu Liu,Kaixuan Li,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 提出了一种基于位置感知混合深度学习架构的统一信道估计框架，用于解决低空无人机通信中的混合近场和远场传播条件下的信道估计挑战。


<details>
  <summary>Details</summary>
Motivation: 低空无人机通信中，由于节点高移动性和大规模天线阵列的使用，导致混合近场和远场传播条件，传统基于远场假设的估计方法无法捕捉近场场景的复杂信道变化，且忽略了实时收发器位置等有价值的几何先验。

Method: 提出位置感知混合深度学习架构，结合CNN进行空间特征提取、BiLSTM网络建模时间演化、多头自注意力机制增强对判别性信道分量的关注，并将实时收发器位置作为几何先验嵌入。

Result: 广泛仿真验证了所提方法的有效性，相比现有基准方法显著提升性能，平均至少减少30.25%的归一化均方误差。

Conclusion: 该统一信道估计框架通过融合深度学习和几何先验，有效解决了低空无人机通信中的混合近场远场信道估计问题，显著提升了估计精度。

Abstract: In low altitude UAV communications, accurate channel estimation remains
challenging due to the dynamic nature of air to ground links, exacerbated by
high node mobility and the use of large scale antenna arrays, which introduce
hybrid near and far field propagation conditions. While conventional estimation
methods rely on far field assumptions, they fail to capture the intricate
channel variations in near-field scenarios and overlook valuable geometric
priors such as real-time transceiver positions. To overcome these limitations,
this paper introduces a unified channel estimation framework based on a
location aware hybrid deep learning architecture. The proposed model
synergistically combines convolutional neural networks (CNNs) for spatial
feature extraction, bidirectional long short term memory (BiLSTM) networks for
modeling temporal evolution, and a multihead self attention mechanism to
enhance focus on discriminative channel components. Furthermore, real-time
transmitter and receiver locations are embedded as geometric priors, improving
sensitivity to distance under near field spherical wavefronts and boosting
model generalization. Extensive simulations validate the effectiveness of the
proposed approach, showing that it outperforms existing benchmarks by a
significant margin, achieving at least a 30.25% reduction in normalized mean
square error (NMSE) on average.

</details>


### [47] [Moving or Predicting? RoleAware-MAPP: A Role-Aware Transformer Framework for Movable Antenna Position Prediction to Secure Wireless Communications](https://arxiv.org/abs/2510.20293)
*Wenxu Wang,Xiaowu Liu,Wei Gong,Yujia Zhao,Kaixuan Li,Qixun Zhang,Zhiyong Feng,Kan Yu*

Main category: cs.IT

TL;DR: 提出RoleAware-MAPP框架，通过Transformer模型结合领域知识解决可移动天线定位问题，显著提升物理层安全性能


<details>
  <summary>Details</summary>
Motivation: 解决可移动天线技术在实际部署中的两大挑战：实时优化计算复杂度和机械移动速度与信道快速变化的时间不匹配问题，同时现有学习方法缺乏对通信领域特定知识的利用

Method: 将MA定位问题重新定义为预测任务，引入基于Transformer的RoleAware-MAPP框架，包含角色感知嵌入、物理信息语义特征和复合损失函数三个关键组件

Result: 在3GPP兼容场景下，平均保密率达到0.3569 bps/Hz，严格正保密容量达到81.52%，分别比最强基线提升48.4%和5.39个百分点

Conclusion: RoleAware-MAPP框架通过有效整合领域知识，显著提升了可移动天线系统的物理层安全性能，在不同用户速度和噪声条件下保持稳健性能

Abstract: Movable antenna (MA) technology provides a promising avenue for actively
shaping wireless channels through dynamic antenna positioning, thereby enabling
electromagnetic radiation reconstruction to enhance physical layer security
(PLS). However, its practical deployment is hindered by two major challenges:
the high computational complexity of real time optimization and a critical
temporal mismatch between slow mechanical movement and rapid channel
variations. Although data driven methods have been introduced to alleviate
online optimization burdens, they are still constrained by suboptimal training
labels derived from conventional solvers or high sample complexity in
reinforcement learning. More importantly, existing learning based approaches
often overlook communication-specific domain knowledge, particularly the
asymmetric roles and adversarial interactions between legitimate users and
eavesdroppers, which are fundamental to PLS. To address these issues, this
paper reformulates the MA positioning problem as a predictive task and
introduces RoleAware-MAPP, a novel Transformer based framework that
incorporates domain knowledge through three key components: role-aware
embeddings that model user specific intentions, physics-informed semantic
features that encapsulate channel propagation characteristics, and a composite
loss function that strategically prioritizes secrecy performance over mere
geometric accuracy. Extensive simulations under 3GPP-compliant scenarios show
that RoleAware-MAPP achieves an average secrecy rate of 0.3569 bps/Hz and a
strictly positive secrecy capacity of 81.52%, outperforming the strongest
baseline by 48.4% and 5.39 percentage points, respectively, while maintaining
robust performance across diverse user velocities and noise conditions.

</details>


### [48] [Ergodic Mutual Information and Outage Probability for SIM-Assisted Holographic MIMO Communications](https://arxiv.org/abs/2510.20307)
*Anastasios Papazafeiropoulos,Pandelis Kourtessis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 本文研究了堆叠智能超表面(SIM)辅助MIMO系统的遍历互信息(EMI)和中断概率，推导了基于统计CSI的紧密闭式中断概率表达式，并应用梯度下降法进行优化。


<details>
  <summary>Details</summary>
Motivation: 目前文献中缺乏对SIM辅助MIMO系统的EMI和中断概率研究，而SIM相比单层超表面具有更好的性能表现。

Method: 使用大随机矩阵理论工具获得互信息分布，推导基于统计CSI的闭式中断概率表达式，并应用梯度下降法进行中断概率最小化。

Result: 仿真结果验证了理论分析，显示相比传统MIMO系统和单层超表面有性能提升，且所提优化算法比交替优化基准节省显著开销。

Conclusion: SIM辅助MIMO系统在性能上优于传统系统，所提出的优化方法具有更高的计算效率。

Abstract: Stacked intelligent metasurface (SIM) is a promising enabler for
next-generation high-capacity networks that exhibit better performance compared
to its single-layer counterpart by means of just wave propagation. However, the
study of ergodic mutual information (EMI) and outage probability for
SIM-assisted multiple-input-multiple-output (MIMO) systems is not available in
the literature. To this end, we obtain the distribution of the MI by using
large random matrix theory (RMT) tools. Next, we derive a tight closed-form
expression for the outage probability based on statistical channel state
information (CSI). Moreover, we apply the gradient descent method for the
minimization of the outage probability. Simulation results verify the
analytical results and provide fundamental insights such as the performance
enhancements compared to conventional MIMO systems and the single-layer
counterpart. Notably the proposed optimization algorithm is faster than the
alternating optimization (AO) benchmark by saving significant overhead.

</details>


### [49] [Robust Analog Lagrange Coded Computing: Theory and Algorithms via Discrete Fourier Transforms](https://arxiv.org/abs/2510.20379)
*Rimpi Borah,J. Harshan*

Main category: cs.IT

TL;DR: 提出了一个安全的模拟拉格朗日编码计算框架，能够抵御拜占庭工作节点的完整性威胁，通过DFT码纠错算法提高计算精度，并利用工作节点信任档案优化任务分配。


<details>
  <summary>Details</summary>
Motivation: 传统的ALCC虽然能保护数据隐私并应对延迟节点，但无法抵御返回错误结果的拜占庭工作节点，这构成了安全漏洞。

Method: 使用离散傅里叶变换码的纠错算法构建新的重构策略，并基于DFT解码器性能理论结果，提出考虑工作节点信任档案的任务分配方法。

Result: 该框架在有限数量的拜占庭工作节点存在时显著提高了计算精度，特别是在掌握工作节点信任信息的情况下。

Conclusion: 提出的安全ALCC框架有效增强了系统对拜占庭攻击的鲁棒性，同时分析了浮点实现精度噪声可能被利用的协同攻击策略。

Abstract: Analog Lagrange Coded Computing (ALCC) is a recently proposed computational
paradigm wherein certain computations over analog datasets are efficiently
performed using distributed worker nodes through floating point representation.
While the vanilla version of ALCC is known to preserve the privacy of the
datasets from the workers and also achieve resilience against stragglers, it is
not robust against Byzantine workers that return erroneous results.
Highlighting this vulnerability, we propose a secure ALCC framework that is
resilient against a wide range of integrity threats from the Byzantine workers.
As a foundational step, we use error-correction algorithms for Discrete Fourier
Transform (DFT) codes to build novel reconstruction strategies for ALCC thereby
improving its computational accuracy in the presence of a bounded number of
Byzantine workers. Furthermore, capitalizing on some theoretical results on the
performance of the DFT decoders, we propose novel strategies for distributing
the ALCC computational tasks to the workers, and show that such methods
significantly improve the accuracy when the workers' trust profiles are
available at the master server. Finally, we study the robustness of the
proposed framework against colluding attacks, and show that interesting attack
strategies can be executed by exploiting the inherent precision noise owing to
floating point implementation.

</details>


### [50] [Adversary-Aware Private Inference over Wireless Channels](https://arxiv.org/abs/2510.20518)
*Mohamed Seif,Malcolm Egan,Andrea J. Goldsmith,H. Vincent Poor*

Main category: cs.IT

TL;DR: 提出了一种用于保护隐私的AI感知框架，设备在将提取的特征传输到模型服务器之前进行特征变换，以防止敏感数据被重建。


<details>
  <summary>Details</summary>
Motivation: 在边缘网络中，传感器和模型服务器通常不位于同一位置，需要传输特征。由于攻击者可能重建敏感个人数据，需要对特征进行变换以降低隐私泄露风险。

Method: 设备在传输前对提取的特征应用变换，保护个体特征隐私。

Result: 提出了一种新的隐私保护AI感知框架，解决了传统差分隐私机制未涉及的个体特征保护问题。

Conclusion: 该框架为边缘AI应用中的隐私保护提供了一种有效解决方案，特别适用于自动驾驶和环境监测等场景。

Abstract: AI-based sensing at wireless edge devices has the potential to significantly
enhance Artificial Intelligence (AI) applications, particularly for vision and
perception tasks such as in autonomous driving and environmental monitoring. AI
systems rely both on efficient model learning and inference. In the inference
phase, features extracted from sensing data are utilized for prediction tasks
(e.g., classification or regression). In edge networks, sensors and model
servers are often not co-located, which requires communication of features. As
sensitive personal data can be reconstructed by an adversary, transformation of
the features are required to reduce the risk of privacy violations. While
differential privacy mechanisms provide a means of protecting finite datasets,
protection of individual features has not been addressed. In this paper, we
propose a novel framework for privacy-preserving AI-based sensing, where
devices apply transformations of extracted features before transmission to a
model server.

</details>


### [51] [Simultaneous Wireless Information and Power Transfer for Fluid Antenna Systems](https://arxiv.org/abs/2510.20569)
*Feilong Zhang,Jianxin Dai,Zhaohui Yang,Kai-Kit Wong,Lingyuxiu Li,Jianglin Ye*

Main category: cs.IT

TL;DR: 提出了一种结合MISO流体天线与传统固定位置天线的新通信系统，通过优化天线位置来提高能量收集效率，在SWIPT场景下显著提升能量接收器的性能。


<details>
  <summary>Details</summary>
Motivation: 流体天线技术通过改变天线位置来提升通信速率，但传统固定位置天线在能量收集效率方面存在局限。本文旨在结合两种天线优势，在SWIPT系统中优化能量传输效率。

Method: 采用MISO流体天线与传统固定天线结合的系统模型，在SWIPT框架下传输相同信号给信息接收器和能量接收器。通过优化发射和接收流体天线位置以及发射协方差矩阵，在满足信息接收器最小SINR约束条件下最大化能量收集效率。

Result: 仿真结果表明，相比传统固定位置天线系统，流体天线系统能显著提升能量接收器的能量收集效率。

Conclusion: 流体天线系统在SWIPT场景中具有显著优势，通过天线位置优化能够有效提升能量传输效率，为未来无线通信系统设计提供了新的技术路径。

Abstract: Fluid antenna is a promising wireless communication technology that enhances
communication rate by changing the antenna positions. This article proposes a
new communication system that combines multiple-input single-output (MISO)
fluid antennas with traditional fixed-position antennas, utilizing antenna
position optimization to improve energy harvesting efficiency. In this model,
we consider simultaneous wireless information and power transfer (SWIPT) which
transmits identical signals from the base station to both information receiver
(IR) and energy receiver (ER). We strive to enhance the power delivered to the
ER by fine-tuning the positions of transmit and receive fluid antennas, along
with optimizing the transmit covariance matrix, subject to a given minimum
signal-to-interference-plus-noise ratio (SINR) constraint at the IR. Simulation
results indicate that fluid antenna systems significantly enhance the energy
harvesting efficiency of the ER compared to traditional fixed-position
antennas.

</details>


### [52] [Stacked Intelligent Metasurfaces for 6G Wireless Networks: Principles, Applications, and Research Directions](https://arxiv.org/abs/2510.20572)
*Enyu Shi,Jiayi Zhang,Zhilong Liu,Ziheng Liu,Arumugam Nallanathan,Merouane Debbah,Shi Jin,Bo Ai*

Main category: cs.IT

TL;DR: 本文综述了基于堆叠智能超表面(SIM)的分布式无线网络在6G中的应用，包括系统架构、信号处理挑战和性能增益案例研究，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要在高度动态环境中提供泛在连接、弹性覆盖和智能服务，而分布式无线架构如无蜂窝大规模MIMO因其可扩展性和公平性受到关注。SIM作为可重构智能表面的演进，提供了增强的多层电磁域处理能力。

Method: 将SIM集成到分布式无线网络中，实现先进的波域操作，包括分层框架、用户关联和联合预编码等信号处理技术。

Result: 案例研究表明SIM辅助的分布式无线网络在干扰管理、能量和频谱效率以及物理层安全性方面取得了显著的性能提升。

Conclusion: SIM为6G分布式无线网络提供了实现可扩展性和智能化的有前景途径，未来研究应关注硬件设计、能耗建模、算法开发和人工智能集成等方向。

Abstract: The sixth-generation (6G) wireless networks are expected to deliver
ubiquitous connectivity, resilient coverage, and intelligence-driven services
in highly dynamic environments. To achieve these goals, distributed wireless
architectures such as cell-free massive multiple-input multiple-output (MIMO)
have attracted significant attention due to their scalability and fairness.
Recently, stacked intelligent metasurfaces (SIMs) have emerged as a promising
evolution of reconfigurable intelligent surfaces, offering multi-layer
electromagnetic domain processing with enhanced controllability and spatial
degrees of freedom. By integrating SIMs into distributed wireless networks,
advanced wave-domain operations can be realized, enabling efficient
interference management, improved energy and spectral efficiency, and robust
physical-layer security. This article provides a comprehensive overview of
SIM-aided distributed wireless networks, including their application scenarios,
classification, and system architectures. Key signal processing challenges,
such as hierarchical frameworks, user association, and joint precoding, are
discussed, followed by case studies demonstrating significant performance
gains. Finally, future research directions in hardware design, energy
consumption modeling, algorithm development, and artificial intelligence
integration are highlighted, aiming to pave the way for scalable and
intelligent 6G distributed wireless networks.

</details>


### [53] [Super-Linear Growth of the Capacity-Achieving Input Support for the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2510.20723)
*Haiyang Wang*

Main category: cs.IT

TL;DR: 本文研究了AWGN信道在幅度约束下容量达成的输入分布支撑点数量的增长特性，证明了支撑点数量K(A)随幅度约束A的增加呈超线性增长。


<details>
  <summary>Details</summary>
Motivation: 虽然Smith(1971)已证明最优输入是离散的且具有有限个支撑点，但关于支撑点数量K(A)随幅度约束A增加的紧致边界仍然开放。本文旨在填补这一理论空白。

Method: 结合输出分布在总变差意义下收敛到均匀分布的性质，以及高斯混合逼近的定量极限，推导了新的解析下界。

Result: 证明了K(A)随A的增长是超线性的，即比线性增长更快。

Conclusion: 本文为幅度约束AWGN信道的最优输入分布支撑点数量提供了新的理论下界，确认了其超线性增长特性。

Abstract: We study the growth of the support size of the capacity-achieving input
distribution for the amplitude-constrained additive white Gaussian noise (AWGN)
channel. While it is known since Smith (1971) that the optimal input is
discrete with finitely many mass points, tight bounds on the number of support
points $K(A)$ as the amplitude constraint $A$ increases remain open. Building
on recent work by Dytso \emph{et al.} (2019) and Mattingly \emph{et al.}
(2018), we derive a new analytical lower bound showing that $K(A)$ grows
super-linearly in $A$. Our approach combines total-variation convergence of the
output distribution to the uniform law with quantitative limits on Gaussian
mixture approximation.

</details>


### [54] [MIMO-Zak-OTFS with Superimposed Spread Pilots](https://arxiv.org/abs/2510.20734)
*Abhishek Bairwa,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出了一种用于MIMO-Zak-OTFS系统的叠加扩频导频设计和有效信道估计方案，通过交叉模糊域分离导频序列，结合turbo迭代来减轻导频数据干扰。


<details>
  <summary>Details</summary>
Motivation: 在MIMO-Zak-OTFS系统中，数据与扩频导频信号叠加在同一帧中，需要有效分离不同发射天线的导频序列以实现良好的信道估计性能。

Method: 设计在交叉模糊域分离的扩频导频序列，通过简单的读取操作估计有效信道抽头，并采用信道估计与检测之间的turbo迭代来减轻导频数据干扰。

Result: 在2×2和3×3 MIMO-Zak-OTFS系统中，使用高斯sinc脉冲成形滤波器对车辆A信道模型进行仿真，所提方案经过三次turbo迭代可获得优异的估计/检测性能。

Conclusion: 所提出的导频设计和估计方案能够有效分离MIMO导频序列，并通过turbo迭代显著提升系统性能，适用于叠加导频的MIMO-Zak-OTFS系统。

Abstract: In this paper, we consider the problem of spread pilot design and effective
channel estimation in multiple-input multiple-output Zak-OTFS (MIMO-Zak-OTFS)
with superimposed spread pilots, where data and spread pilot signals are
superimposed in the same frame. To achieve good estimation performance in a
MIMO setting, the spread pilots at different transmit antennas need to be
effectively separated at the receiver. Towards this, we propose a spread pilot
design that separates the pilot sequences in the cross-ambiguity domain and
enables the estimation of the effective channel taps by a simple read-off
operation. To further alleviate the effect of pilot-data interference on
performance, we carry out turbo iterations between channel estimation and
detection. Simulation results for $2\times 2$ and $3\times 3$ MIMO-Zak-OTFS
with Gaussian-sinc pulse shaping filter for vehicular-A channel model show that
the proposed pilot design and estimation scheme with three turbo iterations can
achieve very good estimation/detection performance.

</details>
