<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.IT](#cs.IT) [Total: 41]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A user subscription model in mobile radio access networks with network slicing](https://arxiv.org/abs/2601.10605)
*José-Ramón Vidal,Luis Guijarro,Vicent Pla*

Main category: cs.NI

TL;DR: 评估网络切片中基于logit模型的用户订阅预测在移动无线场景下的有效性，通过仿真验证模型准确性


<details>
  <summary>Details</summary>
Motivation: 网络切片技术将蜂窝网络逻辑解耦为基础设施提供商和网络切片租户，需要有效的业务模型来管理资源分配和用户订阅。现有的logit模型在用户移动性和无线传播环境下，其假设可能不成立，需要验证模型在移动无线场景下的准确性。

Method: 提出包含资源分配和用户订阅的竞争性业务模型，使用logit模型预测用户订阅均衡。通过计算机仿真进行验证，仿真模型包含完整且现实的用户移动性和无线传播特性。

Result: 通过仿真对比发现，在大多数情况下，logit模型在移动无线场景下仍能提供有效的结果，验证了该模型在此类环境下的适用性。

Conclusion: 尽管用户移动性和无线传播特性可能违反logit模型的某些假设，但该模型在移动无线场景下仍然有效，为网络切片中的资源分配和用户订阅管理提供了可靠的理论基础。

Abstract: Network slicing is an architectural enabling technology that logically decouples the current cellular networks into infrastructure providers (InPs) and Network Slice Tenants (NSTs). The network resources (e.g., radio access resources at each cell) are owned by the InP, and are shared by the NSTs to provide a service to their mobile users. In this context, we proposed a business model that includes resource allocation and user subscription to NSTs in a competitive setting, and provides, among other things, closed-form expressions for the subscription indicators in equilibrium of each NST at each cell. This model relies on the widely adopted logit model to characterize user subscriptions. However, as a consequence of user mobility and radio propagation, some of the underlying assumptions in the logit model do not hold. Therefore, further research is needed to assess the accuracy of the results provided by the logit model in a mobile radio scenario. We carry out a thorough evaluation of the validity of the model by comparing its results against those obtained through computer simulation. Our simulation model includes complete and realistic characterizations of user mobility and radio propagation. From the results, we conclude in most cases the logit model provides valid results in a mobile radio scenario.

</details>


### [2] [Starfield: Demand-Aware Satellite Topology Design for Low-Earth Orbit Mega Constellations](https://arxiv.org/abs/2601.10083)
*Shayan Hamidi Dehshali,Tzu-Hsuan Liao,Shaileshh Bojja Venkatakrishnan*

Main category: cs.NI

TL;DR: Starfield是一种基于需求感知的卫星拓扑设计算法，通过向量场和黎曼度量优化星间链路选择，相比传统+Grid拓扑减少30%跳数和15%拉伸因子。


<details>
  <summary>Details</summary>
Motivation: 现有卫星拓扑（如+Grid和Motif）忽略了区域流量模式、地面站位置和星座几何结构。由于地球人口分布不均和农村地区孤立，流量模式本质不均匀，这为根据流量模式定向星间链路提供了机会。

Method: 首先根据流量流在星座壳层上建立向量场，并在球形流形上定义相应的黎曼度量。该度量结合空间几何为每个潜在星间链路分配距离，然后聚合所有需求流生成每个卫星的链路选择启发式。每个卫星选择具有最小黎曼启发式值的链路及其对应的角度链路。

Result: 对于Phase 1 Starlink，仿真结果显示跳数减少高达30%，拉伸因子改善15%。静态Starfield（Starfield的星间轨道链路匹配修改版）在真实流量模式下相比+Grid实现20%的拉伸因子改善。实验还证明了Starfield在流量需求扰动下的鲁棒性。

Conclusion: Starfield通过需求感知的拓扑设计有效优化了LEO巨型星座的性能，显著减少了跳数和延迟，为下一代卫星互联网提供了更高效的网络架构。

Abstract: Low-Earth orbit (LEO) mega-constellations are emerging as high-capacity backbones for next-generation Internet. Deployment of laser terminals enables high-bandwidth, low-latency inter-satellite links (ISLs); however, their limited number, slow acquisition, and instability make forming a stable satellite topology difficult. Existing patterns like +Grid and Motif ignore regional traffic, ground station placement, and constellation geometry. Given sparse population distribution on Earth and the isolation of rural areas, traffic patterns are inherently non-uniform, providing an opportunity to orient inter-satellite links (ISLs) according to these traffic patterns. In this paper, we propose Starfield, a novel demand-aware satellite topology design heuristic algorithm supported by mathematical analysis. We first formulate a vector field on the constellation's shell according to traffic flows and define a corresponding Riemannian metric on the spherical manifold of the shell. The metric, combined with the spatial geometry, is used to assign a distance to each potential ISL, which we then aggregate over all demand flows to generate a heuristic for each satellite's link selection. Inspired by +Grid, each satellite selects the link with the minimum Riemannian heuristic along with its corresponding angular links. To evaluate Starfield, we developed a custom, link-aware, and link-configurable packet-level simulator, comparing it against +Grid and Random topologies. For the Phase 1 Starlink, simulation results show up to a 30% reduction in hop count and a 15% improvement in stretch factor across multiple traffic distributions. Moreover, static Starfield, an inter-orbital link matching modification of Starfield, achieves a 20% improvement in stretch factor under realistic traffic patterns compared to +Grid. Experiments further demonstrate Starfield's robustness under traffic demand perturbations.

</details>


### [3] [SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks](https://arxiv.org/abs/2601.10544)
*Andrea Piroddi,Riccardo Fonti*

Main category: cs.NI

TL;DR: SDN集成到MANET和IoT网络，通过集中控制提升路由效率、可扩展性和安全性，降低CAPEX/OPEX，改善性能指标


<details>
  <summary>Details</summary>
Motivation: MANET和IoT网络在去中心化动态环境中面临路由效率低、可扩展性有限、安全漏洞等挑战，需要统一解决方案来改善性能

Method: 提出SDN集成框架，利用其集中控制和网络可编程性；建立数学模型评估SDN对CAPEX、OPEX和性能指标的影响

Result: SDN增强的MANET和IoT网络在动态大规模环境中表现出优越的可扩展性、更低延迟、更高吞吐量、更低丢包率，显著提升路由效率和资源优化

Conclusion: SDN集成提供强大可扩展解决方案，能有效管理节点密度增长、动态拓扑和高数据流量，满足现代大规模应用的性能和可靠性需求

Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.

</details>


### [4] [Enhancing Mobile Ad Hoc Networks (MANETs) with Software-Defined Networking (SDN): A Balanced Approach](https://arxiv.org/abs/2601.10556)
*Riccardo Fonti,Andrea Piroddi*

Main category: cs.NI

TL;DR: 该论文探讨了将软件定义网络(SDN)集成到移动自组织网络(MANET)中以优化性能，通过集中控制和网络虚拟化解决MANET的动态拓扑和节点移动性挑战，并开发数学模型分析CAPEX、OPEX和网络效率。


<details>
  <summary>Details</summary>
Motivation: 移动自组织网络(MANET)具有去中心化、动态拓扑和节点移动性等特点，在管理上面临挑战。软件定义网络(SDN)的集中控制和网络虚拟化原则为解决这些挑战提供了有前景的方案，可以优化MANET的可扩展性、成本效益和安全性。

Method: 论文提出了将SDN与MANET集成的方案，利用SDN的集中控制、网络虚拟化等原则来管理MANET。开发了一个数学模型来分析资本支出(CAPEX)、运营支出(OPEX)和网络效率，通过定量分析评估集成方案的性能优化效果。

Result: 研究表明SDN与MANET的集成能够有效优化MANET性能，特别是在可扩展性、成本效率和安全性方面。数学模型分析显示该方案在降低CAPEX和OPEX的同时提高了网络效率，为动态无线网络管理提供了更高效的解决方案。

Conclusion: SDN与MANET的集成为管理动态无线网络挑战提供了有前景的解决方案。通过集中控制和网络虚拟化，该方案能够显著优化MANET的性能，在可扩展性、成本效益和安全性方面都有明显改善，为未来网络架构设计提供了重要参考。

Abstract: Mobile Ad Hoc Networks (MANETs) are decentralized wireless networks, characterized by their dynamic topologies and node mobility. In the era of cutting-edge technologies, integrating Software-Defined Networking (SDN) with MANETs offers a promising solution to manage these challenges more efficiently. This paper presents a balanced discussion of MANETs and SDN, demonstrating how SDN principles, such as centralized control and network virtualization, can optimize MANET performance in terms of scalability, cost-efficiency, and security. A mathematical model is developed to analyze Capital Expenditures (CAPEX), Operational Expenditures (OPEX), and network efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 论文提出了一个分析AI生存风险的框架，基于两个前提：AI将变得极其强大，以及强大的AI会毁灭人类。作者构建了四种人类幸存的场景，并分析了每种场景面临的挑战和应对策略，最后给出了AI毁灭人类的概率估计。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论日益激烈。本文旨在建立一个系统性框架来分析AI的生存风险，帮助理解不同的风险场景及其应对策略。

Method: 基于两个核心前提构建分析框架：1) AI将变得极其强大；2) 极其强大的AI会毁灭人类。通过这两个前提的失败组合，构建了四种人类幸存的场景（生存故事），并分析每种场景的挑战和相应的应对措施。

Result: 提出了一个包含四种生存故事的分类法：1) 科学障碍阻止AI变得极其强大；2) 人类禁止AI研究；3) 极其强大的AI因其目标而不毁灭人类；4) 人类能可靠检测并禁用有毁灭目标的AI系统。分析了每种故事的挑战，并基于此框架给出了P(doom)的粗略估计。

Conclusion: 不同的生存故事面临不同的挑战，需要不同的风险应对策略。该分类框架有助于系统性地思考AI生存风险，并为制定相应的风险缓解措施提供理论基础。最终通过该框架对AI毁灭人类的概率进行了估计。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [6] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes：一个用于GUI任务的主动视觉感知强化学习框架，通过两阶段推理和渐进感知策略，让智能体学习何时、是否以及如何使用视觉工具（如裁剪、缩放），在数据有限的情况下显著提升GUI界面理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法主要依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力。需要开发能够主动进行视觉感知的GUI智能体。

Method: 1. 两阶段推理过程：智能体学习在粗粒度探索和细粒度定位两个阶段中策略性地决定是否以及如何调用视觉工具（裁剪、缩放等）。2. 渐进感知策略：将决策分解为粗粒度探索和细粒度定位，由两级策略协调。3. 空间连续奖励函数：针对工具使用设计，整合位置接近性和区域重叠度，提供密集监督，缓解GUI环境中常见的奖励稀疏问题。

Result: 在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标记样本就实现了44.8%的定位准确率，显著优于监督学习和基于RL的基线方法。

Conclusion: 工具感知的主动感知，通过分阶段策略推理和细粒度奖励反馈实现，对于构建鲁棒且数据高效的GUI智能体至关重要。GUI-Eyes框架展示了在有限数据下显著提升GUI任务性能的潜力。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [7] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec是一个证明携带的协商推荐系统，通过分离自然语言推理和确定性约束执行，在满足治理约束（如长尾曝光、多样性）的同时保持推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现代基于LLM的推荐系统虽然能生成有吸引力的排名列表，但难以可靠地满足治理约束（如最小长尾曝光或多样性要求）。需要一种既能保持推荐质量又能确保约束满足的可验证方法。

Method: PCN-Rec采用证明携带的协商管道：1) 基础推荐器生成候选窗口；2) 两个代理协商（用户倡导者优化相关性，策略代理执行约束）；3) 调解LLM合成top-N列表及结构化证书；4) 确定性验证器检查约束满足；5) 验证失败时使用确定性约束贪婪修复生成合规列表。

Result: 在MovieLens-100K数据集上，PCN-Rec在可行用户中达到98.55%的通过率（n=551，W=80），相比无验证/修复的单LLM基线显著提升。同时保持推荐效用，NDCG@10仅下降0.021（0.403 vs. 0.424），差异具有统计显著性（p<0.05）。

Conclusion: PCN-Rec通过分离推理与约束执行，实现了可验证的约束满足推荐系统，在保持推荐质量的同时可靠地满足治理要求，为可审计的推荐系统提供了新范式。

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [8] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现，人们会花费个人资源惩罚使用LLM完成任务的人，惩罚程度随实际使用量单调增加，且存在"可信度差距"：自称未使用者比实际未使用者受到更严厉惩罚。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速普及，人们对其引发的社会反应产生担忧。先前研究记录了人们对AI用户的负面态度，但尚不清楚这种不赞同是否会转化为实际的代价性行为。

Method: 采用两阶段在线实验设计：第一阶段提供目标参与者，第二阶段491名参与者可以用自己的资金减少先前完成真实努力任务（有或无LLM支持）的同伴的收入。

Result: 参与者平均销毁了完全依赖LLM者36%的收入，惩罚程度随实际LLM使用量单调增加。存在可信度差距：自称未使用者比实际未使用者受到更严厉惩罚；在高使用水平下，实际依赖比自称依赖受到更强烈惩罚。

Conclusion: 这是首个行为证据表明，LLM的效率提升伴随着社会制裁的代价，揭示了AI使用在社会互动中的复杂动态。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [9] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出了一种非交互式端到端推理框架AAI，通过注意力重加权机制激活逻辑推理模式，无需外部资源即可提升LLM的逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法依赖复杂的交互框架或外部符号求解器，存在额外开销和可扩展性限制。需要一种非交互式、端到端的框架，让推理能力在模型内部自然涌现

Method: 提出注意力感知干预（AAI）：1）在few-shot提示中引入结构信息，激活与逻辑推理算子对齐的注意力头；2）在推理时对选定注意力头的注意力分数进行重加权，引导模型利用先验知识

Result: AAI在多种基准测试和模型架构上显著提升逻辑推理性能，同时仅引入可忽略的计算开销

Conclusion: AAI提供了一种高效的非交互式推理框架，通过注意力调制引导模型推理，无需外部资源即可增强LLM的逻辑推理能力，具有良好的泛化性和可分析性

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [10] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: 提出Min-Seek方法，一种新颖的顺序测试时缩放技术，通过动态KV缓存管理解决现有方法在长推理时精度下降和不稳定的问题，无需推理长度微调。


<details>
  <summary>Details</summary>
Motivation: 现有顺序测试时缩放方法虽然能通过延长推理提高模型精度，但随着推理长度进一步增加，会出现精度下降和模型不稳定的问题，且需要推理长度微调。

Method: Min-Seek方法：1) 仅保留一个额外诱导思想的KV对在KV缓存中，提高效率；2) 使用自定义KV缓存，存储不带位置嵌入的键，并在每个新生成思想前动态连续编码；3) 支持超出模型最大上下文长度的推理。

Result: 显著提高了模型在各种推理任务上的精度，稳定了顺序缩放的精度，消除了推理长度微调的需求，在温和条件下具有线性计算复杂度。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时缩放方法，能够在大范围诱导思想下保持模型精度，支持超长推理，具有实际应用价值。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [11] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 本文对大型语言模型对话代理中拟人化现象的伦理研究进行了范围综述，梳理了概念基础、伦理挑战与机遇、方法论方法，并提出了研究议程和设计/治理建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的对话代理日益普及，拟人化现象（将非人类实体赋予人类特质）变得愈发显著。现有文献在不同领域呈现碎片化，对拟人化的定义、操作化和伦理评估存在显著差异，需要进行系统性梳理。

Method: 采用范围综述方法，对五个数据库和三个预印本库中的伦理导向研究进行系统映射，综合分析了概念基础、伦理挑战与机遇、方法论方法三个方面。

Result: 研究发现：在概念上趋向于基于归因的定义，但在操作化上存在显著分歧；伦理框架以风险导向为主；将观察到的交互效应与可操作的治理指导联系起来的实证研究有限。

Conclusion: 提出了研究议程和设计/治理建议，旨在为基于LLM的对话代理中拟人化线索的伦理部署提供指导，强调需要更多实证研究来连接交互效应与治理实践。

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [12] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 论文将人-AI互补性重新定义为可靠认知过程的证据，而非单纯预测准确性的相对指标，从而解决其理论挑战。


<details>
  <summary>Details</summary>
Motivation: 人-AI互补性概念面临理论挑战：缺乏精确理论锚定、仅作为事后预测准确性指标、忽视其他人-AI交互需求、抽象化性能增益的成本特征，导致实证研究中难以实现。

Method: 利用认识论框架，将互补性重新置于"论证性AI"话语中，借鉴计算可靠性主义，将历史互补性实例视为特定人-AI交互作为可靠认知过程的证据。

Result: 互补性与其他可靠性指标（评估人-AI团队与认知标准和社会技术实践的契合度）共同贡献于人-AI团队生成预测时的可靠性程度。

Conclusion: 互补性的角色和价值不在于提供预测准确性的相对度量，而在于帮助校准决策以适应日益塑造日常生活的AI支持过程的可靠性，支持受影响者（患者、管理者、监管者等）的实践推理。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [13] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过A2A通信动态协调智能体，无需预定义工作流，在GAIA基准上超越基于工作流的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两大局限：1) 需要大量人工努力预测和编码可能状态；2) 无法穷尽复杂现实任务的状态空间。

Method: 提出信息流编排的多智能体范式，通过专用信息流编排器持续监控任务进度，使用A2A工具包以自然语言动态协调其他智能体，不依赖预定义工作流。

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL基线（55.15%）提升8.49个百分点，且token消耗相当。案例级分析显示该方法能更灵活监控任务并更稳健处理边缘情况。

Conclusion: 信息流编排的多智能体范式通过动态协调机制克服了基于规则工作流的局限性，在复杂任务处理中展现出更好的灵活性和鲁棒性。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [14] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出了"连续记忆架构"(CMA)来解决传统RAG系统将记忆视为静态查找表的局限性，通过持久化存储、选择性保留、关联路由、时间链和抽象整合等机制，使LLM智能体能够积累、更新和消歧记忆。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索增强生成(RAG)的LLM智能体将记忆视为无状态的查找表：信息永久存在、检索只读、缺乏时间连续性。这种架构无法支持智能体在长期交互中积累、更新或消歧记忆，限制了其在复杂任务中的表现。

Method: 提出"连续记忆架构"(CMA)这一系统类别，不指定具体实现细节，而是定义架构要求：持久化存储、选择性保留、关联路由、时间链和整合为高阶抽象。通过知识更新、时间关联、关联回忆、上下文消歧等实验验证其行为优势。

Result: CMA在暴露RAG结构缺陷的任务上表现出持续的行为优势，特别是在知识更新、时间关联、关联回忆和上下文消歧方面。证明了CMA是实现长期智能体的必要架构原语，同时指出了延迟、漂移和可解释性等开放挑战。

Conclusion: CMA为解决传统RAG系统的记忆局限性提供了新的架构范式，为构建能够积累、更新和消歧记忆的长期智能体奠定了基础，但实际应用中仍需解决延迟、漂移和可解释性等技术挑战。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [15] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种针对计算机使用代理（CUAs）的单次规划方法，通过可信规划器在执行前生成完整的条件分支执行图，实现了对提示注入攻击的鲁棒防御，同时保持了实用性。


<details>
  <summary>Details</summary>
Motivation: AI代理容易受到提示注入攻击，导致凭证窃取或财务损失。现有的鲁棒防御需要架构隔离，但这对需要持续观察UI状态的计算机使用代理（CUAs）构成了根本性挑战，因为安全隔离与连续观察需求相冲突。

Method: 提出单次规划方法：在观察任何可能恶意内容之前，由可信规划器生成包含条件分支的完整执行图。这种方法提供可证明的控制流完整性保证，防止任意指令注入攻击。同时还需要额外措施防止分支导向攻击。

Result: 在OSWorld上评估显示，该方法在保持前沿模型57%性能的同时，将较小开源模型的性能提升了19%，证明严格的安全性和实用性可以在CUAs中共存。

Conclusion: 通过单次规划方法解决了计算机使用代理中安全隔离与连续观察的根本冲突，实现了对提示注入攻击的鲁棒防御，同时保持了系统实用性，为安全AI代理设计提供了新思路。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [16] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 该论文提出了一个基于根因认知的幻觉管理操作框架，通过模型、数据和上下文三个层面的分类干预，结合多层次检测与缓解策略，构建可扩展的可靠生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化的管理方法。

Method: 提出基于持续改进循环的幻觉管理框架，将幻觉来源分为模型、数据和上下文相关因素，整合不确定性估计、推理一致性等多方面检测方法，以及知识基础、置信度校准等分层缓解策略。

Result: 通过分层架构和金融数据提取案例研究展示了框架应用，其中模型、上下文和数据层形成闭环反馈循环，实现渐进式可靠性增强。

Conclusion: 该方法为受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论，能够针对性地解决幻觉问题而非采用通用修复方案。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [17] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是针对中国劳动法领域的专业大语言模型，配合LabourLawBench基准测试，在多项劳动法任务上超越通用模型和现有法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和情境敏感性的专业法律子领域时表现不佳，特别是在中国劳动法这样的专门领域。

Method: 开发LabourLawLLM专门针对中国劳动法，同时创建LabourLawBench基准测试，涵盖法律条款引用、知识问答、案例分类、赔偿计算、命名实体识别和法律案例分析等任务。评估框架结合客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Result: 实验表明LabourLawLLM在各项任务类别上持续优于通用模型和现有法律专用大语言模型，在劳动法领域表现出更高的准确性和可靠性。

Conclusion: 该方法不仅适用于劳动法领域，还为构建其他法律子领域的专业大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [18] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG：一种用于持续个性化LLM的半参数框架，通过漂移驱动选择性适应和严格相关性门控，有效处理用户偏好漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法假设用户偏好静态不变，但真实世界中用户兴趣持续演变，标准持续学习方法无法区分真实偏好漂移与瞬态上下文噪声

Method: SPRInG采用漂移驱动选择性适应，使用基于似然的评分函数识别高新颖性交互，选择性更新用户特定适配器；推理时应用严格相关性门控，通过logit插值融合参数化知识与检索历史

Result: 在长格式个性化生成基准测试中，SPRInG优于现有基线，验证了其在真实世界持续个性化任务中的鲁棒性

Conclusion: SPRInG通过选择性适应和知识融合机制，有效解决了LLM持续个性化中的偏好漂移问题，为动态用户交互场景提供了实用解决方案

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [19] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL：一种无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD数据集上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1）仅依赖正确示例进行上下文学习，忽略了历史错误修复对中的丰富信号；2）测试时扩展方法通常任意分解问题，导致多次运行产生几乎相同的SQL候选，降低集成增益。这些方法还存在明显的准确率-效率权衡问题。

Method: 提出Memo-SQL框架，包含两个核心思想：结构化分解（采用实体级、分层和原子顺序三种策略）和经验感知自校正（构建包含成功查询和历史错误修复对的动态记忆，使用检索增强提示在推理时引入相关示例）。

Result: 在BIRD数据集上达到68.5%的执行准确率，在无需微调的开源方法中创造了新的SOTA，同时比之前的TTS方法节省超过10倍的资源。

Conclusion: Memo-SQL通过结构化分解促进多样化推理，利用历史错误修复对实现更稳健的自校正，无需训练即可显著提升NL2SQL性能，同时保持高效率。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [20] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，通过三种机制实现人格的连贯表达、情境适应和长期演化，为HCI中自然化智能体设计提供支持。


<details>
  <summary>Details</summary>
Motivation: LLM在HCI中应用日益广泛，但现有方法难以实现既细腻又可适应的人格表达。人格对用户参与度、决策和真实感感知至关重要，需要更有效的人格建模框架。

Method: 提出基于荣格心理类型的人格建模框架，包含三个核心机制：1) 主导-辅助协调机制确保核心人格连贯表达；2) 强化-补偿机制实现情境临时适应；3) 反思机制驱动长期人格演化。

Result: 使用迈尔斯-布里格斯类型指标问卷评估人格对齐，并在多样化挑战场景中测试。结果表明演化的人格感知LLM能够支持连贯、情境敏感的交互。

Conclusion: 演化的人格感知LLM能够实现自然化的智能体设计，支持HCI中连贯且情境敏感的交互，为个性化助手和社会模拟等应用提供更真实的体验。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [21] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: 提出PaperScout自主代理，将论文搜索重新定义为顺序决策过程，并引入PSPO优化方法解决多轮代理任务中的粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文搜索方法依赖僵化的预定义工作流，难以处理复杂的条件查询。需要一种能够动态决策的自适应代理框架。

Method: 提出PaperScout自主代理，将论文搜索重构为顺序决策过程，动态决定是否、何时以及如何调用搜索和扩展工具。为解决训练挑战，引入Proximal Sequence Policy Optimization (PSPO)，这是一种过程感知的序列级策略优化方法。

Result: 在合成和真实世界基准测试中，PaperScout在召回率和相关性方面显著优于基于工作流和强化学习的基线方法。

Conclusion: PaperScout的自适应代理框架和PSPO优化策略有效解决了学术论文搜索中的复杂查询问题，验证了序列级优化在多轮代理任务中的重要性。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [22] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep：首个基于多保真度数据的深度学习框架，用于解决弹性塑性固体大变形问题，通过同时利用低保真度（高数量）和高保真度（高精度）数据来解决数据数量与精度之间的困境。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形弹性塑性固体计算中存在固有局限，而现有深度学习技术需要高质量数据集，但在大变形问题中难以获取。数据构建过程中存在数量与精度的两难困境，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，针对大变形问题（以拉伸弯曲为代表应用），同时使用低保真度（数量多但精度低）和高保真度（精度高但数量少）数据进行训练。设计了注意力机制支持的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep框架在多个指标上持续达到最先进性能，并能高效部署于制造应用中。

Conclusion: FilDeep是首个使用多保真度数据解决大变形问题的深度学习框架，成功解决了数据数量与精度之间的困境，为大变形弹性塑性固体计算提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [23] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析100万亿token真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编程辅助类应用流行、智能体推理兴起，并识别出早期用户的"玻璃鞋"留存效应。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM从单次模式生成转向多步推理，但实际使用情况缺乏实证研究。需要了解开发者和终端用户在真实场景中如何与LLM交互。

Method: 利用OpenRouter平台作为AI推理提供商，分析超过100万亿token的真实世界LLM交互数据，涵盖不同任务、地域和时间维度。

Result: 观察到开源模型广泛采用；创意角色扮演（不仅仅是生产力任务）和编程辅助类别异常流行；智能体推理兴起；留存分析发现早期用户的参与持续时间远长于后期用户，称为"玻璃鞋"效应。

Conclusion: LLM的实际使用复杂多样，这些发现对模型构建者、AI开发者和基础设施提供商有重要启示，数据驱动的使用理解可以指导更好的LLM系统设计和部署。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [24] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT：一种基于矩阵规划的结构化思维链框架，通过规范化自然语言表达式、添加显式引用字段和矩阵规划方法，增强LLM在符号推理任务中的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在依赖符号表达式和严格演绎规则的逻辑推理任务上表现不足；神经符号方法依赖外部求解器，对格式敏感且容易因模型输出不稳定而失败；LLM驱动方法缺乏结构化表示和过程级纠错机制。

Method: 提出MatrixCoT框架：1) 对自然语言表达式进行规范化和类型化；2) 添加显式引用字段；3) 引入基于矩阵的规划方法以保持步骤间的全局关系；4) 添加反馈驱动的重新规划机制，在语义等价约束下识别遗漏和缺陷，重写和压缩依赖矩阵。

Result: 在五个逻辑推理基准和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，在处理复杂符号推理任务时增强了鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划，有效提升了LLM的逻辑推理能力，解决了现有方法在符号推理任务中的局限性，为增强LLM的推理能力提供了新方向。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [25] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 本文提出一种新理论见解：学生模型可以在特定子领域超越教师模型，并提出Scheduled Checkpoint Distillation和Adaptive Weighting方法，在多个领域任务中实现学生模型性能匹配甚至超越教师模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署困难，而将微调后的LLM蒸馏到小型学生模型时，师生容量差距导致性能不佳。核心问题是：学生模型何时以及如何在特定领域任务上匹配甚至超越教师模型？

Method: 提出Scheduled Checkpoint Distillation（SCD）方法，通过模拟教师模型在领域任务SFT过程中的收敛过程来减少TFS（教师优势子领域）的缺陷；同时提出样本级Adaptive Weighting（AW）机制来保持学生在SFS（学生优势子领域）的优势。

Result: 在多个领域任务（包括QA、NER、多语言文本分类）上的实验表明，该方法持续优于现有蒸馏方法，使学生模型能够匹配甚至超越其微调教师模型的性能。

Conclusion: 学生模型可以在特定领域任务上超越教师模型，关键在于平衡学生在SFS的优势和TFS的缺陷。提出的SCD和AW方法为实现这一目标提供了有效途径。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [26] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，通过片段级检索增强和多智能体推理，在多个物理化学性质约束下精确生成分子。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在精确多目标控制和数值推理方面存在困难，需要外部结构和反馈来生成满足多个精确数值约束的分子。

Method: 采用两阶段框架：第一阶段通过多智能体推理器进行检索锚定的片段级编辑生成原型；第二阶段使用基于GRPO训练的片段级优化器进行细粒度优化，最小化属性误差并控制编辑复杂度。

Result: 在两组性质约束（QED、LogP、分子量和HOMO、LUMO）下的生成实验中，MolGen在有效性和精确满足多性质目标方面表现优于强LLM和基于图的算法。

Conclusion: MolGen通过片段级推理和可控细化，能够更好地处理分子生成中的多性质约束问题，实现了对数值目标的精确控制。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [27] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型，但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多个领域展现出强大的推理和预测能力，但其从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测表现。

Method: 使用简单的重复购买场景作为代表性案例，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究考察了不同层次的上下文信息对LLM预测准确性的影响。

Result: 1. LLMs虽然优于轻量级统计基线模型，但始终不如专用机器学习模型，显示出其在捕捉定量时间结构方面的有限能力。
2. 适度的上下文信息可以提高LLM的准确性，但添加更多用户级详细信息反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，并为设计未来的上下文感知混合模型提供了指导，这些模型需要整合统计精度和语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [28] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一个漂移感知的数据流系统，通过机器学习自适应控制数据生成过程，解决金融量化中训练与真实表现之间的差距问题


<details>
  <summary>Details</summary>
Motivation: 量化金融中，概念漂移和分布非平稳性导致训练数据与真实市场表现之间存在差距，静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不够用"的理念强调需要能够随市场演化的自适应数据生成方法

Method: 设计了一个漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理过程中。系统包含参数化数据操作模块（单股变换、多股混合、筛选操作）和自适应规划调度器，采用基于梯度的双层优化来控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一在一个可微分框架下，支持溯源感知的重放和持续数据质量监控

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性，提高了风险调整后的收益

Conclusion: 该系统为金融数据的自适应数据管理和学习引导的工作流自动化提供了一个可推广的方法

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [29] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: DecisionLLM：将大型语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，在长序列决策问题上超越传统决策Transformer


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在长序列决策问题中的应用潜力，解决LLMs无法理解连续数值的问题，将强化学习任务转化为序列建模问题

Method: 提出DecisionLLM框架，将轨迹数据作为独立模态，学习轨迹数据与自然语言任务描述的对齐，实现自回归预测未来决策

Result: DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分，建立了模型规模、数据量和数据质量三个维度的缩放规律

Conclusion: LLMs在离线决策任务中表现出强大潜力，为在线竞价等应用开辟了新方向，证明了将轨迹作为独立模态的有效性

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [30] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，用于标准化医学影像AI模型的访问，解决AI实现多样性、文档不一致和可重复性问题，通过容器化包装模型支持DICOM处理，提供统一接口和元数据。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究面临多种AI实现和架构、文档不一致以及可重复性问题的挑战，限制了研究和临床应用。需要标准化平台来促进可访问性和可重复性。

Method: 开发MHub.ai开源容器化平台，将同行评审的AI模型打包成标准化容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，包含参考数据验证模型运行。

Result: 平台包含初始的先进分割、预测和特征提取模型，支持多模态。通过肺分割模型的比较评估展示了临床实用性，公开了分割结果和评估指标，提供交互式仪表板。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化的障碍，增强了透明度和可重复性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [31] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: MMPG：基于多视角图构建和专家混合的蛋白质表示学习框架，通过物理、化学、几何多视角融合提升蛋白质表征能力


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的蛋白质表示学习方法通常采用单一视角的图构建策略，只能捕捉残基相互作用的部分特性，导致蛋白质表示不完整。需要多视角融合方法来获得更全面的蛋白质表征。

Method: 提出MMPG框架：1）从物理、化学、几何三个视角构建蛋白质图；2）使用专家混合（MoE）模块动态路由不同视角到专门专家；3）专家学习内在特征和跨视角交互，捕捉从个体表示到成对跨视角协同再到全局共识的多层次信息。

Result: MMPG在四个不同的下游蛋白质任务上取得了先进性能，验证了MoE能自动专业化专家建模不同层次的交互信息，从个体表示到跨视角协同再到全局共识。

Conclusion: 多视角图构建结合专家混合机制能有效提升蛋白质表示学习效果，通过捕捉多层次交互信息获得更全面的蛋白质表征，在多种蛋白质任务上表现优异。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [32] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 论文提出"通过分解实现组合"的方法，让神经网络智能体通过两个训练步骤学习组合泛化能力，能够描述未见过的图像。


<details>
  <summary>Details</summary>
Motivation: 组合性是人类的认知机制，能够系统地将已知概念组合成新方式。研究旨在探索人工神经网络如何获取和利用组合泛化来描述未见过的图像。

Method: 提出"通过分解实现组合"方法，包含两个顺序训练步骤：1) "分解"步骤：在多目标协调游戏中学习将图像分解为基本概念，建立概念代码本；2) "组合"步骤：利用代码本将基本概念组合成复杂短语来描述新图像。

Result: 智能体能够描述未见过的图像，并且在某些情况下，组合步骤的泛化是零样本实现的，无需额外训练。

Conclusion: 该方法成功展示了神经网络智能体如何通过分解-组合的学习过程获得组合泛化能力，为零样本组合泛化提供了有效途径。

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [33] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个系统评估下采样对高频时间序列信息损失的工作流程，结合形状失真度量和分类性能分析，用于针极肌电图信号分析，以平衡计算负载与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针极肌电图信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，特别是近实时分析。下采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚不明确。

Method: 提出系统评估工作流程，结合形状失真度量、特征机器学习模型分类结果和特征空间分析，量化不同下采样算法和因素对波形完整性和预测性能的影响。使用三分类神经肌肉疾病任务进行实验评估。

Result: 工作流程能识别在显著减少计算负载的同时保留诊断信息的下采样配置。形状感知下采样算法优于标准抽取，能更好地保留峰值结构和整体信号形态。

Conclusion: 研究为选择支持近实时针极肌电图分析的下采样配置提供实用指导，并提出可推广的工作流程，可用于其他高频时间序列应用中平衡数据缩减与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [34] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 提出GFM4GA，一种用于群体异常检测的图基础模型，通过双层级对比学习预训练和参数约束的少样本微调，在AUROC和AUPRC指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 群体异常检测在网络应用中很重要，但面临异常模式多样化的挑战。现有的图基础模型(GFMs)已成功应用于个体异常检测，但无法推广到群体异常检测，因为群体异常需要整体检测，且异常群体中的个体可能看起来正常。

Method: 提出GFM4GA模型，采用双层级对比学习预训练，基于特征估计和群体提取捕获潜在群体异常结构和特征不一致性。在下游任务中，采用参数约束和群体异常比例加权的少样本微调，并通过标记异常邻居确定的群体上下文扩展对未见群体异常的适应能力。

Result: 实验表明GFM4GA超越了现有的群体异常检测器和个体异常检测的GFMs，在AUROC上平均提升2.85%，在AUPRC上平均提升2.55%。

Conclusion: GFM4GA是一种有效的图基础模型，能够成功处理群体异常检测任务，在少样本设置下表现出色，为群体异常检测提供了新的解决方案。

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [35] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG是一个针对企业混合文档（文本+表格）的检索增强生成框架，通过双架构分别处理文本叙事和表格结构，相比传统线性化方法在混合查询上提升了18.4%的nDCG@10性能。


<details>
  <summary>Details</summary>
Motivation: 企业文档通常是文本叙事和表格结构的复杂混合体，当前RAG系统采用线性化方法将多维表格转换为简单文本字符串，但这种做法在数学上已被证明是不充分的，无法有效捕捉表格的空间几何关系。

Method: 提出Topo-RAG框架，采用双架构设计：1）传统密集检索器处理流体叙事文本；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间拓扑关系。这种方法尊重数据的拓扑特性，而非简单地将所有内容视为文本。

Result: 在模拟真实世界复杂性的SEC-25合成企业语料库上评估，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%，显著改善了检索性能。

Conclusion: Topo-RAG挑战了"一切皆文本"的假设，通过尊重数据拓扑结构的双架构设计，不仅提升了检索性能，更重要的是理解了信息的形状和结构，为处理企业混合文档提供了更有效的方法。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [36] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出在数学推理任务中进行步骤级路由，仅将关键步骤（易出错步骤）分配给大模型处理，而让较小模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 多步推理任务（如数学问题求解）容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。现有的LLM路由方法将整个查询分配给一个模型，将所有推理步骤视为同等重要，效率低下。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的考虑长期准确性-成本权衡的策略。

Result: 在MATH-500上，即使最简单的阈值策略也比先前路由方法高出5倍的成本效率，而更高级的策略使用80%更少的大模型token就能达到强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率。

Conclusion: TRIM通过步骤级路由显著提高了多步推理任务的效率，证明了步骤级难度代表了推理的基本特征，所有方法在数学推理任务中都能有效泛化。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [37] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，不依赖推理或代数计算，专注于评估模型能否直接编码空间关系和识别几何属性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估模型基于推理的几何能力（使用代数方法求解），但缺乏对模型内在几何理解能力的评估。需要评估LLMs是否能够直接编码空间关系和识别几何属性。

Method: 创建包含2,500个简单几何问题的基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估包括GPT-4在内的最先进模型，并进行消融实验。

Result: 即使最先进的模型在二元分类任务中最高准确率仅为65%。消融实验表明，仅通过微调无法获得几何理解能力，需要专门的训练方法。

Conclusion: 当前LLMs在原生理解几何概念方面存在显著差距，为未来开发具有真正几何认知能力的模型提供了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [38] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集的过程监督改进长上下文推理中的证据提取质量，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长上下文推理中存在奖励稀疏问题，无法有效惩罚无根据的"幸运猜测"，导致证据检索过程缺乏监督。

Method: 1) 建立证据增强推理范式，验证精确证据提取是关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励进行密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集过程监督有效解决了长上下文推理中的证据提取瓶颈，为强化学习在复杂推理任务中的应用提供了新思路。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [39] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP：一个基于临床推理的HRV解释框架，通过可追溯的八步推理流程和Z-score优先级层次结构，解决LLM在HRV分析中的生理幻觉问题，提升情感分类准确性和临床推理一致性。


<details>
  <summary>Details</summary>
Motivation: 心率变异性（HRV）是自主神经监测的重要非侵入性指标，但大型语言模型（LLM）在HRV解释中存在生理幻觉问题，包括呼吸性窦性心律失常（RSA）污染、非线性指标的短数据不稳定性，以及忽视个体化基线而依赖群体标准。这些问题限制了LLM在临床HRV分析中的应用。

Method: 提出C-GRASP（临床基础推理的情感信号处理）框架，采用防护的RAG增强流程，将HRV解释分解为八个可追溯的推理步骤。核心是Z-score优先级层次结构，强制优先考虑个体化基线变化而非群体统计。通过自动化的RSA感知防护机制防止频域指标污染。

Result: 在DREAMER数据集的414个试验中评估，C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情感分类中达到37.3%的准确率，临床推理一致性（CRC）得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点，防止了原生LLM常见的"群体偏见"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平了道路，实现了从群体标准到个体化临床推理的范式转变。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [40] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal：基于LLM隐藏激活预测查询可回答性的文本转SQL安全拒绝机制，通过Tri-Residual Gated Encoder架构增强问题-模式不匹配检测，提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 在LLM文本转SQL系统中，不可回答和未充分指定的用户查询可能生成错误程序，产生误导结果或违反安全约束，现有拒绝策略依赖输出级指令遵循（易受幻觉影响）或输出不确定性估计（增加复杂性），需要更可靠的解决方案。

Method: 将安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，从LLM中间隐藏激活预测查询可回答性。引入Tri-Residual Gated Encoder轻量探测架构，抑制模式噪声并放大问题-模式不匹配的稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将平均F1提升至88.5%，同时仅增加约2毫秒的探测开销。在多种模糊和不可回答场景下表现出有效性，为文本转SQL系统提供了可附加的高效安全层。

Conclusion: LatentRefusal通过潜在信号拒绝机制有效解决了文本转SQL系统中的安全拒绝问题，提供了一种轻量、高效且可附加的安全层，显著提升系统对不可回答查询的处理能力。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [41] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0通过分层认知缓存架构解决AI在超长时域自主性瓶颈，在机器学习工程任务中实现56.44%的奖牌率。


<details>
  <summary>Details</summary>
Motivation: 当前AI向代理科学发展的瓶颈在于超长时域自主性——在跨越数天或数周的实验周期中保持战略连贯性和迭代修正的能力。虽然大语言模型在短时域推理中表现出色，但在现实研究的高维、延迟反馈环境中容易被执行细节淹没，无法将稀疏反馈整合为连贯的长期指导。

Method: 提出分层认知缓存架构，将上下文管理重构为认知积累过程。该架构受计算机系统启发，通过多层级结构实现经验随时间的结构化区分，能够动态地将瞬时执行轨迹提炼为稳定知识和跨任务智慧，使代理能够将即时执行与长期实验策略解耦。

Result: 在OpenAI的MLE-Bench评估中，使用24小时预算，ML-Master 2.0实现了56.44%的最先进奖牌率。

Conclusion: 超长时域自主性为AI提供了可扩展的蓝图，使其能够在超越人类先例复杂性的领域进行自主探索。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [42] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过显式错误诊断来改善评估质量，解决现有方法忽视关键缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 自动问题生成经常产生事实性幻觉和答案不匹配等关键缺陷，但现有评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏显式错误建模，导致忽视这些缺陷并高估问题质量。

Method: ErrEval将评估重新构建为两阶段过程：错误诊断后接知情评分。第一阶段使用轻量级即插即用错误识别器检测和分类结构、语言和内容方面的常见错误；第二阶段将这些诊断信号作为显式证据指导LLM评估器做出更细粒度和有根据的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示显式诊断的加入提高了与人类判断的一致性。进一步分析证实ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过显式错误诊断增强了问题生成评估，解决了现有评估方法的局限性，为自动问题生成提供了更可靠的质量评估框架。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [43] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA：一个结合LLM、RAG和定制知识库的端到端计算框架，用于从隐私政策中提取个人数据流并构建数据流图进行分析


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言，难以理解且在不同组织间存在不一致性。现有研究虽然使用机器学习分析隐私政策，但结合LLM、RAG和定制知识库的方法有限，需要更有效的自动化分析框架

Method: 开发LADFA框架，包含预处理器、基于LLM的处理器和数据流后处理器。结合检索增强生成（RAG）和从现有研究构建的定制知识库，从非结构化隐私政策文本中提取个人数据流并构建数据流图

Result: 通过汽车行业10个隐私政策的案例研究验证了方法的有效性和准确性。框架能够提取个人数据流并构建数据流图进行分析，且设计灵活可定制，适用于隐私政策分析之外的其他文本分析任务

Conclusion: LADFA框架成功结合LLM、RAG和定制知识库，能够有效分析隐私政策中的个人数据流，为大规模自动化隐私政策分析提供了可行解决方案，且具有扩展到其他文本分析任务的潜力

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [44] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，限制了性能且无法保持基础模型的生成多样性

Method: 采用患者-医生范式，集成token级奖励获取和token级流引导偏好优化（TFPO），通过小型专业医生模型引导大型冻结患者模型，建立所有子轨迹的流一致性

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越了DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，能够在保持生成多样性的同时实现token级对齐，为LLM对齐提供了新的解决方案

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [45] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个针对工业场景的神经符号残差提升框架，通过将遗留模型作为冻结模型，在预测失败的"困难区域"进行针对性修复，实现非侵入式的模型升级。


<details>
  <summary>Details</summary>
Motivation: 在工业表格应用中，梯度提升决策树（GBDTs）占据主导地位，但在高并发生产环境中升级遗留模型面临昂贵的重新训练成本和系统性风险。需要一种安全、低成本的模型进化范式。

Method: 框架包含三个阶段：1) 通过残差找到困难区域；2) 使用大语言模型生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3) 通过轻量级聚合器将专家与遗留模型输出动态集成。

Result: 在Qfin Holdings核心金融风控系统中成功部署，在6个公共数据集和1个私有数据集上显著优于最先进的基线方法，在真实在线数据上表现出优异的性能提升，有效捕捉传统模型遗漏的长尾风险。

Conclusion: NSR-Boost为工业界提供了一个安全、低成本的进化范式，实现了非侵入式的模型升级，解决了遗留模型升级的挑战。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [46] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集仅包含少量图表类型的不足


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和计算机视觉技术的发展，图表理解领域快速演进。多模态大语言模型在图表理解方面表现出高效和准确，但现有基准数据集仅限于少量图表类型，需要更全面的数据集来准确评估模型性能

Method: 基于可视化社区的图表分类法，构建包含30种不同图表类型的ChartComplete数据集。该数据集是分类的图表图像集合，不包含学习信号，以原始形式提供给研究社区

Result: 创建了ChartComplete数据集，覆盖30种图表类型，为图表理解研究提供了更全面的基准测试资源

Conclusion: ChartComplete数据集填补了现有图表理解基准数据集的空白，为研究社区提供了更全面的图表类型覆盖，有助于更准确地评估多模态大语言模型在图表理解方面的性能

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [47] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出Domain-specific Knowledge Graph Fusion (DKGF)任务，通过融合通用知识图谱(GKGs)来丰富领域知识图谱(DKGs)，解决领域相关性和知识粒度不匹配两大挑战。


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱(DKGs)相比通用知识图谱(GKGs)通常覆盖不足，需要从通用知识图谱中融合相关事实来丰富领域知识。

Method: 提出ExeFuse方法，采用Fact-as-Program范式：将每个GKG事实视为潜在语义程序，将抽象关系映射到粒度感知的操作符，通过在目标DKG上执行程序来验证领域相关性。

Result: 构建了两个基准数据集DKGF(W-I)和DKGF(Y-I)，包含21个评估配置。实验验证了任务的重要性和模型的有效性，为DKGF提供了首个标准化测试平台。

Conclusion: DKGF是一个重要且有挑战性的任务，ExeFuse通过统一的概率框架有效解决了领域相关性和知识粒度不匹配问题，为领域知识图谱融合提供了新思路。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [48] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: GRACE是一个神经符号推理的基于原因的遏制架构，通过将规范性推理与工具性决策解耦，确保AI代理的决策既有效又符合道德规范。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在重要场景中自主部署并产生实际影响，确保其决策不仅有效而且符合道德规范变得至关重要。需要一种能够包含各种AI设计、实现可解释性和可验证性的架构。

Method: GRACE采用三层模块化架构：1) 道德模块(MM)使用基于道义逻辑的推理确定允许的宏观行动；2) 决策模块(DMM)封装目标代理，在宏观行动约束下选择最优原始行动；3) 守卫模块监控并强制执行道德合规。MM采用基于原因的形式化方法，提供语义基础。

Result: GRACE能够实现对各种AI设计的道德遏制，提供可解释性、可争议性和可验证性。在LLM治疗助手示例中展示了如何让利益相关者理解、质疑和改进代理行为。

Conclusion: GRACE通过解耦规范性推理和工具性决策，为AI对齐提供了可解释、可验证的架构解决方案，支持形式验证和统计保证，有助于构建值得信赖的自主系统。

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


### [49] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 本文提出一个多层诊断框架，通过微调Llama 3.1 8B、Gemma 2 9B和Mistral模型进行钓鱼检测任务，揭示模型泛化失败的根源，发现架构与数据多样性的协同作用、架构依赖性以及不同架构的泛化能力差异。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大语言模型在专业任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键未解决的问题。需要理解模型泛化失败的根源，以实现可靠的AI系统。

Method: 引入并应用多层诊断框架进行跨架构研究：1）微调Llama 3.1 8B、Gemma 2 9B和Mistral模型进行高风险的钓鱼检测任务；2）使用SHAP分析和机制可解释性技术来揭示泛化失败的根源。

Result: 三个关键发现：1）泛化由架构与数据多样性的强大协同作用驱动，Gemma 2 9B在风格多样的"通才"数据集上训练时达到>91% F1的最优性能；2）泛化高度依赖架构，Llama 3.1 8B在窄域表现良好但无法整合多样数据导致性能显著下降；3）某些架构天生更具泛化能力，Mistral模型在多种训练范式中表现一致且稳健。

Conclusion: 通过识别导致这些失败的缺陷启发式方法，本文提供了诊断和理解泛化失败的具体方法论，强调可靠的AI需要对架构、数据和训练策略之间的相互作用进行深度验证。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [50] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿模型进行综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，其他模型在不同评估维度间存在明显权衡，凸显标准化安全评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力上取得显著进步，但这些进步是否带来相应的安全改进仍不明确，部分原因是现有评估实践局限于单一模态或威胁模型，需要综合评估前沿模型的安全性能。

Method: 对7个前沿模型（GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro、Seedream 4.5）进行统一协议评估，涵盖语言、视觉语言和图像生成场景，整合基准评估、对抗评估、多语言评估和合规评估四种模式。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现最均衡；其他模型在基准安全、对抗对齐、多语言泛化和监管合规间存在明显权衡；语言和视觉语言模态在对抗评估下显著脆弱；文本到图像模型在受监管视觉风险类别中相对对齐更好，但在对抗或语义模糊提示下仍脆弱。

Conclusion: 前沿模型的安全本质上是多维度的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估现实世界风险，指导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [51] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过显式提取LLM解码过程中的潜在安全信号，实现早期检测不安全内容，有效防御越狱攻击同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，但现有防御机制（解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用。研究发现即使成功越狱，模型在生成过程中仍会表现出潜在安全信号，但这些信号被流畅续写的驱动力所压制。

Method: 提出SafeProbing方法，在解码过程中显式提取和利用潜在安全信号进行早期检测。通过激活模型内在的安全意识，在生成不安全内容前进行干预。

Result: 在多种越狱攻击上的实验表明，该方法显著提升安全性，同时在良性输入上保持低误拒率，并保持响应质量。代码已开源。

Conclusion: 在解码过程中激活内在安全意识为防御越狱攻击提供了一个有前景的补充方向，通过利用模型自身的潜在安全信号实现早期检测。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [52] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 该论文主张研究基于大语言模型（LLM）的智能体集体行为是一个关键领域，需要采用交互主义范式来系统分析先验知识、嵌入价值观与社会情境如何共同塑造多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为至关重要，这涉及风险与利益的重要社会影响。LLM的独特性质——包括基于大量预训练知识的初始化、隐含的社会先验，以及通过上下文学习进行适应的能力——要求采用新的理论框架来研究这些系统。

Method: 提出交互主义范式，包含替代性的理论基础、方法论和分析工具，系统研究先验知识、嵌入价值观与社会情境如何相互作用，从而塑造多智能体生成式AI系统中的涌现现象。

Result: 论文提出了四个关键发展方向：理论建设、方法创新、跨学科对话，以及LLM集体系统的开发与部署策略，为这一新兴领域的研究提供了路线图。

Conclusion: 研究基于LLM的智能体集体行为需要新的交互主义范式，该领域的发展将对社会产生深远影响，需要理论、方法和跨学科合作的多方面推进。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [53] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专业智能体处理复杂基因组查询，在GeneTuring基准测试中比当前最优的GeneGPT平均提升12%性能


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但现有方法面临挑战：大型语言模型在基因组问答中受限于对领域特定数据库的访问限制，而当前最优的GeneGPT系统存在API依赖性强、适应性有限的问题

Method: 提出GenomAgent多智能体框架，通过协调多个专业智能体来处理复杂基因组查询。该方法复制了GeneGPT并进行了改进，采用灵活的架构设计，能够超越基因组学扩展到需要专家知识提取的各种科学领域

Result: 在GeneTuring基准测试的九个任务上，GenomAgent平均比GeneGPT性能提升12%，展示了其优越性。该框架的灵活架构使其能够扩展到基因组学以外的科学领域

Conclusion: GenomAgent通过多智能体协调机制有效解决了基因组问答中的数据库访问和适应性限制问题，性能显著优于现有方法，且具有更广泛的适用性

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [54] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合，通过单次不动点计算确定可实现的属性组合，避免枚举子集，性能提升达两个数量级。


<details>
  <summary>Details</summary>
Motivation: 在LTLf综合中，同时满足所有属性可能不可行。传统方法需要枚举属性子集，这在属性数量多时计算代价高昂，需要更高效的解决方案。

Method: 引入布尔目标变量，利用单调性紧凑表示指数级的目标组合，通过单次不动点计算状态与可实现目标集的关系，并综合实现最大可实现集的策略。

Result: 提出的符号化算法显著优于基于枚举的基线方法，性能提升可达两个数量级，能够高效处理多属性综合问题。

Conclusion: 通过符号化表示和单次不动点计算，成功解决了LTLf多属性综合中的组合爆炸问题，为处理复杂属性需求提供了高效实用的解决方案。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [55] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但存在三个令人惊讶的失败模式：简单谜题失败、推理步骤中的"顿悟"动态、以及多重固定点问题，表明HRM更像是"猜测"而非"推理"。通过数据增强、输入扰动和模型引导三种策略，开发了增强版HRM，将数独极端难度的准确率从54.5%提升到96.9%。


<details>
  <summary>Details</summary>
Motivation: 理解分层推理模型（HRM）的优势和潜在失败模式，揭示其推理机制的本质，并基于发现改进模型性能。

Method: 对HRM的推理模式进行机制研究，发现三个关键现象：简单谜题失败、推理步骤中的"顿悟"动态、多重固定点问题。基于"猜测"而非"推理"的洞察，提出三种扩展策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测数量）、模型引导（利用训练随机性增加猜测数量）。

Result: 通过结合所有方法开发的增强版HRM，在Sudoku-Extreme任务上将准确率从54.5%大幅提升到96.9%。机制分析为理解推理模型的"推理"方式提供了新见解。

Conclusion: HRM更像是通过"猜测"而非系统推理来解决问题，这一发现不仅解释了其失败模式，还为通过扩展猜测策略显著提升性能提供了理论基础。研究对理解推理模型的内部工作机制具有重要意义。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [56] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出一个基于文档结构和多样性约束的上下文气泡框架，用于在有限token预算下构建连贯、可引用的文本片段集合，相比传统RAG的top-k检索能减少冗余、提高覆盖率和答案质量。


<details>
  <summary>Details</summary>
Motivation: 传统RAG使用top-k检索存在信息图碎片化、过度检索、内容重复以及查询上下文不足（特别是二阶和三阶方面）的问题，需要在有限上下文窗口中构建更紧凑、信息丰富的上下文集合。

Method: 提出结构感知和多样性约束的上下文气泡框架：1) 利用文档结构组织多粒度文本片段（如章节、行）；2) 使用任务条件结构先验指导检索；3) 从高相关性锚点片段开始，通过平衡查询相关性、边际覆盖和冗余惩罚的约束选择构建上下文气泡；4) 提供完整的检索追踪记录以实现可审计性和确定性调优。

Result: 在企业文档上的实验表明，上下文气泡框架能显著减少冗余上下文，更好地覆盖次要方面，在有限上下文窗口内获得更好的答案质量和引用忠实度。消融研究证明结构先验和多样性约束选择都是必要的。

Conclusion: 提出的上下文气泡框架通过结合文档结构信息和多样性约束，能够构建更紧凑、信息丰富且可审计的上下文集合，有效解决了传统RAG方法在信息碎片化、冗余和覆盖不足方面的问题。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [57] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI对建筑设计任务中表现、创意自我效能和认知负荷的影响，发现AI对新手设计师有显著帮助，但会降低创意自我效能，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑设计任务中的实际影响，特别是对设计表现、创意自我效能和认知负荷的影响，了解AI工具如何影响不同专业水平用户的设计过程。

Method: 36名学生参与两阶段建筑设计任务：独立设计和外部工具辅助设计（生成式AI组vs使用现有建筑项目库的对照组）。专家评估设计成果，参与者自报自我效能和认知负荷，采用双重差分法分析。

Result: 整体上生成式AI无显著表现优势，但显著提升新手设计师表现；使用AI的学生创意自我效能下降；认知负荷无显著差异，但迭代创意生成和视觉反馈提示能更大程度降低认知负荷。

Conclusion: 生成式AI的效果取决于用户专业水平和交互策略，对新手设计师有益但可能削弱创意自信，提示策略对减轻认知负荷很重要。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [58] [Learning-Augmented Perfectly Secure Collaborative Matrix Multiplication](https://arxiv.org/abs/2601.09916)
*Zixuan He,Mohammad Reza Deylam Salehi,Derya Malak,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 提出了一种用于多方计算中矩阵乘法AᵀB的完美安全协议，保证正确性和信息论隐私，并引入了学习增强扩展以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多方计算环境中，如何在保证完美安全性的同时高效执行矩阵乘法运算是一个重要挑战。现有方案在安全性和计算效率之间需要更好的平衡。

Method: 使用稀疏掩码多项式编码子矩阵，结合系数对齐和Beaver式随机性确保完美保密。还引入了基于张量分解的本地块乘法的学习增强扩展。

Result: 协议在阈值限制下提供信息论隐私，任何低于安全阈值的合谋方只能观察到均匀随机份额。恢复阈值达到最优，学习增强版本在矩阵维度增长时提供高达80%的计算效率提升。

Conclusion: 提出的PSMM协议实现了多方计算中矩阵乘法的完美安全性和最优恢复阈值，学习增强扩展在保持隐私和恢复保证的同时显著提高了计算效率。

Abstract: This paper presents a perfectly secure matrix multiplication (PSMM) protocol for multiparty computation (MPC) of $\mathrm{A}^{\top}\mathrm{B}$ over finite fields. The proposed scheme guarantees correctness and information-theoretic privacy against threshold-bounded, semi-honest colluding agents, under explicit local storage constraints. Our scheme encodes submatrices as evaluations of sparse masking polynomials and combines coefficient alignment with Beaver-style randomness to ensure perfect secrecy. We demonstrate that any colluding set of parties below the security threshold observes uniformly random shares, and that the recovery threshold is optimal, matching existing information-theoretic limits. Building on this framework, we introduce a learning-augmented extension that integrates tensor-decomposition-based local block multiplication, capturing both classical and learned low-rank methods. We demonstrate that the proposed learning-based PSMM preserves privacy and recovery guarantees for MPC, while providing scalable computational efficiency gains (up to $80\%$) as the matrix dimensions grow.

</details>


### [59] [High signal-to-noise ratio asymptotics of entropy-constrained Gaussian channel capacity](https://arxiv.org/abs/2601.09864)
*Adway Girish,Shlomo Shamai,Emre Telatar*

Main category: cs.IT

TL;DR: 研究高斯信道在渐近高信噪比下的输入熵约束容量问题，发现容量达到分布是支撑在缩放整数格上的离散高斯分布，输入熵与容量之间的差距随信噪比指数衰减。


<details>
  <summary>Details</summary>
Motivation: 研究高斯信道在输入熵约束下的容量问题，特别是在高信噪比渐近区域，这对于理解实际通信系统中有限精度或量化约束下的性能极限具有重要意义。

Method: 采用渐近分析方法，研究高斯信道在高信噪比极限下的容量达到分布特性，分析离散高斯分布作为最优分布的性质。

Result: 证明当信噪比趋于无穷时，容量达到分布是支撑在缩放整数格上的离散高斯分布，输入熵与容量之间的差距随信噪比指数衰减，并表征了该指数。

Conclusion: 在高信噪比渐近区域，输入熵约束高斯信道的最优输入分布是离散高斯分布，输入熵与容量之间的差距呈指数衰减，为实际量化通信系统提供了理论指导。

Abstract: We study the input-entropy-constrained Gaussian channel capacity problem in the asymptotic high signal-to-noise ratio (SNR) regime. We show that the capacity-achieving distribution as SNR goes to infinity is given by a discrete Gaussian distribution supported on a scaled integer lattice. Further, we show that the gap between the input entropy and the capacity decreases to zero exponentially in SNR, and characterize this exponent.

</details>


### [60] [One-Cold Poisson Channel: A Simple Continuous-Time Channel with Zero Dispersion](https://arxiv.org/abs/2601.09894)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 论文介绍了一种称为"一冷泊松信道"的新型通信信道，其中发射机每次选择衰减一个频带。完美OCPC具有容量1、零信道色散和简并的信息谱分布，是唯一已知具有闭式最优非渐近错误概率公式的非平凡无记忆信道。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提出一种极其简单的连续时间无记忆信道模型，该模型具有闭式最优非渐近错误概率公式，可作为信息的基本度量单位，并可能应用于带可调谐带阻滤波器的光通信。

Method: 引入一冷泊松信道概念，其中发射机每次选择衰减一个频带。特别研究了完美OCPC（频带数量无限的情况），分析了其容量、信道色散、信息谱特性，并推导了最优非渐近错误概率的闭式公式。

Result: 完美OCPC具有容量1、零信道色散，信息谱为在1处的简并分布。这是唯一已知具有闭式最优非渐近错误概率公式的非平凡无记忆信道。OCPC可作为无限可分的信息基本单位，完美反馈下的OCPC可推广前缀码。

Conclusion: 一冷泊松信道是一种极其简单的连续时间无记忆信道模型，具有独特的数学性质，可作为信息的基本度量单位，并可能在实际光通信系统中找到应用。其简单性使其成为研究非渐近编码和信道模拟的理想模型。

Abstract: We introduce the one-cold Poisson channel (OCPC), where the transmitter chooses one of several frequency bands to attenuate at a time. In particular, the perfect OCPC, where the number of bands is unlimited, is an extremely simple continuous-time memoryless channel. It has a capacity 1, zero channel dispersion, and an information spectrum being the degenerate distribution at 1. It is the only known nontrivial (discrete or continuous-time) memoryless channel with a closed-form formula for its optimal non-asymptotic error probability, making it the simplest channel in this sense. A potential application is optical communication with a tunable band rejection filter. Due to its simplicity, we may use it as a basic currency of information that is infinitely divisible, as an alternative to bits which are not infinitely divisible. OCPC with perfect feedback gives a generalization of prefix codes. We also study non-asymptotic coding and channel simulation results for the general OCPC.

</details>


### [61] [Reconstructing Reed-Solomon Codes from Multiple Noisy Channel Outputs](https://arxiv.org/abs/2601.09947)
*Shubhransh Singhvi,Han Mao Kiah,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了针对q元离散无记忆对称替换信道下序列重构问题的高效算法，基于Reed-Solomon码和Koetter-Vardy软判决译码，推导了明确的速率阈值


<details>
  <summary>Details</summary>
Motivation: 解决Levenshtein提出的序列重构问题：发送方传输码字，接收方观察到K个独立的噪声版本。研究在q元离散无记忆对称替换信道下，如何高效重构原始码字

Method: 针对Reed-Solomon码，改进Koetter-Vardy软判决译码算法，提出高效的序列重构算法。通过理论分析推导出明确的速率阈值

Result: 对于足够大的码长和字母表大小，推导出仅依赖于(p,K)的显式速率阈值。当码率R低于该阈值时，能以任意小的错误概率重构传输的码字

Conclusion: 成功将软判决译码技术应用于序列重构问题，为Reed-Solomon码在q元离散无记忆对称替换信道下的高效重构提供了理论保证和实用算法

Abstract: The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a communication setting in which a sender transmits a codeword and the receiver observes K independent noisy versions of this codeword. In this work, we study the problem of efficient reconstruction when each of the $K$ outputs is corrupted by a $q$-ary discrete memoryless symmetric (DMS) substitution channel with substitution probability $p$. Focusing on Reed-Solomon (RS) codes, we adapt the Koetter-Vardy soft-decision decoding algorithm to obtain an efficient reconstruction algorithm. For sufficiently large blocklength and alphabet size, we derive an explicit rate threshold, depending only on $(p, K)$, such that the transmitted codeword can be reconstructed with arbitrarily small probability of error whenever the code rate $R$ lies below this threshold.

</details>


### [62] [Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement](https://arxiv.org/abs/2601.10676)
*Lei Hu,Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子纠缠可以显著改善分布式存储系统的存储-带宽权衡，在某些条件下能同时最小化存储和修复带宽，突破经典系统的限制。


<details>
  <summary>Details</summary>
Motivation: 研究量子资源在分布式存储系统中的应用，探索量子通信如何突破经典修复机制中存储与修复带宽之间的权衡限制。

Method: 在(n,k,d)分布式存储系统中，允许辅助节点通过量子信道向新节点传输经典信息，新节点通过对接收的量子态进行测量来生成存储。分析量子纠缠仅存在于存活节点间的情况。

Result: 完全刻画了存储与修复带宽之间的基本权衡关系。相比经典系统，量子纠缠能显著改善最优存储-带宽权衡，特别是在最小存储再生点。当d≥2k-2时，存在一个操作点能同时最小化存储和修复带宽。

Conclusion: 量子通信揭示了经典设置中不存在的新机制，打破了存储与修复带宽之间的权衡，为分布式存储系统提供了新的优化可能性。

Abstract: This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage--bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.

</details>


### [63] [Private Information Retrieval for Graph-based Replication with Minimal Subpacketization](https://arxiv.org/abs/2601.09957)
*Vayur Shanbhag,Prasad Krishnan*

Main category: cs.IT

TL;DR: 设计新的最小子分组化方案，用于基于图复制的数据库上的信息论私有信息检索，针对星图和一般图实现了单位子分组化（最小化）。


<details>
  <summary>Details</summary>
Motivation: 在基于图复制的私有信息检索系统中，需要在保持高检索率的同时降低子分组化程度，因为子分组化限制了协议执行时的文件大小。现有方案要么子分组化程度高，要么在特定图结构上性能不佳。

Method: 提出了两种新方案：1) 针对星图的专用方案；2) 针对一般图的通用方案，后者使用独立集分解图结构。两种方案都实现了单位子分组化（最小化）。

Result: 星图方案在一般星图上比现有低子分组化方案具有更好的检索率。一般图方案在完全图上比先前方案速率低，但在某些特定图类上能达到更高速率。扩展到多重图时，在完全多重图上比先前方案速率更高。

Conclusion: 成功设计了最小子分组化的私有信息检索方案，针对不同图结构优化了检索率，为基于图复制的数据库系统提供了更实用的隐私保护检索方案。

Abstract: We design new minimal-subpacketization schemes for information-theoretic private information retrieval on graph-based replicated databases. In graph-based replication, the system consists of $K$ files replicated across $N$ servers according to a graph with $N$ vertices and $K$ edges. The client wants to retrieve one desired file, while keeping the index of the desired file private from each server via a query-response protocol. We seek PIR protocols that have (a) high rate, which is the ratio of the file-size to the total download cost, and (b) low subpacketization, which acts as a constraint on the size of the files for executing the protocol. We report two new schemes which have unit-subpacketization (which is minimal): (i) for a special class of graphs known as star graphs, and (ii) for general graphs. Our star-graph scheme has a better rate than previously known schemes with low subpacketization for general star graphs. Our scheme for general graphs uses a decomposition of the graph via independent sets. This scheme achieves a rate lower than prior schemes for the complete graph, however it can achieve higher rates than known for some specific graph classes. An extension of our scheme to the case of multigraphs achieves a higher rate than previous schemes for the complete multi-graph.

</details>


### [64] [On the Leaky Private Information Retrieval with Side Information](https://arxiv.org/abs/2601.09960)
*Yingying Huangfu,Tian Bai*

Main category: cs.IT

TL;DR: 本文研究了带有侧信息的泄露私有信息检索问题，通过放松完美隐私要求来提升通信效率，提出了统一的概率框架量化隐私泄露，并建立了泄露、侧信息和检索效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在私有信息检索中，完美隐私要求往往导致通信效率低下。本文旨在探索在存在侧信息的情况下，通过允许可控的信息泄露来改善通信效率，填补了现有研究中泄露隐私与侧信息结合的研究空白。

Method: 提出了统一的概率框架来构建L-PIR-SI方案，使用参数ε量化隐私泄露（符合差分隐私标准），并分析了可实现的下载成本。

Result: 建立了泄露参数ε与下载成本之间的关系，当ε→0时恢复PIR-SI的容量，当侧信息不存在时退化为已知的泄露PIR边界，首次揭示了泄露、侧信息和检索效率之间的权衡关系。

Conclusion: 本文首次系统研究了泄露私有信息检索与侧信息的结合问题，提出的框架统一了多个重要结果，为隐私与效率的权衡提供了理论基础，扩展了私有信息检索的研究边界。

Abstract: This paper investigates the problem of leaky-private Private Information Retrieval with Side Information (L-PIR-SI), which relaxes the requirement of perfect privacy to achieve improved communication efficiency in the presence of side information. While the capacities of PIR-SI under both $W$-privacy and $(W,S)$-privacy have been partially explored, the impact of controlled information leakage in these settings remains unaddressed. We propose a unified probabilistic framework to construct L-PIR-SI schemes where the privacy leakage is quantified by a parameter $\varepsilon$, consistent with differential privacy standards. We characterize the achievable download costs and show that our results generalize several landmark results in the PIR literature: they recover the capacity of PIR-SI when $\varepsilon \to 0$, and reduce to the known bounds for leaky-PIR when side information is absent. This work provides the first look at the trade-offs between leakage, side information, and retrieval efficiency.

</details>


### [65] [Fundamental Limits of Coded Polynomial Aggregation](https://arxiv.org/abs/2601.10028)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合（CPA）扩展到含掉队者的分布式计算系统，提出基于预设非掉队模式的CPA框架，证明所需响应数少于传统方法，可行性由非掉队模式的交集结构决定。


<details>
  <summary>Details</summary>
Motivation: 传统编码多项式计算需要单独解码每个项，在含掉队者的分布式系统中效率不高。需要开发能直接恢复加权聚合且对掉队者鲁棒的方法。

Method: 扩展CPA到含掉队者的分布式系统，引入基于预设非掉队模式的CPA框架。建立精确恢复的必要充分条件，识别交集大小阈值，提供可行的CPA方案构造方法。

Result: 证明精确恢复所需响应数少于基于单独解码的方法，可行性由非掉队模式的交集结构决定。识别出交集大小阈值，当非掉队集数量足够大时该阈值成为必要充分条件。

Conclusion: 提出的含掉队者CPA框架能有效减少所需响应数，可行性条件由交集结构决定。模拟显示预测阈值处存在急剧的可行性转变，证明边界在实际中是紧的。

Abstract: Coded polynomial aggregation (CPA) enables the master to directly recover a weighted aggregation of polynomial evaluations without individually decoding each term, thereby reducing the number of required worker responses. In this paper, we extend CPA to straggler-aware distributed computing systems and introduce a straggler-aware CPA framework with pre-specified non-straggler patterns, where exact recovery is required only for a given collection of admissible non-straggler sets. Our main result shows that exact recovery of the desired aggregation is achievable with fewer worker responses than required by polynomial coded computing based on individual decoding, and that feasibility is fundamentally characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA and identify an intersection-size threshold that is sufficient to guarantee exact recovery. We further prove that this threshold becomes both necessary and sufficient when the number of admissible non-straggler sets is sufficiently large. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations reveal a sharp feasibility transition at the predicted threshold, providing empirical evidence that the bound is tight in practice.

</details>


### [66] [Optimal Proximity Gap for Folded Reed--Solomon Codes via Subspace Designs](https://arxiv.org/abs/2601.10047)
*Fernando Granha Jeronimo,Lenny Liu,Pranav Rajpal*

Main category: cs.IT

TL;DR: 论文研究了折叠Reed-Solomon码的邻近间隙性质，证明了在最优容量范围内存在(δ,ε)-邻近间隙，扩展了Ben-Sasson等人对仿射子空间的研究。


<details>
  <summary>Details</summary>
Motivation: Ben-Sasson等人证明了仿射子空间在Johnson界范围内对Reed-Solomon码存在邻近间隙。折叠Reed-Solomon码能达到最优列表解码半径（容量），因此自然要问：FRS码是否在最优容量范围内也存在类似的邻近间隙？

Method: 将Ben-Sasson等人的框架扩展到折叠Reed-Solomon码，利用FRS码的列表解码算法特性，证明在最优容量范围内存在(δ,ε)-邻近间隙。该框架也适用于合适的子空间设计码。

Result: 肯定地回答了研究问题：折叠Reed-Solomon码在最优容量范围内确实存在(δ,ε)-邻近间隙，扩展了先前对Reed-Solomon码的结果。

Conclusion: 折叠Reed-Solomon码不仅具有最优列表解码能力，还在邻近间隙性质上表现出与Reed-Solomon码类似的行为，这为理解FRS码与随机线性码的相似性提供了新视角。

Abstract: A collection of sets satisfies a $(δ,\varepsilon)$-proximity gap with respect to some property if for every set in the collection, either (i) all members of the set are $δ$-close to the property in (relative) Hamming distance, or (ii) only a small $\varepsilon$-fraction of members are $δ$-close to the property.
  In a seminal work, Ben-Sasson \textit{et al.}\ showed that the collection of affine subspaces exhibits a $(δ,\varepsilon)$-proximity gap with respect to the property of being Reed--Solomon (RS) codewords with $δ$ up to the so-called Johnson bound for list decoding. Their technique relies on the Guruswami--Sudan list decoding algorithm for RS codes, which is guaranteed to work in the Johnson bound regime.
  Folded Reed--Solomon (FRS) codes are known to achieve the optimal list decoding radius $δ$, a regime known as capacity. Moreover, a rich line of list decoding algorithms was developed for FRS codes. It is then natural to ask if FRS codes can be shown to exhibit an analogous $(δ,\varepsilon)$-proximity gap, but up to the so-called optimal capacity regime. We answer this question in the affirmative (and the framework naturally applies more generally to suitable subspace-design codes).
  An additional motivation to understand proximity gaps for FRS codes is the recent results [BCDZ'25] showing that they exhibit properties similar to random linear codes, which were previously shown to be related to properties of RS codes with random evaluation points in [LMS'25], as well as codes over constant-size alphabet based on AEL [JS'25].

</details>


### [67] [Function Correcting Codes for Maximally-Unbalanced Boolean Functions](https://arxiv.org/abs/2601.10135)
*Rajlaxmi Pandey,Shiven Bajpai,Anjana A Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 该论文研究了用于最大不平衡布尔函数的最优单错误纠正函数校正码，通过距离矩阵结构分析不同FCC类别，并评估它们在AWGN信道下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 函数校正码允许在噪声信道上可靠计算函数而无需完全恢复消息，但现有研究对最优单错误纠正FCC的结构和性能影响了解有限，特别是对于最大不平衡布尔函数。

Method: 通过关联码字距离矩阵分析最优SEFCC的结构，识别基于结构的不同FCC类别，然后在AWGN信道上使用软判决和硬判决解码评估代表性FCC的性能。

Result: 结果显示具有不同距离矩阵结构的FCC在数据误码率和函数错误行为上表现出显著差异，且代码结构的影响强烈依赖于解码策略。

Conclusion: 函数校正码的结构特性对其错误性能有重要影响，解码策略是决定结构影响程度的关键因素，这为设计更高效的FCC提供了指导。

Abstract: Function-Correcting Codes (FCCs) enable reliable computation of a function of a $k$-bit message over noisy channels without requiring full message recovery. In this work, we study optimal single-error correcting FCCs (SEFCCs) for maximally-unbalanced Boolean functions, where $k$ denotes the message length and $t$ denotes the error-correction capability. We analyze the structure of optimal SEFCC constructions through their associated codeword distance matrices and identify distinct FCC classes based on this structure. We then examine the impact of these structural differences on error performance by evaluating representative FCCs over the additive white Gaussian noise (AWGN) channel using both soft-decision and hard-decision decoding. The results show that FCCs with different distance-matrix structures can exhibit markedly different Data BER and function error behavior, and that the influence of code structure depends strongly on the decoding strategy.

</details>


### [68] [On Existence of Girth-8 QC-LDPC Code with Large Column Weight: Combining Mirror-sequence with Classification Modulo Ten](https://arxiv.org/abs/2601.10170)
*Guohua Zhang,Xiangya Liu,Jianhua Zhang,Yi Fang*

Main category: cs.IT

TL;DR: 本文在GCD框架下，通过引入镜像序列和新行重组方案，代数构造了列重为7和8、码长极短、围长为8的QC-LDPC码，相比现有基准将连续循环尺寸下界提升了约20%。


<details>
  <summary>Details</summary>
Motivation: 准循环LDPC码在大围长情况下在信道编码、压缩感知和分布式存储等领域有重要应用。主要挑战是如何用代数方法（而非搜索方法）构造出最短可能长度（或最小循环尺寸）的此类码。

Method: 在先前提出的最大公约数(GCD)框架基础上，引入镜像序列概念并采用新的行重组方案，以代数方式构造列重为7和8、任意行重、围长为8的QC-LDPC码。

Result: 对于列重7和8的情况，连续循环尺寸的下界相比现有基准均提升了约20%。新构造还能提供比最新边界小约25%的循环尺寸。

Conclusion: 通过镜像序列和行重组等创新代数方法，成功构造了列重更高、长度更短的QC-LDPC码，显著改进了循环尺寸下界，为相关应用提供了更优的码构造方案。

Abstract: Quasi-cyclic (QC) LDPC codes with large girths play a crucial role in several research and application fields, including channel coding, compressed sensing and distributed storage systems. A major challenge in respect of the code construction is how to obtain such codes with the shortest possible length (or equivalently, the smallest possible circulant size) using algebraic methods instead of search methods. The greatest-common-divisor (GCD) framework we previously proposed has algebraically constructed QC-LDPC codes with column weights of 5 and 6, very short lengths, and a girth of 8. By introducing the concept of a mirror sequence and adopting a new row-regrouping scheme, QC-LDPC codes with column weights of 7 and 8, very short lengths, and a girth of 8 are proposed for arbitrary row weights in this article via an algebraic manner under the GCD framework. Thanks to these novel algebraic methods, the lower bounds (for column weights 7 and 8) on consecutive circulant sizes are both improved by asymptotically about 20%, compared with the existing benchmarks. Furthermore, these new constructions can also offer circulant sizes asymptotically about 25% smaller than the novel bounds.

</details>


### [69] [A Low-Complexity Architecture for Multi-access Coded Caching Systems with Arbitrary User-cache Access Topology](https://arxiv.org/abs/2601.10175)
*Ting Yang,Minquan Cheng,Xinping Yi,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于图神经网络的通用框架，解决任意拓扑多接入缓存编码问题，在保持接近最优传输负载的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MACC模型依赖高度结构化的连接拓扑，无法处理任意用户-缓存访问拓扑。需要设计适用于任意拓扑的通用低复杂度传输方案。

Method: 1) 提出基于图的通用框架，将传输问题建模为冲突图着色问题；2) 开发基于图神经网络的机器学习框架，构建近似最优的编码多播传输；3) 将索引编码下界扩展到任意拓扑，并提出低复杂度贪婪近似算法。

Result: 学习方案在传输负载上接近DSatur算法和理论下界，同时显著降低计算时间。数值结果表明该方法在不同拓扑和用户数量下具有良好的泛化能力。

Conclusion: 提出的学习框架为任意拓扑MACC问题提供了高效解决方案，在保持接近最优性能的同时大幅降低计算复杂度，具有实际应用价值。

Abstract: This paper studies the multi-access coded caching (MACC) problem under arbitrary user-cache access topologies, extending existing models that rely on highly structured and combinatorially designed connectivity. We consider a MACC system consisting of a single server, multiple cache nodes, and multiple user nodes. Each user can access an arbitrary subset of cache nodes to retrieve cached content. The objective is to design a general and low-complexity delivery scheme under fixed cache placement for arbitrary access topologies. We propose a universal graph-based framework for modeling the MACC delivery problem, where decoding conflicts among requested packets are captured by a conflict graph and the delivery design is reduced to a graph coloring problem. In this formulation, a lower transmission load corresponds to using fewer colors. The classical greedy coloring algorithm DSatur achieves a transmission load close to the index-coding converse bound, providing a tight benchmark, but its computational complexity becomes prohibitive for large-scale graphs. To overcome this limitation, we develop a learning-based framework using graph neural networks that efficiently constructs near-optimal coded multicast transmissions and generalizes across diverse access topologies and varying numbers of users. In addition, we extend the index-coding converse bound for uncoded cache placement to arbitrary access topologies and propose a low-complexity greedy approximation. Numerical results demonstrate that the proposed learning-based scheme achieves transmission loads close to those of DSatur and the converse bound while significantly reducing computational time.

</details>


### [70] [Error-Correcting Codes for the Sum Channel](https://arxiv.org/abs/2601.10256)
*Lyan Abboud,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了一种新的信道模型——和信道，用于分布式存储和DNA数据存储，构建了能够纠正两个删除的编码，并设计了接近最优的单替换纠错码。


<details>
  <summary>Details</summary>
Motivation: 本文的动机源于分布式存储和DNA数据存储应用的需求。在这些应用中，数据通常以矩阵形式存储，需要设计能够有效纠正删除和替换错误的编码方案。和信道模型能够更好地描述这些存储系统中的数据操作和错误模式。

Method: 提出了一种新的信道模型——和信道，该模型输入一个ℓ行二进制矩阵，输出一个(ℓ+1)行矩阵，其中前ℓ行等于输入，最后一行是前ℓ行的奇偶校验和行。针对该模型，作者构建了能够纠正两个删除的编码，冗余度为2⌈log₂log₂n⌉ + O(ℓ²)。对于ℓ=2的情况，建立了⌈log₂log₂n⌉ + O(1)的上界。此外，还设计了能够纠正单个替换的编码，冗余度为⌈log₂(ℓ+1)⌉。

Result: 1. 构建了能够纠正两个删除的编码，冗余度为2⌈log₂log₂n⌉ + O(ℓ²)。2. 当ℓ=2时，证明了⌈log₂log₂n⌉ + O(1)的上界，表明所提编码的冗余度在因子2内是最优的。3. 设计了能够纠正单个替换的编码，冗余度为⌈log₂(ℓ+1)⌉，并证明该编码与最优解相差不超过1比特。

Conclusion: 本文提出的和信道模型为分布式存储和DNA数据存储提供了新的理论框架。所设计的编码方案在纠正删除和替换错误方面具有接近最优的性能，为实际存储系统的错误纠正提供了有效的解决方案。特别是对于ℓ=2的情况，证明了所提编码的冗余度在常数因子内是最优的。

Abstract: We introduce the sum channel, a new channel model motivated by applications in distributed storage and DNA data storage. In the error-free case, it takes as input an $\ell$-row binary matrix and outputs an $(\ell+1)$-row matrix whose first $\ell$ rows equal the input and whose last row is their parity (sum) row. We construct a two-deletion-correcting code with redundancy $2\lceil\log_2\log_2 n\rceil + O(\ell^2)$ for $\ell$-row inputs. When $\ell=2$, we establish an upper bound of $\lceil\log_2\log_2 n\rceil + O(1)$, implying that our redundancy is optimal up to a factor of 2. We also present a code correcting a single substitution with $\lceil \log_2(\ell+1)\rceil$ redundant bits and prove that it is within one bit of optimality.

</details>


### [71] [Transmission Mask Analysis for Range-Doppler Sensing in Half-Duplex ISAC](https://arxiv.org/abs/2601.10259)
*Dikai Liu,Yifeng Xiong,Marco Lops,Fan Liu,Jianhua Zhang*

Main category: cs.IT

TL;DR: 本文分析了半双工ISAC中MASM的周期性传输掩码，推导了其闭式期望距离-多普勒响应，证明了距离旁瓣的多普勒不变性，并揭示了不同动态场景下的最优掩码设计


<details>
  <summary>Details</summary>
Motivation: 研究半双工集成感知与通信（ISAC）中掩码调制（MASM）的周期性传输掩码设计，以优化距离-多普勒响应性能

Method: 分析周期性传输掩码，推导闭式期望距离-多普勒响应$\mathbb{E}\{r(k,l,ν)\}$，研究循环差集（CDS）特别是Singer CDS在不同动态场景下的最优性

Result: 距离旁瓣具有多普勒不变性，将距离旁瓣最优性扩展到二维设置；对于距离主瓣，周期性掩码产生稀疏多普勒旁瓣：在中等动态场景下CDS是最小最大最优的，在高度动态场景下多普勒旁瓣能量是掩码自相关的凹函数

Conclusion: 周期性掩码设计在不同动态场景下表现出不同的最优特性，揭示了主瓣波动与多普勒旁瓣能量之间的固有权衡关系

Abstract: In this paper, we analyze the periodic transmission masks for MASked Modulation (MASM) in half-duplex integrated sensing and communication (ISAC), and derive their closed-form expected range-Doppler response $\mathbb{E}\{r(k,l,ν)\}$. We show that range sidelobes ($k\neq l$) are Doppler-invariant, extending the range-sidelobe optimality to the 2-D setting. For the range mainlobe ($k=l$), periodic masking yields sparse Doppler sidelobes: Cyclic difference sets (CDSs) (in particular Singer CDSs) are minimax-optimal in a moderately dynamic regime, while in a highly dynamic regime the Doppler-sidelobe energy is a concave function of the mask autocorrelation, revealing an inevitable tradeoff with mainlobe fluctuation.

</details>


### [72] [Algebraic Properties of PAC Codes](https://arxiv.org/abs/2601.10262)
*Vlad-Florin Dragoi,Mohammad Rowshan*

Main category: cs.IT

TL;DR: 本文通过代数表示分析极化调整卷积码，定义了一类广义多项式极化码，推导了其结构特性如对偶性、最小距离等


<details>
  <summary>Details</summary>
Motivation: 研究极化调整卷积码(PAC码)和反向PAC码的代数结构，通过极化码和Reed-Muller码的代数表示来统一分析这类编码方案

Method: 使用极化码和Reed-Muller码的代数表示，定义广义多项式极化码类，推导其结构性质如对偶性、最小距离、最小重量码字数、单项式子码维度等

Result: 建立了广义多项式极化码的理论框架，推导了其结构特性，包括对偶关系、最小距离界限，以及最小重量码字数量和单项式子码维度的结构限制

Conclusion: 广义多项式极化码为分析PAC码和反向PAC码提供了统一的代数框架，揭示了这类编码的结构特性，为编码设计提供了理论基础

Abstract: We analyze polarization-adjusted convolutional codes using the algebraic representation of polar and Reed-Muller codes. We define a large class of codes, called generalized polynomial polar codes which include PAC codes and Reverse PAC codes. We derive structural properties of generalized polynomial polar codes, such as duality, minimum distance. We also deduce some structural limits in terms of number of minimum weight codewords, and dimension of monomial sub-code.

</details>


### [73] [On the Capacity of Noisy Frequency-based Channels](https://arxiv.org/abs/2601.10329)
*Yuval Gerzon,Ilan Shomorony,Nir Weinberger*

Main category: cs.IT

TL;DR: 研究噪声频率通道的容量，针对DNA数据存储中的短分子机制，其中信息编码在项目类型的频率而非顺序中。建立了容量上下界，分析了识别噪声对信息存储的影响。


<details>
  <summary>Details</summary>
Motivation: 研究DNA数据存储中短分子机制下的噪声频率通道容量问题。在短分子机制中，信息编码在项目类型的频率而非顺序中，但先前研究主要关注无噪声情况，识别噪声的影响尚未完全表征。

Method: 1. 通过随机降解和数据处理不等式建立容量上界；2. 基于多项采样过程的泊松化建立可达下界，分析带符号间干扰的向量泊松通道；3. 改进Feinstein界中信息密度的集中不等式，显式表征识别噪声导致的互信息加性损失。

Result: 建立了噪声频率通道容量的上下界，显式量化了识别噪声导致的互信息加性损失。将结果应用于DNA存储通道的短分子机制，量化了可靠存储比特总数缩放中的损失。

Conclusion: 该研究为DNA数据存储中短分子机制的噪声频率通道提供了容量分析框架，显式表征了识别噪声的影响，对实际存储系统设计有重要意义。

Abstract: We investigate the capacity of noisy frequency-based channels, motivated by DNA data storage in the short-molecule regime, where information is encoded in the frequency of items types rather than their order. The channel output is a histogram formed by random sampling of items, followed by noisy item identification. While the capacity of the noiseless frequency-based channel has been previously addressed, the effect of identification noise has not been fully characterized. We present a converse bound on the channel capacity that follows from stochastic degradation and the data processing inequality. We then establish an achievable bound, which is based on a Poissonization of the multinomial sampling process, and an analysis of the resulting vector Poisson channel with inter-symbol interference. This analysis refines concentration inequalities for the information density used in Feinstein bound, and explicitly characterizes an additive loss in the mutual information due to identification noise. We apply our results to a DNA storage channel in the short-molecule regime, and quantify the resulting loss in the scaling of the total number of reliably stored bits.

</details>


### [74] [Convertible Codes for Data and Device Heterogeneity](https://arxiv.org/abs/2601.10341)
*Anina Gruica,Benjamin Jany,Stanislav Kruglik*

Main category: cs.IT

TL;DR: 本文研究分布式存储系统中的可转换码，解决数据异构性和设备异构性问题，为Reed-Muller码构建显式转换程序


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统面临两个关键挑战：1) 数据异构性（由非均匀访问需求引起）；2) 设备异构性（由节点可靠性随时间变化引起）。现有方法通常单独处理这两个问题，需要一种能同时解决两者的方案。

Method: 研究可转换码，在合并机制下以最小成本实现代码转换。首先推导线性码转换的读写成本通用下界，然后专注于Reed-Muller码（能高效处理数据异构性），构建显式转换程序，首次将两种异构性结合。

Result: 为Reed-Muller码构建了显式转换程序，首次实现了同时处理数据异构性和设备异构性的分布式数据存储方案。

Conclusion: 可转换码为分布式存储系统提供了一种同时解决数据异构性和设备异构性的有效方法，Reed-Muller码的显式转换程序展示了这一方法的实际可行性。

Abstract: Distributed storage systems must handle both data heterogeneity, arising from non-uniform access demands, and device heterogeneity, caused by time-varying node reliability. In this paper, we study convertible codes, which enable the transformation of one code into another with minimum cost in the merge regime, addressing the latter. We derive general lower bounds on the read and write costs of linear code conversion, applicable to arbitrary linear codes. We then focus on Reed-Muller codes, which efficiently handle data heterogeneity, addressing the former issue, and construct explicit conversion procedures that, for the first time, combine both forms of heterogeneity for distributed data storage.

</details>


### [75] [A New Construction Structure on MISO Coded Caching with Linear Subpacketization: Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10353)
*Bowen Zheng,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于L-HSDP结构的MISO编码缓存方案，在F=K（线性分组）条件下显著降低分组复杂度，仅轻微牺牲和自由度性能


<details>
  <summary>Details</summary>
Motivation: 现有MISO编码缓存方案在和自由度最优时分组复杂度随用户数指数增长，需要设计低分组复杂度的方案

Method: 将MAPDA设计转化为L-HSDP构造问题，基于拉丁方框架设计新的组合结构，实现F=K的线性分组方案

Result: 新方案相比指数分组方案显著降低分组复杂度，相比现有线性分组方案同时获得更高和自由度和更低分组复杂度

Conclusion: 提出的L-HSDP框架有效平衡了和自由度与分组复杂度的权衡，为MISO编码缓存提供了实用的低复杂度方案

Abstract: In the $(L,K,M,N)$ cache-aided multiple-input single-output (MISO) broadcast channel (BC) system, the server is equipped with $L$ antennas and communicates with $K$ single-antenna users through a wireless broadcast channel where the server has a library containing $N$ files, and each user is equipped with a cache of size $M$ files. Under the constraints of uncoded placement and one-shot linear delivery strategies, many schemes achieve the maximum sum Degree-of-Freedom (sum-DoF). However, for general parameters $L$, $M$, and $N$, their subpacketizations increase exponentially with the number of users. We aim to design a MISO coded caching scheme that achieves a large sum-DoF with low subpacketization $F$. An interesting combinatorial structure, called the multiple-antenna placement delivery array (MAPDA), can be used to generate MISO coded caching schemes under these two strategies; moreover, all existing schemes with these strategies can be represented by the corresponding MAPDAs. In this paper, we study the case with $F=K$ (i.e., $F$ grows linearly with $K$) by investigating MAPDAs. Specifically, based on the framework of Latin squares, we transform the design of MAPDA with $F=K$ into the construction of a combinatorial structure called the $L$-half-sum disjoint packing (HSDP). It is worth noting that a $1$-HSDP is exactly the concept of NHSDP, which is used to generate the shared-link coded caching scheme with $F=K$. By constructing $L$-HSDPs, we obtain a class of new schemes with $F=K$. Finally, theoretical and numerical analyses show that our $L$-HSDP schemes significantly reduce subpacketization compared to existing schemes with exponential subpacketization, while only slightly sacrificing sum-DoF, and achieve both a higher sum-DoF and lower subpacketization than the existing schemes with linear subpacketization.

</details>


### [76] [Generalized Weight Structure of Polar Codes: Selected Template Polynomials](https://arxiv.org/abs/2601.10362)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 本文提出一个统一代数框架，用于计算极码（作为递减单项式码）的汉明重量，推导出关键结构模板的闭式表达式，并结合LTA群作用获得明确的多重性公式。


<details>
  <summary>Details</summary>
Motivation: 极码可以视为递减单项式码，具有由下三角仿射（LTA）群支配的丰富代数结构。需要开发一个通用框架来计算由单项式和生成的码字的汉明重量，并系统地表征低重量和中重量谱。

Method: 开发通用框架计算由单项式和生成的码字的汉明重量，将重量表示为规范二元形式，推导关键结构模板（不相交和、嵌套块、互补翻转）的闭式表达式，结合LTA群作用获得明确的多重性公式。

Result: 获得了生成低重量和中重量谱的关键结构模板的闭式表达式，通过结合LTA群作用得到了统一代数方法来表征和枚举码字，提供了明确的多重性公式。

Conclusion: 本文提出了一个统一的代数框架，能够系统计算极码的汉明重量，通过结构模板和LTA群作用的结合，为极码的重量谱表征和码字枚举提供了有效的代数方法。

Abstract: Polar codes can be viewed as decreasing monomial codes, revealing a rich algebraic structure governed by the lower-triangular affine (LTA) group. We develop a general framework to compute the Hamming weight of codewords generated by sums of monomials, express these weights in a canonical dyadic form, and derive closed expressions for key structural templates (disjoint sums, nested blocks, complementary flips) that generate the low and intermediate weight spectrum. Combining these templates with the LTA group action, we obtain explicit multiplicity formulas, yielding a unified algebraic method to characterize and enumerate codewords.

</details>


### [77] [A Hybrid Reliability--Weight Framework for Construction of Polar Codes](https://arxiv.org/abs/2601.10376)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 提出一种结合可靠性和权重的混合比特信道排序方法，用于构造Polar码，在短到中等长度下改善最小距离和权重谱性能


<details>
  <summary>Details</summary>
Motivation: 传统Polar码构造基于比特信道可靠性排序，虽然能保证达到信道容量，但在短到中等长度下可能产生较差的最小距离和权重谱。需要一种能平衡可靠性和最小距离性能的构造方法

Method: 提出混合（可靠性-权重）比特信道排序方法，定义每个比特的成本函数，其中距离项来自最小权重码字的轨道枚举，并用Bhattacharyya型因子进行缩放。该方法在递减单项式码类中最小化截断的SC/ML联合界代理

Result: 混合构造在纯可靠性构造和混合构造之间取得权衡，改善了最小距离、多重性和联合界近似。数值结果表明在BPSK-AWGN信道上，短到中等长度的性能得到提升

Conclusion: 混合比特信道排序方法有效平衡了可靠性和最小距离性能，在有限长度下改善Polar码性能，同时保持渐近容量可达性

Abstract: Polar codes are usually constructed by ranking synthetic bit-channels according to reliability, which guarantees capacity-achieving behavior but can yield poor low-weight spectra at short and moderate lengths. Recent algebraic results express the contribution of individual bit-channels to the multiplicities of minimum and near-minimum weight codewords in closed form. In this work we combine these insights into a mixed (reliability--weight) bit-channel ordering. We define a per-bit cost whose distance term is derived from orbit enumeration of minimum-weight codewords and scaled by a Bhattacharyya-type factor, and show that the resulting mixed construction minimises a truncated SC/ML union-bound surrogate within a class of decreasing monomial codes. We relate the mixed metric to error events in SCL decoding via a pruning/ML decomposition, and prove that mixed designs act as local perturbations of reliability-based constructions whose asymptotic impact vanishes as code-length approaches infinity. Numerical results for short and moderate lengths on BPSK-AWGN, implemented via Gaussian approximation and closed-form weight contributions, illustrate the trade-off between pure reliability-based and mixed constructions in terms of minimum distance, multiplicity, and union-bound approximations. All proofs are deferred to the appendices.

</details>


### [78] [Codebook Design for Limited Feedback in Near-Field XL-MIMO Systems](https://arxiv.org/abs/2601.10391)
*Liujia Yao,Changsheng You,Zixuan Huang,Chao Zhou,Zhaohui Yang,Xiaoyang Li*

Main category: cs.IT

TL;DR: 提出针对XL-MIMO FDD系统的用户分布自适应码本设计，通过优化角度-距离采样和比特分配提升反馈效率


<details>
  <summary>Details</summary>
Motivation: 现有XL-MIMO码本设计（如极域码本）未充分考虑实际用户分布，导致反馈开销过大，需要设计更高效的反馈码本

Method: 1) 针对均匀分布用户场景，建立和速率最大化问题；2) 使用Voronoi划分证明均匀角度采样最优；3) 对距离采样设计，推导接收功率下界，提出几何采样作为高质量次优解；4) 扩展到非均匀分布用户场景，采用交替采样方法；5) 理论分析比特分配策略

Result: 数值结果表明所提码本设计在各种系统设置下具有优越的速率性能和鲁棒性，相比基准方案（包括广泛使用的极域码本）获得显著增益

Conclusion: 提出的用户分布自适应码本设计能有效降低XL-MIMO FDD系统的反馈开销，随着阵列尺寸增大，比特分配应更倾向于距离采样而非角度采样

Abstract: In this paper, we study efficient codebook design for limited feedback in extremely large-scale multiple-input-multiple-output (XL-MIMO) frequency division duplexing (FDD) systems. It is worth noting that existing codebook designs for XL-MIMO, such as polar-domain codebook, have not well taken into account user (location) distribution in practice, thereby incurring excessive feedback overhead. To address this issue, we propose in this paper a novel and efficient feedback codebook tailored to user distribution. To this end, we first consider a typical scenario where users are uniformly distributed within a specific polar-region, based on which a sum-rate maximization problem is formulated to jointly optimize angle-range samples and bit allocation among angle/range feedback. This problem is challenging to solve due to the lack of a closed-form expression for the received power in terms of angle and range samples. By leveraging a Voronoi partitioning approach, we show that uniform angle sampling is optimal for received power maximization. For more challenging range sampling design, we obtain a tight lower-bound on the received power and show that geometric sampling, where the ratio between adjacent samples is constant, can maximize the lower bound and thus serves as a high-quality suboptimal solution. We then extend the proposed framework to accommodate more general non-uniform user distribution via an alternating sampling method. Furthermore, theoretical analysis reveals that as the array size increases, the optimal allocation of feedback bits increasingly favors range samples at the expense of angle samples. Finally, numerical results validate the superior rate performance and robustness of the proposed codebook design under various system setups, achieving significant gains over benchmark schemes, including the widely used polar-domain codebook.

</details>


### [79] [Multiaccess Coded Caching with Heterogeneous Retrieval Costs](https://arxiv.org/abs/2601.10394)
*Wenbo Huang,Minquan Cheng,Kai Wan,Xiaojun Li,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于叠加编码的新型多址编码缓存框架，通过成本感知优化最小化系统总成本（缓存访问成本+广播成本）


<details>
  <summary>Details</summary>
Motivation: 现有MACC系统假设用户从连接的缓存节点检索内容没有通信成本，但实际中不同缓存节点的访问成本不同，服务器传输内容也有成本，需要成本感知的优化方案

Method: 1) 提出基于叠加编码的新型编码缓存框架，将Cheng等人的MACC方案分层；2) 推导成本感知优化问题，优化缓存放置以最小化系统成本；3) 利用最优解的稀疏性提出复杂度降低的结构感知算法

Result: 仿真结果表明，在异构检索成本场景下，所提方案始终优于Cheng等人的方案

Conclusion: 通过成本感知的编码缓存优化，有效降低了MACC系统的总成本，特别是在异构成本环境中表现优异

Abstract: The multiaccess coded caching (MACC) system, as formulated by Hachem {\it et al.}, consists of a central server with a library of $N$ files, connected to $K$ cache-less users via an error-free shared link, and $K$ cache nodes, each equipped with cache memory of size $M$ files. Each user can access $L$ neighboring cache nodes under a cyclic wrap-around topology. Most existing studies operate under the strong assumption that users can retrieve content from their connected cache nodes at no communication cost. In practice, each user retrieves content from its $L$ different connected cache nodes at varying costs. Additionally, the server also incurs certain costs to transmit the content to the users. In this paper, we focus on a cost-aware MACC system and aim to minimize the total system cost, which includes cache-access costs and broadcast costs. Firstly, we propose a novel coded caching framework based on superposition coding, where the MACC schemes of Cheng \textit{et al.} are layered. Then, a cost-aware optimization problem is derived that optimizes cache placement and minimizes system cost. By identifying a sparsity property of the optimal solution, we propose a structure-aware algorithm with reduced complexity. Simulation results demonstrate that our proposed scheme consistently outperforms the scheme of Cheng {\it et al.} in scenarios with heterogeneous retrieval costs.

</details>


### [80] [Placement Delivery Array for Cache-Aided MIMO Systems](https://arxiv.org/abs/2601.10422)
*Yifei Huang,Kai Wan,Minquan Cheng,Jinyan Wang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出MIMO-PDA统一结构，推导出缓存辅助MIMO网络的最大和自由度上界，并构造两种实现最大和自由度的方案，其中第二种方案在保持性能的同时指数级降低子分组复杂度。


<details>
  <summary>Details</summary>
Motivation: 在缓存辅助MIMO网络中，需要同时实现最大和自由度与低子分组复杂度的编码缓存方案。现有方案往往在两者之间存在权衡，需要设计能够同时优化这两个指标的方案。

Method: 引入MIMO-PDA统一组合结构，分析其组合特性，推导和自由度上界。基于上界提出两种MIMO-PDA构造：第一种在严格参数约束下实现线性子分组复杂度；第二种在更宽松约束下实现有序指数级子分组复杂度。

Result: 推导出和自由度上界为min{KG, Gt+G⌈L/G⌉}，与现有最优结果一致。两种构造均实现最大和自由度，第二种构造相比现有方案指数级降低子分组复杂度。

Conclusion: MIMO-PDA框架为缓存辅助MIMO网络提供了统一设计方法，提出的构造方案在保持最大和自由度的同时显著降低了子分组复杂度，解决了现有方案中的关键权衡问题。

Abstract: We consider a $(G,L,K,M,N)$ cache-aided multiple-input multiple-output (MIMO) network, where a server equipped with $L$ antennas and a library of $N$ equal-size files communicates with $K$ users, each equipped with $G$ antennas and a cache of size $M$ files, over a wireless interference channel. Each user requests an arbitrary file from the library. The goal is to design coded caching schemes that simultaneously achieve the maximum sum degrees of freedom (sum-DoF) and low subpacketization. In this paper, we first introduce a unified combinatorial structure, termed the MIMO placement delivery array (MIMO-PDA), which characterizes uncoded placement and one-shot zero-forcing delivery. By analyzing the combinatorial properties of MIMO-PDAs, we derive a sum-DoF upper bound of $\min\{KG, Gt+G\lceil L/G \rceil\}$, where $t=KM/N$, which coincides with the optimal DoF characterization in prior work by Tehrani \emph{et al.}. Based on this upper bound, we present two novel constructions of MIMO-PDAs that achieve the maximum sum-DoF. The first construction achieves linear subpacketization under stringent parameter constraints, while the second achieves ordered exponential subpacketization under substantially milder constraints. Theoretical analysis and numerical comparisons demonstrate that the second construction exponentially reduces subpacketization compared to existing schemes while preserving the maximum sum-DoF.

</details>


### [81] [Energy-Efficient Probabilistic Semantic Communication Over Visible Light Networks With Rate Splitting](https://arxiv.org/abs/2601.10452)
*Zhouxiang Zhao,Zhaohui Yang,Mingzhe Chen,Chen Zhu,Xin Tong,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 该论文研究资源受限的可见光通信系统中基于概率语义通信的能量效率最大化问题，通过联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比来提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 可见光通信具有独特的物理层优势，但其与语义通信等高层次技术的结合仍未被充分探索。在资源受限的VLC系统中，语义压缩会带来额外计算开销，而知识库需要同步更新，这带来了能量效率优化的挑战。

Method: 采用基于概率图的语义通信系统，使用速率分割多址接入同时传输知识和信息数据。提出基于连续凸近似和Dinkelbach方法的交替优化算法，联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比。

Result: 仿真结果表明所提出的方法有效，能够显著提高可见光通信系统中概率语义通信的能量效率。

Conclusion: 该研究成功解决了VLC系统中概率语义通信的能量效率优化问题，提出的联合优化框架为未来无线通信系统中语义通信与物理层技术的融合提供了有效解决方案。

Abstract: Visible light communication (VLC) is emerging as a key technology for future wireless communication systems due to its unique physical-layer advantages over traditional radio-frequency (RF)-based systems. However, its integration with higher-layer techniques, such as semantic communication, remains underexplored. This paper investigates the energy efficiency maximization problem in a resource-constrained VLC-based probabilistic semantic communication (PSCom) system. In the considered model, light-emitting diode (LED) transmitters perform semantic compression to reduce data size, which incurs additional computation overhead. The compressed semantic information is transmitted to the users for semantic inference using a shared knowledge base that requires periodic updates to ensure synchronization. In the PSCom system, the knowledge base is represented by probabilistic graphs. To enable simultaneous transmission of both knowledge and information data, rate splitting multiple access (RSMA) is employed. The optimization problem focuses on maximizing energy efficiency by jointly optimizing transmit beamforming, direct current (DC) bias, common rate allocation, and semantic compression ratio, while accounting for both communication and computation costs. To solve this problem, an alternating optimization algorithm based on successive convex approximation (SCA) and Dinkelbach method is developed. Simulation results demonstrate the effectiveness of the proposed approach.

</details>


### [82] [Joint Source-Channel Coding for ISAC: Distortion Tradeoffs and Separation Theorems](https://arxiv.org/abs/2601.10470)
*Gefei Peng,Youlong Wu*

Main category: cs.IT

TL;DR: 论文研究了集成感知与通信系统中通信与感知性能的权衡关系，证明了分离信源信道编码在该场景下能达到联合最优性。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信系统能够同时实现高效通信和环境感知，但需要明确通信与感知之间的性能权衡关系，这是该领域的核心问题。

Method: 采用联合信源信道编码框架，包含带信道状态估计器和联合编码器的发射机、状态相关无记忆信道、以及带联合解码器的接收机，从信息论角度建立性能权衡关系。

Result: 建立了信道容量、通信和感知过程中的失真以及估计成本之间的权衡关系，证明了分离信源信道编码在该设置下能够达到联合最优性，并通过二进制设置示例验证了理论结果。

Conclusion: 在集成感知与通信系统中，分离信源信道编码能够实现通信与感知性能的联合最优权衡，为系统设计提供了理论基础。

Abstract: Integrated Sensing and Communication (ISAC) systems have garnered significant attention due to their capability to simultaneously achieve efficient communication and environmental sensing. A core objective in this field is characterizing the performance tradeoff between sensing and communication. In this paper, we consider a joint source-channel coding (JSCC) framework for the ISAC system that consists of a transmitter with a channel state estimator and a joint source-channel encoder, a state-dependent memoryless channel, and a receiver with a joint source-channel decoder. From an information-theoretic perspective, we establish the tradeoff relationships among channel capacity, distortions in both communication and sensing processes, and the estimation cost. We prove that the separate source and channel coding can achieve joint optimality in this setting. An illustrative example of a binary setting is also provided to validate our theoretical results.

</details>


### [83] [A Construction Framework of Coded Caching Scheme for Multi-Access MIMO Systems via Knapsack Problem](https://arxiv.org/abs/2601.10484)
*Siying Luo,Youlong Wu,Mingming Zhang,Minquan Cheng,Dianhua Wu*

Main category: cs.IT

TL;DR: 提出基于多天线放置交付阵列(MAPDA)的编码缓存方案，用于组合拓扑多接入MISO网络，通过0-1背包问题优化设计，在提高和自由度(sum-DoF)的同时保持较低的子分组复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有编码缓存方案在组合拓扑多接入MISO网络中面临和自由度与子分组复杂度之间的权衡问题，需要设计能同时实现高sum-DoF和低复杂度的缓存方案。

Method: 将多天线放置交付阵列(MAPDA)设计建模为0-1背包问题来最大化可实现的自由度，将复杂组合缓存结构转化为可处理的优化框架，生成高效缓存放置和灵活交付策略。

Result: 在组合拓扑网络中，所提方案比现有方案获得更高sum-DoF；在相同缓存大小约束下，子分组水平与现有线性子分组方案相当；特定条件下达到理论最大sum-DoF min{L+KM/N, K}并进一步降低子分组复杂度；针对特定组合结构获得优化构造，实现更高sum-DoF和更低子分组。

Conclusion: 提出的基于MAPDA的编码缓存方案有效解决了组合拓扑多接入MISO网络中sum-DoF与子分组复杂度的权衡问题，通过优化框架实现了性能提升和复杂度降低。

Abstract: This paper investigates the coded caching problem in a multi-access multiple-input single-output (MAMISO) network with the combinatorial topology. The considered system consists of a server containing $N$ files, $Λ$ cache nodes, and $K$ cache-less users, where each user can access a unique subset of $r$ cache nodes. The server is equipped with $L$ transmit antennas. Our objective is to design a caching scheme that simultaneously achieves a high sum Degree of Freedom (sum-DoF) and low subpacketization complexity. To address this challenge, we formulate the design of multi-antenna placement delivery arrays (MAPDA) as a $0$--$1$ knapsack problem to maximize the achievable DoF, thereby transforming the complex combinatorial caching structure into a tractable optimization framework that yields efficient cache placement and flexible delivery strategies. Theoretical and numerical analyses demonstrate that: for networks with combinatorial topologies, the proposed scheme achieves a higher sum-DoF than existing schemes. Under identical cache size constraints, the subpacketization level remains comparable to existing linear subpacketization schemes. Moreover, under specific system conditions, the proposed scheme attains the theoretical maximum sum-DoF of $\min\{L+KM/N, K\}$ while achieving further reductions subpacketization. For particular combinatorial structures, we further derive optimized constructions that achieve even higher sum-DoF with lower subpacketization. ```

</details>


### [84] [Coded Caching for Combinatorial Multi-Access Hotplug Networks from $t$-Designs](https://arxiv.org/abs/2601.10503)
*Dhruv Pratap Singh,Anjana A. Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 该论文研究组合多接入网络中的热插拔编码缓存，提出基于t设计的编码缓存方案，在特定内存区间优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有热插拔编码缓存模型在组合多接入网络中存在局限性，用户可访问多个缓存但交付阶段只有部分缓存在线，需要更通用的框架来支持这种场景。

Method: 将热插拔放置交付数组(HpPDA)框架推广到组合多接入设置，提出基于t设计的编码缓存方案，通过适当的参数选择消除冗余多播传输。

Result: 提出的t方案实现了灵活子分组化的一系列速率-内存权衡，在特定内存区间优于现有热插拔编码缓存方案。

Conclusion: 该研究成功将热插拔编码缓存扩展到组合多接入网络，提出的t设计方案在性能和灵活性方面优于现有方法。

Abstract: We study hotplug coded caching in combinatorial multi-access networks, which generalizes existing hotplug coded caching models by allowing users to access multiple caches, while only a subset of caches is online during the delivery phase. We first generalize the Hotplug Placement Delivery Array (HpPDA) framework to the combinatorial multi-access setting. Based on this generalized framework, we propose a t-design-based coded caching scheme for combinatorial multi-access networks. We characterize a class of design parameters under which every active user has access to a sufficient number of coded subfiles to decode its requested file, and show that appropriate parameter choices allow for the elimination of redundant multicast transmissions. As a result, the proposed scheme achieves a family of rate memory trade offs with flexible subpacketization. We present numerical comparisons illustrating that the proposed t-scheme outperforms existing hotplug coded caching schemes in certain memory regimes.

</details>


### [85] [A New Construction Structure on Coded Caching with Linear Subpacketization: Non-Half-Sum Latin Rectangle](https://arxiv.org/abs/2601.10505)
*Yongcheng Yang,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文提出了一种新的组合结构NHSLR，将线性编码缓存的子分组规模从F=K扩展到F=O(K)，实现了线性可扩展的子分组规模，同时进一步降低了传输负载。


<details>
  <summary>Details</summary>
Motivation: 编码缓存虽然能有效缓解网络拥塞，但现有方案在子分组规模和传输负载之间存在权衡：指数或多项式子分组方案负载低但复杂度高，而线性子分组方案（如NHSDP）虽然子分组规模等于用户数K，但传输负载较高。需要设计既能保持线性子分组规模又能降低传输负载的新方案。

Method: 提出了一种新的组合结构——非半和拉丁矩形(NHSLR)，扩展了NHSDP框架，将线性编码缓存方案的子分组规模从F=K扩展到更广泛的F=O(K)场景。通过构造NHSLR，获得了一类新的编码缓存方案。

Result: 新方案实现了线性可扩展的子分组规模，同时相比NHSDP方案进一步降低了传输负载。理论分析和数值实验表明，该方案不仅比现有线性子分组方案具有更低的传输负载，而且性能接近某些指数子分组方案。

Conclusion: NHSLR结构为编码缓存方案设计提供了新思路，在保持线性子分组复杂度的同时显著改善了传输性能，平衡了子分组规模和传输负载之间的矛盾。

Abstract: Coded caching is recognized as an effective method for alleviating network congestion during peak periods by leveraging local caching and coded multicasting gains. The key challenge in designing coded caching schemes lies in simultaneously achieving low subpacketization and low transmission load. Most existing schemes require exponential or polynomial subpacketization levels, while some linear subpacketization schemes often result in excessive transmission load. Recently, Cheng et al. proposed a construction framework for linear coded caching schemes called Non-Half-Sum Disjoint Packing (NHSDP), where the subpacketization equals the number of users $K$. This paper introduces a novel combinatorial structure, termed the Non-Half-Sum Latin Rectangle (NHSLR), which extends the framework of linear coded caching schemes from $F=K$ (i.e., the construction via NHSDP) to a broader scenario with $F=\mathcal{O}(K)$. By constructing NHSLR, we have obtained a new class of coded caching schemes that achieves linearly scalable subpacketization, while further reducing the transmission load compared with the NHSDP scheme. Theoretical and numerical analyses demonstrate that the proposed schemes not only achieves lower transmission load than existing linear subpacketization schemes but also approaches the performance of certain exponential subpacketization schemes.

</details>


### [86] [A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10510)
*Mengyuan Li,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于CMA-NHSDP结构的多接入编码缓存方案，在保持线性子分组化(F=K)的同时实现了较低的传输负载。


<details>
  <summary>Details</summary>
Motivation: 现有多接入编码缓存方案存在矛盾：具有竞争性传输性能的方案子分组化呈指数增长，而具有线性或多项式子分组化的方案总是导致更高的传输负载。需要设计在保持线性子分组化的同时维持低传输负载的方案。

Method: 将Cheng等人提出的线性子分组化编码缓存框架NHSDP扩展到多接入系统，提出新的组合结构CMA-NHSDP。通过构造CMA-NHSDP获得新类别的多接入编码缓存方案。

Result: 理论分析和数值分析表明，该方案在保持线性子分组化(F=K)的同时，比现有线性子分组化方案实现了更低的传输负载。在某些情况下，甚至比具有指数子分组化的现有方案传输负载更低。

Conclusion: 提出的CMA-NHSDP结构成功解决了多接入编码缓存系统中线性子分组化与低传输负载之间的矛盾，为实际系统部署提供了有前景的解决方案。

Abstract: We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.

</details>


### [87] [On the suboptimality of linear codes for binary distributed hypothesis testing](https://arxiv.org/abs/2601.10526)
*Adway Girish,Robinson D. H. Cung,Emre Telatar*

Main category: cs.IT

TL;DR: 研究二进制分布式假设检验问题，两个代理观察相关二进制向量，以相同速率向中央决策者发送压缩信息。分析线性压缩方案，证明截断在某些情况下是最佳线性方案。


<details>
  <summary>Details</summary>
Motivation: 研究分布式假设检验中的线性压缩方案性能，特别是在二进制向量相关性的检测场景中，探索线性方案的最优性及其局限性。

Method: 研究线性压缩方案，分析截断方法，在两种特定情况下证明截断是最佳线性方案：1)检测相同幅度但符号相反的相关系数；2)检测独立性或非独立性。通过数值证据支持猜想。

Result: 证明截断在两种情况下是最佳线性方案，并猜想对于任何符号相反的相关系数检测，截断都是最佳线性编码。对于独立性检测，计算经典随机编码指数，显示截断（及任何线性编码）严格次优。

Conclusion: 线性压缩方案在特定假设检验场景中可以达到最优，但在独立性检测等任务中存在局限性，非线性编码可能提供更好的性能。

Abstract: We study a binary distributed hypothesis testing problem where two agents observe correlated binary vectors and communicate compressed information at the same rate to a central decision maker. In particular, we study linear compression schemes and show that simple truncation is the best linear scheme in two cases: (1) testing opposite signs of the same magnitude of correlation, and (2) testing for or against independence. We conjecture, supported by numerical evidence, that truncation is the best linear code for testing any correlations of opposite signs. Further, for testing against independence, we also compute classical random coding exponents and show that truncation, and consequently any linear code, is strictly suboptimal.

</details>


### [88] [Network Integrated Sensing and Communication](https://arxiv.org/abs/2601.10538)
*Edward Andrews,Lawrence Ong,Duy T. Ngo,Yao Liu,Min Li*

Main category: cs.IT

TL;DR: 本文研究了网络级ISAC系统，分析了中继网络中通信路由与感知覆盖之间的基本权衡关系，为6G异构网络设计提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要集中在链路级设计，但大规模部署需要理解网络级性能。本文旨在研究网络ISAC模型中通信路由与感知覆盖之间的基本权衡关系。

Method: 提出一个新颖的优化框架，捕捉多节点路由与感知覆盖之间的相互作用。针对一维路径网络提供完整的感知-吞吐量区域解析表征，并扩展到一般网络拓扑，建立感知-吞吐量Pareto边界的分段线性特性。

Result: 对于一维路径网络，获得了完整的感知-吞吐量区域解析表征；对于一般网络拓扑，证明了感知-吞吐量Pareto边界是分段线性的，并为每个分段提供了物理解释。

Conclusion: 研究揭示了感知覆盖与通信路由之间的基本权衡关系，为未来6G异构网络的设计提供了关键见解，支持网络级ISAC系统的优化部署。

Abstract: Integrated sensing and communication (ISAC) is a cornerstone technology for 6G networks, offering unified support for high-rate communication and high-accuracy sensing. While existing literature extensively covers link-level designs, the transition toward large-scale deployment necessitates a fundamental understanding of network-level performance. This paper investigates a network ISAC model where a source node communicates with a destination via a relay network, while intermediate nodes concurrently perform cooperative sensing over specific spatial regions. We formulate a novel optimization framework that captures the interplay between multi-node routing and sensing coverage. For a one-dimensional path network, we provide an analytical characterization of the complete sensing-throughput region. Extending this to general network topologies, we establish that the sensing-throughput Pareto boundary is piecewise linear and provide physical interpretations for each segment. Our results reveal the fundamental trade-offs between sensing coverage and communication routing, offering key insights for the design of future 6G heterogeneous networks.

</details>


### [89] [Error-Correcting Codes for Two Bursts of t1-Deletion-t2-Insertion with Low Computational Complexity](https://arxiv.org/abs/2601.10540)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 本文研究能纠正多个(t₁,t₂)-DI错误突发（每个突发包含t₁个删除后接t₂个插入）的纠错码构造，建立了不同错误类型间的等价关系，推导了码率上下界，并提出了低复杂度构造方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用如DNA数据存储和文档同步中，经常出现同时包含插入、删除和替换的突发错误，因此需要开发能够纠正此类错误的信道编码。

Method: 首先建立了三种错误类型间的等价关系，然后推导了两个(t₁,t₂)-DI错误突发码的码率上下界，最后提出了具体的码构造方法。

Result: 提出的码构造相比基于综合征压缩技术的码具有显著更低的计算复杂度，同时建立了错误类型间的等价关系并推导了码率理论界限。

Conclusion: 本文为纠正多个(t₁,t₂)-DI错误突发的纠错码提供了理论基础和实用构造，在DNA数据存储和文档同步等应用中具有重要价值。

Abstract: Burst errors involving simultaneous insertions, deletions, and substitutions occur in practical scenarios, including DNA data storage and document synchronization, motivating developments of channel codes that can correct such errors. In this paper, we address the problem of constructing error-correcting codes (ECCs) capable of handling multiple bursts of $t_1$-deletion-$t_2$-insertion ($(t_1,t_2)$-DI) errors, where each burst consists of $t_1$ deletions followed by $t_2$ insertions in a binary sequence. We make three key contributions: Firstly, we establish the fundamental equivalence of (1) two bursts of $(t_1,t_2)$-DI ECCs, (2) two bursts of $(t_2,t_1)$-DI ECCs, and (3) one burst each of $(t_1,t_2)$-DI and $(t_2,t_1)$-DI ECCs. Then, we derive lower and upper bounds on the code size of two bursts of $(t_1,t_2)$-DI ECCs, which can naturally be extended to the case of multiple bursts. Finally, we present constructions of two bursts of $(t_1,t_2)$-DI ECCs. Compared to the codes obtained by the syndrome compression technique, the resulting codes achieve significantly lower computational complexity.

</details>


### [90] [Sparse Signal Recovery from Random Measurements](https://arxiv.org/abs/2601.10569)
*Siu-Wing Cheng,Man Ting Wong*

Main category: cs.IT

TL;DR: 提出一种无需优化或求解线性系统的简单方法，通过Θ(log n)个随机传感矩阵在O(kn log n)时间内恢复稀疏信号，其中k = Θ(s log n)，s为信号非零坐标数


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法需要解决优化问题或线性系统，计算复杂度高。本文旨在开发一种更简单、更高效的方法来恢复稀疏信号，避免复杂的优化过程

Method: 使用Θ(log n)个随机传感矩阵，通过简单的计算而非优化问题来恢复信号。方法首先确定信号的支持集（非零坐标位置），然后恢复信号值。算法时间复杂度为O(kn log n)，其中k = Θ(s log n)

Result: 方法在理论上保证恢复稀疏信号，时间复杂度显著低于传统优化方法。实验表明，在二进制信号上，该方法与基于优化的方法相比具有竞争力

Conclusion: 提出了一种新颖的压缩感知信号恢复方法，避免了复杂的优化过程，计算效率高，特别适用于稀疏信号恢复，为压缩感知提供了新的简单解决方案

Abstract: Given the compressed sensing measurements of an unknown vector $z \in \mathbb{R}^n$ using random matrices, we present a simple method to determine $z$ without solving any optimization problem or linear system. Our method uses $Θ(\log n)$ random sensing matrices in $\mathbb{R}^{k \times n}$ and runs in $O(kn\log n)$ time, where $k = Θ(s\log n)$ and $s$ is the number of nonzero coordinates in $z$. We adapt our method to determine the support set of $z$ and experimentally compare with some optimization-based methods on binary signals.

</details>


### [91] [Fundamental Limits of Multi-User Distributed Computing of Linearly Separable Functions](https://arxiv.org/abs/2601.10603)
*K. K. Krishnan Namboodiri,Elizabath Peter,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 本文建立了多用户分布式计算线性可分函数的基本极限，提出了在通信与计算权衡下的最优分布式计算方案。


<details>
  <summary>Details</summary>
Motivation: 研究分布式计算中线性可分函数计算的基本极限，核心在于通信与计算之间的权衡：服务器计算能力有限（最多计算M个子函数），通信能力有限（最多向Δ个用户传输线性组合）。目标是设计能降低通信成本的分布式计算方案。

Method: 针对给定的K、L、M、Δ参数，提出联合设计任务分配和传输的分布式计算方案。使用新颖的逆定理证明方案在实数域下的最优性，并使用基于计数论证的另一个逆定理分析有限域下的性能。

Result: 提出的分布式计算方案在实数域下各种条件下达到最优性能，并在有限域下给出了性能表征。

Conclusion: 本文建立了多用户分布式计算线性可分函数的基本极限，提出的方案在通信与计算权衡下达到最优，为分布式计算系统设计提供了理论基础。

Abstract: This work establishes the fundamental limits of the classical problem of multi-user distributed computing of linearly separable functions. In particular, we consider a distributed computing setting involving $L$ users, each requesting a linearly separable function over $K$ basis subfunctions from a master node, who is assisted by $N$ distributed servers. At the core of this problem lies a fundamental tradeoff between communication and computation: each server can compute up to $M$ subfunctions, and each server can communicate linear combinations of their locally computed subfunctions outputs to at most $Δ$ users. The objective is to design a distributed computing scheme that reduces the communication cost (total amount of data from servers to users), and towards this, for any given $K$, $L$, $M$, and $Δ$, we propose a distributed computing scheme that jointly designs the task assignment and transmissions, and shows that the scheme achieves optimal performance in the real field under various conditions using a novel converse. We also characterize the performance of the scheme in the finite field using another converse based on counting arguments.

</details>


### [92] [Basis-Spline Assisted Coded Computing: Strategies and Error Bounds](https://arxiv.org/abs/2601.10616)
*Rimpi Borah,J. Harshan,V. Lalitha*

Main category: cs.IT

TL;DR: 提出基于三次B样条插值的编码计算框架，用于分布式计算中的非多项式函数处理，相比现有Berrut方法在存在大量掉队者时提供更好的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Berrut近似编码计算方法在处理非多项式函数时，由于Berrut插值具有全局支撑特性，当掉队者数量较多时精度会显著下降，需要更稳定的方法。

Method: 提出基于三次B样条插值的编码计算框架，利用B样条的局部支撑和平滑特性，在主机节点重建服务器端函数评估，增强稳定性和精度。

Result: 提供了将B样条插值集成到编码计算中的系统方法，推导了关于服务器数量和掉队者数量的近似误差理论界限，相比Berrut方法在各种非多项式函数上表现显著更优。

Conclusion: 基于B样条的编码计算框架通过利用局部支撑特性，有效解决了现有方法在大量掉队者情况下的精度下降问题，为非多项式函数的分布式计算提供了更可靠的解决方案。

Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.

</details>


### [93] [Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval](https://arxiv.org/abs/2601.10643)
*Chandan Anand,Jayesh Seshadri,Prasad Krishnan,Gowtham R. Kurri*

Main category: cs.IT

TL;DR: 本文证明了Chandan等人提出的弱私有信息检索方案在特定条件下的最优性，并给出了当条件不满足时的反例。


<details>
  <summary>Details</summary>
Motivation: Chandan等人提出了新的弱私有信息检索方案并给出了速率-隐私权衡表达式，但这些权衡是否在各自类别中是最优的尚不清楚。

Method: 通过数学证明和构造反例的方法，分析Sun-Jafar型和Banawan-Ulukus型WPIR方案的最优性条件。

Result: 证明了在无合谋复制存储设置下，Chandan等人的Sun-Jafar型方案是最优的；在满足阈值约束条件下，Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-合谋WPIR方案也是类别最优的；当阈值约束不满足时，可以构造反例实现更高的速率。

Conclusion: 本文确定了Chandan等人提出的WPIR方案在特定条件下的最优性边界，并揭示了当系统参数不满足阈值约束时存在更优方案的可能性。

Abstract: Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.

</details>


### [94] [One-Shot Broadcast Joint Source-Channel Coding with Codebook Diversity](https://arxiv.org/abs/2601.10648)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: 研究单次联合信源信道编码，广播给K个解码器，至少一个解码器在最大失真约束下恢复信源即可。发现使用不相交码本可获得码本分集增益，不同于信道分集增益。提出混合编码方案平衡码本和信道分集，在BSC上优于完全共享或完全不相交码本策略。


<details>
  <summary>Details</summary>
Motivation: 研究多解码器广播场景下的单次联合信源信道编码问题，关注至少一个解码器成功恢复信源的情况。传统方法可能期望信道分集增益，但本文发现码本分集增益是独立存在的增益来源，需要探索如何最优利用这两种分集机制。

Method: 1) 分析不相交码本策略，推导一阶和二阶可达界，通过改进Poisson匹配引理适应多解码器不相交码本场景；2) 提出混合编码方案，将解码器分组以最优平衡码本分集和信道分集；3) 在二进制对称信道上进行数值验证。

Result: 1) 理论上证明了不相交码本可获得码本分集增益，不同于信道分集增益；2) 混合编码方案在二进制对称信道上数值结果显示，优于完全共享码本或完全不相交码本的策略，实现了两种分集机制的最优平衡。

Conclusion: 在单次联合信源信道编码的多解码器广播场景中，码本分集是独立于信道分集的重要增益来源。通过混合编码方案平衡两种分集机制，可以获得比单纯依赖一种分集机制更好的性能，为多用户广播系统设计提供了新思路。

Abstract: We study a one-shot joint source-channel coding setting where the source is encoded once and broadcast to $K$ decoders through independent channels. Success is predicated on at least one decoder recovering the source within a maximum distortion constraint. We find that in the one-shot regime, utilizing disjoint codebooks at each decoder yields a codebook diversity gain, distinct from the channel diversity gain that may be expected when several decoders observe independent realizations of the channel's output but share the same codebook. Coding schemes are introduced that leverage this phenomenon, where first- and second-order achievability bounds are derived via an adaptation of the Poisson matching lemma (Li and Anantharam, 2021) which allows for multiple decoders using disjoint codebooks. We further propose a hybrid coding scheme that partitions decoders into groups to optimally balance codebook and channel diversity. Numerical results on the binary symmetric channel demonstrate that the hybrid approach outperforms strategies where the decoders' codebooks are either fully shared or disjoint.

</details>


### [95] [Synchronizing Probabilities in Model-Driven Lossless Compression](https://arxiv.org/abs/2601.10678)
*Aviv Adler,Jennifer Tang*

Main category: cs.IT

TL;DR: PMATIC是一种概率匹配区间编码算法，能够容忍神经网络预测中的微小差异，解决模型驱动压缩中的预测不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在概率预测方面表现出色，可用于数据压缩。然而，压缩器和解压器需要完全匹配的概率预测，而神经网络由于硬件、软件或计算顺序的微小差异会导致预测不匹配，进而引发解码失败。

Method: 提出概率匹配区间编码（PMATIC），这是一种模型无关算法，能够容忍有界的预测不匹配，且开销较低。PMATIC使用预测概率，可作为模型驱动压缩工具中算术编码器的直接替代品。

Result: 论文展示了PMATIC的理论正确性和性能边界，并在文本数据上验证了结果。当与先进的预测模型配合使用时，PMATIC对预测不匹配具有鲁棒性，同时压缩率优于标准的现代压缩工具。

Conclusion: PMATIC为解决模型驱动压缩中的预测不匹配问题提供了一种有效的解决方案，能够在容忍预测差异的同时实现优于传统方法的压缩性能。

Abstract: It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.

</details>


### [96] [Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes](https://arxiv.org/abs/2601.10682)
*Pin-Hsun Lin,Hadi Aghaee,Christian Deppe,Eduard A. Jorswieck,Holger Boche*

Main category: cs.IT

TL;DR: 基于极化码在二进制输入加性高斯白噪声信道上的二选一不经意传输协议，通过极化变换的自同构实现完美接收者隐私，渐近获得发送者隐私


<details>
  <summary>Details</summary>
Motivation: 开发一种在二进制输入加性高斯白噪声信道上的二选一不经意传输协议，需要同时保证接收者隐私和发送者隐私，并在有限码长下实现可靠传输

Method: 使用极化码，通过极化变换的自同构连接两个解码器视图，公开从对应的自同构群中随机选择编码器；在选定的坏比特信道上故意注入随机性；利用极化变换自同构作为比特级置换的特性进行优化

Result: 在任何有限码长下实现完美接收者隐私（公开编码器分布与接收者选择比特无关）；通过信道极化结合隐私放大渐近获得发送者隐私；推导了松弛的可靠性准则并评估了有限码长性能；优化了可实现的有限码长OT速率

Conclusion: 成功开发了基于极化码的二选一不经意传输协议，实现了完美的接收者隐私和渐近的发送者隐私，通过自同构结构和松弛可靠性准则在有限码长下获得了良好的性能

Abstract: We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver's choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.

</details>


### [97] [Improved Constructions of Reed-Solomon Codes with Optimal Repair Bandwidth](https://arxiv.org/abs/2601.10685)
*Jing Qiu,Weijun Fang,Shu-Tao Xia,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 改进的RS-MSR码构造，消除了素数模s同余1的条件，显著降低了子分组化程度并扩展了可行参数范围。


<details>
  <summary>Details</summary>
Motivation: 现有RS-MSR码构造要求素数满足p_i ≡ 1 (mod s)的条件，这限制了参数选择并导致较大的子分组化程度，需要更灵活的构造方法。

Method: 提出改进的RS-MSR码构造，消除了素数必须满足p_i ≡ 1 (mod s)的同余条件，从而放宽了参数限制。

Result: 新构造将子分组化程度降低了φ(s)^n倍（φ为欧拉函数），显著扩展了RS-MSR码的可行参数范围。

Conclusion: 通过消除同余条件，实现了更高效的RS-MSR码构造，降低了子分组化程度，为分布式存储系统提供了更灵活的编码方案。

Abstract: Maximum-distance-separable (MDS) codes are widely used in distributed storage, yet naive repair of a single erasure in an $[n,k]$ MDS code downloads the entire contents of $k$ nodes. Minimum Storage Regenerating (MSR) codes (Dimakis et al., 2010) minimize repair bandwidth by contacting $d>k$ helpers and downloading only a fraction of data from each. Guruswami and Wootters first proposed a linear repair scheme for Reed-Solomon (RS) codes, showing that they can be repaired with lower bandwidth than the naive approach. The existence of RS codes achieving the MSR point (RS-MSR codes) nevertheless remained open until the breakthrough construction of Tamo, Barg, and Ye, which yields RS-MSR codes with subpacketization $\ell = s \prod_{i=1}^n p_i$, where $p_i$ are distinct primes satisfying $p_i \equiv 1 \pmod{s}$ and $s=d+1-k$.
  In this paper, we present an improved construction of RS-MSR codes by eliminating the congruence condition $p_i \equiv 1 \pmod{s}$. Consequently, our construction reduces the subpacketization by a multiplicative factor of $φ(s)^n$ ( $φ(\cdot)$ is Euler's totient function) and broadens the range of feasible parameters for RS-MSR codes.

</details>


### [98] [Perfect Secret Key Generation for a class of Hypergraphical Sources](https://arxiv.org/abs/2601.10697)
*Manuj Mukherjee,Sagnik Chatterjee,Alhad Sethi*

Main category: cs.IT

TL;DR: 本文扩展了PIN模型的完美密钥生成方案到超图，提出了针对完全t-均匀超图的容量可达方案，以及针对3-均匀超图的2比特完美密钥生成方案。


<details>
  <summary>Details</summary>
Motivation: Nitinawarat和Narayan提出的PIN模型完美密钥生成方案基于图的生成树打包率。本文旨在将这一框架扩展到超图模型，利用超图的组合性质设计类似的完美密钥生成方案。

Method: 1. 针对完全t-均匀超图，利用星形超图打包完全t-均匀超图，设计每个星图产生$\binom{m-2}{t-2}$比特完美密钥的方案。2. 针对3-均匀星形超图（其投影为环），设计2比特完美密钥生成方案，然后扩展到一般3-均匀超图，结合星图打包和图哈密顿打包。

Result: 1. 提出了针对完全t-均匀超图的容量可达完美密钥生成方案。2. 提出了针对3-均匀超图的完美密钥生成方案，并证明该方案对某些超图类是容量可达的。

Conclusion: 成功将PIN模型的完美密钥生成框架扩展到超图模型，利用超图的组合性质设计了有效的密钥生成方案，为超图模型下的安全通信提供了理论基础。

Abstract: Nitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph.
  Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.

</details>
