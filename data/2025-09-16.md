<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 24]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.IT](#cs.IT) [Total: 13]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Energy-Aware Data Center Management: A Sustainable Approach to Reducing Carbon Footprint](https://arxiv.org/abs/2509.10462)
*Rabab Khan Rongon,Krishna Das*

Main category: cs.NI

TL;DR: 这篇论文探讨了数据中心能源敏感管理策略，通过动态负载分配、可再生能源集成和智能制冷等技术，在保持高性能的同时降低能消和碳排放，实现环境和经济双重收益。


<details>
  <summary>Details</summary>
Motivation: 云计算和数据中心基础设施的快速扩展导致能源消耗和碳踹迹大幅增长，带来严重环境挑战，需要发展可持续的运营策略来应对这些挑战。

Method: 研究提出了一个框架，集成了先进节能技术和优化资源利用策略，包括动态工作负载分配、可再生能源集成和智能制冷系统。通过模拟和案例研究进行验证。

Result: 研究证明了这些策略能够在保持高性能的同时有效降低总体能消耗，对运营成本和性能效率产生积极影响，实现环境和经济双重收益。

Conclusion: 这项研究突出了可扩展、能源敏感的数据中心设计的潜力，能够在确保最佳功能的同时显著降低环境影响，为全球应对气候变化做出贡献。

Abstract: The rapid expansion of cloud computing and data center infrastructure has led
to significant energy consumption, posing environmental challenges due to the
growing carbon footprint. This research explores energy-aware management
strategies aimed at creating sustainable data center operations. By integrating
advanced energy-efficient technologies and optimizing resource utilization,
this study proposes a framework to minimize power usage while maintaining high
performance. Key elements include dynamic workload allocation, renewable energy
integration, and intelligent cooling systems, all of which contribute to
reducing overall energy consumption. The study also examines the impact of
these strategies on operational costs and performance efficiency, demonstrating
how sustainable practices can be both environmentally and economically
beneficial. Through simulations and case studies, the research offers practical
insights into reducing carbon emissions in data centers, supporting the
transition towards greener cloud infrastructure. The findings highlight the
potential for scalable, energy-aware data center designs that significantly
lower environmental impact while ensuring optimal functionality, contributing
to the global effort of mitigating climate change.

</details>


### [2] [A Dynamic Service Offloading Algorithm Based on Lyapunov Optimization in Edge Computing](https://arxiv.org/abs/2509.10475)
*Peiyan Yuan,Ming Li,Chenyang Wang,Ledong An,Xiaoyan Zhao,Junna Zhang,Xiangyang Li,Huadong Ma*

Main category: cs.NI

TL;DR: 这篇论文研究协作边缘计算中系统稳定性与载荷分配成本的交换关系，提出了一种基于Lyapunov优化的动态载荷分配算法LDSO，在保证长期稳定性的同时最小化成本。


<details>
  <summary>Details</summary>
Motivation: 现有协作载荷分配方法往往忽视队列稳定性对整体系统性能的重要影响，需要结合系统稳定性来优化载荷成本。

Method: 构建多跳数据传输模型和成本模型（能耗和延迟），引入时变队列模型保持系统稳定性，基于Lyapunov优化提出LDSO动态载荷算法。

Result: 理论分析和实验结果验证LDSO算法在成本效率和系统稳定性方面都显著优于现有最优方法。

Conclusion: 该研究通过结合系统稳定性的动态载荷优化方法，有效解决了协作边缘计算中稳定性与成本的平衡问题，提高了系统性能。

Abstract: This study investigates the trade-off between system stability and offloading
cost in collaborative edge computing. While collaborative offloading among
multiple edge servers enhances resource utilization, existing methods often
overlook the role of queue stability in overall system performance. To address
this, a multi-hop data transmission model is developed, along with a cost model
that captures both energy consumption and delay. A time-varying queue model is
then introduced to maintain system stability. Based on Lyapunov optimization, a
dynamic offloading algorithm (LDSO) is proposed to minimize offloading cost
while ensuring long-term stability. Theoretical analysis and experimental
results verify that the proposed LDSO achieves significant improvements in both
cost efficiency and system stability compared to the state-of-the-art.

</details>


### [3] [The LLM as a Network Operator: A Vision for Generative AI in the 6G Radio Access Network](https://arxiv.org/abs/2509.10478)
*Oluwaseyi Giwa,Michael Adewole,Tobi Awodumila,Pelumi Aderinto*

Main category: cs.NI

TL;DR: 提出了LLM-RAN Operator概念，将大语言模型嵌入RAN控制循环，将高层人类意图转化为最优网络操作，为AI原生RAN控制提供形式化框架和理论保证


<details>
  <summary>Details</summary>
Motivation: 传统自动化无法应对未来AI原生NextG RAN（6G及以后）的极端复杂性管理挑战，需要新的解决方案

Method: 构建基于O-RAN标准的LLM-RAN Operator框架，在非实时RIC中进行LLM驱动的战略指导，在近实时RIC中进行反应式执行，包括策略表达性命题和收敛性定理

Result: 提供了分析工具来推理AI原生RAN控制的可行性和稳定性，识别了安全性、实时性能和物理世界接地等关键研究挑战

Conclusion: 弥合了AI理论与无线系统工程之间的差距，支持AI4NextG愿景，开发知识化、意图驱动的无线网络，将生成式AI集成到RAN核心

Abstract: The management of future AI-native Next-Generation (NextG) Radio Access
Networks (RANs), including 6G and beyond, presents a challenge of immense
complexity that exceeds the capabilities of traditional automation. In
response, we introduce the concept of the LLM-RAN Operator. In this paradigm, a
Large Language Model (LLM) is embedded into the RAN control loop to translate
high-level human intents into optimal network actions. Unlike prior empirical
studies, we present a formal framework for an LLM-RAN operator that builds on
earlier work by making guarantees checkable through an adapter aligned with the
Open RAN (O-RAN) standard, separating strategic LLM-driven guidance in the
Non-Real-Time (RT) RAN intelligent controller (RIC) from reactive execution in
the Near-RT RIC, including a proposition on policy expressiveness and a theorem
on convergence to stable fixed points. By framing the problem with mathematical
rigor, our work provides the analytical tools to reason about the feasibility
and stability of AI-native RAN control. It identifies critical research
challenges in safety, real-time performance, and physical-world grounding. This
paper aims to bridge the gap between AI theory and wireless systems engineering
in the NextG era, aligning with the AI4NextG vision to develop knowledgeable,
intent-driven wireless networks that integrate generative AI into the heart of
the RAN.

</details>


### [4] [Exploring Busy Period for Worst-Case Deadline Failure Probability Analysis](https://arxiv.org/abs/2509.10479)
*Junyi Liu,Xu Jiang,Yuanzhen Mu,Wang Yi,Nan Guan*

Main category: cs.NI

TL;DR: 本文针对概率实时调度分析中仅考虑临界时刻不安全的问题，提出了一种考虑不同繁忙周期起始点的最坏情况截止期限失败概率(WCDFP)分析方法


<details>
  <summary>Details</summary>
Motivation: 传统确定性实时调度分析中只需考虑临界时刻开始的繁忙周期，但概率实时调度分析中仅考虑临界时刻并不安全，需要填补这一研究空白

Method: 系统分析不同繁忙周期起始点的截止期限错过概率，提出最坏情况截止期限失败概率(WCDFP)方法用于概率固定优先级抢占式调度

Result: 实验结果表明，所提方法相比现有最先进方法有显著改进

Conclusion: 在概率实时调度分析中必须考虑多个繁忙周期起始点，WCDFP方法能够提供更安全准确的分析

Abstract: Busy period is a fundamental concept in classical deterministic real-time
scheduling analysis. In this deterministic context, only one busy period -
which starts at the critical instant - needs to be considered, which identifies
the worst-case scenario and thus paves the way for the development of efficient
and safe analysis techniques. However, a recent work has revealed that, in the
context of \textit{probabilistic} real-time scheduling analysis, only
considering critical instant is not safe. In this paper, we address this gap by
systematically analyzing deadline miss probabilities across varying busy period
starting points. We propose a novel method of Worst-Case Deadline Failure
Probability (WCDFP) for probabilistic fixed-priority preemptive scheduling.
Experimental results demonstrate significant improvements over state-of-the-art
methods achieved by our proposed method.

</details>


### [5] [Synergetic Empowerment: Wireless Communications Meets Embodied Intelligence](https://arxiv.org/abs/2509.10481)
*Hongtao Liang,Yihe Diao,YuHang Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.NI

TL;DR: 无线通信与体现智能的协同发展，将无线通信从简单工具转化为集体智能的数字神经系统，同时将定向代理升级为具有出现性能力的超组织体


<details>
  <summary>Details</summary>
Motivation: 无线通信正在进入代理时代，大规模具有体现智能的代理不仅是用户而是积极参与者，完美结合无线通信与体现智能可实现协同增能

Method: 通过感知-认知-执行（PCE）循环视角深入分析体现智能与无线通信如何相互利益，演化为一种共同进化过程

Result: 揭示了PCE每个阶段既对网络容量构成挑战，同时为系统全局优化创造前所未有的机遇，形成基础性的二元性

Conclusion: 识别了关键的开放问题和未来研究方向，为无线通信与体现智能协同发展提供了理论基础和实践指南

Abstract: Wireless communication is evolving into an agent era, where large-scale
agents with inherent embodied intelligence are not just users but active
participants. The perfect combination of wireless communication and embodied
intelligence can achieve a synergetic empowerment and greatly facilitate the
development of agent communication. An overview of this synergetic empowerment
is presented, framing it as a co-evolutionary process that transforms wireless
communication from a simple utility into the digital nervous system of a
collective intelligence, while simultaneously elevating isolated agents into a
unified superorganism with emergent capabilities far exceeding individual
contributions. Moreover, we elaborate how embodied intelligence and wireless
communication mutually benefit each other through the lens of the
perception-cognition-execution (PCE) loop, revealing a fundamental duality
where each PCE stage both challenges network capacity and creates unprecedented
opportunities for system-wide optimization. Furthermore, critical open issues
and future research directions are identified.

</details>


### [6] [SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2509.10486)
*Pengcheng Luo,Yunyang Zhao,Bowen Zhang,Genke Yang,Boon-Hee Soong,Chau Yuen*

Main category: cs.NI

TL;DR: 提出了SABR训练框架，结合行为克隆预训练和强化学习微调，解决现有ABR方法在分布外网络条件下泛化能力差的问题，并在新基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型自适应码率控制方法在训练时依赖有限的网络轨迹数据，忽略了真实网络条件的广泛分布特性，导致在分布外场景中泛化能力较差。

Method: 提出SABR训练框架，采用行为克隆(BC)预训练结合强化学习(RL)微调的方法，并建立了ABRBench-3G和ABRBench-4G+基准测试集，提供广泛覆盖的训练轨迹和专门的OOD测试集。

Result: 实验结果表明，SABR在提出的基准测试中相比Pensieve、Comyco和NetLLM等方法取得了最佳的平均排名，表明其能够在广泛分布下实现更稳定的学习，并提高对未见网络条件的泛化能力。

Conclusion: SABR框架通过结合BC预训练和RL微调，有效解决了ABR方法在OOD场景中的泛化问题，为视频服务的QoE优化提供了更鲁棒的解决方案。

Abstract: With the advent of 5G, the internet has entered a new video-centric era. From
short-video platforms like TikTok to long-video platforms like Bilibili, online
video services are reshaping user consumption habits. Adaptive Bitrate (ABR)
control is widely recognized as a critical factor influencing Quality of
Experience (QoE). Recent learning-based ABR methods have attracted increasing
attention. However, most of them rely on limited network trace sets during
training and overlook the wide-distribution characteristics of real-world
network conditions, resulting in poor generalization in out-of-distribution
(OOD) scenarios. To address this limitation, we propose SABR, a training
framework that combines behavior cloning (BC) pretraining with reinforcement
learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and
ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD
test sets for assessing robustness to unseen network conditions. Experimental
results demonstrate that SABR achieves the best average rank compared with
Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results
indicate that SABR enables more stable learning across wide distributions and
improves generalization to unseen network conditions.

</details>


### [7] [Online Learning Based Efficient Resource Allocation for LoRaWAN Network](https://arxiv.org/abs/2509.10493)
*Ruiqi Wang,Jing Ren,Tongyu Song,Wenjun Li,Xiong Wang,Sheng Wang,Shizhong Xu*

Main category: cs.NI

TL;DR: 这篇论文提出了D-LoRa和CD-LoRa两种在线学习资源分配框架，通过智能地平衡包交付率和能源效率来优化LoRaWAN网络性能。实验结果显示这些方法在实际部署中超过了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRaWAN资源分配方法存在两个主要问题：要么只关注单一指标，要么缺乏对动态频道环境的适应能力，导致性能较差。需要一种能够同时优化包交付率和能源效率的动态资源分配方案。

Method: 提出了两种在线学习框架：
1. D-LoRa：全分布式框架，将问题模型为组合多臂老虎机问题，通过分解联合参数选择和使用专门的分离奖励函数来降低学习复杂度
2. CD-LoRa：混合框架，集成轻量级的中心化初始化阶段，进行一次性准最优频道分配和动作空间剪枝，以加速后续的分布式学习

Result: 通过模拟和实际地域实验验证：
- D-LoRa在非稳态环境中表现优异
- CD-LoRa在稳态条件下实现最快收敛
- 在物理部署中，方法超过现有最优基线，包交付率提高10.8%，能源效率提高26.1%

Conclusion: 这两种在线学习资源分配框架通过智能地平衡PDR和EE持续交易，为LoRaWAN网络提供了可扩展和高效的解决方案。D-LoRa适用于动态环境，CD-LoRa适用于快速收敛的场景，实验结果证明了其在实际部署中的有效性。

Abstract: The deployment of large-scale LoRaWAN networks requires jointly optimizing
conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)
by dynamically allocating transmission parameters, including Carrier Frequency,
Spreading Factor, and Transmission Power. Existing methods often oversimplify
this challenge, focusing on a single metric or lacking the adaptability needed
for dynamic channel environments, leading to suboptimal performance. To address
this, we propose two online learning-based resource allocation frameworks that
intelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,
is a fully distributed framework that models the problem as a Combinatorial
Multi-Armed Bandit. By decomposing the joint parameter selection and employing
specialized, disaggregated reward functions, D-LoRa dramatically reduces
learning complexity and enables nodes to autonomously adapt to network
dynamics. To further enhance performance in LoRaWAN networks, we introduce
CD-LoRa, a hybrid framework that integrates a lightweight, centralized
initialization phase to perform a one-time, quasi-optimal channel assignment
and action space pruning, thereby accelerating subsequent distributed learning.
Extensive simulations and real-world field experiments demonstrate the
superiority of our frameworks, showing that D-LoRa excels in non-stationary
environments while CD-LoRa achieves the fastest convergence in stationary
conditions. In physical deployments, our methods outperform state-of-the-art
baselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their
practical effectiveness for scalable and efficient LoRaWAN networks.

</details>


### [8] [Towards Scalable O-RAN Resource Management: Graph-Augmented Proximal Policy Optimization](https://arxiv.org/abs/2509.10499)
*Duc-Thinh Ngo,Kandaraj Piamrat,Ons Aouedi,Thomas Hassan,Philippe Raipin-Parvédy*

Main category: cs.NI

TL;DR: 该文章提出了一种新的图增强近端策略优化(GPPO)框架，通过结合图神经网络和动作掩码技术，解决O-RAN中功能分割选择和虚拟单元置位的聚合优化问题，在大规模场景中实现了更低部署成本和更高的奖励。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构虽然提供了灵活性和可扩展性，但也带来了资源管理的重大挑战，需要在动态需求和复杂拓扑结构下进行功能分割选择和虚拟单元置位的聚合优化。现有方案常常单独处理这些方面或缺乏在大规模实际场景中的可扩展性。

Method: 提出了图增强近端策略优化(GPPO)框架，利用图神经网络(GNNs)进行拓扑感知特征提取，并集成动作掩码技术来高效导航组合决策空间。该方法聚合优化功能分割和置位决策，抓取O-RAN资源分配的完整复杂性。

Result: 在小规模和大规模O-RAN场景上进行的广泛实验表明，GPPO持续超越了最先进的基线方法，实现了至18%更低的部署成本和25%更高的通用化测试奖励，同时保持了完美的可靠性。

Conclusion: 这些结果强调了GPPO在实际O-RAN部署中的有效性和可扩展性，为解决O-RAN资源管理的复杂挑战提供了一种高效的方案。

Abstract: Open Radio Access Network (O-RAN) architectures enable flexible, scalable,
and cost-efficient mobile networks by disaggregating and virtualizing baseband
functions. However, this flexibility introduces significant challenges for
resource management, requiring joint optimization of functional split selection
and virtualized unit placement under dynamic demands and complex topologies.
Existing solutions often address these aspects separately or lack scalability
in large and real-world scenarios. In this work, we propose a novel
Graph-Augmented Proximal Policy Optimization (GPPO) framework that leverages
Graph Neural Networks (GNNs) for topology-aware feature extraction and
integrates action masking to efficiently navigate the combinatorial decision
space. Our approach jointly optimizes functional split and placement decisions,
capturing the full complexity of O-RAN resource allocation. Extensive
experiments on both small-and large-scale O-RAN scenarios demonstrate that GPPO
consistently outperforms state-of-the-art baselines, achieving up to 18% lower
deployment cost and 25% higher reward in generalization tests, while
maintaining perfect reliability. These results highlight the effectiveness and
scalability of GPPO for practical O-RAN deployments.

</details>


### [9] [An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms](https://arxiv.org/abs/2509.10507)
*Vadim Allayev,Mahbubur Rahman*

Main category: cs.NI

TL;DR: 提出一种异构、去中心化的IoIT对等网络系统，通过联邦学习、元启发和多目标优化来实现可靠性、能源效率和延迟的平衡


<details>
  <summary>Details</summary>
Motivation: 解决中心化IoIT系统存在的性能瓶颈和安全问题，应对嵌入式设备计算资源、能量供应和存储限制的挑战

Method: 构建异构去中心化对等网络，采用联邦学习进行分布式训练，使用元启发算法优化任务分配和路由路径，通过多目标优化平衡矛盾的性能目标

Result: 设计了一种能够在去中心化环境下实现可靠性、能源效率和延迟优化的IoIT系统模型

Conclusion: 异构去中心化网络结合联邦学习和元启发优化能够有效解决IoIT部署中的资源限制问题，为嵌入式AI应用提供更可靠的解决方案

Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the
utility of Internet of Things (IoT) devices with the innovation of embedded AI
algorithms. However, it does not come without challenges, and struggles
regarding available computing resources, energy supply, and storage
limitations. In particular, many impediments to IoIT are linked to the
energy-efficient deployment of machine learning (ML)/deep learning (DL) models
in embedded devices. Research has been conducted to design energy-efficient
IoIT platforms, but these papers often focus on centralized systems, in which
some central entity processes all the data and coordinates actions. This can be
problematic, e.g., serve as bottleneck or lead to security concerns. In a
decentralized system, nodes/devices would self-organize and make their own
decisions. Therefore, to address such issues, we propose a heterogeneous,
decentralized sensing and monitoring IoIT peer-to-peer mesh network system
model. Nodes in the network will coordinate towards several optimization goals:
reliability, energy efficiency, and latency. The system employs federated
learning to train nodes in a distributed manner, metaheuristics to optimize
task allocation and routing paths, and multi-objective optimization to balance
conflicting performance goals.

</details>


### [10] [CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi Head Attention for Heterogeneous Vehicular Networks](https://arxiv.org/abs/2509.10508)
*Aathira G Menon,Prabu Krishnan,Shyam Lal*

Main category: cs.NI

TL;DR: CAR-BRAINet：一种基于深度学习的轻量级波束预测解决方案，专门针对异构车载网络设计，在复杂实时驾驶场景中表现出色，比现有方法频谱效率提升17.94%。


<details>
  <summary>Details</summary>
Motivation: 异构车载网络(HetVNets)通过整合多种通信技术来满足5G/B5G车载网络的多样化连接需求，但在高移动性真实环境中保持稳定连接仍具挑战性。现有波束预测研究大多在理想化场景下进行，缺乏专门针对HetVNets的解决方案。

Method: 提出CAR-BRAINet模型，结合卷积神经网络和强大的多头注意力机制。构建了包含城市、乡村和高速公路三种高质量动态数据集，模拟真实驾驶场景的复杂性，包括3GPP-C-V2X和IEEE 802.11BD MAC协议、多普勒效应、不同距离和SNR水平等因素。

Result: CAR-BRAINet在所有车载场景中都表现有效，实现了精确的波束预测，波束开销最小，频谱效率比现有方法稳定提升17.9422%。

Conclusion: 该研究证明了CAR-BRAINet在复杂异构车载网络中的有效性，无需依赖移动用户的位置角度和天线尺寸，从而减少了冗余的传感器延迟，提供了有前景的性能表现。

Abstract: Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking
different communication technologies such as sub-6GHz, mm-wave and DSRC to meet
diverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address
the humongous user demands-but maintaining a steady connection in a highly
mobile, real-world conditions remain a challenge. Though there has been ample
of studies on beam prediction models a dedicated solution for HetVNets is
sparsely explored. Hence, it is the need of the hour to develop a reliable beam
prediction solution, specifically for HetVNets. This paper introduces a
lightweight deep learning-based solution termed-"CAR-BRAINet" which consists of
convolutional neural networks with a powerful multi-head attention (MHA)
mechanism. Existing literature on beam prediction is largely studied under a
limited, idealised vehicular scenario, often overlooking the real-time
complexities and intricacies of vehicular networks. Therefore, this study aims
to mimic the complexities of a real-time driving scenario by incorporating key
factors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the
effect of Doppler shifts under high velocity and varying distance and SNR
levels into three high-quality dynamic datasets pertaining to urban, rural and
highway vehicular networks. CAR-BRAINet performs effectively across all the
vehicular scenarios, demonstrating precise beam prediction with minimal beam
overhead and a steady improvement of 17.9422% on the spectral efficiency over
the existing methods. Thus, this study justifies the effectiveness of
CAR-BRAINet in complex HetVNets, offering promising performance without relying
on the location angle and antenna dimensions of the mobile users, and thereby
reducing the redundant sensor-latency.

</details>


### [11] [Pair-Bid Auction Model for Optimized Network Slicing in 5G RAN](https://arxiv.org/abs/2509.10533)
*Mengyao Li,Sebastian Troia,Yingqian Zhang,Guido Maier*

Main category: cs.NI

TL;DR: 本文提出了一种基于双层组合拍卖的5G网络切片资源分配模型，通过pair-bid机制和VCG定价优化资源分配和收益，相比基线方法收益提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 5G网络切片技术需要解决多运营商共享物理基础设施时的公平分配和能效优化问题，特别是面对动态用户请求时的复杂资源分配挑战。

Method: 采用双层层次化组合拍卖模型：上层是MNO向MVNO拍卖切片，下层是MVNO向终端用户分配资源。使用pair-bid机制增强竞争效率，VCG定价确保真实竞价。

Result: 仿真验证显示该模型在资源分配和财务性能方面表现有效，相比基线方法实现了12.5%的收益提升。

Conclusion: 提出的双层拍卖模型能够有效解决5G网络切片中的资源分配问题，在保证公平性和能效的同时显著提升运营收益。

Abstract: Network slicing is a key 5G technology that enables multiple virtual networks
to share physical infrastructure, optimizing flexibility and resource
allocation. This involves Mobile Network Operators (MNO), Mobile Virtual
Network Operators (MVNOs), and end users, where MNO leases network slices to
MVNOs, and then provides customized services. This work considers end-to-end
network slicing with a focus on fair sharing and financial-related power
efficiency, modeled as a two level hierarchical combinatorial auction. At the
upper level, an MNO auctions slices to competing MVNOs, while at the lower
level, MVNOs allocate resources to end users through their own auctions.
Dynamic user requests add complexity to the process. Our model optimizes
resource allocation and revenue generation using a pair-bid mechanism and
Vickrey-Clarke-Groves (VCG) pricing. The pair-bid approach enhances competition
and efficiency, while VCG ensures truthful bidding based on marginal system
impact. Simulations validate the model's effectiveness in resource distribution
and financial performance, showing a 12.5% revenue improvement over the
baseline.

</details>


### [12] [ASL360: AI-Enabled Adaptive Streaming of Layered 360° Video over UAV-assisted Wireless Networks](https://arxiv.org/abs/2509.10544)
*Alireza Mohammadhosseini,Jacob Chakareski,Nicholas Mastronarde*

Main category: cs.NI

TL;DR: ASL360是一种基于深度强化学习的自适应调度器，用于5G无线网络中移动VR用户的360度视频流媒体，通过分层编码和动态调整机制显著提升用户体验质量


<details>
  <summary>Details</summary>
Motivation: 解决下一代无线网络中移动VR用户360度视频流媒体的QoE优化问题，特别是在动态和挑战性的网络环境中提供沉浸式视频体验

Method: 采用约束马尔可夫决策过程(CMDP)建模调度决策，使用PPO策略梯度方法寻找最优策略，实现分层视频编码和动态成本组件调整机制

Result: 相比基线方法，平均视频质量提升约2dB，平均重新缓冲时间降低80%，视频质量变化降低57%

Conclusion: 分层和自适应方法在动态网络环境中能有效提升沉浸式视频流媒体应用的用户体验质量

Abstract: We propose ASL360, an adaptive deep reinforcement learning-based scheduler
for on-demand 360{\deg} video streaming to mobile VR users in next generation
wireless networks. We aim to maximize the overall Quality of Experience (QoE)
of the users served over a UAV-assisted 5G wireless network. Our system model
comprises a macro base station (MBS) and a UAV-mounted base station which both
deploy mm-Wave transmission to the users. The 360{\deg} video is encoded into
dependent layers and segmented tiles, allowing a user to schedule downloads of
each layer's segments. Furthermore, each user utilizes multiple buffers to
store the corresponding video layer's segments. We model the scheduling
decision as a Constrained Markov Decision Process (CMDP), where the agent
selects Base or Enhancement layers to maximize the QoE and use a policy
gradient-based method (PPO) to find the optimal policy. Additionally, we
implement a dynamic adjustment mechanism for cost components, allowing the
system to adaptively balance and prioritize the video quality, buffer
occupancy, and quality change based on real-time network and streaming session
conditions. We demonstrate that ASL360 significantly improves the QoE,
achieving approximately 2 dB higher average video quality, 80% lower average
rebuffering time, and 57% lower video quality variation, relative to
competitive baseline methods. Our results show the effectiveness of our layered
and adaptive approach in enhancing the QoE in immersive videostreaming
applications, particularly in dynamic and challenging network environments.

</details>


### [13] [Empowering AI-Native 6G Wireless Networks with Quantum Federated Learning](https://arxiv.org/abs/2509.10559)
*Shaba Shaon,Md Raihan Uddin,Dinh C. Nguyen,Seyyedali Hosseinalipour,Dusit Niyato,Octavia A. Dobre*

Main category: cs.NI

TL;DR: 本文探讨了将量子联邦学习(QFL)集成到AI原生6G网络中，通过量子计算、通信和密码学技术解决传统联邦学习在异构无线网络中面临的设备计算能力有限、连接不可靠、通信间歇性以及安全隐私漏洞等问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在异构动态无线网络中面临设备计算能力有限、连接不可靠、通信间歇性和安全隐私漏洞等瓶颈，需要新的解决方案来支持AI原生6G网络的实时、个性化和隐私保护的边缘智能需求。

Method: 提出量子联邦学习(QFL)框架，在联邦学习工作流中集成量子计算、量子通信和量子密码学技术，使用量子近似优化算法等量子技术来提升性能。

Result: 案例研究表明，采用量子近似优化算法的QFL框架在模型收敛方面优于传统方法，在边缘智能、网络优化以及安全隐私三个关键维度上展现出新的能力。

Conclusion: QFL为AI原生6G网络提供了变革性范式，但实际部署仍面临量子态脆弱性、与经典协议不兼容以及硬件限制等挑战，需要进一步研究以实现可扩展的实际应用。

Abstract: AI-native 6G networks are envisioned to tightly embed artificial intelligence
(AI) into the wireless ecosystem, enabling real-time, personalized, and
privacy-preserving intelligence at the edge. A foundational pillar of this
vision is federated learning (FL), which allows distributed model training
across devices without sharing raw data. However, implementing classical FL
methods faces several bottlenecks in heterogeneous dynamic wireless networks,
including limited device compute capacity, unreliable connectivity,
intermittent communications, and vulnerability to model security and data
privacy breaches. This article investigates the integration of quantum
federated learning (QFL) into AI-native 6G networks, forming a transformative
paradigm capable of overcoming these challenges. By leveraging quantum
techniques across computing, communication, and cryptography within FL
workflows, QFL offers new capabilities along three key dimensions: (i) edge
intelligence, (ii) network optimization, and (iii) security and privacy, which
are studied in this work. We further present a case study demonstrating that a
QFL framework employing the quantum approximate optimization algorithm
outperforms classical methods in model convergence. We conclude the paper by
identifying practical challenges facing QFL deployment, such as quantum state
fragility, incompatibility with classical protocols, and hardware constraints,
and then outline key research directions toward its scalable real-world
adoption.

</details>


### [14] [gNB-based Local Breakout for URLLC in industrial 5G](https://arxiv.org/abs/2509.10617)
*Rajendra Paudyal,Rajendra Upadhyay,Al Nahian Bin Emran,Duminda Wijesekera*

Main category: cs.NI

TL;DR: 通过在gNB本地实现多播数据的本地突四，避免核心网络路径，将工业URLLC应用的续时从6.5-11.5ms降低到1.5-4.0ms，实现了子5ms的低延迟要求。


<details>
  <summary>Details</summary>
Motivation: 工业URLLC工作负荷如协调机器人、自动导航车辆等需要5ms以下延迟和99.999%可靠性。标准5G多播/广播服务中，内部小区组网络交通仍需经过核心网，导致不必要的包延迟。

Method: 提出在gNB本地实现多播突四技术，将符合条件的上行流转换为下行点对多点承载者，同时保持5G核心网的授权、成员管理和策略控制。保持了3GPP安全标准和规范符合性。

Result: 模拟结果显示，移除回传/UPF/AF网段后，端到端延迟从6.5-11.5ms降低到1.5-4.0ms，平均延迟低于2ms，且在不同组组粒度下保持约10ms的稳定巡环间隔。

Conclusion: 该方法为专用5G网络中定制化的内部小区组数据传播提供了一条符合标准的实用路径，是实现工业URLLC低延迟要求的有效解决方案。

Abstract: Industrial URLLC workloads-coordinated robotics, automated guided vehicles,
machine-vision collaboration require sub-5 ms latency and five-nines
reliability. In standardized 5G Multicast/Broadcast Services, intra-cell group
traffic remains anchored in the core using MB-SMF/MB-UPF, and the Application
Function. This incurs a core network path and packet delay that is avoidable
when data transmitters and receivers share a cell. We propose a gNB-local
multicast breakout that pivots eligible uplink flows to a downlink
point-to-multipoint bearer within the gNB, while maintaining authorization,
membership, and policy in the 5G core. The design specifies an eligibility
policy, configured-grant uplink. 3GPP security and compliance are preserved via
unchanged control-plane anchors. A latency budget and simulation indicate that
removing the backhaul/UPF/AF segment reduces end-to-end latency from
approximate 6.5-11.5 ms (anchored to the core) to 1.5-4.0 ms (local breakout),
producing sub-2 ms averages and a stable gap approximate 10 ms between group
sizes. The approach offers a practical, standards-aligned path to deterministic
intra-cell group dissemination in private 5G. We outline multi-cell and
prototype validation as future work.

</details>


### [15] [Deep Learning based Moving Target Defence for Federated Learning against Poisoning Attack in MEC Systems with a 6G Wireless Model](https://arxiv.org/abs/2509.10914)
*Somayeh Kianpisheh,Tarik Taleb,Jari Iinatti,JaeSeung Song*

Main category: cs.NI

TL;DR: 该论文提出了一种结合流量分析和移动目标防御的联邦学习安全框架，通过设备级流量分析预测参与者可靠性，并利用深度强化学习优化拓扑变异策略来防御模型投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中模型投毒攻击威胁严重，传统基于模型的异常检测方法在处理异构模型和复杂攻击时效率不足，需要开发更有效的防御机制。

Method: 采用循环神经网络进行设备流量时间序列分析，结合6G无线模型，设计移动目标防御策略，使用深度强化学习动态优化拓扑变异，根据设备拜占庭状态和通信能力自适应调整参与者。

Result: 在DDoS攻击检测应用中，面对设备级僵尸网络攻击，该方法实现了良好的恶意模型排除效果，并在识别时间和准确率方面均有显著提升。

Conclusion: 通过设备级流量分析和自适应拓扑变异策略，能够有效增强联邦学习对模型投毒攻击的防御能力，提高系统安全性和学习效率。

Abstract: Collaboration opportunities for devices are facilitated with Federated
Learning (FL). Edge computing facilitates aggregation at edge and reduces
latency. To deal with model poisoning attacks, model-based outlier detection
mechanisms may not operate efficiently with hetereogenous models or in
recognition of complex attacks. This paper fosters the defense line against
model poisoning attack by exploiting device-level traffic analysis to
anticipate the reliability of participants. FL is empowered with a topology
mutation strategy, as a Moving Target Defence (MTD) strategy to dynamically
change the participants in learning. Based on the adoption of recurrent neural
networks for time-series analysis of traffic and a 6G wireless model,
optimization framework for MTD strategy is given. A deep reinforcement
mechanism is provided to optimize topology mutation in adaption with the
anticipated Byzantine status of devices and the communication channel
capabilities at devices. For a DDoS attack detection application and under
Botnet attack at devices level, results illustrate acceptable malicious models
exclusion and improvement in recognition time and accuracy.

</details>


### [16] [RU Energy Modeling for O-RAN in ns3-oran](https://arxiv.org/abs/2509.10978)
*Abdul Wadud,Nima Afraz*

Main category: cs.NI

TL;DR: 本文提出了首个支持xApp控制的O-RAN无线电单元(RU)功耗模型，基于ns3-oran模拟器，重点关注硬件级参数化建模和能耗效率评估。


<details>
  <summary>Details</summary>
Motivation: 现有框架如EARTH或VBS-DRX在O-RAN环境中存在局限性，需要开发一个RU中心化的、支持xApp控制的灵活功耗模型来支持能效管理策略。

Method: 使用ns3-oran模拟器构建RU中心化功耗模型，参数化硬件级特征（收发器数量、功放效率、毫米波开销、待机行为等），并与ns-3能量跟踪系统集成。

Result: 模型能够准确表示真实的非线性功率缩放特性，识别RU有效行为的理想工作点，为xApp驱动的能量管理策略提供支持。

Conclusion: 该框架为O-RAN部署中的能量效率评估提供了首个支持xApp控制的RU功耗建模解决方案，有助于开发智能能耗管理策略。

Abstract: This paper presents a detailed and flexible power consumption model for Radio
Units (RUs) in O-RAN using the ns3-oran simulator. This is the first ns3-oran
model supporting xApp control to perform the RU power modeling. In contrast to
existing frameworks like EARTH or VBS-DRX, the proposed framework is RU-centric
and is parameterized by hardware-level features, such as the number of
transceivers, the efficiency of the power amplifier, mmWave overheads, and
standby behavior. It enables simulation-driven assessment of energy efficiency
at various transmit power levels and seamlessly integrates with ns-3's energy
tracking system. To help upcoming xApp-driven energy management strategies in
O-RAN installations, numerical research validates the model's capacity to
represent realistic nonlinear power scaling. It identifies ideal operating
points for effective RU behavior.

</details>


### [17] [Multi-Modal Sensing Aided mmWave Beamforming for V2V Communications with Transformers](https://arxiv.org/abs/2509.11112)
*Muhammad Baqer Mollah,Honggang Wang,Hua Fang*

Main category: cs.NI

TL;DR: 提出多模态传感融合学习框架，通过视觉和GPS数据预测最佳波束，减少毫米波车联网中的波束训练开销


<details>
  <summary>Details</summary>
Motivation: 传统波束成形技术在动态车载环境中存在高训练开销问题，需要交换导频信号和穷举波束测量，减少了通信可用时间

Method: 使用模态特定编码器分别提取视觉和GPS坐标特征，融合多模态特征预测top-k波束，主动建立最佳视距链路

Result: 在四个真实V2V场景中达到77.58%的top-15波束预测准确率，平均功率损失仅2.32dB，搜索空间开销减少76.56%

Conclusion: 多模态传感融合框架能有效降低波束训练开销，提高毫米波车联网通信效率，优于单模态方法

Abstract: Beamforming techniques are utilized in millimeter wave (mmWave) communication
to address the inherent path loss limitation, thereby establishing and
maintaining reliable connections. However, adopting standard defined
beamforming approach in highly dynamic vehicular environments often incurs high
beam training overheads and reduces the available airtime for communications,
which is mainly due to exchanging pilot signals and exhaustive beam
measurements. To this end, we present a multi-modal sensing and fusion learning
framework as a potential alternative solution to reduce such overheads. In this
framework, we first extract the features individually from the visual and GPS
coordinates sensing modalities by modality specific encoders, and subsequently
fuse the multimodal features to obtain predicted top-k beams so that the best
line-of-sight links can be proactively established. To show the
generalizability of the proposed framework, we perform a comprehensive
experiment in four different vehicle-to-vehicle (V2V) scenarios from real-world
multi-modal sensing and communication dataset. From the experiment, we observe
that the proposed framework achieves up to 77.58% accuracy on predicting top-15
beams correctly, outperforms single modalities, incurs roughly as low as 2.32
dB average power loss, and considerably reduces the beam searching space
overheads by 76.56% for top-15 beams with respect to standard defined approach.

</details>


### [18] [On the Feasibility of Inter-Flow Service Degradation Detection](https://arxiv.org/abs/2509.11140)
*Balint Bicski,Adrian Pekar*

Main category: cs.NI

TL;DR: 硬件加速导致网络监控盲区，本文提出互流相关框架，利用可观浏流作为伴随不可观浏流的监测代理，并证明了该方法的可行性和挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现代网络中硬件加速导致的监控盲区问题，这些盲区影响了对服务降级的实时检测能力。

Method: 提出和形式化了一种新的互流相关框架，通过统计分析互流关联特征，并使用标准机器学习模型进行评估。

Result: 统计分析显示互流相关信号常常先于目标流的降级，验证了及时检测的潜力。机器学习模型达到高分类准确率，但特征重要性分析显示模型主要依赖简单的流内特征。

Conclusion: 本研究不仅提供了互流问题的基础分析，还明确指出需要结构感知模型来利用复杂的上下文信息，为未来研究指明了方向。

Abstract: Hardware acceleration in modern networks creates monitoring blind spots by
offloading flows to a non-observable state, hindering real-time service
degradation (SD) detection. To address this, we propose and formalize a novel
inter-flow correlation framework, built on the hypothesis that observable flows
can act as environmental sensors for concurrent, non-observable flows. We
conduct a comprehensive statistical analysis of this inter-flow landscape,
revealing a fundamental trade-off: while the potential for correlation is vast,
the most explicit signals (i.e., co-occurring SD events) are sparse and rarely
perfectly align. Critically, however, our analysis shows these signals
frequently precede degradation in the target flow, validating the potential for
timely detection. We then evaluate the framework using a standard machine
learning model. While the model achieves high classification accuracy, a
feature-importance analysis reveals it relies primarily on simpler intra-flow
features. This key finding demonstrates that harnessing the complex contextual
information requires more than simple models. Our work thus provides not only a
foundational analysis of the inter-flow problem but also a clear outline for
future research into the structure-aware models needed to solve it.

</details>


### [19] [UDFS: Lightweight Representation-Driven Robust Network Traffic Classification](https://arxiv.org/abs/2509.11157)
*Youquan Xian,Xueying Zeng,Mei Huang,Aoxiang Zhou,Xiaoyu Cui,Peng Liu,Lei Cui*

Main category: cs.NI

TL;DR: 提出UDFS表示方法，将整个流量轨迹压缩为二维序列，通过聚合上下游流量来表征每个流，在降低复杂度的同时保持高区分度，并引入自适应阈值机制提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列建模方法存在特征冗余高或计算存储开销大的问题，需要一种既能保持完整信息又降低复杂度的加密流量分析方法。

Method: UDFS表示方法将流量轨迹压缩为二维序列，通过聚合上下游流量表征每个流；采用自适应阈值机制动态调整训练权重和拒绝边界。

Result: 在粗粒度和细粒度数据集上均取得优越的分类性能和鲁棒性，在概念漂移和开放世界场景下表现良好。

Conclusion: UDFS方法有效解决了现有加密流量分析方法的局限性，在保持高区分度的同时显著降低了计算和存储开销，具有实际应用价值。

Abstract: In recent years, sequence features such as packet length have received
considerable attention due to their central role in encrypted traffic analysis.
Existing sequence modeling approaches can be broadly categorized into
flow-level and trace-level methods: the former suffer from high feature
redundancy, limiting their discriminative power, whereas the latter preserve
complete information but incur substantial computational and storage overhead.
To address these limitations, we propose the \textbf{U}p-\textbf{D}own
\textbf{F}low \textbf{S}equence (\textbf{UDFS}) representation, which
compresses an entire trace into a two-dimensional sequence and characterizes
each flow by the aggregate of its upstream and downstream traffic, reducing
complexity while maintaining high discriminability. Furthermore, to address the
challenge of class-specific discriminability differences, we propose an
adaptive threshold mechanism that dynamically adjusts training weights and
rejection boundaries, enhancing the model's classification performance.
Experimental results demonstrate that the proposed method achieves superior
classification performance and robustness on both coarse-grained and
fine-grained datasets, as well as under concept drift and open-world scenarios.
Code and Dataset are available at https://github.com/kid1999/UDFS.

</details>


### [20] [Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation Intelligent Delay-Tolerant Networks](https://arxiv.org/abs/2509.11239)
*Zhekun Huang,Milena Radenkovic*

Main category: cs.NI

TL;DR: 通过机器学习算法在Spray and Wait框架中集成节点评估，提出MLPBasedSprayRouter路由器，在应急通信场景下显著提高了传递比率和降低延迟


<details>
  <summary>Details</summary>
Motivation: 解决延迟容息网络中静态复制策略无法适应急急通信场景下的动态环境问题，提高消息传播效率

Method: 从模拟日志中提取核心特征，训练MLP、SVM和RF分类器预测节点作为中续的适宜性，通过Flask RESTful API实现实时预测，并集成到MLPBasedSprayRouter中

Result: 在实际应急移动场景下，该框架显著提高了传递比率和降低平均延迟，其中MLP分类器表现最佳，在准确性、适应性和推理速度方面都超过SVM和RF

Conclusion: 证明了机器学习集成到DTN路由中的新颖性和实用性，为智慧城市、灾后恢复等动态环境中构建弹性智能通信系统探索了新路径

Abstract: Delay Tolerant Networks (DTNs) are critical for emergency communication in
highly dynamic and challenging scenarios characterized by intermittent
connectivity, frequent disruptions, and unpredictable node mobility. While some
protocols are widely adopted for simplicity and low overhead, their static
replication strategy lacks the ability to adaptively distinguish high-quality
relay nodes, often leading to inefficient and suboptimal message dissemination.
To address this challenge, we propose a novel intelligent routing enhancement
that integrates machine learning-based node evaluation into the Spray and Wait
framework. Several dynamic, core features are extracted from simulation logs
and are used to train multiple classifiers - Multi-Layer Perceptron (MLP),
Support Vector Machine (SVM), and Random Forest (RF) - to predict whether a
node is suitable as a relay under dynamic conditions. The trained models are
deployed via a lightweight Flask-based RESTful API, enabling real-time,
adaptive predictions. We implement the enhanced router MLPBasedSprayRouter,
which selectively forwards messages based on the predicted relay quality. A
caching mechanism is incorporated to reduce computational overhead and ensure
stable, low-latency inference. Extensive experiments under realistic emergency
mobility scenarios demonstrate that the proposed framework significantly
improves delivery ratio while reducing average latency compared to the baseline
protocols. Among all evaluated classifiers, MLP achieved the most robust
performance, consistently outperforming both SVM and RF in terms of accuracy,
adaptability, and inference speed. These results confirm the novelty and
practicality of integrating machine learning into DTN routing, paving the way
for resilient and intelligent communication systems in smart cities, disaster
recovery, and other dynamic environments.

</details>


### [21] [Energy-Aware 6G Network Design: A Survey](https://arxiv.org/abs/2509.11289)
*Rashmi Kamran,Mahesh Ganesh Bhat,Pranav Jha,Shana Moothedath,Manjesh Hanawal,Prasanna Chaporkar*

Main category: cs.NI

TL;DR: 这是一份关于6G网络能源效率与可持续性设计的综述性调研报告，分析了能源收集、AI/ML解决方案、标准化进展等关键技术，并提出了开政研究挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持新型应用和巨大用户数量，导致能源效率和可持续性问题突出，需要将可持续性作为核心设计目标。

Method: 通过综述性调研方法，详细分析了能源收集技术、能源模型参数、能源效率服务分类以及AI/ML基于的解决方案，包含例子案例和3GPP、ITU、IEEE等标准化组织的进展。

Result: 调研揭示了能源效率设计面临的挑战（如能源信息收集开销、用户同意管理），并展示了采用能源效率视角的网络决策带来的好处，为6G网络的可持续发展提供了技术支撑。

Conclusion: 论文强调6G网络必须重视能源效率设计，持续进行标准化和技术创新。最后提出了开政研究问题和挑战，以促进能源效率与网络性能的最优平衡。

Abstract: 6th Generation (6G) mobile networks are envisioned to support several new
capabilities and data-centric applications for unprecedented number of users,
potentially raising significant energy efficiency and sustainability concerns.
This brings focus on sustainability as one of the key objectives in the their
design. To move towards sustainable solution, research and standardization
community is focusing on several key issues like energy information monitoring
and exposure, use of renewable energy, and use of Artificial
Intelligence/Machine Learning (AI/ML) for improving the energy efficiency in 6G
networks. The goal is to build energy-aware solutions that takes into account
the energy information resulting in energy efficient networks. Design of
energy-aware 6G networks brings in new challenges like increased overheads in
gathering and exposing of energy related information, and the associated user
consent management. The aim of this paper is to provide a comprehensive survey
of methods used for design of energy efficient 6G networks, like energy
harvesting, energy models and parameters, classification of energy-aware
services, and AI/ML-based solutions. The survey also includes few use cases
that demonstrate the benefits of incorporating energy awareness into network
decisions. Several ongoing standardization efforts in 3GPP, ITU, and IEEE are
included to provide insights into the ongoing work and highlight the
opportunities for new contributions. We conclude this survey with open research
problems and challenges that can be explored to make energy-aware design
feasible and ensure optimality regarding performance and energy goals for 6G
networks.

</details>


### [22] [Federated Edge Learning for Predictive Maintenance in 6G Small Cell Networks](https://arxiv.org/abs/2509.11421)
*Yusuf Emir Sezgin,Mehmet Özdem,Tuğçe Bilen*

Main category: cs.NI

TL;DR: 提出基于知识定义网络架构的联邦边缘学习框架，用于6G小基站网络的预测性维护，通过本地训练和联邦平均算法实现隐私保护和通信开销降低。


<details>
  <summary>Details</summary>
Motivation: 6G网络对自主性、可靠性和可扩展性提出新需求，但敏感遥测数据传输到中央服务器存在隐私和带宽问题，需要去中心化的智能解决方案。

Method: 采用知识定义网络(KDN)三层架构，基站在本地使用SINR、抖动、延迟等遥测指标独立训练故障预测模型，使用基于阈值的多标签编码方案检测并发故障，通过Flower框架和FedAvg算法进行联邦学习。

Result: 联邦学习模型在准确性和每标签精度方面达到与集中式训练相当的性能，同时保护隐私并减少通信开销。

Conclusion: 联邦边缘学习框架为6G小基站网络提供了一种有效的预测性维护解决方案，在保持性能的同时解决了隐私和带宽限制问题。

Abstract: The rollout of 6G networks introduces unprecedented demands for autonomy,
reliability, and scalability. However, the transmission of sensitive telemetry
data to central servers raises concerns about privacy and bandwidth. To address
this, we propose a federated edge learning framework for predictive maintenance
in 6G small cell networks. The system adopts a Knowledge Defined Networking
(KDN) architecture in Data, Knowledge, and Control Planes to support
decentralized intelligence, telemetry-driven training, and coordinated policy
enforcement. In the proposed model, each base station independently trains a
failure prediction model using local telemetry metrics, including SINR, jitter,
delay, and transport block size, without sharing raw data. A threshold-based
multi-label encoding scheme enables the detection of concurrent fault
conditions. We then conduct a comparative analysis of centralized and federated
training strategies to evaluate their performance in this context. A realistic
simulation environment is implemented using the ns-3 mmWave module,
incorporating hybrid user placement and base station fault injection across
various deployment scenarios. The learning pipeline is orchestrated via the
Flower framework, and model aggregation is performed using the Federated
Averaging (FedAvg) algorithm. Experimental results demonstrate that the
federated model achieves performance comparable to centralized training in
terms of accuracy and per-label precision, while preserving privacy and
reducing communication overhead.

</details>


### [23] [Towards Dynamic Urban Scene Synthesis: The Digital Twin Descriptor Service](https://arxiv.org/abs/2509.11810)
*Ioannis Tsampras,Georgios Stergiopoulos,Tanya Politi,Spyros Denazis*

Main category: cs.NI

TL;DR: 数字双生体描述服务(DTDS)通过NGSI-LD协议整合几何资产和上下文信息，解决现有平台在集成、联盟和动态连接方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有数字双生体平台将数据流和表示分离，仅在可视化层融合，限制了模拟和进一步处理能力；几何中心的文件标准难以整合变化的上下文信息；多提供商联盟支持不足

Method: 提出数字双生体描述服务(DTDS)概念，通过NGSI-LD协议将几何资产和上下文信息的抽象引用融合到单一、可扩展的描述服务中

Result: DTDS提供了上下文数据、表示和运行时同步的动态联盟集成能力，支持异构引擎和模拟器

Conclusion: DTDS概念为现代智慧城市中的数字双生体过程提供了架构组件和描述本体论，有力解决现有平台的集成和联盟挑战

Abstract: Digital twins have been introduced as supporters to city operations, yet
existing scene-descriptor formats and digital twin platforms often lack the
integration, federation, and adaptable connectivity that urban environments
demand. Modern digital twin platforms decouple data streams and representations
into separate architectural planes, fusing them only at the visualization layer
and limiting potential for simulation or further processing of the combined
assets. At the same time, geometry-centric file standards for digital twin
description, and services built on top of them, focus primarily on explicitly
declaring geometry and additional structural or photorealistic parameters,
making integration with evolving context information a complicated process
while limiting compatibility with newer representation methods. Additionally,
multi-provider federation, critical in smart city services where multiple
stakeholders may control distinct infrastructure or representation assets, is
sparsely supported. Consequently, most pilots isolate context and
representation, fusing them per use case with ad hoc components and custom
description files or glue code, which hinders interoperability. To address
these gaps, this paper proposes a novel concept, the 'Digital Twin Descriptor
Service (DTDS)' that fuses abstracted references to geometry assets and context
information within a single, extensible descriptor service through NGSI-LD. The
proposed DTDS provides dynamic and federated integration of context data,
representations, and runtime synchronization across heterogeneous engines and
simulators. This concept paper outlines the DTDS architectural components and
description ontology that enable digital-twin processes in the modern smart
city.

</details>


### [24] [Optimization for Massive 3D-RIS Deployment: A Generative Diffusion Model-Based Approach](https://arxiv.org/abs/2509.11969)
*Kaining Wang,Bo Yang,Zhiwen Yu,Xuelin Cao,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 通过基于次繁模型的概率生成学习方法优化大规模3D可重构智能表面的部署策略，充分利用生成分布来获得最优部署方案


<details>
  <summary>Details</summary>
Motivation: 现有的RIS部署优化方法存在计算复杂度高、对环境变化适应性差以及容易求解局部最优解等挑战

Method: 将目标区域网格化，将RIS部署优化问题模型化为条件生成任务，使用次繁模型生成部署策略的概率分布，通过采样获得最优部署方案

Result: 模拟结果显示，该方法在超越比率和普适性方面都超过传统基准方法

Conclusion: 基于次繁模型的概率生成学习方法能够有效解决大规模3D RIS部署优化问题，提高了性能和适应能力

Abstract: Reconfigurable Intelligent Surfaces (RISs) transform the wireless environment
by modifying the amplitude, phase, and polarization of incoming waves,
significantly improving coverage performance. Notably, optimizing the
deployment of RISs becomes vital, but existing optimization methods face
challenges such as high computational complexity, limited adaptability to
changing environments, and a tendency to converge on local optima. In this
paper, we propose to optimize the deployment of large-scale 3D RISs using a
diffusion model based on probabilistic generative learning. We begin by
dividing the target area into fixed grids, with each grid corresponding to a
potential deployment location. Then, a multi-RIS deployment optimization
problem is formulated, which is difficult to solve directly. By treating RIS
deployment as a conditional generation task, the well-trained diffusion model
can generate the distribution of deployment strategies, and thus, the optimal
deployment strategy can be obtained by sampling from this distribution.
Simulation results demonstrate that the proposed diffusion-based method
outperforms traditional benchmark approaches in terms of exceed ratio and
generalization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 这篇论文通过模糊推理系统建立了基于江格布拉格实际数据的汽车排放预测模型，分析了气象条件对城市交通排放物数量和扩散的影响，为城市规划者提供环保运输管理决策支持。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是当今社会面临的重要问题，需要研究交通排放与气象条件的关系，以帮助城市规划者和政策制定者更有效地管理城市交通运输并关注环境保护。

Method: 采用模糊推理系统(FIS)开发预测模型，基于江格布拉格的实际交通、气象和排放数据进行建模分析，研究不同条件下排放物的变化情况。

Result: 建立了能够预测汽车排放物在不同气象条件下数量和扩散特征的模型，揭示了气象因素对城市空气质量的重要影响。

Conclusion: 该研究为城市运输管理提供了科学的环保决策支持工具，通过系统化方法分析交通排放与气象条件的关联性，有助于提高城市空气质量和环境保护水平。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [26] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 通过两个AI模型的协同工作，实现了仅依靠自由形式自然语言提示来指导简单代理群体行为，无需任务特定调整或工程化适应性函数


<details>
  <summary>Details</summary>
Motivation: 解决人工智能和生物系统中对自然语言理解的缺乏，提供更自然的复杂分布式系统控制方式，迅渡从传统的工程化奖励和命令集到自由形式语言控制

Method: 使用两个AI模型：第一个模型将命令式提示转换为干预措施应用于模拟细胞，第二个模型评价提示描述细胞动力学的准确性，然后通过进化算法优化第一个模型以提高评分

Result: 进化后的系统能够在未见过的提示上完成良好的一般化，无需重新训练，证明了仅依靠自由形式自然语言提示指导人工系统行为的可行性

Conclusion: 该方法为AI-生物学合作提供了具体步骤，展示了用语言替代数学目标函数、固定规则和领域特定编程的潜力，开启了语言作为计算机、机器人和生物系统控制层的未来视野

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [27] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个自演进的文本到图像生成系统，通过自我批判和自我进化机制，仅使用初始提示就能让T2I模型自主改进生成图像质量


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型高度依赖人工干预，需要手动迭代的提示工程，存在可用性挑战和提示欠规范问题

Method: 采用多模态LLM代理作为'批评家'进行自我批判识别图像弱点，以及作为'评判者'进行图像对比的自我进化机制，迭代优化提示

Result: 在复杂T2I任务上的广泛实验表明，Maestro显著优于初始提示和最先进的自动化方法，且效果随MLLM组件先进性而提升

Conclusion: 这项工作为实现自改进的T2I生成提供了一条稳健、可解释且有效的途径

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [28] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 研究发现不同GPT模型具有独特的评估个性：GPT-4o-mini系统一致，GPT-4o擅长错误检测，GPT-5极度保守且变异大。所有GPT模型都存在2:1的负面评估偏见，且评估能力不随通用能力线性提升。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地评估其他AI输出，理解其评估行为对于防止级联偏见至关重要。需要揭示不同AI模型的评估策略和偏见模式。

Method: 分析NVIDIA Describe Anything模型生成的视觉语言描述，由三种GPT变体（GPT-4o、GPT-4o-mini、GPT-5）进行评估。使用Gemini 2.5 Pro作为独立问题生成器进行控制实验，并通过生成问题的语义相似性进行跨家族分析。

Result: GPT-4o-mini表现出系统性一致性，GPT-4o擅长错误检测，GPT-5显示极端保守主义和高变异性。所有GPT模型都存在一致的2:1负面评估偏见。跨家族分析显示GPT模型高度相似，而Gemini表现出显著不同的评估策略。

Conclusion: 评估能力不随通用能力线性扩展，稳健的AI评估需要多样化的架构视角。评估个性是模型固有属性而非人为产物，不同AI家族存在显著差异。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [29] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: GEO-16框架通过16个质量支柱评估网页质量，发现AI搜索引擎在引用网页时更倾向于选择高质量内容，特别是Metadata、Freshness、Semantic HTML和Structured Data等支柱表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着AI答案引擎通过生成响应和引用网页来源来中介领域知识访问，需要建立一个评估框架来衡量被引用网页的质量，并了解引擎如何选择高质量内容。

Method: 使用70个产品意图提示收集了三个引擎（Brave Summary、Google AI Overviews和Perplexity）的1,702个引用，审计了1,100个唯一URL，通过GEO-16框架将页面质量信号转换为支柱分数和标准化GEO分数G（0-1）。

Result: 引擎在引用页面的GEO质量上存在差异，Metadata和Freshness、Semantic HTML、Structured Data等支柱与引用关联最强。整体页面质量是引用的强预测因子，G≥0.70且至少12个支柱命中的页面引用率显著更高。

Conclusion: 研究提供了一个实用的发布者指南，表明AI搜索引擎倾向于引用高质量内容，但研究是观察性的，专注于英语B2B SaaS页面，存在一定局限性。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [30] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 这篇论文通过企业特定基准测诅18种代理构配置，揭示了多代理系统中不同设计维度的相互作用，发现了模型特定的架构偏好和代理性能的显著缺陷


<details>
  <summary>Details</summary>
Motivation: 当前对代理架构中单个组件的研究多为孤立分析，缺乏对复杂多代理系统中不同设计维度如何相互作用的实证理解

Method: 使用企业特定基准测诅平台，评估18种不同的代理构配置，考察四个关键维度：组织策略、代理提示实现方式、内存架构和思考工具集成

Result: 发现了显著的模型特定架构偏好，挖战了代理AI系统中一切通用的范式，同时发现代理在企业任务上的性能显著不佳，最高分模型在复杂任务上仅达35.3%成功率，简单任务上为70.8%

Conclusion: 这些发现应为未来代理系统的设计提供实证支撑，便于在架构组件和模型选择方面做出更有依据的决策

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [31] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文探讨LLM在决策支持中的应用，提出了一种基于人机对话和单调布尔函数的专家心智模型算法，以解决LLM训练数据缺失和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM和RAG技术在决策支持方面有应用潜力，但它们无法解决训练数据缺失和临界知识缺口的问题，导致幻觉和决策准确性不足。

Method: 提出一种基于优化人机对话和单调布尔/k值函数的专家心智模型（EMM）算法，包括四个步骤：因子识别、因子层次结构化、生成广义专家心智模型规范、以及从规范生成详细模型。

Result: 该方法能够发现计算可处理的个人化专家决策心智模型，有效补充LLM在缺失关键信息时的不足。

Conclusion: 通过EMM算法引擎的LLM提示工程方法，可以更有效地利用LLM进行决策支持，解决传统方法在复杂领域知识缺口时的局限性。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [32] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 该论文提出了Logic-constrained Vector Symbolic Architecture (LVSA)框架，通过结合可微分Skolemization模块和神经否定器，解决了知识图谱复杂查询中逻辑一致性和计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 知识图谱复杂查询存在逻辑完备性和计算效率之间的根本性权衡问题。基于Grounding的方法面临组合爆炸，而基于Skolemization的方法往往忽视Skolem函数的显式建模并损害逻辑一致性。

Method: 提出LVSA神经符号框架，包含可微分Skolemization模块、神经否定器，以及逻辑约束驱动的优化协议，统一几何和逻辑要求。

Result: 理论上保证对所有EFO1查询的普遍性；实证上超越最先进的Skolemization方法，相比Grounding基线将推理成本降低数个数量级。

Conclusion: LVSA框架成功解决了复杂查询回答中的逻辑一致性和计算效率的权衡问题，为神经符号推理提供了新的解决方案。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [33] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 该论文对AI领域中以"代理"为中心的范式进行了批判性重新评估，指出该范式存在概念模糊性和人类中心偏见，并提出向系统动态和物质智能框架转移的建议。


<details>
  <summary>Details</summary>
Motivation: 重新考定AI领域中以"代理"为核心的范式的必要性和最优性，因为该范式存在持久的概念模糊性和本质的人类中心偏见，可能构成了限制性框架。

Method: 通过对相关文献的系统性回顾，分析了不同AI框架中的代理范式，重点关注自主性和目标导向性等属性的定义和测量挑战。区分了代理系统、能动系统和非代理系统。

Result: 分析显示对许多AI系统的"代理性"框架虽然在启发式上有用，但可能会误导并模糊基础计算机制，尤其是在大语言模型中。

Conclusion: 建议将重点转向基于系统级动态、世界建模和物质智能的框架。研究非代理和系统性框架对开发健壮、可扩展且潜在非人类形式的普遍智能至关重要，这需要新的架构和对智能本质的基础重新考虑。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [34] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: HaPLa是一种新型的通用越狱攻击技术，通过溯因推理和符号编码绕过LLM的安全防护，在GPT系列模型上达到95%的攻击成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在被滥用于有害目的的风险，需要研究利用其架构和学习范式内在弱点的通用越狱攻击来加强防御

Method: 提出HaPLa技术，包含两种策略：1)溯因推理框架，让LLM推断有害活动的中间步骤而非直接响应有害查询；2)符号编码，轻量级方法混淆有害内容

Result: 实验显示HaPLa在GPT系列模型上达到95%攻击成功率，在所有目标模型上平均70%成功率。符号编码规则分析表明安全调优LLM会显著降低其对良性查询的有用性

Conclusion: 当前LLM主要对显式有害关键词敏感，安全调优面临根本性挑战，需要在安全性和有用性之间取得平衡

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [35] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 通过引入任务相关公共数据，在保持差分隐私保证的前提下显著提升了私有上下文学习的效果，同时具有强劲的成员推断攻击抵御能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中上下文学习存在的私有数据泄漏风险，同时避免差分隐私技术导致模型效用显著下降的问题

Method: 将任务相关公共数据整合到上下文学习框架中，在维持差分隐私保证的前提下，设计了私有上下文学习算法

Result: 实验结果显示该方法在公共数据的协助下显著提升了私有ICL的效用，并且能够有效抵御成员推断攻击

Conclusion: 该研究成功实现了隐私保护与模型效用的良好平衡，为大语言模型的安全应用提供了有效解决方案

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [36] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 这篇论文探讨了将大语言模型(LLMs)与Clarion认知架构相结合，以结合LLMs的计算能力和Clarion的心理学真实性。


<details>
  <summary>Details</summary>
Motivation: 现有认知架构计算能力有限，而LLMs具有强大的计算能力，需要结合两者来处理现实世界的复杂性和心理学真实性。

Method: 利用Clarion架构的隐式-显式二分法，实现Clarion与LLMs的无缝集成，将LLMs的计算能力与Clarion的心理学精细性相结合。

Result: 实现了认知架构与大语言模型的协同结合，为建模复杂现实世界问题提供了更强大的计算能力支持。

Conclusion: 通过将LLMs集成到认知架构中，可以同时满足现实世界复杂性和心理学真实性的要求，这是认知架构发展的重要方向。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [37] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 这篇论文重新思考大语言模型生成的理由评估方法，通过定义具体属性和属性基于的ELO分数来提供更细致的评估，充分解释了二元偏好判断的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM生成的自然语言理由的评估主要依靠二元偏好判断，这种方法不透明且粗糕，无法揭示理由质量的具体属性和差异。

Method: 通过引用现有文献确定了一组关键理由属性，使用自动指标、LLM判断和人工标注进行评估，然后用SHAP分析MT Bench和Chatbot Arena数据集确定哪些属性最能解释人类偏好，最后采用属性特定的ELO分数重新评估模型生成的理由。

Result: 研究发现细粒度的属性评估能够更好地定义理由质量，揭示了更加微观的模型比较结果和洞察。

Conclusion: 属性基于的评估方法能够充分充分解释二元偏好判断的局限性，为建立更可解释和可靠的评估实践提供了指导。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [38] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: 提出了Free-MAD框架，通过基于分数的决策机制和反从众机制，解决了传统多智能体辩论方法的共识限制、错误传播和投票随机性问题，在单轮辩论中显著提升推理性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论方法依赖多轮交互达成共识，存在token开销大、错误传播、多数投票随机性等问题，需要新的框架来克服这些限制。

Method: Free-MAD框架引入基于分数的决策机制评估整个辩论轨迹，而非仅最后一轮；采用反从众机制减少多数意见的过度影响；只需单轮辩论即可完成决策。

Result: 在8个基准数据集上的实验表明，Free-MAD显著提升推理性能，同时大幅降低token成本，在真实攻击场景中表现出更好的鲁棒性。

Conclusion: Free-MAD通过消除共识需求、引入轨迹评估和反从众机制，为多智能体辩论提供了更高效、准确和公平的解决方案，突破了传统方法的局限性。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [39] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic是一个基于有限状态机的多智能体系统，用于桌面自动化，通过动态编排和质量管理实现了57.07%的成功率，在OSWorld基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的桌面自动化智能体在处理复杂多步骤任务时存在协调性差和质量控制不足的问题，需要更可靠的解决方案。

Method: 采用有限状态机架构，包含Controller、Manager、三个Worker（Technician、Operator、Analyst）和Evaluator四个组件，通过FSM路由实现动态执行策略选择和自适应重规划。

Result: 在OSWorld基准测试中达到57.07%的成功率（50步内），显著优于现有方法。

Conclusion: 基于原则的多智能体编排配合持续质量控制，能够为复杂计算环境中的通用桌面自动化提供卓越的可靠性。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [40] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 这篇论文提出了一种验证框架，解决多段体LLM系统中的计算信任挑战，通过概率性审计小随机段落实现高效验证，验证成本远低于重新生成成本。


<details>
  <summary>Details</summary>
Motivation: 多段体LLM系统中存在计算信任挑战，需要确保段体输出真实来自声称的LLM，而非被伪造或由低等模型生成。

Method: 基于自回归模型的确定性可复现性，在计算同质环境中通过概率性审计小随机段落来验证输出真实性，实现不对称努力验证。

Result: 模拟实验显示目标验证比全部重新生成快12倍以上，并可通过可调参数控制检测概率。

Conclusion: 该框架为可审计的LLM系统提供了基础机制，是负责任AI的基础层，也为更复杂的异构多段体系统研究奠定基础。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [41] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出了Patient-Zero框架，无需真实医疗记录即可生成逼真的虚拟患者，通过医学知识注入和动态更新机制提升生成数据的准确性、多样性和交互一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要利用LLMs重写和补全现有医疗记录，存在数据隐私、准确性、多样性限制，且缺乏真实患者般的交互能力。

Method: 采用医学对齐的多步生成架构，通过分层医学知识注入构建完整患者记录；设计动态更新机制优化虚拟患者与人类的交互能力；包含自适应对话策略和实时临床合理性验证。

Result: 实验结果表明模型在准确性、多样性和一致性方面表现良好；使用生成的虚拟患者训练后，现有模型在MedQA数据集上显示出显著改进。

Conclusion: Patient-Zero框架能够生成上下文多样且保持严格医学一致性的患者记录，为解决医疗数据收集挑战提供了有效解决方案。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [42] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态的多智能体框架，通过难度感知的工作流编排、算子分配和LLM路由，实现了对查询难度的自适应处理，在准确性和推理效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架使用静态或任务级工作流，无法根据查询难度动态调整，导致简单查询过度处理或复杂查询处理不足，且忽视了异构LLM的效率-性能权衡。

Method: 提出Difficulty-Aware Agentic Orchestration (DAAO)框架，包含三个模块：变分自编码器(VAE)用于难度估计、模块化算子分配器、成本和性能感知的LLM路由器，动态调整工作流深度、算子选择和LLM分配。

Result: 在六个基准测试中，DAAO在准确性和推理效率方面均优于先前的多智能体系统。

Conclusion: DAAO通过难度感知的动态编排，实现了细粒度的查询特定推理策略，有效解决了现有框架在处理不同难度查询时的效率-性能平衡问题。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [43] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 神经细胞自动机(NCA)是一个强大的生物自组织建模框架，通过可训练的局部决策规则模拟多尺度生物过程，连接生物学与生成式AI，具有构建生物启发的集体智能的潜力。


<details>
  <summary>Details</summary>
Motivation: 扩展经典规则系统，用可训练的微分或进化规则来捕捉生命物质的自适应自调节动力学，为生物自组织和多尺度能力架构提供统一的计算框架。

Method: 将人工神经网络作为局部决策中心和局部化代理之间的交互规则嵌入，模拟从分子到系统级别的多尺度过程，实现迭代状态精炼。

Result: NCA不仅能重现生物启发的目标模式，还能泛化到新条件，表现出对扰动的鲁棒性以及开放式适应和推理能力，在机器人控制和推理任务中显示强大性能。

Conclusion: NCA构成了一个统一的计算精简范式，不仅连接了多尺度生物学与生成式AI的见解，还有潜力设计真正生物启发的、能够进行层次推理和控制的集体智能。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [44] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT采用前端到后端架构，通过对比学习将初步知识状态与基于教学理论的理想知识状态对齐，显式建模稳定知识状态，提升可解释性和教学支持能力


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型主要关注拟合学习者交互序列，往往忽视知识状态本身，导致可解释性降低和教学支持不足

Method: 使用五个编码器实现前端到后端架构，定义基于教学理论的理想知识状态作为对齐标准，并加入对比学习模块增强对齐过程的鲁棒性

Result: 在三个真实数据集上超越七个基线模型，在两个数据集上达到最先进性能，在第三个数据集上表现具有竞争力

Conclusion: AlignKT通过显式建模稳定知识状态并引入教学理论指导的对齐机制，有效提升了知识追踪的可解释性和教学支持能力

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [45] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 这篇视野性论文给出了AIGC的跨领域视角，结合16位多学科学家的见解，提供了AIGC的训练技术、检测方法、社会影响和技术挑战的全面分析


<details>
  <summary>Details</summary>
Motivation: 当前少有研究探讨AIGC在不同领域的最新进展和新兴挑战，需要跨领域视角来填补这一空白

Method: 聚集16位来自多学科的学者，从三个方面提供分析：AIGC的整体概览、各领域社会影响评估、技术挑战与未来研究建议

Result: 构建了一个跨领域的AIGC研究框架，包含了生成式AI训练、内容检测、数字平台传播等多个维度，以及在数字营销、教育、公共健康等领域的应用分析

Conclusion: 该论文为AIGC领域提供了一个维度丰富的视角，指明了当前的研究趋势、正在面临的挑战以及未来的研究方向，对于推动AIGC领域的发展具有重要意义

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [46] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: VideoAgent是一个多智能体框架，通过对话界面生成个性化科学视频，将论文解析为细粒度资源库，并编排包含静态幻灯片和动态动画的叙事流程。


<details>
  <summary>Details</summary>
Motivation: 现有的文档自动化主要关注静态媒体（如海报和幻灯片），缺乏个性化动态编排和多模态内容同步机制，科学视频生成自动化对于知识传播至关重要但具有挑战性。

Method: 引入VideoAgent多智能体框架，将源论文解析为细粒度资源库，根据用户需求编排叙事流程，合成静态幻灯片和动态动画来解释复杂概念。同时提出SciVidEval评估套件，结合多模态内容质量和同步的自动指标以及基于视频测验的人类评估。

Result: 大量实验表明，该方法显著优于现有的商业科学视频生成服务，在科学传播质量方面接近人类水平。

Conclusion: VideoAgent框架成功解决了科学视频生成的个性化动态编排和多模态同步问题，为科学知识传播提供了有效的自动化解决方案。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [47] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 使用LLM作为人类调查响应代理的对齐框架P2P，通过构建多样化人设端神和选择代表性子集来模拟真实人群响应，解决社会科学调查成本高和人口统计偏差问题


<details>
  <summary>Details</summary>
Motivation: 解决社会科学研究中调查部署成本不断上升和调查响应数据中人口统计偏差日益严重的挑战，提供一种成本效益高、可控制的解决方案

Method: 受显示偏好理论启发，将对齐问题形式化为两阶段：构建多样化人设端神模拟可能的响应者类型，然后根据观测数据选择代表性子集近似真实人群。提出P2P系统，通过结构化提示工程、基于熵的采样和回归选择来导向LLM代理的代表性行为模式

Result: 在真实世界意见调查数据集上验证方法有效性，对齐后的代理人群能够高保真度重现聚合响应模式，并展现出实质性的响应多样性，无需人口统计条件化

Conclusion: P2P框架不仅提高了社会科学研究的数据效率，还为研窋多元化对齐的运作化提供了测试床，该方法不依赖个人信息仅使用聚合调查结果，具有更好的普遍性和简洁性

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [48] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 提出基于大语言模型的新框架，从科学摘要中提取污染物源到健康影响的多跳语义链（关系元路径），构建毒性轨迹图追踪污染物传播路径，并通过动态证据协调模块解决语义冲突。


<details>
  <summary>Details</summary>
Motivation: 塑料微粒在环境中广泛存在并积累，导致呼吸系统、胃肠道和神经系统等多种健康风险，需要系统性地追踪污染物从源头到健康影响的复杂因果关系。

Method: 利用大语言模型从科学摘要中提取关系元路径，构建多跳语义链连接污染物源和健康影响，形成毒性轨迹图，并加入动态证据协调模块处理研究发现的语义冲突。

Result: 该方法在从嘈杂科学文本中提取可靠、高价值关系知识方面表现优异，能够有效挖掘领域特定语料库中的复杂因果结构。

Conclusion: 该框架为挖掘污染物传播和健康影响的复杂因果关系提供了可扩展的解决方案，有助于系统理解环境污染物对健康的危害路径。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [49] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 通过动态因果分析优化观测器基软件传感器，利用LTC网络识别并剥离对状态估计因果影响最小的传感器输入，获得更小但更准确的传感器集合。


<details>
  <summary>Details</summary>
Motivation: 传统传感器选择方法依赖线性化可观测性指数戗统计相关性，无法捐描复杂系统的时间演化特征。

Method: 使用液体时间常数(LTC)网络进行动态因果分析：训练LTC观测器→通过受控批动分析量化因果影响→剥离可忽略输入→重复训练直到性能下降。

Result: 在三个机械测试平台上，该方法一致地识别出与基础物理一致的最小传感器集合，同时提高了预测准确性，能够自动区分必要物理测量与噪声。

Conclusion: 该框架通过将传感器选择决策基于动态因果关系而非静态相关性，提高了计算效率和可解释性，在过程工程、生态监测和农业领域具有重要价值。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [50] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: MAPGD是一个多智能体提示梯度下降框架，通过多智能体协作和梯度优化解决传统单轨迹提示工程的局限性，在准确性和效率上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法依赖单一优化轨迹，存在适应性差、效率低、视角狭窄、梯度冲突和计算成本高等问题，需要更鲁棒和高效的优化方法。

Method: MAPGD整合多智能体协作与梯度优化，包含任务清晰化、示例选择、格式设计和风格优化等专门智能体，采用语义梯度协调解决冲突，基于bandit的候选选择实现高效探索-利用平衡，并提供理论收敛保证。

Result: 在分类、生成和推理任务上的实验表明，MAPGD在准确性和效率方面优于单智能体和随机基线方法。消融实验证实了梯度融合、智能体专业化和冲突解决的有效性。

Conclusion: MAPGD提供了一个统一、梯度启发的多智能体方法，实现了鲁棒且可解释的提示优化，为提示工程提供了新的解决方案。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [51] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 大语言模型存在训练数据静态和时效性限制，AI组件通过外部工具和实时数据扩展了能力，但面临安全风险。本文提出一种基于角色的访问控制框架来保护AI组件安全。


<details>
  <summary>Details</summary>
Motivation: 解决LLM模型的静态训练数据限制和需要细调的问题，以及AI组件在工业环境中遂到的安全威胁（如提示注入攻击）。

Method: 提出一种集成角色基于访问控制（RBAC）的框架，为AI组件提供稳健的安全拦截器。该框架重点关注本地部署实现。

Result: 该框架能够有效防范安全威胁，保护AI组件的完整性和可靠性，支持高效可扩展的部署。

Conclusion: 通过集成RBAC机制，可以为AI组件提供强大的安全保障，促进其在工业环境中的应用和发展。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [52] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 基于适应性专家混合模型的降雨预测方法，通过多模态数据整合提升预测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决多源观测数据（雷达、卫星、地面测量）在空间时间分辨率和领域特征异质性导致的数据整合挑战，提高降雨预测准确度

Method: 提出适应性专家混合模型(Adaptive MoE)，每个专家专门处理特定模态或空间-时间模式，使用动态路由器分配输入到最相关专家

Result: 在叙利亚台风Ian的多模态气候数据集上评估，适应性MoE模型显著超过所有基准方法，提升了预测准确性和可解释性

Conclusion: 该模块化设计有效解决了多源异质数据整合挑战，为降雨预测提供了更准确和可解释的方法，同时发布了交互式可视化工具支持决策

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [53] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 这篇论文评估了五种代表性的VLA模型在边缘设备和数据中心GPU平台上的性能扩展、系统指标和能耗情况，发现了架构选择、功耗约束和性能优化的关键见解。


<details>
  <summary>Details</summary>
Motivation: VLA模型作为机器人控制的通用策略强大但性能扩展特性不明，需要在不同硬件平台和功耗约束下进行系统化评估。

Method: 使用LIBERO基准测试集，对五种代表性VLA模型（包括最新基准和新提出的两种架构）在边缘设备和数据中心GPU上进行测试，量化准确率、延迟、吞吐量、峰值内存使用等指标。

Result: 发现三个关键趋势：(1)架构选择影响吞吐量和内存占用；(2)边缘设备在功耗约束下出现非线性性能降级，某些配置甚至超过旧版数据中心GPU；(3)可以在不严重損失准确性的情况下实现高吞吐量。

Conclusion: 这些发现为在不同部署约束下选择和优化VLA模型提供了可操作的见解，并挑战了当前认为数据中心硬件在机器人推理中更优越的假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [54] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个基于智能体的医疗操作系统，通过自然语言指令转换为预定义的医疗命令，实现临床工作流程自动化。


<details>
  <summary>Details</summary>
Motivation: 当前数字医疗系统复杂难用，医护人员需要管理多个工具、重复手动操作，花费大量时间在行政工作上而非患者护理。大语言模型智能体在计算机操作方面的卓越能力为医疗领域提供了新的交互方式可能性。

Method: 开发MedicalOS作为领域特定的抽象层，将人类指令翻译为预定义的医疗命令（如患者查询、病史检索、检查管理、报告生成等），这些命令封装为现成的工具使用机器语言（Python、API、MCP、Linux）。

Result: 在22个专科的214个病例上进行实证验证，显示出高诊断准确性和置信度、临床合理的检查请求，以及一致的结构化报告和药物推荐生成。

Conclusion: MedicalOS为推进临床实践中的工作流程自动化提供了一个可信赖和可扩展的基础。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [55] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 通过使用CTGAN等合成数据生成器生成眼动数据来扩充真实数据集，显著提高了任务解码的准确率，支持了Yarbus的假说。


<details>
  <summary>Details</summary>
Motivation: 验证Yarbus的假说，即可以从观察者的眼动中解码其执行的任务。传统机器学习算法在眼动数据解码任务方面效果不一，需要更多数据来支持这一假说。

Method: 使用CTGAN、CopulaGAN和Gretel AI等合成数据生成器在320个真实眼动数据样本的基础上生成合成数据，并将真实数据与合成数据结合进行分类。测试了多种算法和不同的真实/合成数据组合。

Result: 在真实数据基础上增加5倍数据后，分类准确率从Random Forest的28.1%显著提高到Inception Time的82%。使用额外合成数据集的框架在该数据集上超过了所有现有研究。

Conclusion: 通过合成数据生成来扩充眼动数据集可以显著提高任务解码的准确性，强烈支持了Yarbus的假说，为眼动研究提供了有效的数捠增强方法。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [56] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: MCFR是一种结合大语言模型和模型检查的神经-符号框架，通过将自然语言转换为形式规范并进行属性验证，提高闭域QA系统的推理忠实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有符号引擎在动态、状态基于的多步推理中遇到困难，而大语言模型的推理迹迹经常不忠实，仅作为合理化解释而非因果基础推导。

Method: 提出MCFR框架，将LLM与模型检查相结合，将自然语言翻译为形式规范，并在迁移模型上验证属性。构建EduMC-QA数据集进行评估。

Result: MCFR在推理忠实性和可解释性方面显著改善，为高风险闭域应用提供了可验证的QA解决方案。与ChatGPT、DeepSeek、Claude等先进LLM相比显示出更好的效果。

Conclusion: MCFR框架通过结合神经网络和符号检查技术，有效解决了动态多步推理的挑战，为高要求闭域QA系统提供了可靠的验证方案。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [57] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 这篇综述论文系统性地定义了时间序列推理问题，按推理拓扑结构将文献分为三类：直接一步推理、线性链推理和分支结构推理，并交叉分析了该领域的主要目标、方法和评估实践。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理将时间作为第一类轴，并将中间证据直接融入答案中。该领域需要系统性的组织框架来理解不同推理拓扑结构的优势和局限性，以推动从单纯准确性向大规模可靠性的转变。

Method: 通过推理拓扑结构（直接推理、线性链推理、分支结构推理）与主要目标（传统时间序列分析、解释理解、因果推理决策、时间序列生成）的交叉分析，结合分解验证、集成、工具使用等标签集来组织文献。

Result: 建立了系统性的分类框架，展示了每种拓扑结构在不同场景下的优势和局限性（忠实性和鲁棒性方面），提供了精选的数据集、基准测试资源，并强调了保持证据可见性和时间对齐的评估实践。

Conclusion: 时间序列推理结构需要在接地能力和自我修正与计算成本和可复现性之间取得平衡。未来进展将依赖于将推理质量与效用挂钩的基准测试，以及在偏移感知、流式和长时域设置下权衡成本与风险的闭环测试平台。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [58] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: AMLNet是一个基于知识的多智能体框架，包含监管感知的交易生成器和集成检测管道，用于生成合成反洗钱数据集并实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 反洗钱研究缺乏公开可共享、符合监管要求的交易数据集，限制了相关实验和研究的发展。

Method: 采用知识驱动的多智能体框架，包含两个协调单元：监管感知交易生成器（生成包含洗钱核心阶段和高级类型的合成交易）和集成检测管道。

Result: 生成1,090,173笔合成交易（约0.16%洗钱阳性），监管对齐度达75%，技术保真度得分0.75；检测集成F1得分0.90（精确率0.84，召回率0.97），在外部数据集上也表现良好。

Conclusion: AMLNet提供了多维评估和公开数据集，推动了可重复且符合监管要求的反洗钱实验研究。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [59] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 首次综合评估MLLM在青少年特发性侧弯自我管理中的能力，发现当前模型在解释复杂X光片和知识理解方面有显著局限性


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在青少年特发性侧弯病人自我管理中的应用潜力，以实现个性化辅助疗法

Method: 构建3000张X光片数据库，采用分治框架进行三项任务评估：视觉问答、领域知识考核、病人教育咨询，并提出脊柱关键点提示和RAG知识库改进方法

Result: 模型在脊柱变形位置检测最高准确率仅0.55，方向检测最高准确率仅0.13，RAG技术显著提升了知识考核任务表现

Conclusion: 当前MLLM远未能实现AIS疗护中的个性化辅助，最大挑战在于脊柱变形位置和方向的准确检测能力

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [60] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion是一种高效的编码器，通过构建局部多尺度图和类型特定特征网络，有效建模自动驾驶中异质智能体的多尺度交互，在Waymo数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分捕捉自动驾驶中复杂的社会动力学，特别是多尺度交互的共存和异质智能体的多样化行为。

Method: 构建以每个智能体为中心的局部多尺度图，采用聚合-分解消息传递方案和类型特定特征网络，建模直接成对依赖和复杂群体交互。

Result: 在Waymo Open Motion数据集上实现了最先进的性能，在Soft mAP和minADE等关键指标上创造了新基准。

Conclusion: 基于局部性、显式建模多尺度和异质交互的架构是推进运动预测的有效策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [61] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文提出将监督对比学习(SupCon)应用于模仿学习(IL)的新方法，通过改进状态表示学习来提升智能体在视频游戏环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 目标是学习更好的状态表示，捕捉与动作相关的关键因素，从而更好地建模从观察到演示者执行动作的因果关系（如玩家遇到障碍物时跳跃）。

Method: 将SupCon损失与连续输出空间集成，使SupCon能够不受环境动作类型限制地工作。

Result: 在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验显示，相比仅使用监督动作预测损失函数的基线模型，该方法提高了表示质量、加快了学习收敛速度并具有更好的泛化能力。

Conclusion: 监督对比学习可以有效地提升模仿学习中的状态表示学习效果，为智能体在复杂游戏环境中提供更好的性能表现。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [62] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为全双工模型处理实时全模态流设计的终身记忆代理，能够从原始视听流中识别用户、提供个性化响应，并维护长期用户知识。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理主要面向LLMs，无法处理实时全模态流。需要为终身、实时、具身场景开发能够直接从原始视听流中运作的记忆系统。

Method: 采用三个异步进程：检索过程（动态识别用户并收集相关上下文）、全模态对话过程（生成个性化音频响应）、内存管理过程（检测对话边界并更新长期记忆）。

Result: 检索和内存管理模块在测试集上达到95%以上准确率。与微调的RoboEgo全模态聊天机器人集成后，在实时个性化对话中事实一致性得分超过87%。

Conclusion: EgoMem为未来研究建立了强大基线，特别适用于终身、实时和具身场景的全模态记忆代理系统。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [63] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym是一个开源框架，用于在建筑能源管理中训练强化学习控制策略，集成EnergyPlus模拟器，支持系统级和房间级控制，并能接受外部信号输入。


<details>
  <summary>Details</summary>
Motivation: 现有建筑能源管理缺乏灵活的强化学习实现框架，无法适应各种控制问题和智能电网等灵活环境的需求。

Method: 开发BuildingGym开源工具，集成EnergyPlus模拟器，提供内置RL算法，支持外部信号输入，简化配置过程。

Result: 在冷却负荷管理任务中，内置算法表现出色，验证了BuildingGym在优化冷却策略方面的有效性。

Conclusion: BuildingGym填补了建筑管理者与AI专家之间的鸿沟，为建筑能源管理提供了灵活、高效的强化学习框架解决方案。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [64] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 神经形态计算利用大脑启发的计算原理实现高能效智能系统，动力学系统理论为其提供了统一的理论框架，将噪声作为学习资源，通过微分遗传编程发现实现自适应行为的动力学系统。


<details>
  <summary>Details</summary>
Motivation: 寻求复制人脑高效、灵活和适应性的计算方式，解决传统数字方法计算和能源消耗巨大的问题，建立跨学科的统一理论框架。

Method: 采用动力学系统理论作为理论基础，利用微分计算建模推理、学习和控制，将噪声作为学习资源，使用微分遗传编程发现自适应行为的动力学系统。

Result: 提出了基于动力学系统理论的神经形态计算框架，能够实现高能效的智能系统，为可持续、透明和广泛可及的AI系统奠定基础。

Conclusion: 动力学系统理论为神经形态计算提供了统一的理论基础，通过物理基质的动力学特性实现智能行为的涌现，推动AI科学和可持续发展的进步。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [65] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出相对精确度和相对召回率新指标，通过与多个专家意见比较来更稳定地评估AI医疗诊断性能，发现AI与人类专家的一致性相当或更好


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如精确度、召回率无法考虑专家判断的内在变异性，而Cohen's Kappa等一致性指标缺乏可解释性

Method: 提出RPAD和RRAD新指标，将AI输出与多个专家意见进行比较，通过归一化专家间不一致性来推出更稳定的评估方法，支持自由形式诊断

Result: 在360个医疗对话上验证，最佳模型如DeepSeek-V3达到了与专家共识相当或更高的一致性，自动诊断识别准确率达98%，专家判断的变异性甚至超过AI与人类的差异

Conclusion: 绝对指标在医疗AI评估中有限，需要采用相对指标来更准确地评估AI诊断性能，专家内部的不一致性强调了相对评估方法的必要性

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [66] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号多智能体架构，使用Kripke模型形式化表示智能体信念状态，结合模态逻辑进行可能性与必要性推理，在粒子加速器环境中成功诊断复杂级联故障。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究主要关注模型和数据规模的扩展，但智能体在复杂环境中进行自适应、复杂和自主决策的推理结构和逻辑一致性扩展维度尚未充分探索。

Method: 采用神经符号多智能体架构，将个体智能体信念状态形式化为Kripke模型，使用模态逻辑进行可能性与必要性推理，结合领域特定知识和逻辑约束指导语言模型的假设生成。

Result: 在高保真模拟粒子加速器环境中，系统成功诊断复杂级联故障，结合了语言模型的强大语义直觉与模态逻辑的严格可验证验证。

Conclusion: 该方法展示了构建更鲁棒、可靠和可验证自主智能体的可行路径，通过形式化逻辑约束防止语言模型得出物理或逻辑上不可行的结论。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [67] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的时间图基于多模态医疗推理框架，通过有向图模型化推理过程，支持回溯、精炼推理内容和动态调整原因，以提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的多模态数据需要有效推理进行诊断，但现有多模态推理模型在医疗领域应用有限且诊断准确性不足。需要一种能够处理多模态医疗数据、支持动态变化和提高诊断准确性的推理方法。

Method: 提出一种基于时间图的有向图模型化推理过程，支持回溯、精炼推理内容、动态创建或删除原因。考虑不同时间点的多模态数据进行病患健康跟踪分析。采用多代理时间推理框架进行任务分配和交叉验证。

Result: 基础实验和分析结果证明了该初步方法的新颖性和实际应用价值。

Conclusion: 该研究提出的时间图基于多模态医疗推理框架能够有效处理医疗领域的多模态数据推理问题，通过动态调整推理过程和多代理协同推理，显著提高了诊断准确性，具有良好的应用前景。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [68] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: MusicSwarm是一个去中心化的音乐创作系统，使用冻结的基础模型通过信息素式信号协调，无需权重更新即可产生连贯的长篇音乐作品


<details>
  <summary>Details</summary>
Motivation: 探索如何通过去中心化的群体智能方法实现长时程创造性结构生成，避免传统集中式系统的高计算和数据需求

Method: 使用完全去中心化的群体系统，其中基于小节的智能体通过感知和沉积和声、节奏和结构线索来协调，具有短期记忆适应和共识达成机制

Result: 群体系统在符号、音频和图论分析中均表现出更高质量，提供更大的多样性和结构变化，在创造力指标上领先，形成稳定的互补角色配置和小世界架构

Conclusion: MusicSwarm通过将专业化从参数更新转向交互规则、共享记忆和动态共识，提供了一种计算和数据高效的长时程创造性结构生成方法，可扩展到写作、设计和科学发现等领域

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [69] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文系统回顾了灾害管理中的人机协作模式，识别了四大类别及其子模式，分析了AI如何提升灾害应对能力，并指出了可扩展性、可解释性等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 在高风险灾害场景中，及时明智的决策至关重要，但常受到不确定性、动态环境和资源有限的挑战，需要研究人机协作如何支持灾害管理的各个阶段。

Method: 基于51篇同行评审研究的系统回顾，识别出四大主要类别：人机决策支持系统、任务与资源协调、信任与透明度、模拟与训练，并分析其中的子模式。

Result: 研究发现AI系统可以增强态势感知、提高响应效率、支持复杂决策，但也存在可扩展性、可解释性和系统互操作性等关键局限性。

Conclusion: 需要开发自适应、可信赖和情境感知的人机系统，以提高灾害韧性和公平的恢复结果，这是未来研究的关键方向。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [70] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: MLLMs存在隐式推理风险，无害的单模态输入组合成危险的多模态数据会产生有害输出。研究提出了SSUI数据集和SRPO训练框架来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长链推理过程中难以保持安全对齐，导致无害的单模态输入组合成危险的多模态输出，存在安全隐患。

Method: 提出了SSUI数据集（包含可解释的推理路径）和SRPO训练框架，通过优化推理路径来使MLLM的内部推理过程与人类安全价值观对齐。

Result: SRPO训练的模型在关键安全基准测试（包括新提出的RSBench）上取得了最先进的结果，显著优于开源和顶级商业MLLMs。

Conclusion: 通过SSUI数据集和SRPO框架可以有效解决MLLMs的隐式推理风险，提高模型的安全性表现。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [71] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模型驱动方法，能够在SysML工程模型中自动生成符号规划中的预处理条件、效果和约束等语义信息，以支持AI规划验证系统变体的可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型基于系统工程(MBSE)模型缺乏符号规划语义，无法评估系统变体是否能完成特定任务以及性能效率。

Method: 设计专用的SysML模型模块，引入可重用的规划构造块模板，并通过算法自动生成PDDL格式的预处理预条件、效果和约束文件。

Result: 通过航空装配案例研究验证了方法的可行性，能够在现有工程模型中添加规划语义并生成一致的规划文档。

Conclusion: 该方法实现了工程模型与规划中间产物的原生集成，支持通过AI规划验证系统变体的可行性。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [72] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: JustEva是一个开源评估工具包，用于测量LLM在法律任务中的公平性，包含65个法外因素标签系统、三个核心公平性指标、统计推断方法和可视化功能，实证研究发现当前LLM存在显著公平性缺陷


<details>
  <summary>Details</summary>
Motivation: LLM在法律实践中的集成引发了司法公平性的担忧，特别是由于其"黑盒"特性，需要系统评估工具来确保LLM在法律领域的公平性和可信度

Method: 开发JustEva评估工具包，包含结构化标签系统（65个法外因素）、三个核心公平性指标（不一致性、偏见、不平衡不准确性）、统计推断方法和可视化功能，支持生成结构化输出和统计分析两种实验类型

Result: 实证应用显示当前LLM存在显著的公平性缺陷，缺乏公平可信的法律工具

Conclusion: JustEva为评估和改进法律领域算法公平性提供了便利工具和方法论基础

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [73] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: 提出了双向认知对齐(BiCA)方法，实现人类与AI的相互适应，在协作导航任务中显著提升了成功率、适应性和安全性


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法(如RLHF)是单向的，AI适应人类偏好但将人类认知视为固定。需要转向双向共同对齐范式

Method: 使用可学习协议、表示映射和KL预算约束来实现受控的共同进化，让人类和AI相互适应

Result: 在协作导航中达到85.5%成功率(比基线70.3%提升)，相互适应提升230%，协议收敛提升332%，安全性提升23%

Conclusion: 双向对齐在人类和AI能力交集处产生最优协作，验证了从单向对齐向共同对齐范式的转变

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [74] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: LLM在复杂文本鉴别诊断方面超越医生表现，并能有效模拟专家医学报告，但在图像解读和文献检索方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估仅关注最终诊断，未能全面评估专家讨论所需的多方面推理和展示技能，需要创建更全面的医学AI评估基准。

Method: 使用7102个CPC病例和1021个图像挑战创建CPC-Bench基准，评估领先LLM，并开发Dr. CaBot AI讨论系统来生成书面和幻灯片视频报告。

Result: o3模型在60%病例中排名第一诊断，84%进入前十，优于20名医生基线；图像任务准确率67%；在盲测中74%的病例无法区分AI和人类专家报告。

Conclusion: LLM在文本诊断方面表现出色，能有效模拟专家报告，但图像和文献任务仍需改进。CPC-Bench和CaBot有助于透明追踪医学AI进展。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [75] [A Deep Learning Framework for Joint Channel Acquisition and Communication Optimization in Movable Antenna Systems](https://arxiv.org/abs/2509.10487)
*Ruizhi Zhang,Yuchen Zhang,Lipeng Zhu,Ying Zhang,Rui Zhang*

Main category: cs.IT

TL;DR: 提出端到端深度学习框架解决可移动天线系统中的CSI获取问题，联合优化信道估计、天线位置和预编码设计，在有限反馈和稀疏信道环境下显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统方法假设完美CSI且分别处理信道估计和天线位置优化，实际系统中存在CSI获取不完美的问题，需要更实用的解决方案

Method: 端到端深度学习框架，设计导频信号和量化CSI反馈，联合优化信道估计、MA放置和预编码设计，并扩展到基于统计CSI的天线位置优化

Result: 仿真显示在用户可达和速率方面持续优于传统基准方法，在有限反馈和稀疏信道环境下表现优异，性能接近完美CSI的梯度方法但反馈开销显著降低

Conclusion: 基于学习的可移动天线系统设计对未来无线系统具有有效性和适应性，能够从非完美信道数据中学习优化传输策略

Abstract: This paper presents an end-to-end deep learning framework in a movable
antenna (MA)-enabled multiuser communication system. In contrast to the
conventional works assuming perfect channel state information (CSI), we address
the practical CSI acquisition issue through the design of pilot signals and
quantized CSI feedback, and further incorporate the joint optimization of
channel estimation, MA placement, and precoding design. The proposed mechanism
enables the system to learn an optimized transmission strategy from imperfect
channel data, overcoming the limitations of conventional methods that conduct
channel estimation and antenna position optimization separately. To balance the
performance and overhead, we further extend the proposed framework to optimize
the antenna placement based on the statistical CSI. Simulation results
demonstrate that the proposed approach consistently outperforms traditional
benchmarks in terms of achievable sum-rate of users, especially under limited
feedback and sparse channel environments. Notably, it achieves a performance
comparable to the widely-adopted gradient-based methods with perfect CSI, while
maintaining significantly lower CSI feedback overhead. These results highlight
the effectiveness and adaptability of learning-based MA system design for
future wireless systems.

</details>


### [76] [MAGNET-KG: Maximum-Entropy Geometric Networks for Temporal Knowledge Graphs: Theoretical Foundations and Mathematical Framework](https://arxiv.org/abs/2509.10587)
*Ibne Farabi Shihab*

Main category: cs.IT

TL;DR: 提出了基于最大熵原理、微分几何和信息论的时序知识图谱统一理论框架，建立了评分函数的唯一性特征和几何选择的必要性定理，并推导了具有明确常数的泛化边界。


<details>
  <summary>Details</summary>
Motivation: 为时序知识图谱建模建立理论基础，通过最大熵原理、微分几何和信息论提供形式化的理论保证，解决现有方法缺乏严格理论支撑的问题。

Method: 采用最大熵原理推导评分函数的唯一性特征，利用微分几何建立几何选择的必要性定理，基于信息论推导泛化边界和一致性保证条件。

Result: 证明了评分函数在最大熵原理下的唯一性特征，建立了特定几何选择的必要性定理，推导了具有明确常数的泛化边界，并给出了时序依赖下的收敛性保证条件。

Conclusion: 该框架为时序知识图谱建模提供了严格的理论基础，建立了与微分几何方法的正式联系，为后续研究提供了理论指导和分析工具。

Abstract: We present a unified theoretical framework for temporal knowledge graphs
grounded in maximum-entropy principles, differential geometry, and information
theory. We prove a unique characterization of scoring functions via the
maximum-entropy principle and establish necessity theorems for specific
geometric choices. We further provide rigorous derivations of generalization
bounds with explicit constants and outline conditions under which consistency
guarantees hold under temporal dependence. The framework establishes principled
foundations for temporal knowledge graph modeling with formal connections to
differential geometric methods.

</details>


### [77] [Uniquely-Decodable Coding for Zero-Error Network Function Computation](https://arxiv.org/abs/2509.10775)
*Xuan Guang,Jihang Yang,Ruze Zhang*

Main category: cs.IT

TL;DR: 本文研究零误差网络函数计算中的唯一可解码编码，提出了基于团熵的新下界，改进了现有结果，并证明了唯一可解码编码优于定长编码。


<details>
  <summary>Details</summary>
Motivation: 研究零误差网络函数计算的计算容量问题，从信息论角度分析在无环有向图中，汇聚节点需要多次零误差计算目标函数时的最优编码方案。

Method: 首先证明团熵的新结果（特别是概率图团熵的替换引理），然后基于诱导特征图的团熵建立计算容量的下界，并通过精化信息源的概率分布进一步严格改进该下界。

Result: 提出了适用于任意网络拓扑、任意信息源和任意目标函数的计算容量下界，证明了唯一可解码网络函数计算编码在计算容量方面优于定长编码。

Conclusion: 本文发展的团熵理论和提出的下界为网络函数计算提供了新的理论基础，有助于改进现有结果并指导实际编码设计。

Abstract: We consider uniquely-decodable coding for zero-error network function
computation, where in a directed acyclic graph, the single sink node is
required to compute with zero error a target function multiple times, whose
arguments are the information sources generated at a set of source nodes. We
are interested in the computing capacity from the information theoretic point
of view, which is defined as the infimum of the maximum expected number of bits
transmitted on all the edges for computing the target function once on average.
We first prove some new results on clique entropy, in particular, the
substitution lemma of clique entropy for probabilistic graphs with a certain
condition. With them, we prove a lower bound on the computing capacity
associated with clique entropies of the induced characteristic graphs, where
the obtained lower bound is applicable to arbitrary network topologies,
arbitrary information sources, and arbitrary target functions. By refining the
probability distribution of information sources, we further strictly improve
the obtained lower bound. In addition, we compare uniquely-decodable network
function-computing coding and fixed-length network function-computing coding,
and show that the former indeed outperforms the latter in terms of the
computing capacity. Therein, we provide a novel graph-theoretic explanation of
the key parameter in the best known bound on the computing capacity for
fixed-length network function-computing codes, which would be helpful to
improve the existing results.

</details>


### [78] [A Broadcast Channel Framework for MIMO-OFDM Integrated Sensing and Communication](https://arxiv.org/abs/2509.10878)
*Homa Nikbakht,Husheng Li,Zhu Han,H. Vincent Poor*

Main category: cs.IT

TL;DR: 提出了一个统一的ISAC框架，将通信和感知视为广播信道，通信信号发送给实际用户，感知信号发送给虚拟用户，支持DPC和FDM等复用方案，并提出了波形优化算法。


<details>
  <summary>Details</summary>
Motivation: 6G网络中集成感知与通信(ISAC)是一个重要特性，但缺乏统一框架来同时处理通信和感知两种功能。

Method: 将ISAC建模为广播信道，通信用户和虚拟感知用户分别接收信号；应用脏纸编码(DPC)和频分复用(FDM)等复用方案；提出叠加编码方案和MIMO波形优化算法。

Result: 框架支持现有复用技术的应用，提出了针对感知波形已知/未知情况的编码方案，并开发了考虑杂波和多普勒效应的优化算法。

Conclusion: 该统一框架为ISAC系统的设计和分析提供了理论基础，能够有效协调通信和感知功能，支持多种性能指标的优化。

Abstract: Integrated sensing and communication (ISAC) is expected to be one of the
major features of 6G wireless networks. In an ISAC system, communications and
sensing functionalities are jointly performed using the same waveform,
frequency band and hardware, thereby enabling various use cases such as in
cyber physical systems, digital twin and smart cities. A major challenge to the
design and analysis of ISAC is a unified framework that incorporates the two
distinct functions. By viewing ISAC as a type of broadcast channel, in this
paper, we propose a unified ISAC framework in which communication and sensing
signals are broadcast to the actual communication users and virtual sensing
users. This framework allows the application of existing multiplexing schemes,
such as dirty paper coding (DPC) and frequency division multiplexing (FDM) that
have been intensively studied in data communications and information theory.
Within this framework, we propose different superposition coding schemes, for
cases when the sensing waveform is known or unknown to the communication
receiver. We propose the waveform optimization algorithms in a multiple-input
multiple-output (MIMO) setting accounting for the effects of clutter and
Doppler shift. The proposed framework is numerically evaluated for different
schemes under various sensing and communications performance metrics.

</details>


### [79] [Detectability Thresholds for Network Attacks on Static Graphs and Temporal Networks: Information-Theoretic Limits and Nearly-Optimal Tests](https://arxiv.org/abs/2509.10925)
*Abdulkader Hajjouz,Elena Avksentieva*

Main category: cs.IT

TL;DR: 该论文建立了网络攻击检测的统一理论框架，针对静态图和时间交互网络两种观测模型，给出了信息论下界和上界匹配的检测阈值条件


<details>
  <summary>Details</summary>
Motivation: 网络攻击检测是网络安全的核心问题，但现有研究缺乏统一的理论框架来量化不同网络模型下的检测极限性能，需要建立通用的检测理论来指导实际系统设计

Method: 针对两种模型：(1)静态Erdos-Renyi图中的植入异常社区检测；(2)多元点过程(Poisson或Hawkes)建模的时间交互网络。分别采用非回溯谱统计量和似然累积和(CUSUM)检验方法

Result: 静态模型中检测阈值由k^2 * chi^2 ~ Delta^2/[p(1-p)]控制，当低于c*log n时检测不可能，高于C*log n时谱方法成功。时间模型中检测由KL信息率I控制，阈值T*I >= log n，CUSUM检验在误报水平α下达到近似最优延迟|log α|/I

Conclusion: 建立了网络攻击检测的统一理论框架，给出了匹配信息论极限的检测阈值条件，证明了所提方法的理论最优性，并量化了对边扰动的鲁棒性，为实际系统设计提供了理论指导

Abstract: We develop a consolidated theory for the detectability of network-borne
attacks under two canonical observation models: (i) a static graph drawn from
an Erdos-Renyi background with a planted anomalous community, and (ii) a
temporal interaction network modeled by multivariate point processes (Poisson
or Hawkes). Our main contribution is to match, up to universal constants,
information-theoretic lower and upper bounds that govern when reliable testing
is possible. In the static case, the core quantity is the accumulated edgewise
signal k^2 * chi^2(Bern(p+Delta) || Bern(p)), where chi^2 ~ Delta^2 / [p(1-p)]
for small Delta; detection is impossible when this falls below c * log n, and a
non-backtracking spectral statistic succeeds above C * log n. In the temporal
case, detectability is controlled by the KL information rate I contributed by
internal edges over a window of length T, yielding a threshold T I >= log n; a
likelihood-based cumulative-sum (CUSUM) test achieves first-order optimal delay
approximately abs(log alpha) / I at false-alarm level alpha. We also quantify
robustness to bounded edge perturbations and outline conditional
statistical-computational separations. A brief case study shows how to turn
these bounds into concrete design choices.

</details>


### [80] [Rate-Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees](https://arxiv.org/abs/2509.11054)
*Thomas Y. Chen*

Main category: cs.IT

TL;DR: 本文建立了多模态检索的首个信息论极限，通过次序排名作为损失源编码推导出速率-头真函数，并证明了一种适应性随机量化器能够达到理论极限。


<details>
  <summary>Details</summary>
Motivation: 解决多模态检索中的信息论基础问题，因为现有方法缺乏理论支撑来确定查询需要多少位才能获得高质量检索结果。

Method: 将排名问题演绎为损失源编码问题，推导出速率-头真函数R(D)，证明了反向边界包含模态平衡项和偏斜罚项。构建了随机量化器与适应性温度解码器。

Result: 实验结果显示该方法在控制性高斯混合和Flickr30k数据集上能够达到理论极限的2%以内，显著优于固定温度和简单CLIP基线。

Conclusion: 研究为多模态检索提供了原则性的信息论基础，答复了高质量检索所需的位数问题，为知识感知对比学习、持续学习检索和检索增强生成器提供了设计指南。

Abstract: We establish the first information-theoretic limits for multimodal retrieval.
Casting ranking as lossy source coding, we derive a single-letter
rate-distortion function $R(D)$ for reciprocal-rank distortion and prove a
converse bound that splits into a modality-balanced term plus a skew penalty
$\kappa\,\Delta H$ capturing entropy imbalance and cross-modal redundancy. We
then construct an explicit entropy-weighted stochastic quantizer with an
adaptive, per-modality temperature decoder; a Blahut-Arimoto argument shows
this scheme achieves distortion within $O(n^{-1})$ of $R(D)$ using $n$ training
triples. A VC-type analysis yields the first finite-sample excess-risk bound
whose complexity scales sub-linearly in both the number of modalities and the
entropy gap. Experiments on controlled Gaussian mixtures and Flickr30k confirm
that our adaptive codes sit within two percentage points of the theoretical
frontier, while fixed-temperature and naive CLIP baselines lag significantly.
Taken together, our results give a principled answer to "how many bits per
query are necessary" for high-quality multimodal retrieval and provide design
guidance for entropy-aware contrastive objectives, continual-learning
retrievers, and retrieval-augmented generators.

</details>


### [81] [Active Sequential Hypothesis Testing with Non-Homogeneous Costs](https://arxiv.org/abs/2509.11632)
*George Vershinin,Asaf Cohen,Omer Gurewitz*

Main category: cs.IT

TL;DR: 非齐次序贯假设检验中，通过优化期望信息增益与期望成本的比率而非传统的每步信息成本比，显著降低了平均成本


<details>
  <summary>Details</summary>
Motivation: 研究非齐次序贯假设检验问题，其中决策者需要选择具有异质正成本的动作来识别真实假设，在平均错误约束下最小化总期望成本

Method: 采用标准论证将目标分解为样本均值与策略诱导的每动作平均成本的乘积，提出优化期望信息增益与期望成本比率的设计原则，并将Chernoff方案适配到非齐次设置

Result: 适配的Chernoff方案保持了经典的log(1/δ)缩放特性，在模拟中将平均成本相对于经典Chernoff策略降低了50%，相对于朴素的信息成本比启发式方法降低了90%

Conclusion: 在非齐次序贯假设检验中，优化期望信息增益与期望成本的比率是比传统每步信息成本比更有效的设计原则，能够显著降低总成本

Abstract: We study the Non-Homogeneous Sequential Hypothesis Testing (NHSHT), where a
single active Decision-Maker (DM) selects actions with heterogeneous positive
costs to identify the true hypothesis under an average error constraint
\(\delta\), while minimizing expected total cost paid. Under standard
arguments, we show that the objective decomposes into the product of the mean
number of samples and the mean per-action cost induced by the policy. This
leads to a key design principle: one should optimize the ratio of expectations
(expected information gain per expected cost) rather than the expectation of
per-step information-per-cost ("bit-per-buck"), which can be suboptimal. We
adapt the Chernoff scheme to NHSHT, preserving its classical \(\log 1/\delta\)
scaling. In simulations, the adapted scheme reduces mean cost by up to 50\%
relative to the classic Chernoff policy and by up to 90\% relative to the naive
bit-per-buck heuristic.

</details>


### [82] [Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications](https://arxiv.org/abs/2509.11636)
*Shiyao Jiang,Jian Jiao,Xingjian Zhang,Ye Wang,Dusit Niyato,Qinyu Zhang*

Main category: cs.IT

TL;DR: 提出TALSC框架，通过根据任务损失评估样本重要性的样本稳健性模块（SCM）和可学习加权知识库（LW-KB），有效缓解图像任务无关语义通信中的标签翻转噪声和类不平衡问题，实现更高的语义恢复准确性和结构相似性。


<details>
  <summary>Details</summary>
Motivation: 面对6G网络中多样化大规模数据的传输需求，需要构建任务无关的语义通信系统以提供稳健智能服务。实际应用中知识库存在异构数据偏差（标签翻转噪声和类不平衡），影响语义恢复的稳健性。

Method: 1. 设计TALSC框架，包含作为元学习器的SCM模块和作为学习器的语义编码网络
2. 学习器基于可学习加权知识库（LW-KB）的经验知识更新
3. 元学习器根据任务损失反馈评估样本重要性，调整学习器更新策略
4. 设计SCM-GE方法，嵌入Kolmogorov-Arnold网络（KAN）以平衡参数数量和评估精度

Result: 模拟实验显示，TALSC框架有效缓解了标签翻转噪声和类不平衡问题，在任务无关图像语义通信中实现了至少12%更高的语义恢复准确性（SRA）和多尺度结构相似性（MS-SSIM），超过现有最优方法。

Conclusion: TALSC框架通过样本重要性评估和可学习加权知识库的结合，有效提升了任务无关语义通信系统在异构数据环境下的稳健性和性能，为6G网络的智能服务提供了可靠的技术支撑。

Abstract: With the emergence of diverse and massive data in the upcoming
sixth-generation (6G) networks, the task-agnostic semantic communication system
is regarded to provide robust intelligent services. In this paper, we propose a
task-agnostic learnable weighted-knowledge base semantic communication (TALSC)
framework for robust image transmission to address the real-world heterogeneous
data bias in KB, including label flipping noise and class imbalance. The TALSC
framework incorporates a sample confidence module (SCM) as meta-learner and the
semantic coding networks as learners. The learners are updated based on the
empirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,
the meta-learner evaluates the significance of samples according to the task
loss feedback, and adjusts the update strategy of learners to enhance the
robustness in semantic recovery for unknown tasks. To strike a balance between
SCM parameters and precision of significance evaluation, we design an SCM-grid
extension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)
within SCM, which leverages the concept of spline refinement in KAN and enables
scalable SCM with customizable granularity without retraining. Simulations
demonstrate that the TALSC framework effectively mitigates the effects of
flipping noise and class imbalance in task-agnostic image semantic
communication, achieving at least 12% higher semantic recovery accuracy (SRA)
and multi-scale structural similarity (MS-SSIM) compared to state-of-the-art
methods.

</details>


### [83] [Reflexive Partitions Induced by Rank Support and Non-Reflexive Partitions Induced by Rank Weight](https://arxiv.org/abs/2509.11681)
*Yang Xu,Haibin Kan,Guangyue Han*

Main category: cs.IT

TL;DR: 本文研究了有限模上由秩支撑和秩权重诱导的划分，证明了秩支撑划分的互相对偶性和自反性，并计算了相关的广义Krawtchouk矩阵。对于有限链环上的自由模，证明了当环不是域时秩权重划分是非自反的，并明确刻画了对偶划分。


<details>
  <summary>Details</summary>
Motivation: 研究有限模上秩支撑和秩权重诱导的划分性质，特别是它们的对偶性和自反性，以及这些性质与基础环结构的关系，解决关于秩度量是否诱导关联方案的开放性问题。

Method: 使用非退化配对方法研究划分的互相对偶性，计算广义Krawtchouk矩阵，分析有限链环上自由模的秩权重划分性质，并通过转置等价关系刻画对偶划分。

Result: 证明了秩支撑划分是自反的且具有互相对偶性，而有限链环（非域）上的秩权重划分是非自反的，秩度量在非域环上不诱导关联方案。

Conclusion: 秩支撑划分具有良好的对偶性质，而秩权重划分的自反性取决于基础环是否为域，这一结果解决了相关开放性问题并深化了对有限环上秩度量结构的理解。

Abstract: In this paper, we study partitions of finite modules induced by rank support
and rank weight. First, we show that partitions induced by rank support are
mutually dual with respect to suitable non-degenerate pairings, and hence are
reflexive; moreover, we compute the associated generalized Krawtchouk matrices.
Similar results are established for partitions induced by isomorphic relation
of rank support. These results generalize counterpart results established for
row space partitions and rank partitions of matrix spaces over finite fields.
Next, we show that partitions of free modules over a finite chain ring $R$
induced by rank weight are non-reflexive provided that $R$ is not a field;
moreover, we characterize the dual partitions explicitly. As a corollary, we
show that rank partitions of matrix spaces over $R$ are reflexive if and only
if $R$ is a field; moreover, two matrices belong to the same member of the dual
partition if and only if their transposes are equivalent. In particular, we
show that opposite to matrices over finite fields, rank metric does not induce
an association scheme provided that $R$ is not a field, which further settles
an open question proposed by Blanco-Chac\'{o}n, Boix, Greferath and Hieta-Aho
in \cite{2}.

</details>


### [84] [Permutation decoding of first-order Generalized Reed-Muller codes](https://arxiv.org/abs/2509.11757)
*José Joaquín Bernal,Juan Jacobo Simón*

Main category: cs.IT

TL;DR: 本文提出了一种改进的置换解码算法变体，专门针对一阶广义Reed-Muller码族，能够比现有方法纠正更多错误，并从概率角度分析了算法使用较小PD集的条件。


<details>
  <summary>Details</summary>
Motivation: 针对二进制仿射不变码（特别是一阶Reed-Muller码）的经典置换解码算法进行改进，扩展到一阶广义Reed-Muller码族，旨在提高纠错能力。

Method: 开发了一种改进的置换解码算法变体，专门适用于一阶广义Reed-Muller码，并通过概率分析方法确定算法何时可以使用较小的PD类集合。

Result: 实验示例表明，与已知结果相比，该方法能够纠正更多错误，提高了纠错性能。

Conclusion: 该改进的置换解码算法在一阶广义Reed-Muller码上表现出更好的纠错能力，且从概率角度提供了使用较小解码集合的条件分析，为实际应用提供了理论指导。

Abstract: In [4] we describe a variation of the classical permutation decoding
algorithm that can be applied to any binary affine-invariant code; in
particular, it can be applied to first-order Reed-Muller codes successfully. In
this paper we study how to implement it for the family of first-order
Generalized Reed-Muller codes. Then, we give examples which show that we
improve the number of errors we can correct in comparison with the known
results for this family of codes. Finally, we deal, from a probabilistic point
of view, with the problem of determining when the algorithm only needs to use a
smaller PD-like set.

</details>


### [85] [Energy Efficiency Maximization for Movable Antenna-Enhanced MIMO Downlink System Based on S-CSI](https://arxiv.org/abs/2509.12036)
*Xintai Chen,Biqian Feng,Yongpeng Wu,Xiang-Gen Xia,Chengshan Xiao*

Main category: cs.IT

TL;DR: 提出了一种基于可移动天线(MA)的多用户MIMO下行系统，通过联合优化预编码矩阵和天线位置向量来最大化能量效率(EE)，在统计信道状态信息下实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在能量效率方面存在局限，可移动天线技术能够通过优化天线位置来改善信道条件，从而提升系统性能。

Method: 采用确定性等效技术和交替优化算法，分别优化发射变量(预编码矩阵和发射天线位置向量)和接收天线位置向量，并针对单用户场景进行了算法定制。

Result: 数值结果表明，所提出的MA增强系统相比基于统计信道状态信息的基准方案能显著提高系统能量效率，且在有限移动区域内即可达到最优性能。

Conclusion: 可移动天线技术在多用户MIMO系统中具有巨大潜力，能够有效提升能量效率，为未来无线通信系统设计提供了新的技术路径。

Abstract: This paper presents an innovative movable antenna (MA)-enhanced multi-user
multiple-input multiple-output (MIMO) downlink system. We aim to maximize the
energy efficiency (EE) under statistical channel state information (S-CSI)
through a joint optimization of the precoding matrix and the antenna position
vectors (APVs). To solve the resulting stochastic problem, we first resort to
deterministic equivalent (DE) tecnology to formulate the deterministic
minorizing function of the system EE and the deterministic function of each
user terminal (UT)'s average achievable rate w.r.t. the transmit variables
(i.e., the precoding matrix and the transmit APV) and the corresponding receive
APV, respectively. Then, we propose an alternating optimization (AO) algorithm
to alternatively optimize the transmit variables and the receive APVs to
maximize the formulated deterministic objective functions, respectively.
Finally, the above AO algorithm is tailored for the single-user scenario. Our
numerical results reveal that, the proposed MA-enhanced system can
significantly improve the system EE compared to several benchmark schemes based
on the S-CSI and the optimal performance can be achieved with a finite size of
movement regions for MAs.

</details>


### [86] [Secure Semantic Communication over Wiretap Channels: Rate-Distortion-Equivocation Tradeoff](https://arxiv.org/abs/2509.12142)
*Denis Kozlov,Mahtab Mirmohseni,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 这篇论文研究了一种基于信息论的安全语义通信模型，考虑具有分离保真度和保密约束的损失联合源通道编码问题，并推导了速率-失真-流减区域的上下界。


<details>
  <summary>Details</summary>
Motivation: 为了研究安全语义通信系统，考虑包含相关语义和观测信息的源，以及编码器对源访问范围对系统性能的影响。

Method: 采用损失联合源通道编码(JSCC)方法，提出了包含两个私有部分的新题随机叠加编码方案，并涉及两种源访问情况：Case 1（仅访问观测源）和Case 2（访问语义和观测源）。

Result: 推导出了单字母反向和可达性界限，具体化到高斯和伪诺利分布的情况。数值评估显示了推导界限的有效性。

Conclusion: 该研究提供了一种通用的安全语义通信理论框架，并将现有的源编码和源通道编码问题进行了推广，为实际应用提供了理论指导。

Abstract: This paper investigates an information-theoretic model of secure
semantic-aware communication. For this purpose, we consider the lossy joint
source-channel coding (JSCC) of a memoryless semantic source transmitted over a
memoryless wiretap channel. The source consists of two correlated parts that
represent semantic and observed aspects of the information. Our model assumes
separate fidelity and secrecy constraints on each source component and, in
addition, encompasses two cases for the source output, in order to evaluate the
performance gains if the encoder has an extended access to the source.
Specifically, in Case 1, the encoder has direct access only to the samples from
a single (observed) source component, while in Case 2 it has additional direct
access to the samples of the underlaying semantic information. We derive
single-letter converse and achievability bounds on the
rate-distortion-equivocation region. The converse bound explicitly contains
rate-distortion functions, making it easy to evaluate, especially for some
common distributions. The proposed achievability coding scheme involves novel
stochastic superposition coding with two private parts to enable analysis of
the equivocation for each source component, separately. Our results generalise
some of the previously established source and source-channel coding problems.
The general results are further specialised to Gaussian and Bernoulli sources
transmitted over Gaussian and binary wiretap channels, respectively. The
numerical evaluations illustrate the derived bounds for these distributions.

</details>


### [87] [Information Entropy-Based Scheduling for Communication-Efficient Decentralized Learning](https://arxiv.org/abs/2507.17426)
*Jaiprakash Nagar,Zheng Chen,Marios Kountouris,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 提出基于信息熵的节点和链路调度策略，在资源受限网络中实现高效的去中心化随机梯度下降，显著降低通信成本并加速收敛


<details>
  <summary>Details</summary>
Motivation: 解决资源受限网络中D-SGD算法的通信效率问题，传统方法如介数中心性和MATCHA在通信预算有限时性能不佳

Method: 基于信息熵设计节点和链路重要性度量，随机激活不相交的节点或链路子集，在给定通信成本约束下优化调度概率

Result: 节点调度方面比BC方法收敛更快，通信预算降低60%；高预算时性能相当或更优；链路调度性能与MATCHA相当或更优

Conclusion: 基于信息熵的调度策略能有效提升D-SGD在资源受限网络中的通信效率，为分布式学习提供实用的低通信成本解决方案

Abstract: This paper addresses decentralized stochastic gradient descent (D-SGD) over
resource-constrained networks by introducing node-based and link-based
scheduling strategies to enhance communication efficiency. In each iteration of
the D-SGD algorithm, only a few disjoint subsets of nodes or links are randomly
activated, subject to a given communication cost constraint. We propose a novel
importance metric based on information entropy to determine node and link
scheduling probabilities. We validate the effectiveness of our approach through
extensive simulations, comparing it against state-of-the-art methods, including
betweenness centrality (BC) for node scheduling and \textit{MATCHA} for link
scheduling. The results show that our method consistently outperforms the
BC-based method in the node scheduling case, achieving faster convergence with
up to 60\% lower communication budgets. At higher communication budgets (above
60\%), our method maintains comparable or superior performance. In the link
scheduling case, our method delivers results that are superior to or on par
with those of \textit{MATCHA}.

</details>
