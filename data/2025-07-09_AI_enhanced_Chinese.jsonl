{"id": "2507.05567", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05567", "abs": "https://arxiv.org/abs/2507.05567", "authors": ["Chaofeng Guan", "Shitao Li", "Gaojun Luo", "Zhi Ma", "Hong Wang"], "title": "Lower Bounds for Error Coefficients of Griesmer Optimal Linear Codes via Iteration", "comment": "15 pages, 4 tables", "summary": "The error coefficient of a linear code is defined as the number of\nminimum-weight codewords. In an additive white Gaussian noise channel, optimal\nlinear codes with the smallest error coefficients achieve the best possible\nasymptotic frame error rate (AFER) among all optimal linear codes under maximum\nlikelihood decoding. Such codes are referred to as AFER-optimal linear codes.\n  The Griesmer bound is essential for determining the optimality of linear\ncodes. However, establishing tight lower bounds on the error coefficients of\nGriesmer optimal linear codes is challenging, and the linear programming bound\noften performs inadequately. In this paper, we propose several iterative lower\nbounds for the error coefficients of Griesmer optimal linear codes.\nSpecifically, for binary linear codes, our bounds are tight in most cases when\nthe dimension does not exceed $5$. To evaluate the performance of our bounds\nwhen they are not tight, we also determine the parameters of the remaining\n5-dimensional AFER-optimal linear codes. Our final comparison demonstrates that\neven when our bounds are not tight, they remain very close to the actual\nvalues, with a gap of less than or equal to $2$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u79cd\u8fed\u4ee3\u4e0b\u754c\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1Griesmer\u6700\u4f18\u7ebf\u6027\u7801\u7684\u9519\u8bef\u7cfb\u6570\uff0c\u5e76\u5728\u5927\u591a\u6570\u7ef4\u5ea6\u4e0d\u8d85\u8fc75\u7684\u4e8c\u8fdb\u5236\u7ebf\u6027\u7801\u4e2d\u5b9e\u73b0\u4e86\u7d27\u81f4\u6027\u3002", "motivation": "\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\u4fe1\u9053\u4e2d\uff0c\u5177\u6709\u6700\u5c0f\u9519\u8bef\u7cfb\u6570\u7684\u6700\u4f18\u7ebf\u6027\u7801\u5728\u6700\u5927\u4f3c\u7136\u89e3\u7801\u4e0b\u80fd\u8fbe\u5230\u6700\u4f73\u6e10\u8fd1\u5e27\u9519\u8bef\u7387\uff08AFER\uff09\u3002\u7136\u800c\uff0c\u786e\u5b9aGriesmer\u6700\u4f18\u7ebf\u6027\u7801\u7684\u9519\u8bef\u7cfb\u6570\u7684\u7d27\u81f4\u4e0b\u754c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u79cd\u8fed\u4ee3\u4e0b\u754c\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u4e8c\u8fdb\u5236\u7ebf\u6027\u7801\uff0c\u5728\u7ef4\u5ea6\u4e0d\u8d85\u8fc75\u65f6\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7d27\u81f4\u6027\u3002", "result": "\u5bf9\u4e8e\u7ef4\u5ea6\u4e0d\u8d85\u8fc75\u7684\u4e8c\u8fdb\u5236\u7ebf\u6027\u7801\uff0c\u63d0\u51fa\u7684\u4e0b\u754c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u662f\u7d27\u81f4\u7684\uff1b\u5bf9\u4e8e\u4e0d\u7d27\u81f4\u7684\u60c5\u51b5\uff0c\u4e0e\u5b9e\u9645\u503c\u7684\u5dee\u8ddd\u4e0d\u8d85\u8fc72\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0b\u754c\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u5728\u4e0d\u7d27\u81f4\u65f6\u4e5f\u63a5\u8fd1\u5b9e\u9645\u503c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.05718", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05718", "abs": "https://arxiv.org/abs/2507.05718", "authors": ["Hang Que", "Jie Yang", "Tao Du", "Shuqiang Xia", "Chao-Kai Wen", "Shi Jin"], "title": "Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems", "comment": "Accepted by IEEE Transactions on Communications", "summary": "Simultaneous localization and mapping (SLAM) plays a critical role in\nintegrated sensing and communication (ISAC) systems for sixth-generation (6G)\nmillimeter-wave (mmWave) networks, enabling environmental awareness and precise\nuser equipment (UE) positioning. While cooperative multi-user SLAM has\ndemonstrated potential in leveraging distributed sensing, its application\nwithin multi-modal ISAC systems remains limited, particularly in terms of\ntheoretical modeling and communication-layer integration. This paper proposes a\nnovel multi-modal SLAM framework that addresses these limitations through three\nkey contributions. First, a Bayesian estimation framework is developed for\ncooperative multi-user SLAM, along with a two-stage algorithm for robust radio\nmap construction under dynamic and heterogeneous sensing conditions. Second, a\nmulti-modal localization strategy is introduced, fusing SLAM results with\ncamera-based multi-object tracking and inertial measurement unit (IMU) data via\nan error-aware model, significantly improving UE localization in multi-user\nscenarios. Third, a sensing-aided beam management scheme is proposed, utilizing\nglobal radio maps and localization data to generate UE-specific prior\ninformation for beam selection, thereby reducing inter-user interference and\nenhancing downlink spectral efficiency. Simulation results demonstrate that the\nproposed system improves radio map accuracy by up to 60%, enhances localization\naccuracy by 37.5%, and significantly outperforms traditional methods in both\nindoor and outdoor environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f30\u8ba1\u3001\u591a\u6a21\u6001\u5b9a\u4f4d\u7b56\u7565\u548c\u611f\u77e5\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e866G\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u7684\u5b9a\u4f4d\u548c\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001ISAC\u7cfb\u7edf\u4e2d\u534f\u540c\u591a\u7528\u6237SLAM\u7684\u7406\u8bba\u5efa\u6a21\u548c\u901a\u4fe1\u5c42\u96c6\u6210\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1. \u5f00\u53d1\u8d1d\u53f6\u65af\u4f30\u8ba1\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1b2. \u5f15\u5165\u591a\u6a21\u6001\u5b9a\u4f4d\u7b56\u7565\uff1b3. \u63d0\u51fa\u611f\u77e5\u8f85\u52a9\u6ce2\u675f\u7ba1\u7406\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u5728\u65e0\u7ebf\u7535\u5730\u56fe\u7cbe\u5ea6\u4e0a\u63d0\u534760%\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u63d0\u534737.5%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u7528\u6237\u573a\u666f\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u73af\u5883\u611f\u77e5\u548c\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2507.05781", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05781", "abs": "https://arxiv.org/abs/2507.05781", "authors": ["Bole Liu", "Li Qiao", "Ye Wang", "Zhen Gao", "Yu Ma", "Keke Ying", "Tong Qin"], "title": "Text-Guided Token Communication for Wireless Image Transmission", "comment": null, "summary": "With the emergence of 6G networks and proliferation of visual applications,\nefficient image transmission under adverse channel conditions is critical. We\npresent a text-guided token communication system leveraging pre-trained\nfoundation models for wireless image transmission with low bandwidth. Our\napproach converts images to discrete tokens, applies 5G NR polar coding, and\nemploys text-guided token prediction for reconstruction. Evaluations on\nImageNet show our method outperforms Deep Source Channel Coding with Attention\nModules (ADJSCC) in perceptual quality and semantic preservation at\nSignal-to-Noise Ratios (SNRs) above 0 dB while mitigating the cliff effect at\nlower SNRs. Our system requires no scenario-specific retraining and exhibits\nsuperior cross-dataset generalization, establishing a new paradigm for\nefficient image transmission aligned with human perceptual priorities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u6587\u672c\u5f15\u5bfc\u4ee4\u724c\u901a\u4fe1\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f4e\u5e26\u5bbd\u4e0b\u7684\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u548c\u89c6\u89c9\u5e94\u7528\u7684\u666e\u53ca\uff0c\u6076\u52a3\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u56fe\u50cf\u4f20\u8f93\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u5e94\u75285G NR\u6781\u6027\u7f16\u7801\uff0c\u5e76\u5229\u7528\u6587\u672c\u5f15\u5bfc\u4ee4\u724c\u9884\u6d4b\u8fdb\u884c\u91cd\u5efa\u3002", "result": "\u5728ImageNet\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728SNR\u9ad8\u4e8e0 dB\u65f6\u5728\u611f\u77e5\u8d28\u91cf\u548c\u8bed\u4e49\u4fdd\u7559\u4e0a\u4f18\u4e8eADJSCC\uff0c\u5e76\u5728\u4f4eSNR\u4e0b\u7f13\u89e3\u4e86\u60ac\u5d16\u6548\u5e94\u3002", "conclusion": "\u65e0\u9700\u573a\u666f\u7279\u5b9a\u91cd\u8bad\u7ec3\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9ad8\u6548\u56fe\u50cf\u4f20\u8f93\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.05784", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05784", "abs": "https://arxiv.org/abs/2507.05784", "authors": ["Kan Yu", "Wenxu Wang", "Xiaowu Liu", "Yujia Zhao", "Qixun Zhang", "Zhiyong Feng", "Dong Li"], "title": "Does Movable Antenna Present A Dual-edged Nature? From the Perspective of Physical Layer Security: A Joint Design of Fixed-position Antenna and Movable Antenna", "comment": null, "summary": "In conventional artificial noise (AN)-aided physical-layer security systems,\nfixed-position antenna (FPA) arrays exhibit inherent vulnerability to coverage\ngaps due to their static spatial configuration. Adversarial eavesdroppers can\nstrategically exploit their mobility to infiltrate these spatial nulls of AN\nradiation patterns, thereby evading interference suppression and successfully\nintercepting the confidential communication. To overcome this limitation, in\nthis paper, we investigate a hybrid antenna deployment framework integrating\nFPA arrays and movable antenna (MA) arrays (denoted by FMA co-design) to\naddress the security performance in dynamic wireless environments, based on the\nfact that MA arrays enable channel reconfiguration through localized antenna\nrepositioning, achieving more higher spatial degree of freedom (DoF). Enabled\nby FMA co-design framework, FPA arrays ensure baseline connectivity for\nlegitimate links while MA arrays function as dynamic security enhancers,\nreplacing conventional static AN generation. Furthermore, we formulate a\nnon-convex optimization problem of the secrecy rate maximization through\njointly optimizing MA positioning, FPA beamforming, and MA beamforming under\npractical constraints. the solution employs a dual-algorithm approach: Nesterov\nmomentum-based projected gradient ascent (NMPGA) accelerates convergence in\ncontinuous position optimization, while alternating optimization (AO) handles\ncoupled beamforming design. Experimental evaluations demonstrate that the\nproposed FMA co-design framework achieves significant secrecy performance gains\nover individual optimization benchmarks, yielding 42.34% and 9.12% improvements\nin secrecy rate compared to isolated FPA for AN generation and MA for\nconfidential information baselines, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5929\u7ebf\u90e8\u7f72\u6846\u67b6\uff08FMA co-design\uff09\uff0c\u7ed3\u5408\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\uff08FPA\uff09\u548c\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u9635\u5217\uff0c\u4ee5\u63d0\u5347\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u7269\u7406\u5c42\u5b89\u5168\u6027\u80fd\u3002\u901a\u8fc7\u4f18\u5316MA\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fdd\u5bc6\u7387\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u9635\u5217\u5728\u7269\u7406\u5c42\u5b89\u5168\u7cfb\u7edf\u4e2d\u5b58\u5728\u8986\u76d6\u6f0f\u6d1e\uff0c\u6613\u88ab\u79fb\u52a8\u7a83\u542c\u8005\u5229\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u7ed3\u5408FPA\u548cMA\u7684\u6df7\u5408\u90e8\u7f72\u6846\u67b6\u3002", "method": "\u63d0\u51faFMA co-design\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316MA\u4f4d\u7f6e\u3001FPA\u6ce2\u675f\u6210\u5f62\u548cMA\u6ce2\u675f\u6210\u5f62\uff0c\u91c7\u7528NMPGA\u548cAO\u7b97\u6cd5\u6c42\u89e3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFMA co-design\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u5bc6\u6027\u80fd\uff0c\u4fdd\u5bc6\u7387\u5206\u522b\u6bd4\u5355\u72ec\u4f7f\u7528FPA\u548cMA\u63d0\u9ad8\u4e8642.34%\u548c9.12%\u3002", "conclusion": "FMA co-design\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u5929\u7ebf\u9635\u5217\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.05267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05267", "abs": "https://arxiv.org/abs/2507.05267", "authors": ["Markus B\u00f6ck"], "title": "Strongly Solving $7 \\times 6$ Connect-Four on Consumer Grade Hardware", "comment": null, "summary": "While the game Connect-Four has been solved mathematically and the best move\ncan be effectively computed with search based methods, a strong solution in the\nform of a look-up table was believed to be infeasible. In this paper, we\nrevisit a symbolic search method based on binary decision diagrams to produce\nstrong solutions. With our efficient implementation we were able to produce a\n89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main\nmemory for the standard $7 \\times 6$ board size. In addition to this\nwin-draw-loss evaluation, we include an alpha-beta search in our open source\nartifact to find the move which achieves the fastest win or slowest loss.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\uff08\u57fa\u4e8e\u4e8c\u5143\u51b3\u7b56\u56fe\uff09\u4e3aConnect-Four\u6e38\u620f\u751f\u6210\u5f3a\u89e3\uff0c\u5b9e\u73b0\u4e8689.6 GB\u7684\u67e5\u627e\u8868\uff0c\u5e76\u5728\u5f00\u6e90\u5de5\u5177\u4e2d\u52a0\u5165\u4e86alpha-beta\u641c\u7d22\u4ee5\u4f18\u5316\u80dc\u8d1f\u8def\u5f84\u3002", "motivation": "\u5c3d\u7ba1Connect-Four\u6e38\u620f\u5df2\u6709\u6570\u5b66\u89e3\u6cd5\uff0c\u4f46\u5f3a\u89e3\u7684\u67e5\u627e\u8868\u88ab\u8ba4\u4e3a\u4e0d\u53ef\u884c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\u7684\u9ad8\u6548\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e8c\u5143\u51b3\u7b56\u56fe\u7684\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\uff0c\u9ad8\u6548\u751f\u6210\u67e5\u627e\u8868\uff0c\u5e76\u7ed3\u5408alpha-beta\u641c\u7d22\u4f18\u5316\u80dc\u8d1f\u8def\u5f84\u3002", "result": "\u5728\u5355\u6838CPU\u548c128 GB\u5185\u5b58\u4e0b\uff0c47\u5c0f\u65f6\u5185\u751f\u6210\u4e8689.6 GB\u7684\u67e5\u627e\u8868\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5de5\u5177\u3002", "conclusion": "\u8bc1\u660e\u4e86\u7b26\u53f7\u641c\u7d22\u65b9\u6cd5\u5728\u751f\u6210\u5f3a\u89e3\u67e5\u627e\u8868\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.05813", "categories": ["cs.IT", "cs.ET", "cs.MS", "cs.PF", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05813", "abs": "https://arxiv.org/abs/2507.05813", "authors": ["Ferhat Bayar", "Onur Salan", "Erdogan Aydin", "Haci Ilhan"], "title": "Adaptive Communication Through Exploiting RIS, SSK, and CIM for Improved Reliability and Efficiency", "comment": null, "summary": "In this paper, we present a novel communication system model that integrates\nreconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and code\nindex modulation (CIM) based on Hadamard coding called RIS based transmit\nSSK-CIM (RIS-CIM-TSSK). By leveraging RIS, the system adapts rapidly to dynamic\nenvironments, enhancing error rates and overall reliability. SSK facilitates\nthe transmission of additional passive information while eliminating the need\nfor multiple radio frequency (RF) chains, thereby reducing complexity. CIM\nenhances passive information transmission through frequency domain spreading,\nwhich may increase signal obfuscation. This proposed scheme not only improves\nenergy efficiency but also offers a robust solution for reliable communication\nin modern wireless networks, paving the way for smarter and more adaptable\nimplementations. We consider a suboptimal, low-complexity detector for the\nproposed scheme and also address the blind case for phase adjustment of the\nRIS. Finally, we present the simulation results for the proposed system model\nacross various configurations, including different numbers of receive and\ntransmit antennas, varying reflecting elements of the RIS, and different code\nlengths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRIS\u3001SSK\u548cCIM\u7684\u65b0\u578b\u901a\u4fe1\u7cfb\u7edf\u6a21\u578bRIS-CIM-TSSK\uff0c\u901a\u8fc7RIS\u52a8\u6001\u9002\u5e94\u73af\u5883\uff0c\u63d0\u5347\u80fd\u6548\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u548c\u80fd\u6548\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u3002", "method": "\u7ed3\u5408RIS\u3001SSK\u548cCIM\u6280\u672f\uff0c\u5229\u7528Hadamard\u7f16\u7801\u5b9e\u73b0\u88ab\u52a8\u4fe1\u606f\u4f20\u8f93\uff0c\u63d0\u51fa\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u63d0\u5347\u4e86\u80fd\u6548\u548c\u901a\u4fe1\u53ef\u9760\u6027\u3002", "conclusion": "RIS-CIM-TSSK\u4e3a\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05597", "categories": ["cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.05597", "abs": "https://arxiv.org/abs/2507.05597", "authors": ["Yiming Zhao", "Xuanqi Meng", "Xinyu Tong", "Xiulong Liu", "Xin Xie", "Wenyu Qu"], "title": "Baton: Compensate for Missing Wi-Fi Features for Practical Device-free Tracking", "comment": "17 pages, 20 figures. Accepted and published in IEEE Transactions on\n  Mobile Computing on April 10, 2025. This is the accepted version. Final\n  published version: https://ieeexplore.ieee.org/document/10962318", "summary": "Wi-Fi contact-free sensing systems have attracted widespread attention due to\ntheir ubiquity and convenience. The integrated sensing and communication (ISAC)\ntechnology utilizes off-the-shelf Wi-Fi communication signals for sensing,\nwhich further promotes the deployment of intelligent sensing applications.\nHowever, current Wi-Fi sensing systems often require prolonged and unnecessary\ncommunication between transceivers, and brief communication interruptions will\nlead to significant performance degradation. This paper proposes Baton, the\nfirst system capable of accurately tracking targets even under severe Wi-Fi\nfeature deficiencies. To be specific, we explore the relevance of the Wi-Fi\nfeature matrix from both horizontal and vertical dimensions. The horizontal\ndimension reveals feature correlation across different Wi-Fi links, while the\nvertical dimension reveals feature correlation among different time slots.\nBased on the above principle, we propose the Simultaneous Tracking And\nPredicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi\nfeatures over time and across different links, akin to passing a baton. We\nimplement the system on commercial devices, and the experimental results show\nthat our system outperforms existing solutions with a median tracking error of\n0.46m, even when the communication duty cycle is as low as 20.00%. Compared\nwith the state-of-the-art, our system reduces the tracking error by 79.19% in\nscenarios with severe Wi-Fi feature deficiencies.", "AI": {"tldr": "Baton\u7cfb\u7edf\u901a\u8fc7\u6c34\u5e73\u548c\u5782\u76f4\u7ef4\u5ea6\u5206\u6790Wi-Fi\u7279\u5f81\u77e9\u9635\u76f8\u5173\u6027\uff0c\u63d0\u51faSTAP\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347Wi-Fi\u611f\u77e5\u7cfb\u7edf\u5728\u7279\u5f81\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u76ee\u6807\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Wi-Fi\u611f\u77e5\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u901a\u4fe1\u4e14\u5bf9\u901a\u4fe1\u4e2d\u65ad\u654f\u611f\uff0c\u6027\u80fd\u6613\u53d7\u4e25\u91cd\u5f71\u54cd\u3002", "method": "\u63a2\u7d22Wi-Fi\u7279\u5f81\u77e9\u9635\u5728\u6c34\u5e73\u548c\u5782\u76f4\u7ef4\u5ea6\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51faSTAP\u7b97\u6cd5\u5b9e\u73b0\u7279\u5f81\u65e0\u7f1d\u4f20\u9012\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBaton\u7cfb\u7edf\u5728\u901a\u4fe1\u5360\u7a7a\u6bd4\u4f4e\u81f320%\u65f6\uff0c\u8ffd\u8e2a\u8bef\u5dee\u4e2d\u4f4d\u6570\u4ec5\u4e3a0.46m\uff0c\u8f83\u73b0\u6709\u6280\u672f\u964d\u4f4e79.19%\u3002", "conclusion": "Baton\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86Wi-Fi\u611f\u77e5\u5728\u7279\u5f81\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.05283", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.", "AI": {"tldr": "Chat2SPaT\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u7528\u6237\u5bf9\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u7684\u534a\u7ed3\u6784\u5316\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7cbe\u786e\u7684\u4fe1\u53f7\u76f8\u4f4d\u4e0e\u65f6\u95f4\uff08SPaT\uff09\u7ed3\u679c\uff0c\u51c6\u786e\u7387\u8d85\u8fc794%\u3002", "motivation": "\u4f20\u7edf\u5b9a\u65f6\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u9700\u8981\u7e41\u7410\u7684\u624b\u52a8\u64cd\u4f5c\uff0c\u4e14\u4e00\u4e2a\u4ea4\u53c9\u53e3\u5e38\u5173\u8054\u591a\u4e2a\u8ba1\u5212\uff0c\u5bfc\u81f4\u91cd\u590d\u8f93\u5165\u3002Chat2SPaT\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7LLM\u7406\u89e3\u7528\u6237\u63cf\u8ff0\u5e76\u751f\u6210JSON\u683c\u5f0f\u7684\u76f8\u4f4d\u5e8f\u5217\u548c\u5c5e\u6027\uff0c\u518d\u901a\u8fc7Python\u811a\u672c\u7ec4\u88c5\u5b8c\u6574\u4fe1\u53f7\u63a7\u5236\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChat2SPaT\u5728300\u591a\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u51c6\u786e\u7387\u8d85\u8fc794%\u3002", "conclusion": "Chat2SPaT\u4e3a\u4ea4\u901a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6613\u7528\u7684\u8ba1\u5212\u7ba1\u7406\u5de5\u5177\uff0c\u5e76\u4e3aLLM\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8303\u4f8b\u3002"}}
{"id": "2507.05731", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05731", "abs": "https://arxiv.org/abs/2507.05731", "authors": ["Yuxin Zhang", "Jiahao Yang", "Zhe Chen", "Wenjun Zhu", "Jin Zhao", "Yue Gao"], "title": "A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation", "comment": "11 pages, 12 figures", "summary": "Recently, large vision-language models (LVLMs) unleash powerful analysis\ncapabilities for low Earth orbit (LEO) satellite Earth observation images in\nthe data center. However, fast satellite motion, brief satellite-ground station\n(GS) contact windows, and large size of the images pose a data download\nchallenge. To enable near real-time Earth observation applications (e.g.,\ndisaster and extreme weather monitoring), we should explore how to deploy LVLM\nin LEO satellite networks, and design SpaceVerse, an efficient satellite-ground\nsynergistic LVLM inference system. To this end, firstly, we deploy compact\nLVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs\nto handle computationally intensive tasks. Then, we propose a computing and\ncommunication co-design framework comprised of a progressive confidence network\nand an attention-based multi-scale preprocessing, used to identify on-satellite\ninferring data, and reduce data redundancy before satellite-GS transmission,\nseparately. We implement and evaluate SpaceVerse on real-world LEO satellite\nconstellations and datasets, achieving a 31.2% average gain in accuracy and a\n51.2% reduction in latency compared to state-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpaceVerse\u7684\u9ad8\u6548\u536b\u661f-\u5730\u9762\u534f\u540cLVLM\u63a8\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u56fe\u50cf\u4e0b\u8f7d\u548c\u5b9e\u65f6\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u5feb\u901f\u536b\u661f\u8fd0\u52a8\u3001\u77ed\u6682\u7684\u536b\u661f-\u5730\u9762\u7ad9\u63a5\u89e6\u7a97\u53e3\u4ee5\u53ca\u5927\u5c3a\u5bf8\u56fe\u50cf\u5bfc\u81f4\u6570\u636e\u4e0b\u8f7d\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5730\u7403\u89c2\u6d4b\u5e94\u7528\uff08\u5982\u707e\u5bb3\u548c\u6781\u7aef\u5929\u6c14\u76d1\u6d4b\uff09\u7684\u53d1\u5c55\u3002", "method": "\u5728\u536b\u661f\u4e0a\u90e8\u7f72\u7d27\u51d1\u578bLVLM\u5904\u7406\u8f7b\u91cf\u4efb\u52a1\uff0c\u5730\u9762\u7ad9\u8fd0\u884c\u5e38\u89c4LVLM\u5904\u7406\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\uff1b\u63d0\u51fa\u8ba1\u7b97\u4e0e\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5305\u62ec\u6e10\u8fdb\u7f6e\u4fe1\u7f51\u7edc\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5c3a\u5ea6\u9884\u5904\u7406\u3002", "result": "\u5728\u771f\u5b9eLEO\u536b\u661f\u661f\u5ea7\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b031.2%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u548c51.2%\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "SpaceVerse\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f-\u5730\u9762\u534f\u540cLVLM\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u65f6\u5730\u7403\u89c2\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05878", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05878", "abs": "https://arxiv.org/abs/2507.05878", "authors": ["Yujia Zhao", "Zhiyong Feng", "Kan Yu", "Qixun Zhang", "Dong Li"], "title": "An Effective Equivalence Model of Analyzing PLS of Multiple Eavesdroppers Facing Low-altitude Communication Systems", "comment": null, "summary": "In low-altitude wireless communications, the increased complexity of wireless\nchannels and the uncertainty of eavesdroppers (Eves)--caused by diverse\naltitudes, speeds, and obstacles--pose significant challenges to physical layer\nsecurity (PLS) technologies based on fixed-position antennas (FPAs),\nparticularly in terms of beamforming capabilities and spatial efficiency. In\ncontrast, movable antennas (MAs) offer a flexible solution by enabling channel\nreconstruction through antenna movement, effectively compensating for the\nlimitations of FPAs. In this paper, we aim to derive a closed-form expression\nfor the secrecy rate, a key metric in PLS, which is often unattainable in\ncurrent studies due to the uncertainty of Eves. We construct an equivalent\nmodel that leverages the reconfigurable nature of MAs, equating the secrecy\nrates obtained by multiple Eves with single FPAs to those achieved by a single\nvirtual Eve equipped with an MA array. To minimize the gap between these two\ntypes of secrecy rates, we formulate and solve an optimization problem by\njointly designing the equivalent distance between the transmitter and the\nvirtual Eve} and the antenna positions of MAs at the virtual Eve. Numerical\nsimulations validate the effectiveness of the proposed equivalent model,\noffering a new perspective for PLS strategies. This work provides significant\ninsights for network designers on how system parameters affect PLS performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MAs\uff09\u7684\u7269\u7406\u5c42\u5b89\u5168\uff08PLS\uff09\u7b49\u6548\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\uff08FPAs\uff09\u5728\u4f4e\u7a7a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u6700\u5c0f\u5316\u4fdd\u5bc6\u7387\u5dee\u8ddd\u3002", "motivation": "\u4f4e\u7a7a\u65e0\u7ebf\u901a\u4fe1\u4e2d\uff0c\u65e0\u7ebf\u4fe1\u9053\u7684\u590d\u6742\u6027\u548c\u7a83\u542c\u8005\uff08Eves\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u57fa\u4e8eFPAs\u7684PLS\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6ce2\u675f\u6210\u5f62\u548c\u7a7a\u95f4\u6548\u7387\u65b9\u9762\u3002MAs\u7684\u7075\u6d3b\u6027\u4e3a\u4fe1\u9053\u91cd\u6784\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7b49\u6548\u6a21\u578b\uff0c\u5c06\u591a\u4e2aEves\u7684\u4fdd\u5bc6\u7387\u4e0e\u5355\u4e2a\u865a\u62dfEve\uff08\u914d\u5907MA\u9635\u5217\uff09\u7684\u4fdd\u5bc6\u7387\u7b49\u540c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b49\u6548\u8ddd\u79bb\u548cMA\u4f4d\u7f6e\u6765\u6700\u5c0f\u5316\u4fdd\u5bc6\u7387\u5dee\u8ddd\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7b49\u6548\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e3aPLS\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7f51\u7edc\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u53c2\u6570\u5982\u4f55\u5f71\u54cdPLS\u6027\u80fd\u7684\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.05297", "categories": ["cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.05297", "abs": "https://arxiv.org/abs/2507.05297", "authors": ["Zijun Meng"], "title": "Fuzzy Classification Aggregation for a Continuum of Agents", "comment": null, "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean.", "AI": {"tldr": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u8fde\u7eed\u4e2a\u4f53\u5206\u7c7b\u7684\u6700\u4f18\u3001\u72ec\u7acb\u4e14\u96f6\u4e00\u81f4\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u5fc5\u987b\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u3002", "motivation": "\u7814\u7a76\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u7684\u6027\u8d28\uff0c\u7279\u522b\u662f\u5728\u591a\u5bf9\u8c61\u591a\u7c7b\u578b\u5206\u7c7b\u4e2d\u7684\u6700\u4f18\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\uff0c\u5206\u6790\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\uff08\u6700\u4f18\u3001\u72ec\u7acb\u3001\u96f6\u4e00\u81f4\uff09\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u3002", "result": "\u8bc1\u660e\u4e86\u6b64\u7c7b\u51fd\u6570\u5fc5\u987b\u662f\u52a0\u6743\u7b97\u672f\u5e73\u5747\u3002", "conclusion": "\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\uff0c\u52a0\u6743\u7b97\u672f\u5e73\u5747\u662f\u552f\u4e00\u6ee1\u8db3\u8981\u6c42\u7684\u6a21\u7cca\u5206\u7c7b\u805a\u5408\u51fd\u6570\u3002"}}
{"id": "2507.05829", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05829", "abs": "https://arxiv.org/abs/2507.05829", "authors": ["Zekai Sun", "Xiuxian Guan", "Zheng Lin", "Zihan Fang", "Xiangming Cai", "Zhe Chen", "Fangming Liu", "Heming Cui", "Jie Xiong", "Wei Ni", "Chau Yuen"], "title": "Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing", "comment": "14 pages, 19 figures", "summary": "Deploying deep neural networks (DNNs) on resource-constrained mobile devices\npresents significant challenges, particularly in achieving real-time\nperformance while simultaneously coping with limited computational resources\nand battery life. While Mobile Edge Computing (MEC) offers collaborative\ninference with GPU servers as a promising solution, existing approaches\nprimarily rely on layer-wise model partitioning and undergo significant\ntransmission bottlenecks caused by the sequential execution of DNN operations.\nTo address this challenge, we present Intra-DP, a high-performance\ncollaborative inference system optimized for DNN inference on MEC. Intra DP\nemploys a novel parallel computing technique based on local operators (i.e.,\noperators whose minimum unit input is not the entire input tensor, such as the\nconvolution kernel). By decomposing their computations (operations) into\nseveral independent sub-operations and overlapping the computation and\ntransmission of different sub-operations through parallel execution, Intra-DP\nmitigates transmission bottlenecks in MEC, achieving fast and energy-efficient\ninference. The evaluation demonstrates that Intra-DP reduces per-inference\nlatency by up to 50% and energy consumption by up to 75% compared to\nstate-of-the-art baselines, without sacrificing accuracy.", "AI": {"tldr": "Intra-DP\u662f\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u4f18\u5316\u7684\u9ad8\u6027\u80fd\u534f\u4f5c\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u6280\u672f\u51cf\u5c11\u4f20\u8f93\u74f6\u9888\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u9762\u4e34\u5b9e\u65f6\u6027\u80fd\u548c\u8d44\u6e90\u9650\u5236\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u987a\u5e8f\u6267\u884cDNN\u64cd\u4f5c\u5bfc\u81f4\u4f20\u8f93\u74f6\u9888\u3002", "method": "Intra-DP\u91c7\u7528\u57fa\u4e8e\u5c40\u90e8\u7b97\u5b50\u7684\u5e76\u884c\u8ba1\u7b97\u6280\u672f\uff0c\u5c06\u8ba1\u7b97\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u6267\u884c\u91cd\u53e0\u8ba1\u7b97\u4e0e\u4f20\u8f93\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cIntra-DP\u5c06\u6bcf\u6b21\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e50%\uff0c\u80fd\u8017\u51cf\u5c1175%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "Intra-DP\u6709\u6548\u89e3\u51b3\u4e86MEC\u4e2d\u7684\u4f20\u8f93\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8282\u80fd\u7684DNN\u63a8\u7406\u3002"}}
{"id": "2507.05488", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05488", "abs": "https://arxiv.org/abs/2507.05488", "authors": ["Subhasis Dasgupta", "Jon Stephens", "Amarnath Gupta"], "title": "OLG++: A Semantic Extension of Obligation Logic Graph", "comment": null, "summary": "We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)\nfor modeling regulatory and legal rules in municipal and interjurisdictional\ncontexts. OLG++ introduces richer node and edge types, including spatial,\ntemporal, party group, defeasibility, and logical grouping constructs, enabling\nnuanced representations of legal obligations, exceptions, and hierarchies. The\nmodel supports structured reasoning over rules with contextual conditions,\nprecedence, and complex triggers. We demonstrate its expressiveness through\nexamples from food business regulations, showing how OLG++ supports legal\nquestion answering using property graph queries. OLG++ also improves over\nLegalRuleML by providing native support for subClassOf, spatial constraints,\nand reified exception structures. Our examples show that OLG++ is more\nexpressive than prior graph-based models for legal knowledge representation.", "AI": {"tldr": "OLG++\u662fObligation Logic Graph\uff08OLG\uff09\u7684\u8bed\u4e49\u6269\u5c55\uff0c\u7528\u4e8e\u5efa\u6a21\u5e02\u653f\u548c\u8de8\u8f96\u533a\u80cc\u666f\u4e0b\u7684\u6cd5\u89c4\u548c\u6cd5\u5f8b\u89c4\u5219\u3002\u5b83\u5f15\u5165\u4e86\u66f4\u4e30\u5bcc\u7684\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\uff0c\u652f\u6301\u590d\u6742\u7684\u6cd5\u5f8b\u4e49\u52a1\u3001\u4f8b\u5916\u548c\u5c42\u6b21\u7ed3\u6784\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\u6a21\u578b\uff08\u5982LegalRuleML\uff09\u5728\u8868\u8fbe\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u903b\u8f91\u5206\u7ec4\u7b49\u590d\u6742\u7ea6\u675f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u5efa\u6a21\u5de5\u5177\u3002", "method": "OLG++\u6269\u5c55\u4e86OLG\uff0c\u65b0\u589e\u4e86\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u7fa4\u4f53\u3001\u53ef\u5e9f\u6b62\u6027\u548c\u903b\u8f91\u5206\u7ec4\u7b49\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\uff0c\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u548c\u590d\u6742\u89e6\u53d1\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u98df\u54c1\u4e1a\u52a1\u6cd5\u89c4\u7684\u6848\u4f8b\uff0cOLG++\u5c55\u793a\u4e86\u5176\u5728\u6cd5\u5f8b\u95ee\u7b54\u548c\u5c5e\u6027\u56fe\u67e5\u8be2\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u56fe\u6a21\u578b\u3002", "conclusion": "OLG++\u5728\u8868\u8fbe\u590d\u6742\u6cd5\u5f8b\u89c4\u5219\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2507.05876", "categories": ["cs.NI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.05876", "abs": "https://arxiv.org/abs/2507.05876", "authors": ["Nehal Baganal Krishna", "Anam Tahir", "Firas Khamis", "Mina Tahmasbi Arashloo", "Michael Zink", "Amr Rizk"], "title": "OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning", "comment": "17 pages, 11 figures", "summary": "Asynchronous Distributed Reinforcement Learning (DRL) can suffer from\ndegraded convergence when model updates become stale, often the result of\nnetwork congestion and packet loss during large-scale training. This work\nintroduces a network data-plane acceleration architecture that mitigates such\nstaleness by enabling inline processing of DRL model updates as they traverse\nthe accelerator engine. To this end, we design and prototype a novel queueing\nmechanism that opportunistically combines compatible updates sharing a network\nelement, reducing redundant traffic and preserving update utility.\nComplementing this we provide a lightweight transmission control mechanism at\nthe worker nodes that is guided by feedback from the in-network accelerator. To\nassess model utility at line rate, we introduce the Age-of-Model (AoM) metric\nas a proxy for staleness and verify global fairness and responsiveness\nproperties using a formal verification method. Our evaluations demonstrate that\nthis architecture significantly reduces update staleness and congestion,\nultimately improving the convergence rate in asynchronous DRL workloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u7edc\u6570\u636e\u5e73\u9762\u52a0\u901f\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u8054\u5904\u7406DRL\u6a21\u578b\u66f4\u65b0\u51cf\u5c11\u9648\u65e7\u6027\uff0c\u63d0\u5347\u5f02\u6b65DRL\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5f02\u6b65\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u56e0\u7f51\u7edc\u62e5\u585e\u548c\u6570\u636e\u5305\u4e22\u5931\u5bfc\u81f4\u6a21\u578b\u66f4\u65b0\u9648\u65e7\uff0c\u5f71\u54cd\u6536\u655b\u901f\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u961f\u5217\u673a\u5236\uff0c\u7ed3\u5408\u517c\u5bb9\u7684\u66f4\u65b0\u4ee5\u51cf\u5c11\u5197\u4f59\u6d41\u91cf\uff1b\u540c\u65f6\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u4f20\u8f93\u63a7\u5236\u673a\u5236\uff0c\u5e76\u901a\u8fc7Age-of-Model\uff08AoM\uff09\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6548\u7528\u3002", "result": "\u8be5\u67b6\u6784\u663e\u8457\u51cf\u5c11\u4e86\u66f4\u65b0\u9648\u65e7\u6027\u548c\u62e5\u585e\uff0c\u63d0\u5347\u4e86\u5f02\u6b65DRL\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u7f51\u7edc\u6570\u636e\u5e73\u9762\u52a0\u901f\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6b65DRL\u4e2d\u7684\u66f4\u65b0\u9648\u65e7\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2507.05495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05495", "abs": "https://arxiv.org/abs/2507.05495", "authors": ["Prahaladh Chandrahasan", "Jiahe Jin", "Zhihan Zhang", "Tevin Wang", "Andy Tang", "Lucy Mo", "Morteza Ziyadi", "Leonardo F. R. Ribeiro", "Zimeng Qiu", "Markus Dreyer", "Akari Asai", "Chenyan Xiong"], "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents", "comment": null, "summary": "Effectively evaluating deep research agents that autonomously search the web,\nanalyze information, and generate reports remains a major challenge,\nparticularly when it comes to assessing long reports and giving detailed\nfeedback on their intermediate steps. To address these gaps, we introduce Deep\nResearch Comparator, a platform that offers a holistic framework for deep\nresearch agent hosting, side-by-side comparison, fine-grained human feedback\ncollection, and ranking calculation. Given a user query, our platform displays\nthe final reports from two different agents along with their intermediate steps\nduring generation. Annotators can evaluate the overall quality of final reports\nbased on side-by-side comparison, and also provide detailed feedback separately\nby assessing intermediate steps or specific text spans within the final report.\nFurthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This\nscaffold serves as a baseline that facilitates the easy integration of various\nlarge language models to transform them into deep research agents for\nevaluation. To demonstrate the platform's utility for deep research agent\ndevelopment, we have collected real user preference data from 17 annotators on\nthree deep research agents. A demo video of our platform can be found at\nhttps://www.youtube.com/watch?v=g4d2dnbdseg.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Deep Research Comparator\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u751f\u6210\u62a5\u544a\uff0c\u652f\u6301\u5bf9\u6bd4\u5206\u6790\u3001\u7ec6\u7c92\u5ea6\u53cd\u9988\u548c\u6392\u540d\u8ba1\u7b97\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u751f\u6210\u62a5\u544a\u7684\u8bc4\u4f30\u96be\u9898\uff0c\u5c24\u5176\u662f\u957f\u62a5\u544a\u548c\u4e2d\u95f4\u6b65\u9aa4\u7684\u8be6\u7ec6\u53cd\u9988\u3002", "method": "\u5f00\u53d1\u4e86Deep Research Comparator\u5e73\u53f0\uff0c\u652f\u6301\u4ee3\u7406\u62a5\u544a\u5bf9\u6bd4\u3001\u4e2d\u95f4\u6b65\u9aa4\u8bc4\u4f30\u548c\u53cd\u9988\u6536\u96c6\uff1b\u540c\u65f6\u63d0\u51fa\u4e86Simple Deepresearch\u4f5c\u4e3a\u57fa\u7ebf\u4ee3\u7406\u6846\u67b6\u3002", "result": "\u6536\u96c6\u4e8617\u4f4d\u6807\u6ce8\u8005\u5bf9\u4e09\u4e2a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u771f\u5b9e\u504f\u597d\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.06001", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2507.06001", "abs": "https://arxiv.org/abs/2507.06001", "authors": ["Carlo Segat", "Sandro Rodriguez Garzo", "Axel K\u00fcpper"], "title": "Programmable Governance for Group-Controlled Decentralized Identifiers", "comment": null, "summary": "Self-Sovereign Identity (SSI) is a paradigm for digital identity management\nthat offers unique privacy advantages. A key technology in SSI is Decentralized\nIdentifiers (DIDs) and their associated metadata, DID Documents (DDOs). DDOs\ncontain crucial verification material such as the public keys of the entity\nidentified by the DID (i.e., the DID subject) and are often anchored on a\ndistributed ledger to ensure security and availability. Long-lived DIDs need to\nsupport updates (e.g., key rotation). Ideally, only the DID subject should\nauthorize DDO updates. However, in practice, update capabilities may be shared\nor delegated. While the DID specification acknowledges such scenarios, it does\nnot define how updates should be authorized when multiple entities jointly\ncontrol a DID (i.e., group control). This article examines the implementation\nof an on-chain, trustless mechanism enabling DID controllers under group\ncontrol to program their governance rules. The main research question is the\nfollowing: Can a technical mechanism be developed to orchestrate on-chain group\ncontrol of a DDO in a ledger-agnostic and adaptable manner?", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\uff08DID\uff09\u7684\u7fa4\u4f53\u63a7\u5236\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u94fe\u4e0a\u673a\u5236\u5b9e\u73b0\u65e0\u9700\u4fe1\u4efb\u7684DID\u6587\u6863\uff08DDO\uff09\u66f4\u65b0\u6388\u6743\u3002", "motivation": "\u5728\u7fa4\u4f53\u63a7\u5236DID\u65f6\uff0c\u73b0\u6709\u89c4\u8303\u672a\u660e\u786e\u5982\u4f55\u6388\u6743DDO\u66f4\u65b0\uff0c\u4e9f\u9700\u4e00\u79cd\u7075\u6d3b\u4e14\u4e0e\u8d26\u672c\u65e0\u5173\u7684\u6280\u672f\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u94fe\u4e0a\u673a\u5236\uff0c\u5141\u8bb8\u7fa4\u4f53\u63a7\u5236\u5668\u7f16\u7a0b\u5176\u6cbb\u7406\u89c4\u5219\uff0c\u5b9e\u73b0DDO\u7684\u66f4\u65b0\u6388\u6743\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u5f00\u53d1\u4e00\u79cd\u6280\u672f\u673a\u5236\uff0c\u4ee5\u8d26\u672c\u65e0\u5173\u548c\u53ef\u9002\u914d\u7684\u65b9\u5f0f\u534f\u8c03\u7fa4\u4f53\u63a7\u5236\u7684DDO\u66f4\u65b0\u3002", "conclusion": "\u8be5\u673a\u5236\u4e3a\u7fa4\u4f53\u63a7\u5236DID\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c4\u8303\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.05515", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05515", "abs": "https://arxiv.org/abs/2507.05515", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Pablo Cesar", "Chuang Yu", "Zhaochun Ren", "Junxiao Wang"], "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality", "comment": "20 pages", "summary": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e13\u4e3aAR\u8bad\u7ec3\u8bbe\u8ba1\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5148\u8fdbVLM\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u547c\u5401\u6539\u8fdb\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "motivation": "\u63a2\u7d22VLM\u5728AR\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u4e3a\u76f2\u4eba\u548c\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u5e73\u7b49\u7684AI\u5b66\u4e60\u673a\u4f1a\u3002", "method": "\u6784\u5efa\u7cfb\u7edf\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e5d\u79cd\u5148\u8fdbVLM\u6a21\u578b\uff08\u5305\u62ecGPT-4o\uff09\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5148\u8fdb\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7ec4\u88c5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8F1\u5206\u6570\u4ec5\u4e3a40.54%\uff0c\u663e\u793a\u9700\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u3002", "conclusion": "\u9700\u589e\u5f3a\u6570\u636e\u96c6\u548c\u57fa\u51c6\u7814\u7a76\uff0c\u4ee5\u63d0\u5347VLM\u5728AR\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63a8\u52a8\u793e\u4f1a\u5305\u5bb9\u6027\u3002"}}
{"id": "2507.05519", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05519", "abs": "https://arxiv.org/abs/2507.05519", "authors": ["Gopal Gupta", "Abhiramon Rajasekharan", "Alexis R. Tudor", "Elmer Salazar", "Joaqu\u00edn Arias"], "title": "Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System", "comment": null, "summary": "We consider the problem of implementing deontic modal logic. We show how\n(deontic) modal operators can be expressed elegantly using default negation\n(negation-as-failure) and strong negation present in answer set programming\n(ASP). We propose using global constraints of ASP to represent obligations and\nimpermissibilities of deontic modal logic. We show that our proposed\nrepresentation results in the various paradoxes of deontic modal logic being\nelegantly resolved.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u4e2d\u4f18\u96c5\u5730\u5b9e\u73b0\u9053\u4e49\u6a21\u6001\u903b\u8f91\uff0c\u5229\u7528\u9ed8\u8ba4\u5426\u5b9a\u548c\u5f3a\u5426\u5b9a\u8868\u793a\u6a21\u6001\u7b97\u5b50\uff0c\u5e76\u901a\u8fc7ASP\u7684\u5168\u5c40\u7ea6\u675f\u89e3\u51b3\u9053\u4e49\u903b\u8f91\u7684\u6096\u8bba\u3002", "motivation": "\u89e3\u51b3\u9053\u4e49\u6a21\u6001\u903b\u8f91\u7684\u5b9e\u73b0\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u7b54\u6848\u96c6\u7f16\u7a0b\u4e2d\u7684\u8868\u8fbe\u65b9\u5f0f\u3002", "method": "\u5229\u7528ASP\u4e2d\u7684\u9ed8\u8ba4\u5426\u5b9a\u548c\u5f3a\u5426\u5b9a\u8868\u793a\u9053\u4e49\u6a21\u6001\u7b97\u5b50\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u7ea6\u675f\u8868\u793a\u4e49\u52a1\u548c\u7981\u6b62\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4f18\u96c5\u5730\u89e3\u51b3\u9053\u4e49\u6a21\u6001\u903b\u8f91\u4e2d\u7684\u5404\u79cd\u6096\u8bba\u3002", "conclusion": "ASP\u4e3a\u9053\u4e49\u6a21\u6001\u903b\u8f91\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u5e76\u80fd\u89e3\u51b3\u5176\u6096\u8bba\u95ee\u9898\u3002"}}
{"id": "2507.05520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05520", "abs": "https://arxiv.org/abs/2507.05520", "authors": ["Karishma Thakrar", "Shreyas Basavatia", "Akshay Daftardar"], "title": "Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis", "comment": "2025 ImageCLEF MEDIQA-MAGIC Challenge", "summary": "The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized\nby researchers from Microsoft, Stanford University, and the Hospital Clinic of\nBarcelona, focuses on multimodal dermatology question answering and\nsegmentation, using real-world patient queries and images. This work addresses\nthe Closed Visual Question Answering (CVQA) task, where the goal is to select\nthe correct answer to multiple-choice clinical questions based on both\nuser-submitted images and accompanying symptom descriptions. The proposed\napproach combines three core components: (1) fine-tuning open-source multimodal\nmodels from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)\nintroducing a structured reasoning layer that reconciles and adjudicates\nbetween candidate model outputs, and (3) incorporating agentic\nretrieval-augmented generation (agentic RAG), which adds relevant information\nfrom the American Academy of Dermatology's symptom and condition database to\nfill in gaps in patient context. The team achieved second place with a\nsubmission that scored sixth, demonstrating competitive performance and high\naccuracy. Beyond competitive benchmarks, this research addresses a practical\nchallenge in telemedicine: diagnostic decisions must often be made\nasynchronously, with limited input and with high accuracy and interpretability.\nBy emulating the systematic reasoning patterns employed by dermatologists when\nevaluating skin conditions, this architecture provided a pathway toward more\nreliable automated diagnostic support systems.", "AI": {"tldr": "2025 ImageCLEF MEDIQA-MAGIC\u6311\u6218\u8d5b\u5173\u6ce8\u591a\u6a21\u6001\u76ae\u80a4\u75c5\u95ee\u7b54\u4e0e\u5206\u5272\uff0c\u63d0\u51fa\u7ed3\u5408\u5fae\u8c03\u591a\u6a21\u6001\u6a21\u578b\u3001\u7ed3\u6784\u5316\u63a8\u7406\u5c42\u548c\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u8fdc\u7a0b\u533b\u7597\u4e2d\u57fa\u4e8e\u6709\u9650\u8f93\u5165\u7684\u5f02\u6b65\u8bca\u65ad\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408\u5fae\u8c03\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\uff08Qwen\u3001Gemma\u3001LLaMA\uff09\u3001\u7ed3\u6784\u5316\u63a8\u7406\u5c42\u548c\u4ee3\u7406\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002", "result": "\u56e2\u961f\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u63d0\u4ea4\u4f5c\u54c1\u6392\u540d\u7b2c\u516d\uff0c\u8868\u73b0\u7ade\u4e89\u529b\u548c\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u6a21\u62df\u76ae\u80a4\u79d1\u533b\u751f\u7684\u7cfb\u7edf\u63a8\u7406\u6a21\u5f0f\uff0c\u4e3a\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8def\u5f84\u3002"}}
{"id": "2507.05528", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05528", "abs": "https://arxiv.org/abs/2507.05528", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "comment": "14 pages", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWikiHowAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u6a21\u62df\u6559\u5b66\u4e92\u52a8\uff0c\u8bc4\u4f30\u6559\u5b66\u6548\u679c\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u89c4\u6a21\u8bfe\u7a0b\u5185\u5bb9\uff0c\u4e14\u7f3a\u4e4f\u8bc4\u4f30\u6559\u5b66\u8d28\u91cf\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faWikiHowAgent\uff0c\u6574\u5408\u6559\u5e08\u548c\u5b66\u4e60\u8005\u667a\u80fd\u4f53\u3001\u4e92\u52a8\u7ba1\u7406\u5668\u548c\u8bc4\u4f30\u5668\uff0c\u57fa\u4e8e14,287\u7bc7\u6559\u7a0b\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u5de5\u4f5c\u6d41\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "WikiHowAgent\u4e3a\u6559\u5b66\u4e92\u52a8\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.05538", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05538", "abs": "https://arxiv.org/abs/2507.05538", "authors": ["Subhabrata Majumdar", "Brian Pendleton", "Abhishek Gupta"], "title": "Red Teaming AI Red Teaming", "comment": null, "summary": "Red teaming has evolved from its origins in military applications to become a\nwidely adopted methodology in cybersecurity and AI. In this paper, we take a\ncritical look at the practice of AI red teaming. We argue that despite its\ncurrent popularity in AI governance, there exists a significant gap between red\nteaming's original intent as a critical thinking exercise and its narrow focus\non discovering model-level flaws in the context of generative AI. Current AI\nred teaming efforts focus predominantly on individual model vulnerabilities\nwhile overlooking the broader sociotechnical systems and emergent behaviors\nthat arise from complex interactions between models, users, and environments.\nTo address this deficiency, we propose a comprehensive framework\noperationalizing red teaming in AI systems at two levels: macro-level system\nred teaming spanning the entire AI development lifecycle, and micro-level model\nred teaming. Drawing on cybersecurity experience and systems theory, we further\npropose a set of recommendations. In these, we emphasize that effective AI red\nteaming requires multifunctional teams that examine emergent risks, systemic\nvulnerabilities, and the interplay between technical and social factors.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5730\u5ba1\u89c6\u4e86AI\u7ea2\u961f\u6d4b\u8bd5\u7684\u5b9e\u8df5\uff0c\u6307\u51fa\u5176\u5f53\u524d\u5728AI\u6cbb\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6db5\u76d6\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u7684\u7efc\u5408\u6846\u67b6\u3002", "motivation": "\u63a2\u8ba8AI\u7ea2\u961f\u6d4b\u8bd5\u5728\u751f\u6210\u5f0fAI\u80cc\u666f\u4e0b\u8fc7\u4e8e\u5173\u6ce8\u6a21\u578b\u7ea7\u7f3a\u9677\u800c\u5ffd\u89c6\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53cc\u5c42\u6b21\u6846\u67b6\uff08\u5b8f\u89c2\u7cfb\u7edf\u7ea7\u548c\u5fae\u89c2\u6a21\u578b\u7ea7\u7ea2\u961f\u6d4b\u8bd5\uff09\uff0c\u5e76\u7ed3\u5408\u7f51\u7edc\u5b89\u5168\u7ecf\u9a8c\u548c\u7cfb\u7edf\u7406\u8bba\u63d0\u51fa\u5efa\u8bae\u3002", "result": "\u5f3a\u8c03\u6709\u6548\u7684AI\u7ea2\u961f\u6d4b\u8bd5\u9700\u8981\u591a\u804c\u80fd\u56e2\u961f\uff0c\u5173\u6ce8\u6d8c\u73b0\u98ce\u9669\u3001\u7cfb\u7edf\u6027\u6f0f\u6d1e\u53ca\u6280\u672f\u4e0e\u793e\u4f1a\u56e0\u7d20\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u547c\u5401\u66f4\u5168\u9762\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9AI\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u4ea4\u4e92\u548c\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2507.05541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05541", "abs": "https://arxiv.org/abs/2507.05541", "authors": ["Shovito Barua Soumma", "Asiful Arefeen", "Stephanie M. Carpenter", "Melanie Hingle", "Hassan Ghasemzadeh"], "title": "SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation", "comment": "In review", "summary": "Counterfactual explanations (CFs) offer human-centric insights into machine\nlearning predictions by highlighting minimal changes required to alter an\noutcome. Therefore, CFs can be used as (i) interventions for abnormality\nprevention and (ii) augmented data for training robust models. In this work, we\nexplore large language models (LLMs), specifically GPT-4o-mini, for generating\nCFs in a zero-shot and three-shot setting. We evaluate our approach on two\ndatasets: the AI-Readi flagship dataset for stress prediction and a public\ndataset for heart disease detection. Compared to traditional methods such as\nDiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high\nplausibility (up to 99%), strong validity (up to 0.99), and competitive\nsparsity. Moreover, using LLM-generated CFs as augmented samples improves\ndownstream classifier performance (an average accuracy gain of 5%), especially\nin low-data regimes. This demonstrates the potential of prompt-based generative\ntechniques to enhance explainability and robustness in clinical and\nphysiological prediction tasks. Code base: github.com/anonymous/SenseCF.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528GPT-4o-mini\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u7684\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u548c\u4e09\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFs\uff09\u80fd\u4e3a\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u63d0\u4f9b\u76f4\u89c2\u7684\u5e72\u9884\u548c\u6570\u636e\u589e\u5f3a\u624b\u6bb5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210CFs\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528GPT-4o-mini\u5728\u96f6\u6837\u672c\u548c\u4e09\u6837\u672c\u8bbe\u7f6e\u4e0b\u751f\u6210CFs\uff0c\u5e76\u5728AI-Readi\u548c\u5fc3\u810f\u75c5\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "LLM\u751f\u6210\u7684CFs\u5728\u5408\u7406\u6027\uff0899%\uff09\u3001\u6709\u6548\u6027\uff080.99\uff09\u548c\u7a00\u758f\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4f5c\u4e3a\u589e\u5f3a\u6570\u636e\u80fd\u63d0\u5347\u5206\u7c7b\u5668\u51c6\u786e\u73875%\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u751f\u6210\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u548c\u751f\u7406\u9884\u6d4b\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.05566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05566", "abs": "https://arxiv.org/abs/2507.05566", "authors": ["David Bensa\u00efd", "Noam Rotstein", "Roy Velich", "Daniel Bensa\u00efd", "Ron Kimmel"], "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.", "AI": {"tldr": "SingLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u89e3\u51b3LoRA\u4e2d\u7684\u5c3a\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "LoRA\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u5b58\u5728\u5c3a\u5ea6\u51b2\u7a81\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0cSingLoRA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SingLoRA\u5c06\u6743\u91cd\u66f4\u65b0\u5206\u89e3\u4e3a\u5355\u4f4e\u79e9\u77e9\u9635\u4e0e\u5176\u8f6c\u7f6e\u7684\u4e58\u79ef\uff0c\u6d88\u9664\u5c3a\u5ea6\u51b2\u7a81\u5e76\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982LLama 7B\u5fae\u8c03\u51c6\u786e\u738791.3%\uff0c\u4f18\u4e8eLoRA\u548cLoRA+\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "SingLoRA\u901a\u8fc7\u7b80\u5316\u8bbe\u8ba1\u89e3\u51b3\u4e86LoRA\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u5fae\u8c03\u3002"}}
{"id": "2507.05587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05587", "abs": "https://arxiv.org/abs/2507.05587", "authors": ["Elija Perrier"], "title": "Towards Measurement Theory for Artificial Intelligence", "comment": "Under review for Iliad Conference 2025", "summary": "We motivate and outline a programme for a formal theory of measurement of\nartificial intelligence. We argue that formalising measurement for AI will\nallow researchers, practitioners, and regulators to: (i) make comparisons\nbetween systems and the evaluation methods applied to them; (ii) connect\nfrontier AI evaluations with established quantitative risk analysis techniques\ndrawn from engineering and safety science; and (iii) foreground how what counts\nas AI capability is contingent upon the measurement operations and scales we\nelect to use. We sketch a layered measurement stack, distinguish direct from\nindirect observables, and signpost how these ingredients provide a pathway\ntoward a unified, calibratable taxonomy of AI phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u6d4b\u91cf\u7684\u6b63\u5f0f\u7406\u8bba\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316\u6d4b\u91cf\u65b9\u6cd5\u4fc3\u8fdb\u7cfb\u7edf\u6bd4\u8f83\u3001\u98ce\u9669\u5206\u6790\u53ca\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u4e3aAI\u7814\u7a76\u3001\u5b9e\u8df5\u548c\u76d1\u7ba1\u63d0\u4f9b\u7edf\u4e00\u7684\u6d4b\u91cf\u6807\u51c6\uff0c\u4ee5\u652f\u6301\u7cfb\u7edf\u6bd4\u8f83\u3001\u98ce\u9669\u5206\u6790\u53ca\u80fd\u529b\u8bc4\u4f30\u7684\u5ba2\u89c2\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u7684\u6d4b\u91cf\u6846\u67b6\uff0c\u533a\u5206\u76f4\u63a5\u4e0e\u95f4\u63a5\u53ef\u89c2\u6d4b\u6027\uff0c\u5e76\u6784\u5efa\u53ef\u6821\u51c6\u7684AI\u73b0\u8c61\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u4e3aAI\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u652f\u6301\u7cfb\u7edf\u6bd4\u8f83\u3001\u98ce\u9669\u5206\u6790\u53ca\u80fd\u529b\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u3002", "conclusion": "\u6807\u51c6\u5316AI\u6d4b\u91cf\u6709\u52a9\u4e8e\u63d0\u5347\u7814\u7a76\u7684\u53ef\u6bd4\u6027\u548c\u98ce\u9669\u7ba1\u7406\u7684\u79d1\u5b66\u6027\u3002"}}
{"id": "2507.05591", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05591", "abs": "https://arxiv.org/abs/2507.05591", "authors": ["Wei Zhang", "Juan Chen", "En Zhu", "Wenhong Cheng", "YunPeng Li", "Yanbo J. Wang"], "title": "MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models", "comment": null, "summary": "Automated depression diagnosis aims to analyze multimodal information from\ninterview videos to predict participants' depression scores. Previous studies\noften lack clear explanations of how these scores were determined, limiting\ntheir adoption in clinical practice. While the advent of LLMs provides a\npossible pathway for explainable depression diagnosis, current LLMs capable of\nprocessing multimodal data lack training on interview data, resulting in poor\ndiagnostic performance when used directly. In this paper, we propose a novel\nmultimodal large language model (MLlm-DR) that can understand multimodal\ninformation inputs and supports explainable depression diagnosis. MLlm-DR\nintegrates a smaller LLMs and a lightweight query module (LQ-former).\nSpecifically, the smaller LLMs is designed to generate depression scores and\ncorresponding evaluation rationales. To enhance its logical reasoning for\ndomain-specific tasks while maintaining practicality, we constructed a robust\ntraining dataset to fine-tune it. Meanwhile, the LQ-former captures\ndepression-related features from speech and visual data, aiding the model's\nability to process multimodal information, to achieve comprehensive depression\ndiagnosis. Our approach achieves state-of-the-art results on two\ninterview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its\neffectiveness and superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLlm-DR\uff09\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u6291\u90c1\u75c7\u8bca\u65ad\uff0c\u7ed3\u5408\u5c0f\u578bLLM\u548c\u8f7b\u91cf\u7ea7\u67e5\u8be2\u6a21\u5757\uff08LQ-former\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6291\u90c1\u75c7\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u4e14\u76f4\u63a5\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u7684LLM\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u53c8\u80fd\u63d0\u4f9b\u89e3\u91ca\u7684\u6a21\u578b\u3002", "method": "MLlm-DR\u6574\u5408\u5c0f\u578bLLM\u751f\u6210\u6291\u90c1\u75c7\u8bc4\u5206\u548c\u8bc4\u4f30\u7406\u7531\uff0c\u5e76\u901a\u8fc7LQ-former\u4ece\u8bed\u97f3\u548c\u89c6\u89c9\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728CMDC\u548cE-DAIC-WOZ\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "MLlm-DR\u4e3a\u6291\u90c1\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.05613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05613", "abs": "https://arxiv.org/abs/2507.05613", "authors": ["Lei Fan", "Fangxue Liu", "Cheng Chen"], "title": "Domain adaptation of large language models for geotechnical applications", "comment": null, "summary": "Recent developments in large language models (LLMs) are opening up new\nopportunities in geotechnical engineering and engineering geology. While\ngeneral-purpose LLMs possess broad capabilities, effective application in\ngeotechnics often requires domain-specific adaptation. Such tailored LLMs are\nincreasingly employed to streamline geotechnical workflows. This paper presents\nthe first survey of the adaptation and application of LLMs in geotechnical\nengineering. It outlines key methodologies for adaptation to geotechnical\ndomain, including prompt engineering, retrieval-augmented generation,\ndomain-adaptive pretraining, and fine-tuning. The survey examines the\nstate-of-the-art applications of geotechnical-adapted LLMs, including\ngeological interpretation, subsurface characterization, site planning, design\ncalculations, numerical modeling, safety and risk assessment, and educational\ntutoring. It also analyzes benefits and limitations of geotechnical-adapted\nLLMs, and identifies promising directions for future research in this\ninterdisciplinary discipline. The findings serve as a valuable resource for\npractitioners seeking to integrate LLMs into geotechnical practice, while also\nproviding a foundation to stimulate further investigation within the academic\ncommunity.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u9002\u5e94\u4e0e\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u65b9\u6cd5\u3001\u5e94\u7528\u9886\u57df\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLMs\u7684\u53d1\u5c55\uff0c\u5176\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\u9010\u6e10\u663e\u73b0\uff0c\u4f46\u9700\u9886\u57df\u7279\u5b9a\u9002\u5e94\u624d\u80fd\u6709\u6548\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u5c06LLMs\u9002\u5e94\u4e8e\u5ca9\u571f\u5de5\u7a0b\u9886\u57df\u3002", "result": "\u7efc\u8ff0\u4e86LLMs\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u591a\u79cd\u5e94\u7528\uff0c\u5982\u5730\u8d28\u89e3\u91ca\u3001\u5730\u4e0b\u8868\u5f81\u3001\u8bbe\u8ba1\u8ba1\u7b97\u7b49\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "conclusion": "\u672c\u6587\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6574\u5408LLMs\u7684\u6307\u5bfc\uff0c\u540c\u65f6\u4e3a\u5b66\u672f\u754c\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.05624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05624", "abs": "https://arxiv.org/abs/2507.05624", "authors": ["Wei Zhang", "Juan Chen", "Yanbo J. Wang", "En Zhu", "Xuan Yang", "Yiduo Wang"], "title": "ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion", "comment": null, "summary": "Multimodal emotion and intent recognition is essential for automated\nhuman-computer interaction, It aims to analyze users' speech, text, and visual\ninformation to predict their emotions or intent. One of the significant\nchallenges is that missing modalities due to sensor malfunctions or incomplete\ndata. Traditional methods that attempt to reconstruct missing information often\nsuffer from over-coupling and imprecise generation processes, leading to\nsuboptimal outcomes. To address these issues, we introduce an Attention-based\nDiffusion model for Missing Modalities feature Completion (ADMC). Our framework\nindependently trains feature extraction networks for each modality, preserving\ntheir unique characteristics and avoiding over-coupling. The Attention-based\nDiffusion Network (ADN) generates missing modality features that closely align\nwith authentic multimodal distribution, enhancing performance across all\nmissing-modality scenarios. Moreover, ADN's cross-modal generation offers\nimproved recognition even in full-modality contexts. Our approach achieves\nstate-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating\nits effectiveness in both missing and complete modality scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6269\u6563\u7684\u6a21\u578b\uff08ADMC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u548c\u751f\u6210\u7f3a\u5931\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\uff0c\u6a21\u6001\u7f3a\u5931\u662f\u5e38\u89c1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u8fc7\u5ea6\u8026\u5408\u548c\u751f\u6210\u4e0d\u7cbe\u786e\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faADMC\u6846\u67b6\uff0c\u72ec\u7acb\u8bad\u7ec3\u5404\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6269\u6563\u7f51\u7edc\uff08ADN\uff09\u751f\u6210\u7f3a\u5931\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728IEMOCAP\u548cMIntRec\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7f3a\u5931\u548c\u5b8c\u6574\u6a21\u6001\u573a\u666f\u3002", "conclusion": "ADMC\u901a\u8fc7\u907f\u514d\u8fc7\u5ea6\u8026\u5408\u548c\u7cbe\u786e\u751f\u6210\u7f3a\u5931\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u7684\u6548\u679c\u3002"}}
{"id": "2507.05629", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05629", "abs": "https://arxiv.org/abs/2507.05629", "authors": ["Yuan An", "John Liu", "Niyam Acharya", "Ruhma Hashmi"], "title": "Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses", "comment": null, "summary": "Retrieval practice is a well-established pedagogical technique known to\nsignificantly enhance student learning and knowledge retention. However,\ngenerating high-quality retrieval practice questions is often time-consuming\nand labor intensive for instructors, especially in rapidly evolving technical\nsubjects. Large Language Models (LLMs) offer the potential to automate this\nprocess by generating questions in response to prompts, yet the effectiveness\nof LLM-generated retrieval practice on student learning remains to be\nestablished. In this study, we conducted an empirical study involving two\ncollege-level data science courses, with approximately 60 students. We compared\nlearning outcomes during one week in which students received LLM-generated\nmultiple-choice retrieval practice questions to those from a week in which no\nsuch questions were provided. Results indicate that students exposed to\nLLM-generated retrieval practice achieved significantly higher knowledge\nretention, with an average accuracy of 89%, compared to 73% in the week without\nsuch practice. These findings suggest that LLM-generated retrieval questions\ncan effectively support student learning and may provide a scalable solution\nfor integrating retrieval practice into real-time teaching. However, despite\nthese encouraging outcomes and the potential time-saving benefits, cautions\nmust be taken, as the quality of LLM-generated questions can vary. Instructors\nmust still manually verify and revise the generated questions before releasing\nthem to students.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u68c0\u7d22\u7ec3\u4e60\u95ee\u9898\u80fd\u663e\u8457\u63d0\u9ad8\u5b66\u751f\u77e5\u8bc6\u4fdd\u7559\u7387\uff0c\u4f46\u9700\u4eba\u5de5\u9a8c\u8bc1\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6559\u5e08\u751f\u6210\u9ad8\u8d28\u91cf\u68c0\u7d22\u7ec3\u4e60\u95ee\u9898\u8017\u65f6\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u81ea\u52a8\u751f\u6210\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "method": "\u572860\u540d\u5b66\u751f\u7684\u6570\u636e\u79d1\u5b66\u8bfe\u7a0b\u4e2d\uff0c\u5bf9\u6bd4LLM\u751f\u6210\u7684\u591a\u9009\u9898\u4e0e\u65e0\u7ec3\u4e60\u5468\u7684\u5b66\u4e60\u6548\u679c\u3002", "result": "\u4f7f\u7528LLM\u751f\u6210\u95ee\u9898\u7684\u5b66\u751f\u77e5\u8bc6\u4fdd\u7559\u7387\uff0889%\uff09\u663e\u8457\u9ad8\u4e8e\u65e0\u7ec3\u4e60\u5468\uff0873%\uff09\u3002", "conclusion": "LLM\u751f\u6210\u95ee\u9898\u53ef\u6709\u6548\u652f\u6301\u5b66\u4e60\uff0c\u4f46\u9700\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u8d28\u91cf\u3002"}}
{"id": "2507.05638", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.05638", "abs": "https://arxiv.org/abs/2507.05638", "authors": ["Litian Zhang", "Xiaoming Zhang", "Bingyu Yan", "Ziyi Zhou", "Bo Zhang", "Zhenyu Guan", "Xi Zhang", "Chaozhuo Li"], "title": "LLMs are Introvert", "comment": null, "summary": "The exponential growth of social media and generative AI has transformed\ninformation dissemination, fostering connectivity but also accelerating the\nspread of misinformation. Understanding information propagation dynamics and\ndeveloping effective control strategies is essential to mitigate harmful\ncontent. Traditional models, such as SIR, provide basic insights but\ninadequately capture the complexities of online interactions. Advanced methods,\nincluding attention mechanisms and graph neural networks, enhance accuracy but\ntypically overlook user psychology and behavioral dynamics. Large language\nmodels (LLMs), with their human-like reasoning, offer new potential for\nsimulating psychological aspects of information spread. We introduce an\nLLM-based simulation environment capturing agents' evolving attitudes,\nemotions, and responses. Initial experiments, however, revealed significant\ngaps between LLM-generated behaviors and authentic human dynamics, especially\nin stance detection and psychological realism. A detailed evaluation through\nSocial Information Processing Theory identified major discrepancies in\ngoal-setting and feedback evaluation, stemming from the lack of emotional\nprocessing in standard LLM training. To address these issues, we propose the\nSocial Information Processing-based Chain of Thought (SIP-CoT) mechanism\nenhanced by emotion-guided memory. This method improves the interpretation of\nsocial cues, personalization of goals, and evaluation of feedback. Experimental\nresults confirm that SIP-CoT-enhanced LLM agents more effectively process\nsocial information, demonstrating behaviors, attitudes, and emotions closer to\nreal human interactions. In summary, this research highlights critical\nlimitations in current LLM-based propagation simulations and demonstrates how\nintegrating SIP-CoT and emotional memory significantly enhances the social\nintelligence and realism of LLM agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u5728\u6a21\u62df\u4fe1\u606f\u4f20\u64ad\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51faSIP-CoT\u673a\u5236\u7ed3\u5408\u60c5\u611f\u8bb0\u5fc6\u4ee5\u63d0\u5347\u6a21\u62df\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7684\u5feb\u901f\u53d1\u5c55\u4e0e\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\u52a0\u5267\u4e86\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u4f20\u7edf\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u5728\u7ebf\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u9ad8\u7ea7\u65b9\u6cd5\u53c8\u5ffd\u7565\u4e86\u7528\u6237\u5fc3\u7406\u548c\u884c\u4e3a\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u4fe1\u606f\u5904\u7406\u7406\u8bba\uff08SIP\uff09\u7684SIP-CoT\u673a\u5236\uff0c\u7ed3\u5408\u60c5\u611f\u8bb0\u5fc6\uff0c\u6539\u8fdbLLM\u4ee3\u7406\u5bf9\u793e\u4ea4\u7ebf\u7d22\u7684\u89e3\u91ca\u3001\u76ee\u6807\u4e2a\u6027\u5316\u53ca\u53cd\u9988\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSIP-CoT\u589e\u5f3a\u7684LLM\u4ee3\u7406\u80fd\u66f4\u6709\u6548\u5730\u5904\u7406\u793e\u4ea4\u4fe1\u606f\uff0c\u884c\u4e3a\u3001\u6001\u5ea6\u548c\u60c5\u611f\u66f4\u63a5\u8fd1\u771f\u5b9e\u4eba\u7c7b\u4e92\u52a8\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u6a21\u62df\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660eSIP-CoT\u548c\u60c5\u611f\u8bb0\u5fc6\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u793e\u4f1a\u667a\u80fd\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2507.05651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05651", "abs": "https://arxiv.org/abs/2507.05651", "authors": ["Tianxing Wu", "Lizhe Cao", "Shuang Wang", "Jiming Wang", "Shutong Zhu", "Yerong Wu", "Yuqing Feng"], "title": "City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data", "comment": "9 pages, accepted by IJCAI 2025", "summary": "To advance the United Nations Sustainable Development Goal on promoting\nsustained, inclusive, and sustainable economic growth, foreign direct\ninvestment (FDI) plays a crucial role in catalyzing economic expansion and\nfostering innovation. Precise city-level FDI prediction is quite important for\nlocal government and is commonly studied based on economic data (e.g., GDP).\nHowever, such economic data could be prone to manipulation, making predictions\nless reliable. To address this issue, we try to leverage large-scale judicial\ndata which reflects judicial performance influencing local investment security\nand returns, for city-level FDI prediction. Based on this, we first build an\nindex system for the evaluation of judicial performance over twelve million\npublicly available adjudication documents according to which a tabular dataset\nis reformulated. We then propose a new Tabular Learning method on Judicial Data\n(TLJD) for city-level FDI prediction. TLJD integrates row data and column data\nin our built tabular dataset for judicial performance indicator encoding, and\nutilizes a mixture of experts model to adjust the weights of different\nindicators considering regional variations. To validate the effectiveness of\nTLJD, we design cross-city and cross-time tasks for city-level FDI predictions.\nExtensive experiments on both tasks demonstrate the superiority of TLJD (reach\nto at least 0.92 R2) over the other ten state-of-the-art baselines in different\nevaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53f8\u6cd5\u6570\u636e\u7684\u57ce\u5e02\u7ea7\u5916\u56fd\u76f4\u63a5\u6295\u8d44\uff08FDI\uff09\u9884\u6d4b\u65b9\u6cd5\uff08TLJD\uff09\uff0c\u901a\u8fc7\u6574\u5408\u53f8\u6cd5\u6027\u80fd\u6307\u6807\u548c\u533a\u57df\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ecf\u6d4e\u6570\u636e\uff08\u5982GDP\uff09\u7684FDI\u9884\u6d4b\u53ef\u80fd\u56e0\u6570\u636e\u64cd\u7eb5\u800c\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u66ff\u4ee3\u6570\u636e\u6e90\u3002\u53f8\u6cd5\u6570\u636e\u80fd\u53cd\u6620\u6295\u8d44\u5b89\u5168\u548c\u56de\u62a5\uff0c\u662f\u7406\u60f3\u9009\u62e9\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e1200\u4e07\u4efd\u516c\u5f00\u88c1\u51b3\u6587\u4ef6\u7684\u53f8\u6cd5\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb\uff0c\u5e76\u63d0\u51faTLJD\u65b9\u6cd5\uff0c\u6574\u5408\u884c\u6570\u636e\u548c\u5217\u6570\u636e\uff0c\u5229\u7528\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u8c03\u6574\u6307\u6807\u6743\u91cd\u3002", "result": "TLJD\u5728\u8de8\u57ce\u5e02\u548c\u8de8\u65f6\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08R2\u81f3\u5c110.92\uff09\uff0c\u4f18\u4e8e\u5176\u4ed6\u5341\u79cd\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u53f8\u6cd5\u6570\u636e\u662fFDI\u9884\u6d4b\u7684\u6709\u6548\u8865\u5145\uff0cTLJD\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.05716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05716", "abs": "https://arxiv.org/abs/2507.05716", "authors": ["Dipayan Sengupta", "Saumya Panda"], "title": "Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology", "comment": "13 pages, 3 tables", "summary": "Background: Evaluating AI-generated treatment plans is a key challenge as AI\nexpands beyond diagnostics, especially with new reasoning models. This study\ncompares plans from human experts and two AI models (a generalist and a\nreasoner), assessed by both human peers and a superior AI judge.\n  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI\n(o3) generated treatment plans for five complex dermatology cases. The\nanonymized, normalized plans were scored in two phases: 1) by the ten human\nexperts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical\nrubric.\n  Results: A profound 'evaluator effect' was observed. Human experts scored\npeer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;\np=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th\n(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI\nplans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It\nranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.\n  Conclusions: The perceived quality of a clinical plan is fundamentally\ndependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by\nhuman experts, was judged as superior by a sophisticated AI, revealing a deep\ngap between experience-based clinical heuristics and data-driven algorithmic\nlogic. This paradox presents a critical challenge for AI integration,\nsuggesting the future requires synergistic, explainable human-AI systems that\nbridge this reasoning gap to augment clinical care.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e13\u5bb6\u548c\u4e24\u79cdAI\u6a21\u578b\uff08\u901a\u7528\u578b\u548c\u63a8\u7406\u578b\uff09\u751f\u6210\u7684\u76ae\u80a4\u75c5\u6cbb\u7597\u8ba1\u5212\uff0c\u53d1\u73b0\u8bc4\u4f30\u8005\u7684\u6027\u8d28\u663e\u8457\u5f71\u54cd\u7ed3\u679c\uff1a\u4eba\u7c7b\u4e13\u5bb6\u504f\u597d\u540c\u884c\u8ba1\u5212\uff0c\u800cAI\u8bc4\u4f30\u8005\u66f4\u9752\u7750AI\u8ba1\u5212\u3002", "motivation": "\u968f\u7740AI\u5728\u533b\u7597\u9886\u57df\u7684\u6269\u5c55\uff0c\u8bc4\u4f30\u5176\u751f\u6210\u7684\u6cbb\u7597\u8ba1\u5212\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u65b0\u578b\u63a8\u7406\u6a21\u578b\u7684\u51fa\u73b0\u3002\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u751f\u6210\u8ba1\u5212\u7684\u5dee\u5f02\u53ca\u5176\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5341\u540d\u76ae\u80a4\u79d1\u533b\u751f\u3001\u901a\u7528AI\uff08GPT-4o\uff09\u548c\u63a8\u7406AI\uff08o3\uff09\u4e3a\u4e94\u4e2a\u590d\u6742\u76ae\u80a4\u75c5\u6848\u4f8b\u751f\u6210\u6cbb\u7597\u8ba1\u5212\u3002\u8ba1\u5212\u7ecf\u533f\u540d\u548c\u6807\u51c6\u5316\u540e\uff0c\u7531\u4eba\u7c7b\u4e13\u5bb6\u548c\u9ad8\u7ea7AI\uff08Gemini 2.5 Pro\uff09\u5206\u522b\u8bc4\u5206\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u663e\u8457\u9ad8\u4e8eAI\u8ba1\u5212\uff087.62 vs. 7.16\uff09\uff0c\u800cAI\u8bc4\u4f30\u8005\u5219\u76f8\u53cd\uff087.75 vs. 6.79\uff09\u3002\u63a8\u7406AI\uff08o3\uff09\u88abAI\u8bc4\u4f30\u8005\u8bc4\u4e3a\u6700\u4f73\uff088.20\uff09\uff0c\u4f46\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u8f83\u4f4e\uff086.97\uff09\u3002", "conclusion": "\u4e34\u5e8a\u8ba1\u5212\u7684\u8d28\u91cf\u611f\u77e5\u53d6\u51b3\u4e8e\u8bc4\u4f30\u8005\u7684\u6027\u8d28\uff0c\u63ed\u793a\u4e86\u7ecf\u9a8c\u542f\u53d1\u5f0f\u4e0e\u6570\u636e\u9a71\u52a8\u903b\u8f91\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u672a\u6765\u9700\u53d1\u5c55\u534f\u540c\u3001\u53ef\u89e3\u91ca\u7684\u4eba\u673a\u7cfb\u7edf\u4ee5\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002"}}
{"id": "2507.05755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05755", "abs": "https://arxiv.org/abs/2507.05755", "authors": ["Lukas Kuhn", "Florian Buettner"], "title": "An autonomous agent for auditing and improving the reliability of clinical AI models", "comment": null, "summary": "The deployment of AI models in clinical practice faces a critical challenge:\nmodels achieving expert-level performance on benchmarks can fail\ncatastrophically when confronted with real-world variations in medical imaging.\nMinor shifts in scanner hardware, lighting or demographics can erode accuracy,\nbut currently reliability auditing to identify such catastrophic failure cases\nbefore deployment is a bespoke and time-consuming process. Practitioners lack\naccessible and interpretable tools to expose and repair hidden failure modes.\nHere we introduce ModelAuditor, a self-reflective agent that converses with\nusers, selects task-specific metrics, and simulates context-dependent,\nclinically relevant distribution shifts. ModelAuditor then generates\ninterpretable reports explaining how much performance likely degrades during\ndeployment, discussing specific likely failure modes and identifying root\ncauses and mitigation strategies. Our comprehensive evaluation across three\nreal-world clinical scenarios - inter-institutional variation in\nhistopathology, demographic shifts in dermatology, and equipment heterogeneity\nin chest radiography - demonstrates that ModelAuditor is able correctly\nidentify context-specific failure modes of state-of-the-art models such as the\nestablished SIIM-ISIC melanoma classifier. Its targeted recommendations recover\n15-25% of performance lost under real-world distribution shift, substantially\noutperforming both baseline models and state-of-the-art augmentation methods.\nThese improvements are achieved through a multi-agent architecture and execute\non consumer hardware in under 10 minutes, costing less than US$0.50 per audit.", "AI": {"tldr": "ModelAuditor\u662f\u4e00\u4e2a\u81ea\u6211\u53cd\u601d\u5de5\u5177\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u4fee\u590dAI\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u6f5c\u5728\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u5206\u5e03\u53d8\u5316\u751f\u6210\u53ef\u89e3\u91ca\u62a5\u544a\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AI\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u56e0\u771f\u5b9e\u4e16\u754c\u53d8\u5316\uff08\u5982\u786c\u4ef6\u3001\u5149\u7167\u6216\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u8bbf\u95ee\u4e14\u53ef\u89e3\u91ca\u7684\u53ef\u9760\u6027\u5ba1\u8ba1\u5de5\u5177\u3002", "method": "ModelAuditor\u901a\u8fc7\u4e0e\u7528\u6237\u5bf9\u8bdd\u3001\u9009\u62e9\u4efb\u52a1\u7279\u5b9a\u6307\u6807\u3001\u6a21\u62df\u4e34\u5e8a\u76f8\u5173\u5206\u5e03\u53d8\u5316\uff0c\u751f\u6210\u89e3\u91ca\u6027\u80fd\u4e0b\u964d\u3001\u5931\u8d25\u6a21\u5f0f\u548c\u7f13\u89e3\u7b56\u7565\u7684\u62a5\u544a\u3002", "result": "\u5728\u4e09\u79cd\u4e34\u5e8a\u573a\u666f\u4e2d\uff0cModelAuditor\u80fd\u51c6\u786e\u8bc6\u522b\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u5efa\u8bae\u6062\u590d15-25%\u7684\u6027\u80fd\u635f\u5931\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u589e\u5f3a\u65b9\u6cd5\u3002", "conclusion": "ModelAuditor\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.05765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05765", "abs": "https://arxiv.org/abs/2507.05765", "authors": ["Bruno Jammes", "Edgar Hernando Sep\u00falveda-Oviedo", "Corinne Alonso"], "title": "Real-time monitoring of the SoH of lithium-ion batteries", "comment": "in French language, Symposium de G{\\'e}nie {\\'E}lectrique SGE 2025,\n  Jul 2025, Toulouse, France", "summary": "Real-time monitoring of the state of health (SoH) of batteries remains a\nmajor challenge, particularly in microgrids where operational constraints limit\nthe use of traditional methods. As part of the 4BLife project, we propose an\ninnovative method based on the analysis of a discharge pulse at the end of the\ncharge phase. The parameters of the equivalent electrical model describing the\nvoltage evolution across the battery terminals during this current pulse are\nthen used to estimate the SoH. Based on the experimental data acquired so far,\nthe initial results demonstrate the relevance of the proposed approach. After\ntraining using the parameters of two batteries with a capacity degradation of\naround 85%, we successfully predicted the degradation of two other batteries,\ncycled down to approximately 90% SoH, with a mean absolute error of around 1%\nin the worst case, and an explainability score of the estimator close to 0.9.\nIf these performances are confirmed, this method can be easily integrated into\nbattery management systems (BMS) and paves the way for optimized battery\nmanagement under continuous operation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5145\u7535\u7ed3\u675f\u9636\u6bb5\u653e\u7535\u8109\u51b2\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b\u7535\u6c60\u5065\u5eb7\u72b6\u6001\uff08SoH\uff09\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u9884\u6d4b\u7cbe\u5ea6\u9ad8\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "motivation": "\u5fae\u7535\u7f51\u4e2d\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u7684\u5b9e\u65f6\u76d1\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\uff0c\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5145\u7535\u7ed3\u675f\u9636\u6bb5\u7684\u653e\u7535\u8109\u51b2\uff0c\u5229\u7528\u7b49\u6548\u7535\u8def\u6a21\u578b\u53c2\u6570\u4f30\u8ba1SoH\u3002", "result": "\u5b9e\u9a8c\u6570\u636e\u663e\u793a\uff0c\u9884\u6d4b\u9000\u5316\u7535\u6c60\u7684SoH\u65f6\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7ea61%\uff0c\u89e3\u91ca\u6027\u5f97\u5206\u63a5\u8fd10.9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u96c6\u6210\u5230\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u4e2d\uff0c\u4f18\u5316\u6301\u7eed\u8fd0\u884c\u4e0b\u7684\u7535\u6c60\u7ba1\u7406\u3002"}}
{"id": "2507.05791", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05791", "abs": "https://arxiv.org/abs/2507.05791", "authors": ["Yan Yang", "Dongxu Li", "Yutong Dai", "Yuhao Yang", "Ziyang Luo", "Zirui Zhao", "Zhiyuan Hu", "Junzhe Huang", "Amrita Saha", "Zeyuan Chen", "Ran Xu", "Liyuan Pan", "Caiming Xiong", "Junnan Li"], "title": "GTA1: GUI Test-time Scaling Agent", "comment": null, "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTA1\u7684GUI\u6d4b\u8bd5\u65f6\u6269\u5c55\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u89c4\u5212\u548c\u89c6\u89c9\u5143\u7d20\u4ea4\u4e92\u7684\u4e24\u5927\u6311\u6218\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3GUI\u4ee3\u7406\u5728\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6b67\u4e49\u6027\u548c\u590d\u6742\u754c\u9762\u4e2d\u89c6\u89c9\u5143\u7d20\u4ea4\u4e92\u7684\u51c6\u786e\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u63d0\u6848\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8\u89c6\u89c9\u5143\u7d20\u4ea4\u4e92\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f8b\u5982\u5728Screenspot-Pro\u3001Screenspot-V2\u548cOSWorld-G\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a50.1%\u300192.4%\u548c67.7%\u3002", "conclusion": "GTA1\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.05816", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05816", "abs": "https://arxiv.org/abs/2507.05816", "authors": ["Shuai Zhao", "Yulin Zhang", "Luwei Xiao", "Xinyi Wu", "Yanhao Jia", "Zhongliang Guo", "Xiaobao Wu", "Cong-Duy Nguyen", "Guoming Zhang", "Anh Tuan Luu"], "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity", "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9884\u6d4b\u65e9\u4ea7\u513f\u89c6\u7f51\u819c\u75c5\u53d8\uff08ROP\uff09\u98ce\u9669\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u4e2d\u6587\u6570\u636e\u96c6CROP\u548c\u8bc4\u4f30\u6846\u67b6Affective-ROPTester\uff0c\u53d1\u73b0\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u548c\u60c5\u611f\u63d0\u793a\u5de5\u7a0b\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6548\u679c\u5e76\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u63a2\u7d22LLM\u5728ROP\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u7814\u7a76\u60c5\u611f\u56e0\u7d20\u5bf9\u6a21\u578b\u9884\u6d4b\u548c\u504f\u89c1\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faCROP\u6570\u636e\u96c6\u548cAffective-ROPTester\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08Instruction-based\u3001CoT\u3001ICL\uff09\u5e76\u7ed3\u5408\u60c5\u611f\u5143\u7d20\u8bc4\u4f30LLM\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u4ec5\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u65f6\u9884\u6d4b\u6548\u679c\u6709\u9650\uff0c\u4f46\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1b\u60c5\u611f\u504f\u89c1\u660e\u663e\uff0c\u79ef\u6781\u60c5\u611f\u63d0\u793a\u6709\u52a9\u4e8e\u51cf\u5c11\u504f\u89c1\u3002", "conclusion": "\u60c5\u611f\u654f\u611f\u7684\u63d0\u793a\u5de5\u7a0b\u5bf9\u63d0\u5347\u8bca\u65ad\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0cAffective-ROPTester\u4e3a\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u548c\u504f\u89c1\u7f13\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.05868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05868", "abs": "https://arxiv.org/abs/2507.05868", "authors": ["Alo\u00efs Rautureau", "\u00c9ric Piette"], "title": "CogniPlay: a work-in-progress Human-like model for General Game Playing", "comment": "5 pages, 1 figure", "summary": "While AI systems have equaled or surpassed human performance in a wide\nvariety of games such as Chess, Go, or Dota 2, describing these systems as\ntruly \"human-like\" remains far-fetched. Despite their success, they fail to\nreplicate the pattern-based, intuitive decision-making processes observed in\nhuman cognition. This paper presents an overview of findings from cognitive\npsychology and previous efforts to model human-like behavior in artificial\nagents, discusses their applicability to General Game Playing (GGP) and\nintroduces our work-in-progress model based on these observations: CogniPlay.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u7cfb\u7edf\u5728\u6e38\u620f\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u4ecd\u7f3a\u4e4f\u4eba\u7c7b\u76f4\u89c9\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684\u6a21\u578bCogniPlay\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u8868\u73b0\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u4eba\u7c7b\u76f4\u89c9\u548c\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u7684\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u8ba4\u77e5\u5fc3\u7406\u5b66\u548c\u524d\u4eba\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCogniPlay\u7684\u6a21\u578b\uff0c\u65e8\u5728\u6a21\u62df\u4eba\u7c7b\u76f4\u89c9\u51b3\u7b56\u3002", "result": "\u8bba\u6587\u4ecb\u7ecd\u4e86CogniPlay\u6a21\u578b\u7684\u521d\u6b65\u8fdb\u5c55\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "CogniPlay\u662f\u8fc8\u5411\u66f4\u4eba\u7c7b\u5316AI\u51b3\u7b56\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6548\u679c\u3002"}}
{"id": "2507.05886", "categories": ["cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.05886", "abs": "https://arxiv.org/abs/2507.05886", "authors": ["Aaron Bembenek"], "title": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better", "comment": "6 pages, 4 figures", "summary": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u6a21\u578b\u2014\u2014\u795e\u7ecf\u7b26\u53f7\u8f6c\u6362\u7cfb\u7edf\uff0c\u7528\u4e8e\u6784\u5efa\u795e\u7ecf\u7b26\u53f7\u81ea\u52a8\u63a8\u7406\u5de5\u5177\uff0c\u7ed3\u5408\u7b26\u53f7\u7b97\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7b26\u53f7\u81ea\u52a8\u63a8\u7406\u7cfb\u7edf\u7684\u6784\u5efa\u7f3a\u4e4f\u539f\u5219\u6027\u7f16\u7a0b\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u6216\u4fdd\u6301\u7b26\u53f7\u7b97\u6cd5\u7684\u5f3a\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u8f6c\u6362\u7cfb\u7edf\uff0c\u5c06\u7b26\u53f7\u72b6\u6001\u4e0e\u76f4\u89c9\u914d\u5bf9\uff0c\u5e76\u5728\u7b26\u53f7\u548c\u76f4\u89c9\u4e0a\u5e76\u884c\u6267\u884c\u72b6\u6001\u8f6c\u6362\u3002", "result": "\u8be5\u6a21\u578b\u6709\u671b\u6269\u5c55\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u7b26\u53f7\u7b97\u6cd5\u7684\u5f3a\u4fdd\u8bc1\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u8f6c\u6362\u7cfb\u7edf\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u795e\u7ecf\u7b26\u53f7\u81ea\u52a8\u63a8\u7406\u5de5\u5177\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u53ef\u901a\u8fc7\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u3002"}}
{"id": "2507.05891", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05891", "abs": "https://arxiv.org/abs/2507.05891", "authors": ["Robert Leppich", "Michael Stenger", "Andr\u00e9 Bauer", "Samuel Kounev"], "title": "Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection", "comment": null, "summary": "With the advent of Transformers, time series forecasting has seen significant\nadvances, yet it remains challenging due to the need for effective sequence\nrepresentation, memory construction, and accurate target projection. Time\nseries forecasting remains a challenging task, demanding effective sequence\nrepresentation, meaningful information extraction, and precise future\nprojection. Each dataset and forecasting configuration constitutes a distinct\ntask, each posing unique challenges the model must overcome to produce accurate\npredictions. To systematically address these task-specific difficulties, this\nwork decomposes the time series forecasting pipeline into three core stages:\ninput sequence representation, information extraction and memory construction,\nand final target projection. Within each stage, we investigate a range of\narchitectural configurations to assess the effectiveness of various modules,\nsuch as convolutional layers for feature extraction and self-attention\nmechanisms for information extraction, across diverse forecasting tasks,\nincluding evaluations on seven benchmark datasets. Our models achieve\nstate-of-the-art forecasting accuracy while greatly enhancing computational\nefficiency, with reduced training and inference times and a lower parameter\ncount. The source code is available at\nhttps://github.com/RobertLeppich/REP-Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6d41\u7a0b\u4e3a\u4e09\u4e2a\u6838\u5fc3\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u4e0d\u540c\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5e8f\u5217\u8868\u793a\u3001\u4fe1\u606f\u63d0\u53d6\u548c\u672a\u6765\u9884\u6d4b\u7684\u6311\u6218\u3002", "method": "\u5c06\u9884\u6d4b\u6d41\u7a0b\u5206\u89e3\u4e3a\u8f93\u5165\u5e8f\u5217\u8868\u793a\u3001\u4fe1\u606f\u63d0\u53d6\u4e0e\u8bb0\u5fc6\u6784\u5efa\u3001\u76ee\u6807\u9884\u6d4b\u4e09\u9636\u6bb5\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u5757\u914d\u7f6e\u3002", "result": "\u6a21\u578b\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u6a21\u5757\u8bc4\u4f30\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.05894", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05894", "abs": "https://arxiv.org/abs/2507.05894", "authors": ["Fathinah Izzati", "Xinyue Li", "Yuxuan Wu", "Gus Xia"], "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation", "comment": null, "summary": "Humans can imagine various atmospheres and settings when listening to music,\nenvisioning movie scenes that complement each piece. For example, slow,\nmelancholic music might evoke scenes of heartbreak, while upbeat melodies\nsuggest celebration. This paper explores whether a Music Language Model, e.g.\nMU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),\nwhich requires cross-modal information from video and music to train. To\nimprove upon existing music captioning models which focusing solely on musical\nelements, we introduce MusiScene, a music captioning model designed to imagine\nscenes that complement each music. In this paper, (1) we construct a\nlarge-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music\nUnderstanding LLaMA for the MSI task to create MusiScene, and (3) we conduct\ncomprehensive evaluations and prove that our MusiScene is more capable of\ngenerating contextually relevant captions compared to MU-LLaMA. We leverage the\ngenerated MSI captions to enhance Video Background Music Generation (VBMG) from\ntext.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMusiScene\u6a21\u578b\uff0c\u901a\u8fc7\u97f3\u4e50\u573a\u666f\u60f3\u8c61\u4efb\u52a1\uff08MSI\uff09\u751f\u6210\u4e0e\u97f3\u4e50\u5339\u914d\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e76\u7528\u4e8e\u63d0\u5347\u89c6\u9891\u80cc\u666f\u97f3\u4e50\u751f\u6210\uff08VBMG\uff09\u3002", "motivation": "\u63a2\u7d22\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u97f3\u4e50\u60f3\u8c61\u573a\u666f\uff0c\u5f25\u8865\u73b0\u6709\u97f3\u4e50\u63cf\u8ff0\u6a21\u578b\u4ec5\u5173\u6ce8\u97f3\u4e50\u5143\u7d20\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u9891-\u97f3\u9891\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u5fae\u8c03Music Understanding LLaMA\u4ee5\u521b\u5efaMusiScene\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "MusiScene\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u63cf\u8ff0\u4e0a\u4f18\u4e8eMU-LLaMA\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eVBMG\u4efb\u52a1\u3002", "conclusion": "MusiScene\u5c55\u793a\u4e86\u97f3\u4e50\u573a\u666f\u60f3\u8c61\u7684\u6f5c\u529b\uff0c\u4e3a\u8de8\u6a21\u6001\u97f3\u4e50\u7406\u89e3\u4e0e\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.05934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05934", "abs": "https://arxiv.org/abs/2507.05934", "authors": ["Baojiao Xiong", "Boheng Chen", "Chengzhi Wang", "Daxiong Luo", "Dongsheng Xu", "Dongyang Liu", "Fan Yang", "Fangyuan Li", "Fei Teng", "Feng Wang", "Fukang Qin", "Fuquan Peng", "Guanxin Tan", "Guozhi Wang", "Haibo Yu", "Haohao Gao", "Heng Liu", "Hongbo Yang", "Hongjian Zou", "Houzheng Shen", "Hu Meng", "Huan Li", "Hui Tan", "Jiali Chen", "Jianzhao Chen", "Jinliang Zhu", "Kai Wang", "Lei Wu", "Liangbing Liu", "Liuyang Bian", "Liyan He", "Long Liu", "Peiwen Li", "Penggang Shi", "Qi Ding", "Rui Hu", "Shuai Cao", "Shuai Ren", "Shuang Peng", "Teng Xie", "Weiji Chen", "Weilin Xiang", "Weixin Wu", "Xi Yin", "Xiaoxin Chen", "Xu Chen", "Yafei Wen", "Yan Hu", "Yanzhou Yang", "Yina Xie", "Yinghao Chen", "Yixuan Liao", "Yu Geng", "Yuanjiang Ouyang", "Yuanzhuo Yang", "Yuehua He", "Yushuai Peng", "Zhaoxiong Wang", "Zheng Wang", "Zhibo Zhou", "Ziyang Wu"], "title": "BlueLM-2.5-3B Technical Report", "comment": null, "summary": "We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large\nLanguage Model (MLLM) designed for efficient edge-device deployment, offering\nstrong general-purpose and reasoning capabilities. To the best of our\nknowledge, this is the first 3B-scale MLLM to support both thinking and\nnon-thinking modes, while also enabling explicit control over thinking token\nbudget. BlueLM-2.5-3B is developed through diversified data curation, key data\nresampling, hybrid heterogeneous reinforcement learning, and a high-performance\ntraining infrastructure. Our model achieves superior multimodal capacity while\npreserving competitive pure-text performance with only 2.9 billion parameters.\nWe conduct comprehensive evaluations across a broad range of multimodal and\ntext-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable\nperformance to Qwen3-4B on text-only benchmarks, and trails the larger\nKimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In\nnon-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal\nbenchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.\nAll of the aforementioned performance is achieved with substantially less total\ntraining data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to\nthe advancement of high-performance, on-device MLLMs and provides meaningful\ninsights to the research community.", "AI": {"tldr": "BlueLM-2.5-3B\u662f\u4e00\u6b3e\u7d27\u51d1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u601d\u8003\u548c\u514d\u601d\u8003\u6a21\u5f0f\uff0c\u5e76\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fd\u3001\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u548c\u591a\u6a21\u6001\u4efb\u52a1\u7684\u7ade\u4e89\u529b\u3002", "method": "\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u6574\u7406\u3001\u5173\u952e\u6570\u636e\u91cd\u91c7\u6837\u3001\u6df7\u5408\u5f02\u6784\u5f3a\u5316\u5b66\u4e60\u548c\u9ad8\u6548\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1\u3002", "result": "\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u6548\u7387\u9ad8\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "BlueLM-2.5-3B\u4e3a\u9ad8\u6027\u80fd\u8fb9\u7f18\u8bbe\u5907MLLM\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5bf9\u7814\u7a76\u793e\u533a\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2507.05938", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05938", "abs": "https://arxiv.org/abs/2507.05938", "authors": ["Yucheng Sheng", "Jiacheng Wang", "Xingyu Zhou", "Le Liang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "A Wireless Foundation Model for Multi-Task Prediction", "comment": null, "summary": "With the growing complexity and dynamics of the mobile communication\nnetworks, accurately predicting key system parameters, such as channel state\ninformation (CSI), user location, and network traffic, has become essential for\na wide range of physical (PHY)-layer and medium access control (MAC)-layer\ntasks. Although traditional deep learning (DL)-based methods have been widely\napplied to such prediction tasks, they often struggle to generalize across\ndifferent scenarios and tasks. In response, we propose a unified foundation\nmodel for multi-task prediction in wireless networks that supports diverse\nprediction intervals. The proposed model enforces univariate decomposition to\nunify heterogeneous tasks, encodes granularity for interval awareness, and uses\na causal Transformer backbone for accurate predictions. Additionally, we\nintroduce a patch masking strategy during training to support arbitrary input\nlengths. After trained on large-scale datasets, the proposed foundation model\ndemonstrates strong generalization to unseen scenarios and achieves zero-shot\nperformance on new tasks that surpass traditional full-shot baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u591a\u4efb\u52a1\u9884\u6d4b\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u4e0d\u540c\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u589e\u52a0\uff0c\u51c6\u786e\u9884\u6d4b\u5173\u952e\u7cfb\u7edf\u53c2\u6570\uff08\u5982CSI\u3001\u7528\u6237\u4f4d\u7f6e\u548c\u7f51\u7edc\u6d41\u91cf\uff09\u5bf9PHY\u548cMAC\u5c42\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u573a\u666f\u548c\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5355\u53d8\u91cf\u5206\u89e3\u7edf\u4e00\u5f02\u6784\u4efb\u52a1\uff0c\u7f16\u7801\u7c92\u5ea6\u4ee5\u5b9e\u73b0\u533a\u95f4\u611f\u77e5\uff0c\u4f7f\u7528\u56e0\u679cTransformer\u4e3b\u5e72\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u8865\u4e01\u63a9\u7801\u7b56\u7565\u652f\u6301\u4efb\u610f\u8f93\u5165\u957f\u5ea6\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6027\u80fd\uff0c\u8d85\u8d8a\u4f20\u7edf\u5168\u6837\u672c\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u7840\u6a21\u578b\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05976", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.05976", "abs": "https://arxiv.org/abs/2507.05976", "authors": ["Alessandro Umbrico", "Guido Bologna", "Luca Coraci", "Francesca Fracasso", "Silvia Gola", "Gabriella Cortellessa"], "title": "Enhancing the Interpretability of Rule-based Explanations through Information Retrieval", "comment": null, "summary": "The lack of transparency of data-driven Artificial Intelligence techniques\nlimits their interpretability and acceptance into healthcare decision-making\nprocesses. We propose an attribution-based approach to improve the\ninterpretability of Explainable AI-based predictions in the specific context of\narm lymphedema's risk assessment after lymph nodal radiotherapy in breast\ncancer. The proposed method performs a statistical analysis of the attributes\nin the rule-based prediction model using standard metrics from Information\nRetrieval techniques. This analysis computes the relevance of each attribute to\nthe prediction and provides users with interpretable information about the\nimpact of risk factors. The results of a user study that compared the output\ngenerated by the proposed approach with the raw output of the Explainable AI\nmodel suggested higher levels of interpretability and usefulness in the context\nof predicting lymphedema risk.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u53ef\u89e3\u91caAI\u5728\u4e73\u817a\u764c\u6dcb\u5df4\u7ed3\u653e\u7597\u540e\u6dcb\u5df4\u6c34\u80bf\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u63a5\u53d7\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684AI\u6280\u672f\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u63a5\u53d7\u5ea6\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u5bf9\u57fa\u4e8e\u89c4\u5219\u7684\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u5c5e\u6027\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u5c5e\u6027\u5bf9\u9884\u6d4b\u7684\u76f8\u5173\u6027\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8f93\u51fa\u6bd4\u539f\u59cb\u53ef\u89e3\u91caAI\u6a21\u578b\u7684\u8f93\u51fa\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86AI\u9884\u6d4b\u5728\u533b\u7597\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2507.05984", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.05984", "abs": "https://arxiv.org/abs/2507.05984", "authors": ["Zhijun Guo", "Alvina Lai", "Julia Ive", "Alexandru Petcu", "Yutong Wang", "Luyuan Qi", "Johan H Thygesen", "Kezhi Li"], "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening", "comment": null, "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.", "AI": {"tldr": "HopeBot\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u6291\u90c1\u75c7\u7b5b\u67e5\uff0c\u76f8\u6bd4\u9759\u6001\u5de5\u5177\u66f4\u5177\u4e92\u52a8\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u9759\u6001\u5de5\u5177\u5982PHQ-9\u7f3a\u4e4f\u4e92\u52a8\u6027\u548c\u9002\u5e94\u6027\uff0cHopeBot\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5b9e\u65f6\u6f84\u6e05\u6280\u672f\u5f00\u53d1HopeBot\uff0c\u5e76\u5728132\u540d\u6210\u5e74\u4eba\u4e2d\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "HopeBot\u4e0e\u81ea\u8bc4\u7248\u672c\u5f97\u5206\u9ad8\u5ea6\u4e00\u81f4\uff08ICC=0.91\uff09\uff0c71%\u53c2\u4e0e\u8005\u66f4\u4fe1\u4efb\u804a\u5929\u673a\u5668\u4eba\uff0c87.1%\u613f\u610f\u91cd\u590d\u4f7f\u7528\u6216\u63a8\u8350\u3002", "conclusion": "\u8bed\u97f3\u9a71\u52a8\u7684LLM\u804a\u5929\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u6291\u90c1\u75c7\u7b5b\u67e5\u7684\u53ef\u6269\u5c55\u3001\u4f4e\u8d1f\u62c5\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2507.06013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06013", "abs": "https://arxiv.org/abs/2507.06013", "authors": ["Kushal Gajjar", "Harshit Sikchi", "Arpit Singh Gautam", "Marc Hammons", "Saurabh Jha"], "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation", "comment": null, "summary": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation.", "AI": {"tldr": "CogniSQL-R1-Zero\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u51c6\u786e\u4e14\u53ef\u6267\u884c\u7684SQL\u67e5\u8be2\uff0c\u5728Text2SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u8f6cSQL\uff08Text-to-SQL\uff09\u4efb\u52a1\u4e2d\u590d\u6742\u67e5\u8be2\u751f\u6210\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u4e2d\u95f4\u76d1\u7763\u548c\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5956\u52b1\u4fe1\u53f7\uff08\u6267\u884c\u6b63\u786e\u6027\u548c\u683c\u5f0f\u6807\u7b7e\u5408\u89c4\u6027\uff09\uff0c\u65e0\u9700\u4e2d\u95f4\u76d1\u7763\u6216\u6df7\u5408\u6d41\u7a0b\u3002", "result": "\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5305\u62ecSFT CodeS-7B\u3001DeepSeek-Coder 236B\u548cMistral 123B\u5728\u5185\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06029", "abs": "https://arxiv.org/abs/2507.06029", "authors": ["Courtney Ford", "Mark T. Keane"], "title": "Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions", "comment": "7 pages, 5 figures, 1 table. Accepted at IJCAI 2025 Workshop on\n  User-Aligned Assessment of Adaptive AI Systems", "summary": "Explainable AI (XAI) methods often struggle to generate clear, interpretable\noutputs for users without domain expertise. We introduce Feature-Guided\nNeighbor Selection (FGNS), a post hoc method that enhances interpretability by\nselecting class-representative examples using both local and global feature\nimportance. In a user study (N = 98) evaluating Kannada script classifications,\nFGNS significantly improved non-experts' ability to identify model errors while\nmaintaining appropriate agreement with correct predictions. Participants made\nfaster and more accurate decisions compared to those given traditional k-NN\nexplanations. Quantitative analysis shows that FGNS selects neighbors that\nbetter reflect class characteristics rather than merely minimizing\nfeature-space distance, leading to more consistent selection and tighter\nclustering around class prototypes. These results support FGNS as a step toward\nmore human-aligned model assessment, although further work is needed to address\nthe gap between explanation quality and perceived trust.", "AI": {"tldr": "FGNS\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u91cd\u8981\u6027\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u4e13\u5bb6\u7528\u6237\u5bf9\u6a21\u578b\u9519\u8bef\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6b63\u786e\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u751f\u6210\u6e05\u6670\u3001\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faFeature-Guided Neighbor Selection (FGNS)\uff0c\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u91cd\u8981\u6027\u9009\u62e9\u7c7b\u4ee3\u8868\u6027\u6837\u672c\u3002", "result": "\u5728Kannada\u811a\u672c\u5206\u7c7b\u7684\u7528\u6237\u7814\u7a76\u4e2d\uff0cFGNS\u663e\u8457\u63d0\u5347\u4e86\u975e\u4e13\u5bb6\u7684\u51b3\u7b56\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4e14\u9009\u62e9\u7684\u90bb\u5c45\u66f4\u7b26\u5408\u7c7b\u522b\u7279\u5f81\u3002", "conclusion": "FGNS\u662f\u8fc8\u5411\u66f4\u4eba\u6027\u5316\u6a21\u578b\u8bc4\u4f30\u7684\u4e00\u6b65\uff0c\u4f46\u89e3\u91ca\u8d28\u91cf\u4e0e\u7528\u6237\u4fe1\u4efb\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06042", "categories": ["cs.AI", "03B42, 03B48"], "pdf": "https://arxiv.org/pdf/2507.06042", "abs": "https://arxiv.org/abs/2507.06042", "authors": ["Tommaso Flaminio", "Lluis Godo", "Ram\u00f3n Pino P\u00e9rez", "Lluis Subirana"], "title": "On Lockean beliefs that are deductively closed and minimal change", "comment": "18 pages, to appear in the proceedings of JELIA 2025", "summary": "Within the formal setting of the Lockean thesis, an agent belief set is\ndefined in terms of degrees of confidence and these are described in\nprobabilistic terms. This approach is of established interest, notwithstanding\nsome limitations that make its use troublesome in some contexts, like, for\ninstance, in belief change theory. Precisely, Lockean belief sets are not\ngenerally closed under (classical) logical deduction. The aim of the present\npaper is twofold: on one side we provide two characterizations of those belief\nsets that are closed under classical logic deduction, and on the other we\npropose an approach to probabilistic update that allows us for a minimal\nrevision of those beliefs, i.e., a revision obtained by making the fewest\npossible changes to the existing belief set while still accommodating the new\ninformation. In particular, we show how we can deductively close a belief set\nvia a minimal revision.", "AI": {"tldr": "\u8bba\u6587\u5728Lockean\u7406\u8bba\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u6982\u7387\u63cf\u8ff0\u7f6e\u4fe1\u5ea6\u5b9a\u4e49\u4fe1\u5ff5\u96c6\uff0c\u5e76\u89e3\u51b3\u5176\u4e0d\u95ed\u5408\u4e8e\u7ecf\u5178\u903b\u8f91\u63a8\u7406\u7684\u95ee\u9898\u3002\u63d0\u51fa\u4e24\u79cd\u95ed\u5408\u6027\u8868\u5f81\u53ca\u6700\u5c0f\u4fee\u6b63\u7684\u4fe1\u5ff5\u66f4\u65b0\u65b9\u6cd5\u3002", "motivation": "Lockean\u4fe1\u5ff5\u96c6\u5728\u7ecf\u5178\u903b\u8f91\u63a8\u7406\u4e0b\u901a\u5e38\u4e0d\u95ed\u5408\uff0c\u9650\u5236\u4e86\u5176\u5728\u4fe1\u5ff5\u53d8\u5316\u7406\u8bba\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u4f9b\u4e24\u79cd\u95ed\u5408\u6027\u8868\u5f81\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6700\u5c0f\u4fee\u6b63\u7684\u4fe1\u5ff5\u66f4\u65b0\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u4fe1\u5ff5\u96c6\u7684\u95ed\u5408\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6700\u5c0f\u4fee\u6b63\u4f7f\u4fe1\u5ff5\u96c6\u5728\u7ecf\u5178\u903b\u8f91\u4e0b\u95ed\u5408\u3002", "conclusion": "\u672c\u6587\u4e3aLockean\u4fe1\u5ff5\u96c6\u7684\u95ed\u5408\u6027\u548c\u6700\u5c0f\u4fee\u6b63\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.06057", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06057", "abs": "https://arxiv.org/abs/2507.06057", "authors": ["Bo Pang", "Yalu Ouyang", "Hangfei Xu", "Ziqi Jia", "Panpan Li", "Shengzhao Wen", "Lu Wang", "Shiyong Li", "Yanpeng Wang"], "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models", "comment": null, "summary": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models -- C32B, S32B, R32B -- from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation", "AI": {"tldr": "FEVO\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u91d1\u878d\u9886\u57df\u5e94\u7528LLM\u7684\u8fdb\u5c55\u6709\u9650\uff0c\u7f3a\u4e4f\u9488\u5bf9\u91d1\u878d\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "FEVO\u6846\u67b6\u91c7\u7528\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u3001\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u96c6FEVO-Train\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "FEVO-R32B\u5728\u4e94\u4e2a\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u9a8c\u8bc1\u4e86\u91d1\u878d\u77e5\u8bc6\u6269\u5c55\u548c\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "FEVO\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u6027\u80fd\uff0c\u4e3a\u91d1\u878d\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06077", "abs": "https://arxiv.org/abs/2507.06077", "authors": ["Iman Rahimi", "Isha Patel"], "title": "AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study", "comment": null, "summary": "This paper tackles the urgent need for efficient energy management in\nhealthcare facilities, where fluctuating demands challenge operational\nefficiency and sustainability. Traditional methods often prove inadequate,\ncausing inefficiencies and higher costs. To address this, the study presents an\nAI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm\n(GA), and SHAP (Shapley Additive Explanations), specifically designed for\nhealthcare energy management. Although LSTM is widely used for time-series\nforecasting, its application in healthcare energy prediction remains\nunderexplored. The results reveal that LSTM significantly outperforms ARIMA and\nProphet models in forecasting complex, non-linear demand patterns. LSTM\nachieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)\nof 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:\n87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm\nis applied to optimize model parameters and improve load balancing strategies,\nenabling adaptive responses to real-time energy fluctuations. SHAP analysis\nfurther enhances model transparency by explaining the influence of different\nfeatures on predictions, fostering trust in decision-making processes. This\nintegrated LSTM-GA-SHAP approach offers a robust solution for improving\nforecasting accuracy, boosting energy efficiency, and advancing sustainability\nin healthcare facilities. Future research may explore real-time deployment and\nhybridization with reinforcement learning for continuous optimization. Overall,\nthe study establishes a solid foundation for using AI in healthcare energy\nmanagement, highlighting its scalability, efficiency, and resilience potential.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u3001\u9057\u4f20\u7b97\u6cd5\u548cSHAP\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u7597\u8bbe\u65bd\u7684\u80fd\u6e90\u7ba1\u7406\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u533b\u7597\u8bbe\u65bd\u80fd\u6e90\u9700\u6c42\u6ce2\u52a8\u5927\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528LSTM\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u53c2\u6570\u548c\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u5e76\u901a\u8fc7SHAP\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "LSTM\u5728\u9884\u6d4b\u590d\u6742\u975e\u7ebf\u6027\u9700\u6c42\u65f6\u8868\u73b0\u4f18\u5f02\uff08MAE: 21.69, RMSE: 29.96\uff09\uff0c\u8fdc\u8d85Prophet\u548cARIMA\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u7597\u80fd\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5b9e\u65f6\u90e8\u7f72\u548c\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u3002"}}
{"id": "2507.06134", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06134", "abs": "https://arxiv.org/abs/2507.06134", "authors": ["Sanidhya Vijayvargiya", "Aditya Bharat Soni", "Xuhui Zhou", "Zora Zhiruo Wang", "Nouha Dziri", "Graham Neubig", "Maarten Sap"], "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety", "comment": "19 pages, 10 figures", "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment.", "AI": {"tldr": "OpenAgentSafety\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u5177\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u6db5\u76d6\u516b\u7c7b\u98ce\u9669\uff0c\u652f\u6301\u591a\u4efb\u52a1\u548c\u591a\u7528\u6237\u573a\u666f\uff0c\u7ed3\u5408\u89c4\u5219\u5206\u6790\u548cLLM\u8bc4\u4f30\uff0c\u53d1\u73b0\u591a\u4e2aLLM\u5b58\u5728\u663e\u8457\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u5b89\u5168\u6027\u8bc4\u4f30\u591a\u4f9d\u8d56\u6a21\u62df\u73af\u5883\u6216\u72ed\u7a84\u4efb\u52a1\u57df\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u771f\u5b9e\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faOpenAgentSafety\u6846\u67b6\uff0c\u652f\u6301\u771f\u5b9e\u5de5\u5177\u4ea4\u4e92\uff08\u5982\u6d4f\u89c8\u5668\u3001\u4ee3\u7801\u6267\u884c\u73af\u5883\u7b49\uff09\uff0c\u6db5\u76d6350\u591a\u9879\u4efb\u52a1\uff0c\u7ed3\u5408\u89c4\u5219\u548cLLM\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u591a\u4e2aLLM\u5728\u5b89\u5168\u8106\u5f31\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u6bd4\u4f8b\u4ece51.2%\u523072.7%\u3002", "conclusion": "OpenAgentSafety\u63ed\u793a\u4e86AI\u4ee3\u7406\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u5b9e\u9645\u90e8\u7f72\u524d\u9700\u66f4\u5f3a\u4fdd\u969c\u63aa\u65bd\u3002"}}
{"id": "2507.06187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06187", "abs": "https://arxiv.org/abs/2507.06187", "authors": ["Scott Geng", "Hamish Ivison", "Chun-Liang Li", "Maarten Sap", "Jerry Li", "Ranjay Krishna", "Pang Wei Koh"], "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains", "comment": "COLM 2025", "summary": "Improvements in language models are often driven by improving the quality of\nthe data we train them on, which can be limiting when strong supervision is\nscarce. In this work, we show that paired preference data consisting of\nindividually weak data points can enable gains beyond the strength of each\nindividual data point. We formulate the delta learning hypothesis to explain\nthis phenomenon, positing that the relative quality delta between points\nsuffices to drive learning via preference tuning--even when supervised\nfinetuning on the weak data hurts. We validate our hypothesis in controlled\nexperiments and at scale, where we post-train 8B models on preference data\ngenerated by pairing a small 3B model's responses with outputs from an even\nsmaller 1.5B model to create a meaningful delta. Strikingly, on a standard\n11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the\nperformance of Tulu 3, a state-of-the-art open model tuned from the same base\nmodel while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta\nlearning enables simpler and cheaper open recipes for state-of-the-art\npost-training. To better understand delta learning, we prove in logistic\nregression that the performance gap between two weak teacher models provides\nuseful signal for improving a stronger student. Overall, our work shows that\nmodels can learn surprisingly well from paired data that might typically be\nconsidered weak.", "AI": {"tldr": "\u901a\u8fc7\u914d\u5bf9\u504f\u597d\u6570\u636e\uff08\u5373\u4f7f\u5355\u4e2a\u6570\u636e\u70b9\u8f83\u5f31\uff09\u53ef\u4ee5\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u51fadelta\u5b66\u4e60\u5047\u8bbe\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5339\u914d\u4e86\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5f3a\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u5982\u4f55\u5229\u7528\u5f31\u6570\u636e\u70b9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fadelta\u5b66\u4e60\u5047\u8bbe\uff0c\u5229\u7528\u914d\u5bf9\u504f\u597d\u6570\u636e\u7684\u76f8\u5bf9\u8d28\u91cf\u5dee\u5f02\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u4e86\u5148\u8fdb\u6a21\u578bTulu 3\u7684\u6027\u80fd\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "delta\u5b66\u4e60\u4e3a\u5229\u7528\u5f31\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u7b80\u5355\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.06213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06213", "abs": "https://arxiv.org/abs/2507.06213", "authors": ["Cl\u00e9ment Yvernes", "Emilie Devijver", "Marianne Clausel", "Eric Gaussier"], "title": "Identifiability in Causal Abstractions: A Hierarchy of Criteria", "comment": "Accepted at the CAR Workshop at UAI2025", "summary": "Identifying the effect of a treatment from observational data typically\nrequires assuming a fully specified causal diagram. However, such diagrams are\nrarely known in practice, especially in complex or high-dimensional settings.\nTo overcome this limitation, recent works have explored the use of causal\nabstractions-simplified representations that retain partial causal information.\nIn this paper, we consider causal abstractions formalized as collections of\ncausal diagrams, and focus on the identifiability of causal queries within such\ncollections. We introduce and formalize several identifiability criteria under\nthis setting. Our main contribution is to organize these criteria into a\nstructured hierarchy, highlighting their relationships. This hierarchical view\nenables a clearer understanding of what can be identified under varying levels\nof causal knowledge. We illustrate our framework through examples from the\nliterature and provide tools to reason about identifiability when full causal\nknowledge is unavailable.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u89c2\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\u5904\u7406\u6548\u5e94\u65f6\u56e0\u679c\u56fe\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u62bd\u8c61\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u53ef\u8bc6\u522b\u6027\u6807\u51c6\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "motivation": "\u5728\u590d\u6742\u6216\u9ad8\u7ef4\u73af\u5883\u4e2d\uff0c\u5b8c\u6574\u7684\u56e0\u679c\u56fe\u901a\u5e38\u672a\u77e5\uff0c\u9650\u5236\u4e86\u5904\u7406\u6548\u5e94\u7684\u8bc6\u522b\u3002\u56e0\u679c\u62bd\u8c61\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u90e8\u5206\u4fdd\u7559\u56e0\u679c\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u5c06\u56e0\u679c\u62bd\u8c61\u5f62\u5f0f\u5316\u4e3a\u56e0\u679c\u56fe\u7684\u96c6\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u51e0\u79cd\u53ef\u8bc6\u522b\u6027\u6807\u51c6\uff0c\u6784\u5efa\u4e86\u8fd9\u4e9b\u6807\u51c6\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u901a\u8fc7\u5c42\u6b21\u7ed3\u6784\u660e\u786e\u4e86\u4e0d\u540c\u56e0\u679c\u77e5\u8bc6\u6c34\u5e73\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5de5\u5177\u6765\u63a8\u7406\u5728\u7f3a\u4e4f\u5b8c\u6574\u56e0\u679c\u77e5\u8bc6\u65f6\u7684\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u56e0\u679c\u56fe\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u5904\u7406\u6548\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.06221", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.06221", "abs": "https://arxiv.org/abs/2507.06221", "authors": ["Yuxuan Lu", "Yifan Wu", "Jason Hartline", "Michael J. Curry"], "title": "Aligned Textual Scoring Rules", "comment": null, "summary": "Scoring rules elicit probabilistic predictions from a strategic agent by\nscoring the prediction against a ground truth state. A scoring rule is proper\nif, from the agent's perspective, reporting the true belief maximizes the\nexpected score. With the development of language models, Wu and Hartline (2024)\nproposes a reduction from textual information elicitation to the numerical\n(i.e. probabilistic) information elicitation problem, which achieves provable\nproperness for textual elicitation. However, not all proper scoring rules are\nwell aligned with human preference over text. Our paper designs the Aligned\nScoring rule (ASR) for text by optimizing and minimizing the mean squared error\nbetween a proper scoring rule and a reference score (e.g. human score). Our\nexperiments show that our ASR outperforms previous methods in aligning with\nhuman preference while maintaining properness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u9f50\u8bc4\u5206\u89c4\u5219\uff08ASR\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u4e0e\u53c2\u8003\u5206\u6570\uff08\u5982\u4eba\u7c7b\u8bc4\u5206\uff09\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u4fe1\u606f\u6fc0\u53d1\u4e2d\u8bc4\u5206\u89c4\u5219\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bc4\u5206\u89c4\u5219\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u73b0\u6709\u7684\u8bc4\u5206\u89c4\u5219\u867d\u7136\u5728\u6570\u503c\u4fe1\u606f\u6fc0\u53d1\u4e2d\u5177\u6709\u6b63\u786e\u6027\uff0c\u4f46\u5728\u6587\u672c\u4fe1\u606f\u6fc0\u53d1\u4e2d\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6b63\u786e\u6027\u53c8\u80fd\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u8bc4\u5206\u89c4\u5219\u3002", "method": "\u8bbe\u8ba1\u4e86Aligned Scoring Rule\uff08ASR\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u6700\u5c0f\u5316\u4e00\u4e2a\u6b63\u786e\u8bc4\u5206\u89c4\u5219\u4e0e\u53c2\u8003\u5206\u6570\uff08\u5982\u4eba\u7c7b\u8bc4\u5206\uff09\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASR\u5728\u4fdd\u6301\u8bc4\u5206\u89c4\u5219\u6b63\u786e\u6027\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "ASR\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bc4\u5206\u89c4\u5219\uff0c\u80fd\u591f\u5728\u6587\u672c\u4fe1\u606f\u6fc0\u53d1\u4e2d\u540c\u65f6\u6ee1\u8db3\u6b63\u786e\u6027\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u9700\u6c42\u3002"}}
