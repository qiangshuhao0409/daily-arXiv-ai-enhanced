<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 分析了量子纠缠纯化协议在非瞬时经典通信网络条件下的实际可行性，确定了成功区域和资源需求


<details>
  <summary>Details</summary>
Motivation: 量子网络中纠缠对在存储期间会受到环境退相干影响，而纠缠纯化所需的经典通信延迟会进一步加剧退相干问题，需要评估实际网络条件下的协议性能

Method: 采用微观Lindblad方法处理量子动力学，使用当前城域IP网络延迟统计数据和量子内存测试平台参数，对BBPSSW和DEJMPS协议进行综合性能评估

Result: 确定了纠缠纯化成功与失败的区域边界（等保真度等高线），计算了完成多轮纯化所需的总纠缠对数量，以及超过应用特定阈值的纯化纠缠对稳态吞吐量

Conclusion: 为在当前和近未来网络中部署纠缠纯化提供了延迟预算、内存质量目标和资源开销估计

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [2] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 该论文研究队列网络中路由调度与未知传输成本的联合优化问题，提出了一种结合Lyapunov漂移惩罚和多臂老虎机技术的网络控制策略，实现了O(√T log T)的次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 传统老虎机方法无法直接应用于队列网络的路由调度问题，因为需要考虑网络稳定性约束。网络控制器需要在保证吞吐量的同时优化传输成本，这涉及到探索-利用权衡和队列动态的复杂交互。

Method: 采用Lyapunov漂移惩罚优化技术和多臂老虎机方法相结合的策略。首先证明最优成本的下界由静态优化问题决定，然后设计网络控制策略来同时处理队列稳定性和成本优化。

Result: 提出的策略实现了O(√T log T)的次线性遗憾界，相对于完全了解到达和成本信息的最优策略。仿真实验验证了该策略确实具有次线性遗憾性能。

Conclusion: 该研究成功解决了队列网络中未知传输成本下的联合路由调度问题，提出的Lyapunov-老虎机混合策略在保证网络稳定性的同时实现了接近最优的成本性能，为网络控制领域提供了新的解决方案。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [3] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 开发基于无人机的移动网络性能测量平台，用于农村和地形复杂地区的网络覆盖测试，通过空中操作收集RAN信号和端到端性能指标


<details>
  <summary>Details</summary>
Motivation: 传统众包方法在农村地区因人口密度低和地形复杂而效果不佳，需要更高效的网络性能测量解决方案

Method: 使用无人机搭载计算单元和商用蜂窝调制解调器，通过空中操作收集信号数据，利用地理空间映射和统计技术分析数据

Result: 高空接收信号功率因视距条件改善而增强，但信号质量因邻区干扰增加而下降；系统在大部分区域保持可接受的信号质量和吞吐量性能

Conclusion: 强无线电信号指标并不一定意味着测试区域的持续空间覆盖，无人机平台为农村网络测试提供了有效解决方案

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [4] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 这篇论文是一个关于IEEE 802.11mc FTM协议在室内定位中应用的综述性评估，分析了180多篇研究论文，包括FTM的实际精度、提升方法、与其他系统的结合、应用场景和安全问题


<details>
  <summary>Details</summary>
Motivation: 虽然室内定位技术有许多研究评论，但专门针对FTM协议及其最新增强功能的综述性研究仍缺失，需要填补这一空白

Method: 对过180多篇与FTM相关的研究论文进行分类和评估，包括实际精度、机器学习改进方法、与其他室内定位系统的结合、应用场景和安全问题等多个方面

Result: 识别了FTM在室内定位中的重要作用，总结了最重要的研究成果，并指出了需要进一步研究的开放问题

Conclusion: IEEE 802.11mc FTM协议在室内定位领域具有重要潜力，本文填补了该领域综述性研究的空白，为未来研究提供了重要参考

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [5] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 提出了一种基于网络协作的车辆边缘计算任务卸载方案，通过预测边缘服务器瞬时处理能力和考虑队列离散变量，实现全局最优的延迟减少性能


<details>
  <summary>Details</summary>
Motivation: 解决车辆边缘计算网络中资源利用不足和负载拥塞的问题，最小化车辆用户的整体等待延迟

Method: 基于网络协作平衡资源利用的任务卸载解决方案，通过预测边缘服务器瞬时处理能力识别过载服务器，考虑队列离散变量进行精确估计

Result: 理论和数值双重评估显示，该方案相比现有方法实现了全局最优的延迟减少性能，在真实地图虚拟环境中的可行性测试得到验证

Conclusion: 预测边缘服务器瞬时处理能力有助于识别过载服务器，考虑队列离散变量的精确估计能有效解决组合挑战，实现最优性能

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


### [6] [Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding](https://arxiv.org/abs/2509.04219)
*Leandro Márcio Bertholdo,Renan Barreto Paredes,Gabriela de Lima Marin,Cesar A. H. Loureiro,Milton Kaoru Kashiwakura Pedro de Botelho Marcos*

Main category: cs.NI

TL;DR: 巴西里约格拉蒂尔州气候灾害期间通信网络弹性研究，通过数据集构建和分析揭示光纤网络、数据中心和用户行为的脏脏性趋势


<details>
  <summary>Details</summary>
Motivation: 分析2024年5月巴西里约格拉蒂尔州严重气候灾害对通信基础设施的影响，研究通信网络在极端事件中的弹性和脏脏性

Method: 构建综合性通信数据集，包括互联网测量、光纤切断报告和互联网交换中心路由数据，并与水文和运营因素相关联

Result: 初步发现了连接恢复趋势、基础设施脏脏性以及用户行为变化，描绘了信息通信技术基础设施在极端条件下面临的挑战

Conclusion: 该数据集和预分析为灾害恢复策略和健壮通信系统开发的未来研究提供了支持，有助于提高通信网络的弹性和耐受性

Abstract: In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused
widespread damage to infrastructure, impacting over 400 cities and 2.3 million
people. This study presents the construction of comprehensive
telecommunications datasets during this climatic event, encompassing Internet
measurements, fiber cut reports, and Internet Exchange routing data. By
correlating network disruptions with hydrological and operational factors, the
dataset offers insights into the resilience of fiber networks, data centers,
and Internet traffic during critical events. For each scenario, we investigate
failures related to the Information and Communication Technology infrastructure
and highlight the challenges faced when its resilience is critically tested.
Preliminary findings reveal trends in connectivity restoration, infrastructure
vulnerabilities, and user behavior changes. These datasets and pre-analysis aim
to support future research on disaster recovery strategies and the development
of robust telecommunications systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 基于页面图结构的GUI代理框架PG-Agent，通过将序列操作转换为图结构并结合RAG技术，提升GUI代理在新场景中的适应能力


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理使用序列操作作为知识，无法抓取页面间复杂的过渡关系，导致代理难以深度感知GUI环境并法法法到新场景

Method: 设计自动化流水线将序列操作转换为页面图，显式建模页面图结构；结合RAG技术从图中检索GUI感知指南；提出PG-Agent多代理框架与任务分解策略

Result: 在多个标准测试集上进行了广泛实验，证明了PG-Agent的有效性，即使用有限的操作序列构建页面图也能获得良好效果

Conclusion: PG-Agent通过图结构表征和RAG技术，有效解决了GUI代理在新场景中的适应性问题，为GUI代理的应用提供了新的解决方案

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [8] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 该论文研究准马尔可夫因果模型中部分可识别查询的概率边界计算问题，提出了基于列生成技术的新算法，通过辅助线性整数程序序列计算概率边界，实验证明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在准马尔可夫因果模型中，当外生变量未完全指定时，无法精确计算感兴趣的概率值，因此需要研究如何计算紧概率边界。

Method: 提出新算法利用内生变量的输入概率简化多线性规划构建，对单干预场景应用列生成技术，通过一系列辅助线性整数程序计算概率边界。

Result: 实验结果表明列生成技术在计算效率上优于现有方法，并证明了外生变量多项式基数表示的可能性。

Conclusion: 该研究为准马尔可夫因果模型中的部分可识别查询提供了有效的概率边界计算方法，列生成技术展现出优越性能。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [9] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出Diffusion-AC框架，将扩散概率模型应用于空中交通冲突检测与解决，通过多模态决策解决传统DRL方法的单模态偏差问题，在密集交通场景中显著提升安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在冲突检测与解决中存在"单模态偏差"，导致决策灵活性不足，在复杂动态约束下容易出现"决策死锁"，需要新的方法来提升决策多样性和安全性。

Method: 提出Diffusion-AC框架，将策略建模为由价值函数引导的反向去噪过程，生成丰富、高质量的多模态动作分布；并设计密度递进安全课程(DPSC)训练机制，从稀疏到密集交通环境逐步学习。

Result: 在最具挑战性的高密度场景中，Diffusion-AC保持94.1%的高成功率，相比次优基线减少约59%的近空中碰撞事件，显著提升系统安全边际。

Conclusion: 扩散概率模型为安全关键的冲突解决任务提供了有效的多模态决策能力，使智能体能够灵活切换到有效的替代机动策略，实现了性能的显著飞跃。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [10] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 提出了一种新的启发式搜索算法，将控制参数作为真正的决策点处理，而不是作为约束条件，通过延迟部分扩展策略在无限决策空间中进行搜索


<details>
  <summary>Details</summary>
Motivation: 现有方法将控制参数作为嵌入式约束处理，而不是作为搜索空间中的决策点，这限制了搜索效率。需要一种能够显式处理控制参数作为真正决策点的方法

Method: 开发了最佳优先启发式搜索算法，采用延迟部分扩展策略，在控制参数定义的无限决策空间中进行系统搜索

Result: 算法在涉及控制参数的规划问题求解中表现出竞争力，证明了在特定条件下的极限完备性

Conclusion: 该方法为处理控制参数提供了一种高效的替代方案，通过将控制参数作为决策点而非约束条件，提升了搜索性能

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [11] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 该论文提出了一种动态规划框架，让LLM智能体能够灵活决定何时进行规划，通过两阶段训练（监督微调+强化学习）在长时程任务中实现更高效的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct要求LLM在每次行动前都进行规划，这计算成本高且在长时程任务中性能下降，而完全不规划又会限制性能。需要一种动态决定规划时机的解决方案。

Method: 提出两阶段训练流程：1）在多样化合成数据上进行监督微调，为动态规划做准备；2）在长时程环境中使用强化学习来精炼这种能力。

Result: 在Crafter环境中的实验表明，动态规划智能体更样本高效，能持续实现更复杂的目标，且能够有效利用人类编写的规划来超越独立能力。

Conclusion: 这是首个探索训练LLM智能体进行动态测试时计算分配的研究，为更高效、自适应和可控的智能体系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [12] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 提出了KG-SMILE框架，通过扰动分析和线性代理模型为Graph RAG提供token和组件级的可解释性，提升生成AI的透明度和可信度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型产生幻觉和不可验证声明的问题，特别是在医疗等敏感领域需要高精度的场景下，RAG方法虽然能提高准确性但仍缺乏透明度

Method: 开发了方法无关的基于扰动的框架，通过控制扰动、计算相似度和训练加权线性代理模型，识别对生成输出影响最大的图实体和关系

Result: KG-SMILE能够产生稳定且与人类认知一致的解释，在保真度、忠实度、一致性、稳定性和准确性等指标上表现良好

Conclusion: 该框架能够在保持模型有效性的同时提升可解释性，促进机器学习技术的透明度和可信度

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [13] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC是一个新的AI推理测试平台，基于因果世界模型构建，用于评估语言模型在低数据和分布外环境下的推理能力，提供观察、干预和反事实反馈。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI系统在有限数据和分布偏移下适应新问题的推理挑战，需要建立一个能够系统评估模型在因果推理方面能力的测试环境。

Method: 基于结构因果模型构建完全指定的因果世界模型，通过原则性数据增强提供观察、干预和反事实反馈，作为少样本上下文学习演示。

Result: 开发了CausalARC测试平台，展示了其在四个语言模型评估场景中的应用：测试时训练的抽象推理、上下文学习的反事实推理、程序合成以及因果发现与逻辑推理。

Conclusion: CausalARC为评估AI系统在因果推理方面的能力提供了一个系统化的测试环境，特别是在低数据和分布外场景下的表现。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [14] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 提出了Embodied-LM神经符号系统，通过基于图像模式的具身认知结构来增强LLM的逻辑推理能力，使用ASP进行空间推理，在逻辑演绎问题上表现出更好的可解释性和推理效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言理解方面取得显著进展，但在逻辑推理方面仍然容易出错，缺乏人类类似的稳健心理表征能力。

Method: 引入基于图像模式（源自感觉运动经验的重复认知模式）的神经符号系统，使用答案集编程（ASP）进行声明式空间推理操作化空间认知结构。

Result: 实验证明LLM能够通过具身认知结构解释场景，这些结构可形式化为可执行程序，产生的表征支持有效的逻辑推理并增强可解释性。

Conclusion: 虽然当前实现专注于空间原语，但为纳入更复杂和动态表征建立了计算基础，展示了具身认知结构在增强LLM逻辑推理方面的潜力。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [15] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 论文揭示了RL提升LLM推理能力的机制：存在从低层技能到高层战略规划的两阶段学习动态，并提出HICRA算法专注于规划token的优化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RL能有效提升大语言模型的复杂推理能力，但其成功背后的机制仍不明确，需要揭示RL训练过程中的关键动态和瓶颈。

Method: 提出HIerarchy-Aware Credit Assignment (HICRA)算法，通过集中优化高影响力的规划token来解决现有RL算法优化压力分散的问题。

Result: HICRA显著优于强基线方法，验证了关注战略瓶颈对解锁高级推理能力的关键作用。语义熵被证明是衡量战略探索的优越指标。

Conclusion: RL训练中的推理层次涌现类似于人类认知的高层战略规划与低层程序执行分离，专注于战略瓶颈的优化是提升推理能力的关键。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [16] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 本文研究了时间序列分类中SHAP解释方法的优化，发现分段数量比具体分段算法对解释质量影响更大，等长分段优于大多数定制算法，并提出了一种新的基于长度的归一化技术来提升解释质量。


<details>
  <summary>Details</summary>
Motivation: SHAP方法在时间序列分类中计算复杂度高，虽然通过分段聚合可以降低计算成本，但最优分段策略选择仍是一个开放问题，需要研究不同分段算法如何影响解释质量。

Method: 研究了8种时间序列分割算法，使用InterpretTime和AUC Difference两种评估方法，在多变量和单变量时间序列上进行实验，并提出了基于长度的归因归一化技术。

Result: 发现分段数量对解释质量的影响大于具体分段方法，等长分段在大多数情况下优于定制算法，新提出的归一化技术能持续提升归因质量。

Conclusion: 对于时间序列SHAP解释，分段策略选择应优先考虑分段数量而非复杂算法，简单的等长分段配合长度加权归一化可获得最佳解释效果。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [17] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 人格化红队测试方法PersonaTeaming，通过在对手提示生成中引入不同人设，显著提高了AI模型的攻击成功率，同时保持提示多样性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化红队测试方法忽视了人员身份和背景对测试策略的影响，无法发现广泛的潜在风险。

Method: 提出PersonaTeaming方法，包括：1)基于"红队专家"和"普通用户"人设的提示突变技术；2)动态人设生成算法；3)新的"突变距离"指标来衡量多样性。

Result: 实验结果显示人设突变方法将攻击成功率提高了最高达144.1%，同时保持了提示的多样性。

Conclusion: 证明了将人员身份引入自动化红队测试的有效性，为自动化与人工红队测试的补充关系提供了新的研究方向。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [18] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 该研究系统分析了大型语言模型的人格特质，发现指令对齐能稳定特质表达但自报告特质无法可靠预测行为，人格注入对行为影响有限


<details>
  <summary>Details</summary>
Motivation: 理解LLMs是否表现出类似人类的人格特质模式，现有研究主要依赖简化的自我报告而缺乏行为验证

Method: 从三个维度系统分析LLM人格：(1)训练阶段特质动态演变；(2)自报告特质在行为任务中的预测效度；(3)人格注入等干预措施的影响

Result: 指令对齐稳定特质表达并强化特质相关性，但自报告特质无法可靠预测行为，人格注入能引导自报告但行为影响有限

Conclusion: LLM表面特质表达与行为一致性存在差异，挑战了关于LLM人格的假设，强调需要对对齐和可解释性进行更深入评估

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [19] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 这篇论文通过实验证明大语言模型在人类实验中作为代理参与者时存在内部不一致性问题，虽然能生成与人类相似的回答，但在不同实验环境下行为不一致


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型作为人类实验参与者替代品的可靠性，重点关注代理是否在不同实验设置下保持内部一致性

Method: 设计了能够(a)揭示代理内部状态和(b)在基础对话环境中检验代理行为的研究方案，通过一组行为假设来评估代理的内部一致性

Result: 发现不同模型家族和不同模型大小的LLMs都存在显著的内部不一致性，虽然能生成与人类对应者相似的回答，但无法保持内部一致性

Conclusion: 大语言模型作为人类实验参与者的替代品存在重要的能力缺口，其内部不一致性影响了在人类实验研究中的准确性和可靠性

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [20] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard是一个增强的RAG框架，通过并行查询技术文档和安全文档，并分配单独的检索预算，显著提高了海上风电维护中LLM的安全召回率


<details>
  <summary>Details</summary>
Motivation: 传统LLM在高度专业化或意外场景中经常失败，海上风电维护对准确性和安全性要求极高，需要确保技术深度和安全覆盖

Method: 提出RAGuard框架，并行查询两个索引（技术手册和安全关键文档），分配单独的检索预算；开发SafetyClamp扩展，获取更大的候选池并进行硬钳位安全保证

Result: 安全召回率从RAG的几乎0%提高到RAGuard的50%以上，同时保持技术召回率在60%以上

Conclusion: RAGuard和SafetyClamp有潜力为关键维护环境中LLM驱动的决策支持建立新的安全保证标准

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [21] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 基于大语言模型构建的供应链规划组件（SCPA）框架，能够理解域知识、分解任务、使用工具并生成有根据的规划报告，在JD.com实际场景中有效提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 供应链管理中需要应对需求预测、库存管理、补货等多重挑战，如何在确保解释性、效率和可靠性的前提下进行长期规划和动态调整是一个实际难题。大语言模型的发展为解决这些问题提供了新工具。

Method: 构建了一个供应链规划组件（SCPA）框架，该框架能够：1）理解领域知识和运营商需求；2）将复杂任务分解为子任务；3）利用或创建工具来处理各个子任务；4）生成基于证据的规划报告。

Result: 在JD.com的实际供应链场景中部署并验证了该框架的可行性，有效减少了人工劳动强度，同时提高了规划的准确性、库存可用性以及其他关键指标的表现。

Conclusion: 这项研究证明了大语言模型以助手组件在供应链管理领域的实际应用价值，为解决复杂的供应链规划问题提供了一种高效、可靠且可解释的新方法。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [22] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 提出了Meta-Policy Deliberation Framework (MPDF)，让LLM智能体学习去中心化的元认知策略，通过SoftRankPO算法实现稳定训练，在数学和通用推理任务上相比现有方法获得4-5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统的固定协作协议忽视了智能体内部的审议能力，将其视为被动执行者，无法根据内部认知状态（如不确定性或置信度）调整策略。

Method: 引入MPDF框架，智能体学习对高层元认知动作（坚持、精炼、让步）的去中心化策略。开发SoftRankPO强化学习算法，通过基于奖励排名的平滑正态分位数映射来稳定训练。

Result: 在五个数学和通用推理基准测试中，MPDF与SoftRankPO相比六种最先进的启发式和基于学习的多智能体推理算法，平均准确率绝对提升4-5%。

Conclusion: 这项工作提出了学习自适应、元认知策略的新范式，将重点从设计固定协议转向学习动态审议策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [23] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 评估大语言模型在无家可归政策制定中与领域专家的一致性，开发包含四个地理区域的决策场景基准，并通过基于代理的模型模拟社会影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在高风险领域的应用增加，需要评估其在复杂社会政策制定（特别是影响全球1.5亿人的无家可归问题）中与专家意见的一致性。

Method: 开发包含四个地理区域（美国南本德、西班牙巴塞罗那、南非约翰内斯堡、中国澳门）决策场景的新基准，基于能力发展方法框架，建立连接基准政策与基于代理模型的自动化流程。

Result: 研究结果显示LLMs在社会政策制定方面具有潜力，通过与本地领域专家合作引入负责任的护栏和情境校准，可以大规模提供有价值的替代政策见解。

Conclusion: LLMs在负责任的使用框架下，能够为复杂的社会政策制定提供有价值的规模化见解，特别是在无家可归问题缓解方面展现出应用前景。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [24] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 基于Model Context Protocol的无需训练、防幽灵的OMOP CDM医学术语映射系统，通过外部资源查询提高映射效率和准确性


<details>
  <summary>Details</summary>
Motivation: OMOP CDM标准化过程中的源医学术语映射工作耗时费力且容易出错，而大语言模型存在幽灵问题不适合直接临床部署

Method: 采用Model Context Protocol(MCP)标准化框架，允许LLM与外部资源和工具交互，实现实时词汇查询和结构化推理输出

Result: 系统显著提高了映射效率和准确性，支持可解释的映射过程，适用于探索性和生产环境

Conclusion: 该方法在不需训练的情况下有效防止LLM幽灵，为OMOP CDM数据标准化提供了高效、准确的自动化映射解决方案

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [25] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 一种多模态AI框架，通过社交媒体数据分析游客对历史城区的视觉关注、色彩偏好和情感反应，为可持续城市规划提供数据支撑


<details>
  <summary>Details</summary>
Motivation: 理解游客对历史城区的感知对于可持续、人本为本的城市规划至关重要，现有研究缺乏多维度的整合分析框架

Method: 使用多模态社交媒体数据，整合了焦点提取、色彩主题分析和情感挖掘。具体包括：使用精调语义分割模型识别视觉关注区域；通过聚类方法提取主导色彩并分析空间分布；采用混合情感分析方法（规则基础和多任务BERT模型）评估游客评论

Result: 发现了视觉期望与实际环境之间的显著差异，反映了风格偏好和感知偏差。在美学吸引力和情感反应方面呈现出空间差异。满意度评估涵盖四个维度：游客活动、建筑环境、服务设施和业态类型

Conclusion: 该框架提供了一种整合的、数据驱动的方法来解码游客感知，为旅游业、遗产保护和公共空间设计的管理决策提供了有价值的见解和支撑

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [26] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 使用知识图形式进行生成式AI可靠性评估，通过比较确定性KG和LLM生成KG的结构偏差来检测幻觉和语义异常


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型的可靠性问题（幻觉、语义漏洒、偏见），充当黑盒模型评估的透明性和可扩展性挑战

Method: 构建两个并行知识图形：确定性KG（规则基础）和LLM生成KG（实时数据），使用KG指标（ICR、IPR、CI）计算结构偏差，通过动态阈值进行异常检测

Result: 建立了一个自动化实时监控框架，能够主动识别和标记显著偏差，及时检测语义异常或幻觉

Conclusion: 通过结构化、指标驱动的KG比较方法，提供了一个健壮且可扩展的生成式AI可靠性评估框架

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [27] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: 提出E&E混合策略，结合局部新颖性搜索和目标导向探索，使用视觉语言模型生成语义目标，在连续细胞自动机中发现更多样化的模式


<details>
  <summary>Details</summary>
Motivation: 传统新颖性搜索在连续细胞自动机中容易陷入局部最优，无法探索遥远未开发区域，需要新的探索策略来突破局部新颖性边界

Method: E&E混合策略：交替进行局部新颖性驱动的扩展和目标导向的远征，使用VLM生成语言目标描述假设模式，在语义空间中进行评估和目标生成

Result: 在Flow Lenia连续细胞自动机上测试，E&E比现有方法发现更多样化解决方案，谱系分析显示远征产生的解决方案对长期探索有不成比例的影响

Conclusion: E&E能够突破局部新颖性边界，以人类可理解的方式探索行为空间，为人工生命等领域的开放式探索提供了有前景的模板

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [28] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: 本文介绍了Facebook Marketplace Assistant (FaMA)，一个基于LLM的智能代理助手，通过自然语言交互简化C2C电商平台的复杂操作，将传统GUI界面转变为对话式入口，实现了98%的任务成功率和2倍的交互速度提升。


<details>
  <summary>Details</summary>
Motivation: C2C电商平台的复杂GUI界面给买卖双方带来了繁琐的操作体验，传统交互方式效率低下，需要一种更直观、高效的交互方式来简化市场活动管理。

Method: 开发基于大语言模型的智能代理助手FaMA，通过自然语言命令解析自动化关键高摩擦工作流程，为卖家提供简化列表更新和批量消息功能，为买家提供对话式产品搜索。

Result: FaMA在解决市场复杂任务上达到98%的成功率，交互时间最多可缩短2倍，证明了对话式AI代理作为传统应用界面轻量级替代方案的有效性。

Conclusion: 基于LLM的智能代理助手为C2C电商平台提供了更轻量、易用的交互范式，通过对话式界面显著提升了用户体验和操作效率，代表了从反应式生成系统向目标导向自主代理的重要转变。

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [29] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: DeepMedix-R1是一个用于胸部X光片解读的医疗基础模型，通过三阶段训练实现透明推理和局部可解释性，在报告生成和视觉问答任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前医疗基础模型通常以黑盒方式生成答案，缺乏透明的推理过程和局部可解释性，这阻碍了其在临床实践中的部署应用。

Method: 采用顺序训练流程：先在精选的CXR指令数据上微调获得基础解读能力，然后通过高质量合成推理样本实现冷启动推理，最后通过在线强化学习优化推理质量和生成性能。

Result: 在报告生成任务上分别比LLaVA-Rad和MedGemma提升14.54%和31.32%，在视觉问答任务上分别比MedGemma和CheXagent提升57.75%和23.06%。专家评审显示推理步骤的可解释性和临床合理性显著优于Qwen2.5-VL-7B模型。

Conclusion: DeepMedix-R1推动了医疗基础模型向整体性、透明性和临床可操作性方向发展，为CXR解读提供了更可靠的解决方案。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [30] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: WorMI框架通过将大型语言模型与领域特定世界模型进行测试时组合，实现具身智能体在未见领域的零样本和少样本自适应，无需大量数据收集或重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI中智能体在新领域需要大量数据收集和重新训练才能适应的挑战，提高跨领域适应性和数据效率。

Method: 采用原型化世界模型检索方法，利用基于轨迹的抽象表示匹配，开发世界级复合注意力机制来整合多个世界模型的知识并对其齐中间表示。

Result: 在VirtualHome和ALFWorld基准测试中表现出优于多个基于LLM的方法的零样本和少样本性能。

Conclusion: WorMI框架展示了在需要适应性和数据效率的具身智能体场景中可扩展实际部署的潜力。

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [31] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: MPR是一个混合框架，通过元策略记忆和双重机制（软内存引导解码和硬规则可接受性检查）来提升LLM智能体的跨任务适应性和执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有反思策略产生的痕迹是临时性的、任务特定的，无法跨任务重用；而基于强化学习的方法需要大量参数更新和计算资源。需要一种既能重用纠正知识又不需要模型权重更新的方法。

Method: 提出Meta-Policy Reflexion (MPR)框架，将LLM生成的反思整合为结构化的元策略记忆(MPM)，在推理时通过软内存引导解码和硬规则可接受性检查(HAC)两种机制应用该记忆。

Result: 实验结果表明，与Reflexion基线相比，MPR在执行准确性和鲁棒性方面取得了持续提升，规则可接受性检查进一步提高了稳定性。

Conclusion: MPR能够外部化可重用的纠正知识而不需要模型权重更新，强制执行领域约束以减少不安全或无效动作，同时保持了基于语言的反思的适应性。

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [32] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO是一个基于大语言模型的框架，用于自动增强伪布尔优化(PBO)局部搜索求解器，在多个基准测试中显著优于现有局部搜索方法，并与最先进求解器保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 伪布尔优化(PBO)是组合问题建模的强大框架，局部搜索求解器性能优秀但设计需要大量专家努力和手动调优。虽然大语言模型在算法设计自动化方面显示出潜力，但在优化PBO求解器方面的应用尚未探索。

Method: 提出AutoPBO框架，利用大语言模型自动增强PBO局部搜索求解器，在四个公共基准测试上进行实验评估，包括真实世界基准、PB竞赛基准、整数线性规划优化基准和手工组合基准。

Result: AutoPBO相比之前的局部搜索方法有显著改进，同时与六种最先进的竞争对手（包括局部搜索PBO求解器、完整PB求解器和混合整数规划求解器）保持竞争力。

Conclusion: AutoPBO为自动化局部搜索求解器设计提供了一种有前景的方法，展示了LLM在优化组合优化求解器方面的潜力。

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [33] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: 提出了CoT-Space理论框架，将LLM推理从离散的token预测任务重新定义为连续推理级语义空间中的优化过程，解释了最优CoT长度的收敛性


<details>
  <summary>Details</summary>
Motivation: 传统token级RL框架无法与多步推理过程（如Chain-of-Thought）的推理级性质对齐，存在显著理论空白

Method: 引入CoT-Space框架，从噪声视角和风险视角分析推理过程，将LLM推理重新构建为连续语义空间中的优化问题

Result: 证明了最优CoT长度的收敛是欠拟合和过拟合之间基本权衡的自然结果，实验提供了强有力的实证验证

Conclusion: 该框架不仅为过度思考等实证现象提供了连贯解释，还为开发更有效和可泛化的推理智能体奠定了坚实的理论基础

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [34] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: Oruga是一个实现表示系统理论(RST)的系统，包含核心数据结构、通信语言和结构转换引擎，旨在让机器像人类一样灵活使用不同表示形式


<details>
  <summary>Details</summary>
Motivation: 人类能够灵活使用图表、变换表示形式和跨领域类比，希望让机器具备这种能力以更好地与人类兼容

Method: 基于表示系统理论(RST)，开发了Oruga系统，包含核心数据结构、专用通信语言和结构转换引擎，采用结构转移方法进行表示转换

Result: 实现了Oruga系统的核心架构和语言，展示了结构转移方法能够执行的表示转换示例

Conclusion: Oruga系统成功实现了RST理论的关键方面，为机器获得人类式灵活表示能力提供了可行框架

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [35] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: 研究表明，在神经符号LLM推理中，形式语言的选择是一个被忽视但关键的因素，不同形式语言对LLM的语法和语义推理能力有显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多种任务上表现优异，但其形式推理能力仍然不足。神经符号LLM推理方法虽然前景广阔，但其成功的关键因素尚不清楚，特别是形式语言选择的影响未被充分研究。

Method: 通过比较四种形式语言在三个数据集和七个大型语言模型上的表现，分析形式语言选择对神经符号推理的影响，并探讨不同LLM之间的差异效应。

Result: 研究发现形式语言的选择显著影响LLM的语法和语义推理能力，不同LLM对形式语言的响应存在差异，证实了中间语言挑战的重要性。

Conclusion: 形式语言的选择是神经符号LLM推理成功的关键因素之一，需要根据具体任务和模型特性仔细选择合适的形式语言，这一发现为提升LLM形式推理能力提供了重要指导。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [36] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 结合强化学习和搜索路径规划，通过预计算近最优路径来约束求解器搜索空间，在保持燃油消耗接近最优的同时，将计算速度提升最高50%


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下需要快速重新计算航班路径，传统求解器计算速度较慢，需要加速航线优化过程

Method: 训练RL代理基于位置和大气数据预计算近最优路径，在运行时用这些路径约束底层路径规划求解器，在初始猜测的特定距离内寻找解决方案

Result: 燃油消耗与传统无约束求解器几乎相同（偏差通常在1%以内），计算速度相比传统单独求解器提升最高50%

Conclusion: 该方法有效减少了求解器搜索空间大小，显著加速了航线优化，虽然不能保证全局最优性，但在实际应用中表现出色

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [37] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: 研究DQN和CFR算法在Leduc Hold'em扑克游戏中是否表现出诈唬行为，发现两种算法都以不同方式诈唬，但成功率相似，表明诈唬是游戏本质而非算法特性


<details>
  <summary>Details</summary>
Motivation: 虽然诈唬是人类扑克游戏中的基本技能，但现有计算机扑克研究主要关注胜率等性能指标，而忽视了诈唬行为的研究

Method: 设计实验让基于强化学习的DQN算法和基于博弈论的CFR算法在Leduc Hold'em中对战，记录并分析它们的行动模式

Result: 两种算法都表现出诈唬行为，但方式不同：诈唬尝试率有差异，但成功诈唬（对手弃牌）的比例大致相同

Conclusion: 诈唬是扑克游戏的本质特征，而非特定算法的特性。未来研究应关注不同诈唬风格和完整扑克游戏

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [38] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 论文认为即使AI在能力上超越人类，但由于缺乏中枢神经系统带来的情感体验和道德理解，AI永远无法真正取代人类成为宇宙的领导者


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能够真正超越人类成为宇宙的领导者，挑战了当前关于AGI将全面超越人类的普遍观点

Method: 通过哲学分析和神经科学视角，比较人类中枢神经系统与AI系统的本质差异，论证情感体验和道德理解的重要性

Result: 提出中枢神经系统是人类独特优势的核心，情感体验是道德发展和领导资格的基础，这是AI无法通过技术手段复制的

Conclusion: DNA而非硅基技术才是宇宙领导权的最佳基础，人类的中枢神经系统赋予了我们AI无法替代的道德领导资格

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [39] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 通过基于向导动作图的预定义域特定语言，形式化模型烹饪过程中的复杂操作、转移和并发结构


<details>
  <summary>Details</summary>
Motivation: 烹饪过程具有内在的复杂性和模糊性，需要形式化表达方法来支持精确的机器理解和自动化执行

Method: 设计了一种可扩展的域特定语言，将菜谱表示为向导动作图，包含过程、转移、环境、并发性和组合结构

Result: 初步手工评估在英式全套早餐菜谱上证明了该DSL的表达能力和适用性

Conclusion: 这是构建以动作为中心的烹饪本体论的初步尝试，通过时间图实现结构化机器理解和可扩展的烹饪自动化

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [40] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 该论文研究了马尔可夫逻辑网络（MLN）在域大小趋于无穷大时的分布特性，分析了三种具体MLN示例的极限行为，并比较了量化自由MLN与提升贝叶斯网络的渐近表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究MLN在无限大域上的分布特性，了解软约束权重对极限行为的影响，以及比较不同概率图模型在渐近意义上的表达能力差异。

Method: 通过分析三种具体MLN示例：(1)单一元关系符号的量化自由MLN；(2)偏好较少三角形的图MLN；(3)偏好较少高度数顶点的图MLN，研究其随机结构在域大小趋于无穷时的性质。

Result: 发现不同软约束会导致完全不同的极限行为，权重可能影响也可能不影响极限行为；证明了量化自由MLN与提升贝叶斯网络在渐近意义上不可比较；在大域上MLN分布与均匀分布集中在完全不同的可能世界空间区域。

Conclusion: MLN的极限行为高度依赖于所使用的软约束类型，不同形式的概率图模型在渐近表达能力上存在本质差异，这为理解大规模概率推理系统的理论性质提供了重要见解。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [41] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 一种利用Delphi研究结构和Kano模型的结构化方法，通过故事设计专家评估AI生成游戏故事的质量，以指导游戏开发者优先考虑满意度关键因素。


<details>
  <summary>Details</summary>
Motivation: 为了系统化评估AI生成游戏故事的质量，并为游戏开发者提供优先级指南，以提高与生成式AI协同创作的故事质量和玩家满意度。

Method: 采用Delphi研究结构，组织故事设计专家小组，综合文献中的故事质量维度和专家见解，并将其映射到Kano模型框架中来分析各维度对玩家满意度的影响。

Result: 研究得出了一套结构化的评估指标体系，能够识别和优先考虑那些对玩家满意度最关键的故事质量维度，为游戏开发者提供了具体的优先级指南。

Conclusion: 该结构化方法有效地支持了AI生成游戏故事的质量评估，通过Kano模型框架帮助开发者更好地优先考虑那些能最大程度提升玩家满意度的故事质量维度。

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [42] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: EvoEmo是一个进化强化学习框架，通过优化动态情感表达来提升LLM在复杂多轮谈判中的表现，相比传统策略获得更高成功率和效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在谈判中忽视了情感的功能性作用，仅生成被动的情感响应，使其容易受到对手操纵和战略利用

Method: 将情感状态转换建模为马尔可夫决策过程，采用基于种群的遗传优化算法，在不同谈判场景中演化高奖励情感策略

Result: EvoEmo在广泛实验和消融研究中始终优于基线方法（普通策略和固定情感策略），实现了更高的成功率、效率和买家节省

Conclusion: 自适应情感表达对于实现更有效的多轮谈判LLM代理至关重要，EvoEmo框架为此提供了有效解决方案

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [43] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 本文分析了AlphaZero在测试环境可能发生变化时的部署问题，提出了简单修改框架的方法来显著提升性能，即使在低规划预算下也有效


<details>
  <summary>Details</summary>
Motivation: AlphaZero通常假设训练和测试环境不变，这限制了其适用性。研究如何在环境可能变化的测试环境中部署AlphaZero代理

Method: 对标准AlphaZero框架进行简单修改的组合方法，特别针对低规划预算设置

Result: 显著提升了在变化测试环境中的性能表现

Conclusion: 通过简单的框架修改可以有效增强AlphaZero在环境变化场景下的适应性和性能

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [44] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: MBTI-in-Thoughts框架通过MBTI人格类型提示工程增强LLM代理，实现基于心理学的人格调节，无需微调即可控制认知和情感行为。


<details>
  <summary>Details</summary>
Motivation: 将心理学理论（如MBTI）与LLM行为设计结合，通过人格原型调节来增强AI代理的有效性和行为一致性。

Method: 使用提示工程为LLM代理注入不同MBTI人格原型，通过官方16Personalities测试自动验证特质持久性，支持多智能体通信协议实验。

Result: 人格调节在不同任务中产生一致的行为偏差：情感表达型代理在叙事生成中表现优异，分析型代理在博弈论设置中采用更稳定策略，自我反思能改善合作和推理质量。

Conclusion: 该框架为心理增强型AI代理奠定了基础，无需微调即可实现人格调节，且方法可推广到其他心理学框架（如Big Five、HEXACO、Enneagram）。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [45] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 该论文提出了一种概念级外部记忆方法，通过从推理轨迹中提取可重用的模块化抽象概念，实现无需权重更新的测试时持续学习，在ARC-AGI基准上相对基线提升7.5%。


<details>
  <summary>Details</summary>
Motivation: 虽然推理时扩展使LLMs能够进行更长的推理追踪，但这些追踪中发现的模式和见解在上下文窗口重置后立即被丢弃。外部记忆是持久化这些发现的自然方式，但现有方法主要基于实例级记忆，缺乏可重用性和扩展性。

Method: 提出概念级记忆方法：从解决方案轨迹中提取可重用的模块化自然语言抽象概念；为新的查询选择性检索和集成相关概念到提示中；引入新的抽象策略从推演中提取要点和检索新查询的条目。

Result: 在ARC-AGI基准上获得7.5%的相对增益，性能随推理计算持续扩展；抽象概念在所有测试的推理计算规模上都优于基线；动态更新记忆在测试时优于固定记忆设置。

Conclusion: 概念级记忆提供了一种有效的测试时持续学习方法，通过抽象和重用推理模式实现自我改进，证明了外部记忆在提升LLMs推理能力方面的重要价值。

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [46] [Distributed MIMO With Over-the-Air Phase Calibration Integrated Into the TDD Flow](https://arxiv.org/abs/2509.03722)
*Khac-Hoang Ngo,Erik G. Larsson*

Main category: cs.IT

TL;DR: 本文提出了一种在分布式MIMO系统中通过调整TDD时隙结构来集成AP间空中相位校准测量的方法，分析了校准资源与频谱效率的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 分布式MIMO系统中多个AP需要周期性相位校准才能实现联合相干波束成形，传统方法需要专门的校准时间，影响系统效率。

Method: 通过调整TDD时隙结构的上行/下行切换点，创建短时间段供AP间进行空中测量，并将此技术扩展到大型网络。

Result: 分析了共轭波束成形和迫零波束成形下校准资源投入与频谱效率之间的权衡关系，证明了该方法的可行性。

Conclusion: 通过将空中AP间测量集成到TDD流程中，可以实现分布式MIMO系统的相位校准，该方法具有良好的可扩展性和实用性。

Abstract: Reciprocity-based, joint coherent downlink beamforming from multiple access
points (APs) in distributed multiple-input multiple-output (MIMO) with
independent local oscillators (LOs) requires the APs to be periodically
phase-calibrated (a.k.a. phase-synchronized or phase-aligned). Such phase
alignment can be accomplished by bidirectional over-the-air measurements
between the APs. In this paper, we show how such over-the-air measurements can
be integrated into the time-division duplexing (TDD) flow by appropriately
shifting the uplink/downlink switching points of the TDD slot structure,
creating short time segments during which APs can measure on one another. We
also show how this technique scales to large networks. Furthermore, we
analytically characterize the tradeoff between the amount of resources spent on
calibration measurements and the resulting spectral efficiency of the system,
when conjugate beamforming or zero-forcing beamforming is used. The results
demonstrate the feasibility of distributed MIMO with phase-calibration through
over-the-air inter-AP measurements integrated into the TDD flow.

</details>


### [47] [Two-Timescale Sum-Rate Maximization for Movable Antenna Enhanced Systems](https://arxiv.org/abs/2509.04062)
*Xintai Chen,Biqian Feng,Yongpeng Wu,Derrick Wing Kwan Ng,Robert Schober*

Main category: cs.IT

TL;DR: 提出了一种基于可移动天线(MA)的多用户MIMO下行系统，通过两时间尺度优化来最大化平均可实现和速率，利用瞬时信道状态信息进行接收天线位置优化，统计信道信息进行发射天线位置和协方差矩阵设计。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在信道条件变化时性能受限，可移动天线能够通过动态调整位置来优化信道条件，从而提升多用户MIMO系统的性能。

Method: 采用两时间尺度分解方法：短时问题使用梯度上升算法优化接收天线位置向量；长时问题使用约束随机逐次凸近似(CSSCA)算法优化发射天线位置和协方差矩阵。还提出了平面移动模式和基于原始-对偶分解的随机逐次凸近似(PDD-SSCA)算法。

Result: 数值结果表明，相比基准方案，所提出的两时间尺度MA增强系统设计在平均可实现和速率和问题可行性方面都有显著提升，适用于一般移动模式和平面移动模式。

Conclusion: 可移动天线技术结合两时间尺度优化方法能够有效提升多用户MIMO系统的性能，提出的算法在保证性能的同时降低了计算复杂度，为实际应用提供了可行性。

Abstract: This paper studies a novel movable antenna (MA)-enhanced multiuser
multiple-input multiple-output downlink system designed to improve wireless
communication performance. We aim to maximize the average achievable sum rate
through two-timescale optimization exploiting instantaneous channel state
information at the receiver (I-CSIR) for receive antenna position vector (APV)
design and statistical channel state information at the transmitter (S-CSIT)
for transmit APV and covariance matrix design. We first decompose the resulting
stochastic optimization problem into a series of short-term problems and one
long-term problem. Then, a gradient ascent algorithm is proposed to obtain
suboptimal receive APVs for the short-term problems for given I-CSIR samples.
Based on the output of the gradient ascent algorithm, a series of convex
objective/feasibility surrogates for the long-term problem are constructed and
solved utilizing the constrained stochastic successive convex approximation
(CSSCA) algorithm. Furthermore, we propose a planar movement mode for the
receive MAs to facilitate efficient antenna movement and the development of a
low-complexity primal-dual decomposition-based stochastic successive convex
approximation (PDD-SSCA) algorithm, which finds Karush-Kuhn-Tucker (KKT)
solutions almost surely. Our numerical results reveal that, for both the
general and the planar movement modes, the proposed two-timescale MA-enhanced
system design significantly improves the average achievable sum rate and the
feasibility of the formulated problem compared to benchmark schemes.

</details>


### [48] [Design of RIS-UAV-Assisted LEO Satellite Constellation Communication](https://arxiv.org/abs/2509.04136)
*Wenfei Yao,Xiaoming Chen,Qi Wang,Xingyu Peng*

Main category: cs.IT

TL;DR: 这篇论文提出了一种新颖的低地轨卫星组网通信框架，通过重构智能表面无人机(RIS-UAV)来改善卫星与地面用户设备的通信质量，并使用统计频道状态信息来降低频道量获取开销。


<details>
  <summary>Details</summary>
Motivation: 解决低地轨卫星与地面用户设备之间长距离通信质量不佳的问题，同时减少多卫星协作中频道状态信息获取的开销。

Method: 使用统计频道状态信息(sCSI)，推导用户设备的步径速率表达式，并通过交替优化(AO)算法聚合优化卫星核心网绘形、RIS相位移和UAV赨迹。

Result: 大量模拟实验表明，所提算法在频谱效率方面显著优于基准算法。

Conclusion: 该方案能够有效提升卫星通信系统的性能，为6G网络提供了一种有效的解决方案。

Abstract: Low Earth orbit (LEO) satellite constellations play a pivotal role in
sixth-generation (6G) wireless networks by providing global coverage, massive
connections, and huge capacity. In this paper, we present a novel LEO satellite
constellation communication framework, where a reconfigurable intelligent
surface-mounted unmanned aerial vehicle (RIS-UAV) is deployed to improve the
communication quality of multiple terrestrial user equipments (UEs) under the
condition of long distance between satellite and ground. To reduce the overhead
for channel state information (CSI) acquisition with multiple-satellite
collaboration, statistical CSI (sCSI) is utilized in the system. In such a
situation, we first derive an approximated but exact expression for ergodic
rate of each UE. Then, we aim to maximize the minimum approximated UE ergodic
rate by the proposed alternating optimization (AO)-based algorithm that jointly
optimizes LEO satellite beamforming, RIS phase shift, and UAV trajectory.
Finally, extensive simulations are conducted to demonstrate the superiority of
the proposed algorithm in terms of spectrum efficiency over baseline
algorithms.

</details>


### [49] [Non-Reed-Solomon Type MDS Codes from Elliptic Curves](https://arxiv.org/abs/2509.04247)
*Puyin Wang,Wei Liu,Jinquan Luo,Dengxin Zhai*

Main category: cs.IT

TL;DR: 本文提出了基于椭圆曲线的新MDS码族，长度接近理论最大值，且可证明不等价于Reed-Solomon码。通过使用仿射点支撑的除子和多点除子，构造了长度约为(q+1+⌊2√q⌋)/2的码，证明了已知上界的紧性。


<details>
  <summary>Details</summary>
Motivation: 构建长度接近理论最大值且与Reed-Solomon码不等价的新MDS码族，突破以往依赖无穷远点的构造限制，探索更一般的除子选择。

Method: 利用椭圆曲线，考虑仿射点支撑的除子和由多个不同点组成的除子，采用更广泛的框架构造MDS码。通过计算生成矩阵Schur积的秩来证明与RS码的不等价性。

Result: 成功构造了长度约为(q+1+⌊2√q⌋)/2的MDS码，这一长度接近已知上界，证明了上界的紧性。详细比较显示这些码未被先前结果覆盖，且明确证明了与RS码的不等价性。

Conclusion: 提出的椭圆曲线MDS码构造方法具有更一般的除子选择，能够达到接近理论最大长度，且与RS码严格不等价，为MDS码构造提供了新的途径并验证了相关上界的紧性。

Abstract: In this paper, we present a new family of MDS codes derived from elliptic
curves. These codes attain lengths close to the theoretical maximum and are
provably inequivalent to Reed-Solomon (RS) codes. Unlike many previous
constructions that rely on the point at infinity, our approach allows for more
general choices: we consider divisors supported on affine points and divisors
consisting of multiple distinct points. This broader framework enables the
construction of codes with length approximately $(q + 1 + \lfloor 2\sqrt{q}
\rfloor)/2$, further illustrating the tightness of known upper bounds on
elliptic MDS code lengths. A detailed comparison shows that our codes are not
covered by earlier results. Moreover, we show that their inequivalence to RS
codes by explicitly computing the rank of the Schur product of their generator
matrices.

</details>
