<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 62]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Performance Guarantees for Data Freshness in Resource-Constrained Adversarial IoT Systems](https://arxiv.org/abs/2512.18155)
*Aresh Dadlani,Muthukrishnan Senthil Kumar,Omid Ardakanian,Ioanis Nikolaidis*

Main category: cs.NI

TL;DR: 研究物联网监控系统中对抗性攻击对信息时效性的影响，提出考虑队列动态的对抗性AoI模型，推导闭式表达式和边界分析


<details>
  <summary>Details</summary>
Motivation: 物联网实时监控系统规模扩大时易受对抗性攻击，现有模型忽略队列动态和资源约束，需要更全面的分析框架

Method: 基于G-队列框架研究双源M/G/1/1系统，对抗源通过泊松过程注入负到达和独立同分布服务减速，使用矩生成函数推导闭式表达式，引入最坏情况约束攻击模型和随机占优论证

Result: 推导出任意数量源的平均和峰值AoI闭式表达式，建立分析性AoI边界，数值结果验证分析并展示资源受限对抗干扰的影响

Conclusion: 该研究提供了对抗性攻击下信息时效性的全面分析框架，揭示了资源受限对抗干扰对系统性能的影响，为物联网安全监控系统设计提供理论基础

Abstract: Timely updates are critical for real-time monitoring and control applications powered by the Internet of Things (IoT). As these systems scale, they become increasingly vulnerable to adversarial attacks, where malicious agents interfere with legitimate transmissions to reduce data rates, thereby inflating the age of information (AoI). Existing adversarial AoI models often assume stationary channels and overlook queueing dynamics arising from compromised sensing sources operating under resource constraints. Motivated by the G-queue framework, this paper investigates a two-source M/G/1/1 system in which one source is adversarial and disrupts the update process by injecting negative arrivals according to a Poisson process and inducing i.i.d. service slowdowns, bounded in attack rate and duration. Using moment generating functions, we then derive closed-form expressions for average and peak AoI for an arbitrary number of sources. Moreover, we introduce a worst-case constrained attack model and employ stochastic dominance arguments to establish analytical AoI bounds. Numerical results validate the analysis and highlight the impact of resource-limited adversarial interference under general service time distributions.

</details>


### [2] [TCP BBR Performance over Wi-Fi~6: AQM Impacts and Cross-Layer Insights](https://arxiv.org/abs/2512.18259)
*Shyam Kumar Shrestha,Shiva Raj Pokhrel,Jonathan Kua*

Main category: cs.NI

TL;DR: BBRv3在Wi-Fi 6家庭网络中的性能评估：FIFO导致延迟增加和不稳定，FQ-CoDel恢复公平性，CAKE提供最佳整体性能，但可能触发短暂重传突发。


<details>
  <summary>Details</summary>
Motivation: 评估TCP BBRv3在现代Wi-Fi 6家庭网络中的性能，特别是在不同AQM（主动队列管理）方案下的表现，为家庭Wi-Fi网络配置提供实践指导。

Method: 使用完全无线测试床和跨层模型，将Wi-Fi调度、路由器队列和BBRv3的节奏动态联系起来，比较BBR和CUBIC在不同AQM（FIFO、FQ-CoDel、CAKE）下的上行、下行和双向流量性能。

Result: FIFO破坏节奏稳定性并增加延迟，通常让CUBIC占优；FQ-CoDel恢复公平性并控制延迟；CAKE提供最佳整体性能，保持低延迟并使BBRv3的发送和交付速率对齐。发现CAKE快速排空队列可能触发BBRv3带宽探测期间的短暂重传突发。

Conclusion: 建议在家庭Wi-Fi中使用FQ-CoDel或CAKE，避免未管理的FIFO，并指出BBRv3的探测机制有Wi-Fi感知调优的潜力。结果源于可变Wi-Fi服务速率、AQM延迟控制和BBRv3飞行限制的相互作用。

Abstract: We evaluate TCP BBRv3 on Wi-Fi 6 home networks under modern AQM schemes using a fully wireless testbed and a simple cross-layer model linking Wi-Fi scheduling, router queueing, and BBRv3's pacing dynamics. Comparing BBR Internet traffic with CUBIC across different AQMs (FIFO, FQ-CoDel, and CAKE) for uplink, downlink, and bidirectional traffic, we find that FIFO destabilizes pacing and raises delay, often letting CUBIC dominate; FQ-CoDel restores fairness and controls latency; and CAKE delivers the best overall performance by keeping delay low and aligning BBRv3's sending and delivered rates. We also identify a Wi-Fi-specific effect where CAKE's rapid queue draining, while improving pacing alignment, can trigger brief retransmission bursts during BBRv3's bandwidth probes. These results follow from the interaction of variable Wi-Fi service rates, AQM delay control, and BBRv3's inflight limits, leading to practical guidance to use FQ-CoDel or CAKE and avoid unmanaged FIFO in home Wi-Fi, with potential for Wi-Fi-aware tuning of BBRv3's probing.

</details>


### [3] [Wireless Copilot: An AI-Powered Partner for Navigating Next-Generation Wireless Complexity](https://arxiv.org/abs/2512.18582)
*Haoxiang Luo,Ruichen Zhang,Yinqiu Liu,Gang Sun,Hongfang Yu,Dusit Niyato,Shiwen Mao,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出"无线副驾驶"AI助手，通过集成大语言模型与认知框架，将人类意图转化为可执行的6G网络操作，解决传统自动化无法应对的6G网络复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 第六代(6G)无线网络的运行复杂性已超出传统自动化和人工监管的极限，需要新的智能解决方案来弥合人类专业知识与机器规模复杂性之间的鸿沟。

Method: 提出"无线副驾驶"框架，集成大语言模型(LLMs)与稳健的认知框架，作为无线基础设施与网络运营商之间的新层，能够将高级人类意图转化为精确、优化且可验证的网络操作。

Result: 在低空无线网络(LAWNets)的应用案例中，展示了基于意图的资源分配具有优越的适应性，验证了该框架在6G网络设计、配置、评估和优化中的有效性。

Conclusion: 无线副驾驶能够实现更高效、智能和可信的6G系统管理，为创建全面的人机协作生态系统奠定了基础，并指出了未来研究方向。

Abstract: The sixth-generation (6G) of wireless networks introduces a level of operational complexity that exceeds the limits of traditional automation and manual oversight. This paper introduces the "Wireless Copilot", an AI-powered technical assistant designed to function as a collaborative partner for human network designers, engineers, and operators. We posit that by integrating Large Language Models (LLMs) with a robust cognitive framework. It will surpass the existing AI tools and interact with wireless devices, transmitting the user's intentions into the actual network execution process. Then, Wireless Copilot can translate high-level human intent into precise, optimized, and verifiable network actions. This framework bridges the gap between human expertise and machine-scale complexity, enabling more efficient, intelligent, and trustworthy management of 6G systems. Wireless Copilot will be a novel layer between the wireless infrastructure and the network operators. Moreover, we explore Wireless Copilot's methodology and analyze its application in Low-Altitude Wireless Networks (LAWNets) assisting 6G networking, including network design, configuration, evaluation, and optimization. Additionally, we present a case study on intent-based LAWNets resource allocation, demonstrating its superior adaptability compared to others. Finally, we outline future research directions toward creating a comprehensive human-AI collaborative ecosystem for the 6G era.

</details>


### [4] [How Many Pinching Antennas Are Enough?](https://arxiv.org/abs/2512.18761)
*Dimitrios Tyrovolas,Sotiris A. Tegos,Yue Xiao,Panagiotis D. Diamantoulakis,Sotiris Ioannidis,Christos K. Liaskos,George K. Karagiannidis,Stylianos D. Asimonis*

Main category: cs.NI

TL;DR: 分析离散位置可编程天线系统在可编程无线环境中的性能，推导中断概率和遍历数据率的闭式表达式，并提出量化离散与连续配置性能差距的指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设可编程天线位置可沿波导连续调整，但实际只有有限个固定位置可用。本文针对这种现实约束，分析二态可编程天线系统的性能。

Method: 考虑可编程天线位置的空间离散性，推导中断概率和遍历可实现数据率的闭式解析表达式，引入"夹持离散化效率"来量化离散与连续配置的性能差距。

Result: 数值结果验证了分析框架，表明有限数量的可编程天线即可实现接近连续的性能，为可编程天线系统在可编程无线环境中的设计和部署提供了实用见解。

Conclusion: 通过考虑实际离散位置约束，本文为可编程天线系统的性能分析提供了理论框架，并证明有限数量的天线即可有效逼近理想连续配置的性能。

Abstract: Programmable wireless environments (PWEs) have emerged as a key paradigm for next-generation communication networks, aiming to transform wireless propagation from an uncontrollable phenomenon into a reconfigurable process that can adapt to diverse service requirements. In this framework, pinching-antenna systems (PASs) have recently been proposed as a promising enabling technology, as they allow the radiation location and effective propagation distance to be adjusted by selectively exciting radiating points along a dielectric waveguide. However, most existing studies on PASs rely on the idealized assumption that pinching-antenna (PA) positions can be continuously adjusted along the waveguide, while realistically only a finite set of pinching locations is available. Motivated by this, this paper analyzes the performance of two-state PASs, where the PA positions are fixed and only their activation state can be controlled. By explicitly accounting for the spatial discreteness of the available pinching points, closed-form analytical expressions for the outage probability and the ergodic achievable data rate are derived. In addition, we introduce the pinching discretization efficiency to quantify the performance gap between discrete and continuous pinching configurations, enabling a direct assessment of the number of PAs required to approximate the ideal continuous case. Finally, numerical results validate the analytical framework and show that near-continuous performance can be achieved with a limited number of PAs, offering useful insights for the design and deployment of PASs in PWEs.

</details>


### [5] [QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits](https://arxiv.org/abs/2512.18915)
*Ivan Čilić,Ivana Podnar Žarko,Pantelis Frangoudis,Schahram Dustdar*

Main category: cs.NI

TL;DR: QEdgeProxy：一种用于计算连续体的去中心化QoS感知负载均衡器，将负载均衡建模为异构奖励的多玩家多臂老虎机问题，使用核密度估计来最大化客户端QoS目标满足概率。


<details>
  <summary>Details</summary>
Motivation: 随着计算从云端转向边缘以减少处理延迟和网络流量，计算连续体形成了动态环境，难以满足严格的QoS要求并避免服务实例过载。现有方法通常优先考虑全局指标，忽略了每个客户端的QoS，而这对于延迟敏感和可靠性关键的应用至关重要。

Method: 提出QEdgeProxy，一种去中心化的QoS感知负载均衡器，作为物联网设备和服务实例之间的代理。将负载均衡问题建模为异构奖励的多玩家多臂老虎机问题，每个负载均衡器自主选择服务实例，使用核密度估计来估计QoS成功概率，最大化客户端QoS目标满足概率。还包含自适应探索机制，以快速从性能变化和非平稳条件中恢复。

Result: 在K3s集群上部署的模拟计算连续体测试平台上进行评估，使用真实的网络条件和延迟敏感的边缘AI工作负载。结果显示，QEdgeProxy在每客户端QoS满意度方面显著优于基于邻近性和强化学习的基线方法，同时能有效适应负载激增和实例可用性变化。

Conclusion: QEdgeProxy通过将负载均衡建模为MP-MAB问题并使用核密度估计，成功解决了计算连续体中满足每客户端QoS要求的挑战，在动态环境中表现出优越的适应性和性能。

Abstract: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where it is challenging to meet strict Quality of Service (QoS) requirements and avoid service instance overload. Existing methods often prioritize global metrics, overlooking per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards, where each load balancer autonomously selects service instances that maximize the probability of meeting its clients' QoS targets by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. It also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and instance availability changes.

</details>


### [6] [Optimal 3D Directional WPT Charging via UAV for 3D Wireless Rechargeable Sensor Networks](https://arxiv.org/abs/2512.19075)
*Zhenguo Gao,Hui Li,Yiqin Chen,Qingyu Gao,Zhufang Kuang,Shih-Hau Fang,Hsiao-Chun Wu*

Main category: cs.NI

TL;DR: 本文提出FELKH-3D算法解决三维无线可充电传感器网络中无人机定向充电调度问题，通过功能等效方向集和LKH启发式算法优化充电路径。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注2D-WRSNs，缺乏针对真实3D-WRSNs的设计。三维网络中节点的空间分布特性增加了充电调度任务的复杂性，需要系统化框架解决这一问题。

Method: 提出三步框架FELKH-3D：1) 设计生成最小尺寸功能等效方向集的算法，解决无限充电方向空间问题；2) 使用Lin-Kernighan启发式算法确定无人机最优充电路径；3) 证明方法的优化性。

Result: 仿真实验表明FELKH-3D算法优于其他经典算法，能够有效解决三维无线可充电传感器网络的无人机定向充电调度问题。

Conclusion: FELKH-3D为3D-WRSNs中的无人机定向充电调度提供了系统化解决方案，通过功能等效方向集和LKH算法有效处理了三维空间中的复杂调度问题。

Abstract: The high mobility and flexible deployment capability of UAVs make them an impressive option for charging nodes in Wireless Rechargeable Sensor Networks (WRSNs) using Directional Wireless Power Transfer (WPT) technology. However, existing studies largely focus on 2D-WRSNs, lacking designs catering to real 3D-WRSNs. The spatial distribution characteristics of nodes in a 3D-WRSN further increase the complexity of the charging scheduling task, thus requiring a systematic framework to solve this problem. In this paper, we investigated the Directional UAV Charging Scheduling problem for 3D-WRSNs (DCS-3D) and established its NP-hard property, and then proposed a three-step framework named as directional charging scheduling algorithm using Functional Equivalent (FuncEqv) direction set and Lin-Kernighan heuristic (LKH) for 3D-WRSNs (FELKH-3D) to solve it. In FELKH-3D, the challenge of infinite charging direction space is solved by designing an algorithm generating a minimum-size direction set guaranteed to be FuncEqv to the infinite set of whole sphere surface, and the optimaility of the method was proved.To determine the optimal charging tour for the UAV, the LKH algorithm is employed.Simulation experiments demonstrated the superiority of FELKH-3D over other classical algorithms.

</details>


### [7] [BEVCooper: Accurate and Communication-Efficient Bird's-Eye-View Perception in Vehicular Networks](https://arxiv.org/abs/2512.19082)
*Jiawei Hou,Peng Yang,Xiangxiang Dai,Mingliu Liu,Conghao Zhou*

Main category: cs.NI

TL;DR: BEVCooper：一种协作感知框架，通过在线学习选择最有价值的协作车辆，并自适应优化BEV特征压缩，在动态V2V信道条件下实现高精度、低延迟的鸟瞰图构建。


<details>
  <summary>Details</summary>
Motivation: 传统BEV地图在遮挡或远距离区域感知质量下降，需要协作感知来提升精度，但V2V通信带宽有限，需要智能选择协作车辆并优化特征传输。

Method: 1) 定义BEV特征效用评估指标；2) 基于在线学习的协作车辆选择策略，优先选择最有价值的邻居车辆；3) 自适应融合机制，根据环境动态和V2V信道质量优化特征压缩。

Result: 在真实测试平台上，BEVCooper相比现有方法提升BEV感知精度达63.18%，降低端到端延迟67.9%，仅增加1.8%计算开销。

Conclusion: BEVCooper在动态车辆拓扑和V2V信道条件下实现了渐进最优的车辆选择和自适应特征融合，显著提升了BEV地图构建的精度和效率。

Abstract: Bird's-Eye-View (BEV) is critical to connected and automated vehicles (CAVs) as it can provide unified and precise representation of vehicular surroundings. However, quality of the raw sensing data may degrade in occluded or distant regions, undermining the fidelity of constructed BEV map. In this paper, we propose BEVCooper, a novel collaborative perception framework that can guarantee accurate and low-latency BEV map construction. We first define an effective metric to evaluate the utility of BEV features from neighboring CAVs. Then, based on this, we develop an online learning-based collaborative CAV selection strategy that captures the ever-changing BEV feature utility of neighboring vehicles, enabling the ego CAV to prioritize the most valuable sources under bandwidth-constrained vehicle-to-vehicle (V2V) links. Furthermore, we design an adaptive fusion mechanism that optimizes BEV feature compression based on the environment dynamics and real-time V2V channel quality, effectively balancing feature transmission latency and accuracy of the constructed BEV map. Theoretical analysis demonstrates that, BEVCooper achieves asymptotically optimal CAV selection and adaptive feature fusion under dynamic vehicular topology and V2V channel conditions. Extensive experiments on real-world testbed show that, compared with state-of-the-art benchmarks, the proposed BEVCooper enhances BEV perception accuracy by up to $63.18\%$ and reduces end-to-end latency by $67.9\%$, with only $1.8\%$ additional computational overhead.

</details>


### [8] [On Network-Aware Semantic Communication and Edge-Cloud Collaborative Intelligence Systems](https://arxiv.org/abs/2512.19563)
*Murdadha Nasif,Ahmed Refaey Hussein*

Main category: cs.NI

TL;DR: 该综述论文系统性地总结了边缘-云协同智能中的语义通信技术，探讨了其在带宽、时延和资源受限环境下的应用，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 下一代智能服务面临严格的带宽、时延和资源约束，需要从传统的比特级完美传输转向任务相关的语义表示传输，以实现通信开销、推理精度、计算负载和端到端时延之间的自适应权衡。

Method: 提供了边缘-云接口语义通信的系统级综合综述，涵盖协同智能架构模型、表示学习和语义抽象技术、网络感知和资源自适应语义编码策略，以及学习驱动的优化和编排机制。

Result: 将语义通信置于实际运营环境中考虑，包括安全性、信任度、韧性和可扩展性，并与零信任网络、物理层安全和新兴边缘-云控制范式建立联系。

Conclusion: 语义通信是AI原生网络和6G就绪智能系统的关键构建模块，论文识别了开放挑战和研究方向，强调了其在下一代智能服务中的基础作用。

Abstract: Semantic communication and edge-cloud collaborative intelligence are increasingly recognized as foundational enablers for next-generation intelligent services operating under stringent bandwidth, latency, and resource constraints. By shifting the communication objective from bit-perfect delivery toward the transmission of task-relevant semantic representations, semantic communication enables adaptive tradeoffs among communication overhead, inference accuracy, computational load, and end-to-end latency. This survey provides a comprehensive and system-level synthesis of recent advances in semantic communication at the edge-cloud interface, encompassing architectural models for collaborative intelligence, representation learning and semantic abstraction techniques, network-aware and resource-adaptive semantic encoding strategies, and learning-driven optimization and orchestration mechanisms. Beyond efficiency considerations, the survey situates semantic communication within practical operational contexts, including security, trust, resilience, and scalability, drawing connections to zero-trust networking, physical-layer security, and emerging edge-cloud control paradigms. Finally, open challenges and research directions are identified, highlighting the role of semantic communication as a key building block for AI-native networking and 6G-ready intelligent systems.

</details>


### [9] [CORE: Compensable Reward as a Catalyst for Improving Offline RL in Wireless Networks](https://arxiv.org/abs/2512.19671)
*Lipeng Zu,Hansong Zhou,Yu Qian,Shayok Chakraborty,Yukun Yuan,Linke Guo,Xiaonan Zhang*

Main category: cs.NI

TL;DR: CORE：针对无线环境的离线RL框架，通过行为嵌入聚类识别专家轨迹，使用对比VAE分离专家/非专家行为，构建可补偿奖励指导策略学习，是无线网络领域离线RL的早期系统探索。


<details>
  <summary>Details</summary>
Motivation: 现实世界无线数据收集成本高且缺乏足够的专家演示，导致现有离线RL方法容易过拟合次优行为且性能不稳定。无线领域对离线RL的采用仍然有限，需要针对该领域特点的系统性探索。

Method: 1. 通过行为嵌入聚类从噪声数据集中识别潜在专家轨迹；2. 使用对比目标训练条件变分自编码器，在潜在空间中分离专家和非专家行为；3. 基于学习到的表示构建反映专家可能性的可补偿奖励；4. 开发考虑无线数据固有结构特性的领域对齐算法。

Result: CORE框架能够有效指导在有限或不完美监督下的策略学习，为无线网络领域的离线RL应用提供了基础性见解和实证证据。

Conclusion: 该研究代表了无线网络领域离线RL的早期系统性探索，旨在通过领域对齐的算法设计，考虑无线数据固有特性，推动离线RL在无线社区的更广泛接受和应用。

Abstract: Real-world wireless data are expensive to collect and often lack sufficient expert demonstrations, causing existing offline RL methods to overfit suboptimal behaviors and exhibit unstable performance. To address this issue, we propose CORE, an offline RL framework specifically designed for wireless environments. CORE identifies latent expert trajectories from noisy datasets via behavior embedding clustering, and trains a conditional variational autoencoder with a contrastive objective to separate expert and non-expert behaviors in latent space. Based on the learned representations, CORE constructs compensable rewards that reflect expert-likelihood, effectively guiding policy learning under limited or imperfect supervision. More broadly, this work represents one of the early systematic explorations of offline RL in wireless networking, where prior adoption remains limited. Beyond introducing offline RL techniques to this domain, we further examine intrinsic wireless data characteristics and develop a domain-aligned algorithm that explicitly accounts for their structural properties. While offline RL has not yet been fully established as a standard methodology in the wireless community, our study aims to provide foundational insights and empirical evidence to support its broader acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout](https://arxiv.org/abs/2512.18034)
*Joshua Gibson,Kapil Dhakal*

Main category: cs.AI

TL;DR: 该论文研究将CDCL与VSIDS启发式作为计算引擎用于离散设施布局问题，开发了CNF建模，比较了CDCL、CP-SAT和MILP的性能，并提出了两种混合架构来结合CDCL的可行性搜索与CP-SAT的优化能力。


<details>
  <summary>Details</summary>
Motivation: 离散设施布局问题具有密集的逻辑结构，传统方法如CP-SAT和MILP在大规模问题上存在可扩展性问题。需要探索更高效的求解方法，特别是利用CDCL在SAT求解中的优势来处理这类组合分配问题。

Method: 1) 将设施布局问题建模为带有邻接、分离和槽位可用性约束的CNF公式；2) 在统一基准框架下比较CDCL、CP-SAT和MILP的性能；3) 提出两种混合架构：快速枚举可行布局的架构和使用CDCL生成热启动解以加速精确优化的架构。

Result: CDCL在可行性检测方面表现出接近常数的运行时间行为，而CP-SAT和MILP分别显示多项式和指数级扩展。混合方法能显著减少求解时间同时保持正确性保证。

Conclusion: CDCL在处理大规模离散布局问题的可行性检测方面具有优势，但需要与优化方法结合。混合架构在速度和最优性之间提供了有效的权衡，阐明了子句学习搜索与精确优化方法之间的算法权衡。

Abstract: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.

</details>


### [11] [Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability](https://arxiv.org/abs/2512.18092)
*Ge Yan,Tuomas Oikarinen,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 论文首次为神经元识别提供理论分析，建立了忠实性和稳定性的理论保证，并提出了带覆盖概率保证的概念预测集方法。


<details>
  <summary>Details</summary>
Motivation: 虽然Network Dissection和CLIP-Dissect等神经元识别算法在经验上很成功，但缺乏严格的理论基础，这对于实现可信赖和可靠的解释至关重要。

Method: 将神经元识别视为机器学习的逆过程，推导相似性度量（如准确率、AUROC、IoU）的泛化边界以保证忠实性；提出自助集成程序量化稳定性，以及BE方法生成具有覆盖概率保证的概念预测集。

Result: 在合成和真实数据上的实验验证了理论结果，证明了方法的实用性，为可信赖的神经元识别迈出了重要一步。

Conclusion: 该工作首次为神经元识别提供了理论分析框架，解决了忠实性和稳定性两个基本挑战，为实现可信赖的神经元解释提供了理论基础和方法支持。

Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.

</details>


### [12] [Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks](https://arxiv.org/abs/2512.18094)
*Boxuan Wang,Zhuoyun Li,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 该研究将小世界网络理论应用于LLM多智能体系统设计，通过实验证明小世界连接能稳定共识轨迹，并提出了基于不确定性的自适应重连方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统大多采用全连接图、简单稀疏环或临时动态选择，缺乏结构化指导。研究者希望探索小世界网络作为多智能体系统设计先验的可能性，以平衡局部聚类和长程整合。

Method: 1) 将神经科学和复杂网络的小世界理论见解桥接到多智能体系统；2) 以多智能体辩论为受控测试平台，比较不同连接拓扑；3) 引入基于不确定性的重连方案，使用LLM导向的不确定性信号（如语义熵）在认知分歧的智能体间添加长程捷径。

Result: 实验结果显示：小世界连接在保持相同准确性和代币成本的同时，显著稳定了共识轨迹。基于不确定性的自适应重连方案能够根据任务难度和智能体异质性生成可控的小世界结构。

Conclusion: 小世界先验可作为多智能体系统设计的稳定器、鲁棒性增强器、可扩展协调器，并为涌现认知角色提供归纳偏置，为LLM多智能体系统设计提供了结构化指导框架。

Abstract: Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.

</details>


### [13] [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)
*Zijun Wang,Yijiahao Qi,Hanqiu Chen,Zishen Wan,Gongjin Sun,Dongyang Li,Shuyi Pei,Cong Hao*

Main category: cs.AI

TL;DR: 提出一种算法-系统协同设计的MoA推理服务框架，通过层次化树拓扑、运行时自适应机制和流水线执行，显著降低延迟（最高90%）同时保持准确率（±1%以内）。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Agents推理存在密集的智能体间通信和低硬件利用率问题，这共同增加了服务延迟，需要解决这些瓶颈。

Method: 采用三部分协同设计：1) 用层次化树拓扑替代密集连接图，引入结构化稀疏通信；2) 基于语义一致性和置信度的运行时自适应机制，选择性终止或跳过下游智能体调用；3) 流水线化智能体执行，重叠增量预填充与解码过程。

Result: 在代表性任务上，该方法大幅降低端到端延迟（最高90%），同时保持与密集连接MoA基线相当的准确率（±1%以内），在某些场景下还能提高准确率。

Conclusion: 通过算法-系统协同设计，成功解决了MoA推理中的通信密集和硬件利用率低的问题，实现了延迟显著降低而准确率基本保持，为高效MoA服务提供了有效方案。

Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.

</details>


### [14] [Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications](https://arxiv.org/abs/2512.18135)
*Cristiano da Costa Cunha,Wei Liu,Tim French,Ajmal Mian*

Main category: cs.AI

TL;DR: 这篇综述系统回顾了因果推断与强化学习的交叉领域，将现有方法分为因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性等类别，并讨论了该领域的挑战、应用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于相关性决策，面临分布偏移、混杂变量和动态环境等挑战，导致可解释性差、鲁棒性不足和泛化失败。因果强化学习通过建模因果关系来解决这些问题。

Method: 采用系统综述方法，对因果推断与强化学习交叉领域的研究进行分类分析，包括因果表示学习、反事实策略优化、离线因果RL、因果迁移学习和因果可解释性等五个主要类别。

Result: 通过结构化分析，识别了该领域的主要挑战，强调了在实际应用中的实证成功案例，并讨论了未解决的问题，为开发鲁棒、可泛化和可解释的AI系统提供了理论框架。

Conclusion: 因果强化学习为解决传统RL的局限性提供了有前景的解决方案，通过明确建模因果关系，有望开发出更鲁棒、可泛化和可解释的人工智能系统，未来研究应继续探索这一交叉领域。

Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.

</details>


### [15] [Propose, Solve, Verify: Self-Play Through Formal Verification](https://arxiv.org/abs/2512.18160)
*Alex Wilf,Pranjal Aggarwal,Bryan Parno,Daniel Fried,Louis-Philippe Morency,Paul Pu Liang,Sean Welleck*

Main category: cs.AI

TL;DR: PSV框架通过自我对弈训练语言模型进行代码生成，利用形式化验证提供可靠正确性信号，显著提升代码生成性能


<details>
  <summary>Details</summary>
Motivation: 探索在没有人类数据的情况下，仅通过自我对弈训练大型语言模型的有效性，特别是在代码生成领域，传统的单元测试奖励机制脆弱且容易传播错误

Method: 提出Propose, Solve, Verify (PSV)框架：使用形式化验证信号训练提议者生成具有挑战性的合成问题，并通过专家迭代训练求解器

Result: PSV-Verus模型在三个基准测试中，pass@1指标比仅推理和专家迭代基线提升高达9.6倍，性能随生成问题数量和训练迭代次数而扩展

Conclusion: 形式化验证和难度感知的提议是成功自我对弈的关键要素，PSV框架在代码生成领域展示了纯自我对弈训练的有效性

Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.

</details>


### [16] [NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI](https://arxiv.org/abs/2512.18177)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: NEURO-GUARD：结合视觉Transformer与语言驱动推理的知识引导医疗AI框架，通过检索增强生成机制实现自我验证，在糖尿病视网膜病变分类等任务上超越纯数据驱动基线，提升准确率和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗AI面临准确性与可解释性的平衡难题，特别是在数据有限、视觉线索细微、临床决策高风险的环境中。现有视觉模型多为黑盒预测，可解释性差且跨域泛化能力弱，限制了临床实际应用。

Method: 提出NEURO-GUARD框架，将视觉Transformer与语言驱动推理结合，采用检索增强生成机制进行自我验证。大语言模型迭代生成、评估和优化医疗图像特征提取代码，并将此过程基于临床指南和专家知识，逐步提升特征检测和分类能力。

Result: 在糖尿病视网膜病变分类的四个基准数据集（APTOS、EyePACS、Messidor-1、Messidor-2）上，NEURO-GUARD比纯ViT基线准确率提升6.2%（84.69% vs. 78.4%），跨域泛化能力提升5%。在MRI癫痫检测任务上也表现出优越的跨域鲁棒性。

Conclusion: NEURO-GUARD将符号化医学推理与亚符号化视觉学习相结合，实现了可解释、知识感知且可泛化的医疗图像诊断，在多个数据集上达到最先进性能，为医疗AI的实际临床应用提供了有前景的解决方案。

Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.

</details>


### [17] [NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework](https://arxiv.org/abs/2512.18189)
*Zihao Deng,Yijia Li,Renrui Zhang,Peijun Ye*

Main category: cs.AI

TL;DR: NL2CA：一种从自然语言描述自动形式化认知决策规则的方法，通过LLM翻译为LTL逻辑，经无监督批评树精炼，转换为可执行的生产规则，并构建可通过认知强化学习优化的认知智能体。


<details>
  <summary>Details</summary>
Motivation: 认知计算模型能形式化、可解释地表征人类决策过程，但开发过程劳动密集。需要自动化方法从自然语言描述中提取认知决策规则，减少人工干预。

Method: 1) 使用微调的大语言模型将文本翻译为线性时序逻辑(LTL)；2) 通过无监督批评树精炼逻辑；3) 将输出转换为与符号认知框架兼容的可执行生产规则；4) 基于规则构建认知智能体，并通过认知强化学习根据真实行为数据优化。

Result: 1) NL-to-LTL翻译：CriticNL2LTL模块在专家和大规模基准测试中表现一致，无需人工反馈；2) 认知驾驶模拟：从人类访谈自动构建的智能体成功学习了约70个不同关键场景中的多样化决策模式。

Conclusion: NL2CA能够从非结构化文本数据实现可扩展、可解释且与人类对齐的认知建模，为自动设计符号认知智能体提供了新范式。

Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.

</details>


### [18] [External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning](https://arxiv.org/abs/2512.18190)
*Jian Yan*

Main category: cs.AI

TL;DR: 提出External Hippocampus框架，从认知动力学角度建模语言模型推理，通过降维投影构建拓扑认知地图，实现推理过程中的能量流精确导航和干预，无需额外训练即可显著提升小模型多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统权重空间优化方法计算量大且难以在推理时进行干预。小模型在多步推理中容易出现"认知死锁"问题，需要一种高效可控的解决方案来提升推理能力。

Method: 从认知动力学视角将语言模型推理建模为语义空间中的信息能量流动。通过降维投影构建拓扑认知地图，在测试时对能量流进行精确导航和干预。利用温度扰动等技术重启停滞的能量流，解决"认知漩涡"和低熵势阱问题。

Result: 在≤7B参数模型上：地图引导方法在500个挑战性问题中达到81.20%准确率（相对基线+16.80%）；推理时间减少≥15倍；发现推理停滞表现为"认知漩涡"和低熵势阱；温度扰动能有效重启能量流。

Conclusion: External Hippocampus框架为小模型推理提供了高效可控的拓扑感知解决方案，无需额外训练，具有自主增长能力，能有效解决多步推理中的认知死锁问题，显著提升推理性能和效率。

Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.

</details>


### [19] [Sophia: A Persistent Agent Framework of Artificial Life](https://arxiv.org/abs/2512.18202)
*Mingyang Sun,Feng Hong,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出System 3架构和Sophia持久智能体，为LLM智能体添加元认知层，实现身份连续性和长期自适应，显著提升复杂任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体主要专注于感知（System 1）和推理（System 2），但缺乏维持身份连续性、验证推理、将短期行动与长期生存对齐的元认知层。大多数架构是静态和反应式的，局限于手动定义的狭窄场景。

Method: 提出System 3作为第三层架构，将心理学概念映射到具体计算模块。开发Sophia持久智能体包装器，包含四个协同机制：过程监督思维搜索、叙事记忆、用户与自我建模、混合奖励系统，形成持续自我改进循环。

Result: 定量：Sophia自主启动和执行内在任务，重复操作推理步骤减少80%，元认知持久性使高复杂度任务成功率提升40%。定性：System 3展现出连贯的叙事身份和内在任务组织能力。

Conclusion: 通过融合心理学洞察和轻量级强化学习核心，持久智能体架构为人工生命提供了可行的实践路径，实现了身份连续性和透明的行为解释。

Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.

</details>


### [20] [MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification](https://arxiv.org/abs/2512.18256)
*Sirui Li,Wangyue Lu,Xiaorui Shi,Ke Weng,Haozhe Sun,Minghe Yu,Tiancheng Zhang,Ge Yu,Hengyu Liu,Lun Du*

Main category: cs.AI

TL;DR: MSC-180是一个基于MSC2020数学学科分类的定理证明基准测试，包含180个形式化验证问题，覆盖60个数学分支，用于评估LLM定理证明器的领域覆盖和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的定理证明器存在领域覆盖有限和数学推理泛化能力弱的问题，需要更系统、更具区分度的基准测试来推动具有真正数学推理能力的AI系统发展。

Method: 基于MSC2020数学学科分类构建MSC-180基准，包含180个形式化验证问题（每个数学分支3个问题），覆盖从本科到研究生难度，经过领域专家多轮验证和精炼。引入变异系数(CV)作为评估指标量化跨数学领域的性能变异性。

Result: 在pass@32设置下，最佳模型仅达到18.89%的整体通过率，存在显著的领域偏见（最大领域覆盖率41.7%）和难度差距（研究生级别问题通过率显著更低）。CV值比统计高变异性阈值高4-6倍，表明模型仍依赖训练语料的模式匹配而非可迁移的推理机制。

Conclusion: MSC-180及其多维评估框架为驱动下一代具有真正数学推理能力的AI系统发展提供了具有区分度和系统性的基准测试，揭示了当前LLM定理证明器在系统泛化能力上的局限性。

Abstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.

</details>


### [21] [Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration](https://arxiv.org/abs/2512.18265)
*Himabindu Thogaru,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.AI

TL;DR: 提出一个结合知识图谱和LLM代理的协作智能系统，让制造规划人员通过自然语言界面分析仿真数据，实现人机协作的制造瓶颈识别与决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿真的制造数据分析方法在人类决策者和关键运营洞察之间设置了障碍，限制了制造规划中的有效协作。需要建立人机协作系统来弥合这一差距。

Method: 开发了一个集成知识图谱和基于大语言模型代理的协作智能框架，将仿真数据转换为语义丰富的表示，通过自然语言界面让规划人员与运营洞察自然交互，协作LLM代理采用迭代推理生成精确查询并透明验证。

Result: 在运营查询中系统通过自然语言交互达到接近完美的准确性；在需要协作分析的调查场景中，框架有效支持人类专家发现相互关联的运营问题，增强理解和决策能力。

Conclusion: 该工作通过创建直观的可操作洞察方法推进了协作制造，在演变的制造生态系统中减少认知负荷同时增强人类分析能力，保持人类监督和决策权威。

Abstract: Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.

</details>


### [22] [Monitoring Monitorability](https://arxiv.org/abs/2512.18311)
*Melody Y. Guan,Miles Wang,Micah Carroll,Zehao Dou,Annie Y. Wei,Marcus Williams,Benjamin Arnav,Joost Huizinga,Ian Kivlichan,Mia Glaese,Jakub Pachocki,Bowen Baker*

Main category: cs.AI

TL;DR: 该论文提出了AI系统决策可监控性的评估框架和度量标准，通过实验发现思维链监控比仅监控行动更有效，且模型可监控性随推理计算量增加而提升。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力不断增强，需要对其决策过程进行监控以确保安全部署。当前基于思维链的监控方法可能在不同训练过程、数据源或系统扩展下变得脆弱，因此需要系统评估和度量可监控性。

Method: 提出了三种评估原型（干预、过程和结果属性）和新的可监控性度量标准，构建了广泛的评估套件。实验比较了前沿模型的可监控性，研究了推理计算量、强化学习优化和预训练模型大小对可监控性的影响。

Result: 思维链监控比仅监控行动更有效；大多数前沿模型具有相当但非完美的可监控性；更长的思维链通常更易监控；强化学习优化不会显著降低可监控性；通过增加弱监控器的测试时计算量可以提升监控能力；让监控器访问思维链能改善监控效果。

Conclusion: AI系统的可监控性可以通过系统评估框架进行度量和改进。思维链监控是有效的安全机制，通过增加推理计算量、让监控器访问思维链以及使用后续问题等方法可以提升监控能力，为安全部署强大AI系统提供了实用指导。

Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.

</details>


### [23] [Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation](https://arxiv.org/abs/2512.18412)
*Mykyta Lapin,Kostiantyn Bokhan,Yurii Parzhyn*

Main category: cs.AI

TL;DR: 提出一种基于结构图的无反向传播小样本轮廓图像分类方法，通过构建属性图表示图像结构，形成概念吸引子进行透明分类


<details>
  <summary>Details</summary>
Motivation: 设计一种无需反向传播的小样本分类架构，使结构成为解释的载体，实现透明决策和可追溯的分类过程

Method: 轮廓矢量化后构建点/线二分图，通过结构化和参数化约简消除噪声，迭代组合样本形成概念图，使用近似图编辑距离进行图-概念匹配分类

Result: 在MNIST子集上（每类5-6个基础样本），单次训练获得约82%的准确率，决策完全可追溯，误分类可通过显式结构相似性解释

Conclusion: 结构图方案结合概念吸引子实现了无需反向传播的小样本学习，通过显式图结构提供内置解释，但受限于图编辑距离计算成本和骨架化质量

Abstract: We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.

</details>


### [24] [Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System](https://arxiv.org/abs/2512.18450)
*Xavier Rafael-Palou,Jose Munuera,Ana Jimenez-Pastor,Richard Osuala,Karim Lekadir,Oliver Diaz*

Main category: cs.AI

TL;DR: 提出基于代理的多中心临床AI系统漂移检测框架，通过站点特定的漂移监控代理提升多机构环境下的模型性能监测效果


<details>
  <summary>Details</summary>
Motivation: 现代临床决策支持系统服务于多个独立医学影像机构时，由于患者群体、成像硬件和采集协议的差异，预测性能可能在不同站点间退化。现有方法多依赖集中式监控，忽视了站点特定的漂移动态。

Method: 提出基于代理的框架，为每个站点分配漂移监控代理，进行批量模型输出与参考分布的对比。分析了多种多中心监控方案，包括站点特定、全局、仅生产数据和自适应参考获取方式。

Result: 在真实世界乳腺癌影像数据上，所有多中心方案都优于集中式监控，漂移检测F1分数提升高达10.3%。在没有站点特定参考的情况下，自适应方案表现最佳，漂移检测F1分数74.3%，漂移严重程度分类83.7%。

Conclusion: 自适应、站点感知的基于代理的漂移监控能够增强多中心临床决策支持系统的可靠性，为多机构AI系统部署提供了有效的性能监测方案。

Abstract: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.

</details>


### [25] [Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations](https://arxiv.org/abs/2512.18483)
*Rahul Yumlembam,Biju Issac,Seibu Mary Jacob,Longzhi Yang,Deepa Krishnan*

Main category: cs.AI

TL;DR: 提出结合显式和隐式图表示与时序建模的内网威胁检测框架，在CERT数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 内网威胁检测具有挑战性，因为恶意活动隐蔽且由受信任用户执行。现有方法难以捕捉复杂的用户行为模式，需要同时考虑结构关系和时序依赖。

Method: 1) 构建显式图：基于组织规则建模用户活动间的直接关系；2) 学习隐式图：使用Gumbel-Softmax技巧从特征相似性中发现潜在行为关系；3) 分别用GCN处理两种图生成节点嵌入；4) 通过注意力机制拼接和精炼特征；5) 使用Bi-LSTM捕捉时序依赖；6) 基于概率阈值标记异常活动。

Result: 在CERT r5.2数据集上：AUC 98.62，检测率100%，误报率0.05；在更具挑战性的r6.2数据集上：AUC 88.48，检测率80.15%，误报率0.15。优于现有最先进方法。

Conclusion: 结合图表示和时序建模能有效捕捉内网威胁的复杂行为模式，提出的框架在检测性能和鲁棒性方面表现优异，为内网威胁检测提供了有效解决方案。

Abstract: Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.

</details>


### [26] [Large Language Models as Discounted Bayesian Filters](https://arxiv.org/abs/2512.18489)
*Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: LLMs在动态随机环境中的在线推理能力评估：发现其信念更新类似贝叶斯后验，但更准确描述为指数遗忘滤波器，存在系统性的旧证据折扣现象。


<details>
  <summary>Details</summary>
Motivation: LLMs在少样本学习中表现出色，但在动态随机环境中的推理能力不透明。现有研究主要关注静态任务，忽视了信念需要持续更新的在线适应能力，这对LLMs作为世界模型或智能体至关重要。

Method: 引入贝叶斯滤波框架评估LLMs的在线推理能力，使用概率探测套件涵盖多元离散分布（如骰子滚动）和连续分布（如高斯过程），其中真实参数随时间变化。

Result: LLM的信念更新类似贝叶斯后验，但更准确描述为指数遗忘滤波器，具有模型特定的折扣因子小于1。这揭示了旧证据的系统性折扣现象，在不同模型架构间差异显著。虽然固有先验常常校准不当，但更新机制本身保持结构化和原则性。

Conclusion: LLMs在动态环境中的推理具有结构化的更新机制，但存在系统性偏差。通过提示策略可以有效重新校准先验，以最小计算成本改善性能。这为LLMs作为世界模型和智能体的应用提供了重要见解。

Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.

</details>


### [27] [Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V](https://arxiv.org/abs/2512.18564)
*John Chen,Sihan Cheng,Can Gurkan,Ryan Lay,Moez Salahuddin*

Main category: cs.AI

TL;DR: Vox Deorum是一个混合LLM+X架构，用于4X策略游戏《文明V》，让LLM负责宏观战略推理，子系统处理战术执行，实现了具有竞争力的游戏表现和多样化的游戏风格。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言推理方面的能力使其在4X和大战略游戏中具有独特优势，能够实现更自然的人机交互（如协作和谈判）。然而，这些游戏的复杂性和长时程特性带来了挑战，同时延迟和成本因素也阻碍了LLM的实际部署。

Method: 提出Vox Deorum混合架构，采用分层技术设计：LLM负责宏观战略推理，将战术执行委托给子系统（如算法AI或未来的强化学习AI）。在《文明V》Vox Populi模组上进行验证。

Result: 通过2,327场完整游戏测试，比较了两个开源LLM与Vox Populi增强AI的表现。结果显示LLM实现了具有竞争力的端到端游戏表现，同时展现出与算法AI显著不同且彼此之间也不同的游戏风格。

Conclusion: 该工作为在商业4X游戏中集成LLM建立了一个可行的架构，为游戏设计和智能体AI研究开辟了新机会。

Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.

</details>


### [28] [ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning](https://arxiv.org/abs/2512.18571)
*Weijie Zhou,Xuangtang Xiong,Ye Tian,Lijun Yue,Xinyu Wu,Wei Li,Chaoyang Zhao,Honghui Dong,Ming Tang,Jinqiao Wang,Zhengyou Zhang*

Main category: cs.AI

TL;DR: ESearch-R1：一个成本感知的具身推理框架，通过HC-GRPO算法统一交互对话、情景记忆检索和物理导航，在模糊指令下优化信息获取与异构成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）驱动的具身代理在面对模糊自然语言指令时，无法平衡物理探索的高成本与人类交互的认知成本。它们通常将消歧视为被动感知问题，缺乏最小化总任务执行成本的战略推理能力。

Method: 提出ESearch-R1成本感知具身推理框架，将交互对话（Ask）、情景记忆检索（GetMemory）和物理导航（Navigate）统一为单一决策过程。引入HC-GRPO（异构成本感知组相对策略优化）算法，通过采样推理轨迹组并强化那些在信息增益和异构成本（如导航时间、人类注意力）之间达到最优权衡的轨迹来优化MLLM。

Result: 在AI2-THOR环境中的大量实验表明，ESearch-R1显著优于基于ReAct的标准代理。它提高了任务成功率，同时将总运营成本降低了约50%，验证了GRPO在使MLLM代理与物理世界约束对齐方面的有效性。

Conclusion: ESearch-R1框架通过成本感知推理和HC-GRPO优化，有效解决了具身代理在模糊指令下的成本权衡问题，为MLLM代理在物理世界约束下的高效决策提供了新方法。

Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.

</details>


### [29] [Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction](https://arxiv.org/abs/2512.18605)
*Qinglin Zeng,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 提出反思置信度框架，将低置信度信号从终止指标转变为反思触发器，让模型在置信度低时进行自我分析纠正，而非直接停止生成，在数学推理任务上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于集成的方法如self-consistency需要多个推理轨迹，计算开销大。早期停止策略如DeepConf虽然通过终止低置信度轨迹降低成本，但会丢弃不完整的推理路径，浪费部分计算资源。

Method: 提出反思置信度框架：当模型置信度低于阈值时，不停止生成，而是生成反思提示来分析当前推理状态、识别潜在错误，并沿着纠正后的轨迹继续生成。

Result: 在数学推理基准测试（包括AIME 2025）上，相比先进的早期停止基线方法，在可比计算成本下实现了显著的准确率提升。

Conclusion: 主动自我纠正比被动丢弃更有效，反思置信度框架能够将低置信度信号转化为改进机会，在保持效率的同时提升推理质量。

Abstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.
  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.

</details>


### [30] [Assignment-Routing Optimization: Solvers for Problems Under Constraints](https://arxiv.org/abs/2512.18618)
*Yuan Qilong,Michal Pavelka*

Main category: cs.AI

TL;DR: 提出针对联合路由分配问题的MIP求解器，在机器人包装规划场景中显著优于现有方法，实现全局最优且计算时间稳定。


<details>
  <summary>Details</summary>
Motivation: 解决实际包装规划中更丰富的约束条件，包括多个占位符选项、时间框架限制和多类别物品包装，扩展先前基于Gurobi和割平面子环消除的精确MIP求解器。

Method: 开发针对实际包装规划场景的定制MIP求解器，扩展先前精确MIP求解器，结合Gurobi和割平面子环消除技术，处理多占位符选项、时间限制和多类别包装等约束。

Result: 在46个移动操作数据集上的实验表明，MIP方法实现全局最优且计算时间稳定低，比基于抖动的精确求解器快一个数量级，相比贪婪基线保持最优距离，平均偏差14%。

Conclusion: MIP基础的JRA优化在机器人包装、运动规划和复杂物流中具有实际应用价值，证明其效率和解决方案质量。

Abstract: We study the Joint Routing-Assignment (JRA) problem in which items must be assigned one-to-one to placeholders while simultaneously determining a Hamiltonian cycle visiting all nodes exactly once. Extending previous exact MIP solvers with Gurobi and cutting-plane subtour elimination, we develop a solver tailored for practical packaging-planning scenarios with richer constraints.These include multiple placeholder options, time-frame restrictions, and multi-class item packaging. Experiments on 46 mobile manipulation datasets demonstrate that the proposed MIP approach achieves global optima with stable and low computation times, significantly outperforming the shaking-based exact solver by up to an orders of magnitude. Compared to greedy baselines, the MIP solutions achieve consistent optimal distances with an average deviation of 14% for simple heuristics, confirming both efficiency and solution quality. The results highlight the practical applicability of MIP-based JRA optimization for robotic packaging, motion planning, and complex logistics .

</details>


### [31] [ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning](https://arxiv.org/abs/2512.18619)
*Zhenhao Zhou,Dan Negrut*

Main category: cs.AI

TL;DR: ChronoDreamer是一个用于接触丰富机器人操作的动作条件世界模型，通过时空变换器和MaskGIT风格掩码预测来预测未来视频帧、接触分布和关节角度，并使用VLM进行安全动作筛选。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够预测接触丰富机器人操作场景中未来状态的世界模型，特别是要处理刚性和可变形物体的复杂接触交互，同时确保动作执行的安全性。

Method: 使用时空变换器架构，采用MaskGIT风格的掩码预测训练。将接触编码为深度加权高斯泼溅图像，将3D力渲染为适合视觉骨干网络的相机对齐格式。推理时使用视觉语言模型评估预测轨迹的碰撞可能性，进行拒绝采样。

Result: 模型在非接触运动中保持空间一致性，生成合理的接触预测。基于LLM的评估器能够区分碰撞和非碰撞轨迹。在DreamerBench仿真数据集上进行了训练和评估。

Conclusion: ChronoDreamer成功构建了一个能够预测复杂接触交互的世界模型，并通过VLM-based安全评估机制实现了动作执行前的安全筛选，为接触丰富的机器人操作提供了有效的解决方案。

Abstract: We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.

</details>


### [32] [ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting](https://arxiv.org/abs/2512.18661)
*Hafiz Saif Ur Rehman,Ling Liu,Kaleem Ullah Qasim*

Main category: cs.AI

TL;DR: ASTIF是一个用于加密货币价格预测的混合智能系统，通过基于置信度的元学习实时调整预测策略，整合语义市场信号和数值趋势，在非平稳环境中优于现有深度学习和Transformer基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列预测模型大多依赖静态架构，难以整合异构知识源或适应快速的市场机制转变。传统方法仅依赖历史价格序列，忽略了政策不确定性和市场叙事等语义驱动因素。

Method: 提出ASTIF框架，包含三个互补组件：1) 使用MirrorPrompt的双通道小语言模型提取语义市场信号和数值趋势；2) 混合LSTM随机森林模型捕捉序列时间依赖；3) 基于置信度的元学习器作为自适应推理层，根据实时不确定性调节各预测器的贡献。

Result: 在2020-2024年AI相关加密货币和主要科技股的多样化数据集上，ASTIF优于领先的深度学习和Transformer基线模型（如Informer、TFT）。消融研究证实了自适应元学习机制的关键作用，能在市场波动期间成功通过转移语义和时间通道的依赖来降低风险。

Conclusion: 该研究为在非平稳环境中融合定量和定性数据提供了一个可扩展的、基于知识的解决方案，展示了自适应语义-时间整合在金融预测中的有效性。

Abstract: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.
  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.

</details>


### [33] [Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking](https://arxiv.org/abs/2512.18665)
*Dmitry Bennett,Fernand Gobet*

Main category: cs.AI

TL;DR: CogAct计算模型通过组块机制实现概念学习，能自适应学习从简单逻辑函数到文学、国际象棋和音乐等自然原始概念，相比其他模型更具适应性，并能模拟个体主观概念空间。


<details>
  <summary>Details</summary>
Motivation: 解决认知科学中关于短期和长期记忆中多种概念形成与检索的基本心理过程问题，特别是组块机制在概念学习中的核心作用，以及如何建立能够适应复杂自然概念且能模拟个体主观体验的计算模型。

Method: 提出CogAct计算模型，基于组块、注意、STM和LTM等基本认知过程和结构。模型能自适应学习多种类型概念，包括简单逻辑函数、人工类别以及文学、国际象棋和音乐等自然原始概念。通过设计考虑主观性和个体经验的人类基准测试，在真实复杂类别中模拟个体主观概念空间。

Result: CogAct模型成功实现了自适应概念学习，能够处理其他心理模型难以应对的自然原始概念。模型能模拟个体人类参与者的主观概念空间，捕捉音乐领域的主观判断，且无需依赖预构建知识结构。与深度学习模型相比，CogAct在模拟主观概念空间方面表现出优势。

Conclusion: 组块机制在概念学习中起关键作用，CogAct模型成功将概念学习和复杂性适应整合到更广泛的认知心理学理论中。该方法为心理学应用提供了新方向，从建模平均参与者转向捕捉主观概念空间。

Abstract: A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.

</details>


### [34] [IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling](https://arxiv.org/abs/2512.18669)
*Jones David,Shreya Ghosh*

Main category: cs.AI

TL;DR: IntelliCode是一个基于多智能体LLM的辅导系统，通过集中化、版本化的学习者状态实现长期、透明的教学支持


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统通常是单轮对话助手，缺乏对学习者知识的持久表示，难以提供有原则、透明和长期的教学支持

Method: 构建围绕集中化版本化学习者状态的多智能体LLM辅导系统，包含技能评估、学习者画像、渐进提示、课程选择、间隔重复和参与度监控六个专业智能体，通过StateGraph协调器统一管理

Result: 系统展示了端到端的辅导流程，模拟学习者验证显示稳定的状态更新、渐进提示提高任务成功率、多样化的课程覆盖

Conclusion: IntelliCode展示了持久学习者建模、协调的多智能体推理和有原则的教学设计相结合，能够产生透明可靠的LLM驱动辅导

Abstract: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.
  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.

</details>


### [35] [Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model](https://arxiv.org/abs/2512.18687)
*Yosuke Taniuchi,Chie Hieida,Atsushi Noritake,Kazushi Ikeda,Masaki Isoda*

Main category: cs.AI

TL;DR: 猴子社会比较研究：通过三种计算模型分析，发现猴子更依赖客观奖励差异而非主观价值推断来进行社会比较


<details>
  <summary>Details</summary>
Motivation: 研究猴子在社会比较中如何处理他人奖励信息，探索它们是仅识别客观奖励差异，还是能推断他人的主观价值评估

Method: 开发了三种计算模型：内部预测模型（推断伙伴主观价值）、无比较模型（忽略伙伴信息）、外部比较模型（直接纳入伙伴客观奖励），使用多层多模态潜在狄利克雷分配方法训练模型

Result: 外部比较模型在兰德指数上获得最高分类分数（0.88 vs 内部预测模型的0.79），表明社会比较主要依赖客观奖励差异而非主观状态推断

Conclusion: 猴子的社会比较过程基于客观奖励差异，而非对他人主观价值状态的推断，这为理解灵长类社会认知的计算机制提供了新见解

Abstract: Social comparison -- the process of evaluating one's rewards relative to others -- plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.

</details>


### [36] [KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing](https://arxiv.org/abs/2512.18709)
*Zhifei Li,Lifan Chen,Jiali Yi,Xiaoju Hou,Yue Zhao,Wenxin Huang,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: KeenKT模型使用NIG分布表示学生知识状态，通过注意力机制和对比学习提升知识追踪的准确性和鲁棒性，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要依赖单点估计，无法区分学生真实能力与偶然表现（如爆发或粗心），导致对知识掌握程度的判断存在模糊性。

Method: 提出KeenKT模型：1) 使用正态-逆高斯(NIG)分布表示每个交互时的知识状态；2) 设计基于NIG距离的注意力机制建模知识状态动态演化；3) 引入扩散去噪重构损失和分布对比学习损失增强鲁棒性。

Result: 在六个公开数据集上的实验表明，KeenKT在预测准确性和对行为波动的敏感性方面优于最先进的知识追踪模型，AUC最大提升5.85%，ACC最大提升6.89%。

Conclusion: KeenKT通过分布表示和多种优化技术有效解决了知识状态模糊性问题，提高了知识追踪的准确性和鲁棒性。

Abstract: Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.

</details>


### [37] [Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth](https://arxiv.org/abs/2512.18732)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架，将概念增长建模为在最小描述长度准则下评估的容许基扩展，认为概念学习仅在现有表示无法解释经验时发生。


<details>
  <summary>Details</summary>
Motivation: 大多数学习和推理模型都预设了固定的表示基础，但概念学习实际上需要表示基础本身的扩展。本文旨在探究在什么结构条件下，表示基础能够以原则性和选择性的方式扩展。

Method: 提出几何框架，将经验表示为当前概念子空间中的向量，残差分量捕获系统性表示失败。候选概念扩展被限制为低秩、容许变换，并在最小描述长度准则下评估。

Result: 证明任何MDL接受的扩展都可以选择使其新方向完全位于经验诱导的残差跨度内，而与该跨度正交的扩展会严格增加描述长度而被拒绝。这为想象力和概念创新提供了保守解释。

Conclusion: 该框架将概念发展描述为误差驱动、几何约束的基扩展过程，阐明了想象力在学习和理论变革中的作用和限制，区分了表示反事实与因果或价值层面的反事实。

Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.

</details>


### [38] [MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking](https://arxiv.org/abs/2512.18755)
*Jianyi Zhang,Shizhao Liu,Ziyin Zhou,Zhen Li*

Main category: cs.AI

TL;DR: MEEA是一种基于心理学"单纯曝光效应"的黑盒攻击框架，通过重复低毒性语义暴露逐步侵蚀LLM的安全对齐边界，在多轮对话中实现更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究大多假设静态安全边界，忽略了上下文交互对模型行为的动态影响，导致攻击的稳定性和泛化性有限。需要一种能评估多轮安全鲁棒性的自动化框架。

Method: MEEA构建语义递进的提示链，采用模拟退火策略进行优化，优化目标包括语义相似性、毒性和越狱效果。通过重复低毒性语义暴露逐步改变模型的有效安全阈值。

Result: 在GPT-4、Claude-3.5、DeepSeek-R1等闭源和开源模型上，MEEA比7个代表性基线平均攻击成功率提升超过20%，验证了退火优化和上下文暴露机制的必要性。

Conclusion: LLM安全行为本质上是动态且历史依赖的，挑战了静态对齐边界的常见假设，强调需要交互感知的安全评估和防御机制。

Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA

</details>


### [39] [The Dead Salmons of AI Interpretability](https://arxiv.org/abs/2512.18792)
*Maxime Méloux,Giada Dirupo,François Portet,Maxime Peyrard*

Main category: cs.AI

TL;DR: 论文提出将AI可解释性方法视为统计模型参数估计，强调需要对抗虚假发现、量化不确定性，并检验有意义的替代计算假设。


<details>
  <summary>Details</summary>
Motivation: AI可解释性领域存在类似"死鲑鱼"的虚假发现现象，许多方法（特征归因、探测、稀疏自编码等）甚至能为随机初始化的神经网络产生看似合理的解释。这暴露了当前可解释性方法在统计推断上的缺陷。

Method: 提出统计-因果重构框架：将计算系统的解释视为统计模型的参数，从计算轨迹中推断。强调解释方法应作为统计估计器，需要检验明确的替代计算假设，并在假设的统计模型下量化不确定性。

Result: 该框架揭示了可解释性查询的可识别性等关键理论问题，这些对理解领域对虚假发现、泛化能力差和高方差的敏感性至关重要。

Conclusion: 将可解释性置于统计推断的标准工具包中，为将AI可解释性转变为实用且严谨的科学开辟了有前景的途径，有助于对抗虚假发现并提高方法的可靠性。

Abstract: In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.

</details>


### [40] [HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare](https://arxiv.org/abs/2512.18829)
*Aditya Siddhant*

Main category: cs.AI

TL;DR: HARBOR模型在行为健康风险评估中超越传统方法和通用LLM，准确率达69%


<details>
  <summary>Details</summary>
Motivation: 行为健康风险评估面临挑战，因为患者数据高度多模态且情绪障碍具有时间动态性。虽然大语言模型展现强大推理能力，但在结构化临床风险评分中的效果尚不明确。

Method: 提出HARBOR模型预测Harbor风险评分（HRS），范围从-3（重度抑郁）到+3（躁狂）。同时发布PEARL数据集，包含4年纵向行为健康数据，涵盖生理、行为和自我报告的心理健康信号。

Result: HARBOR在多个评估设置和消融实验中表现最佳，达到69%准确率，而逻辑回归为54%，最强专有LLM基线为29%。

Conclusion: HARBOR模型在行为健康风险评估中优于传统方法和现成LLM，为临床风险评分提供了有效解决方案。

Abstract: Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.

</details>


### [41] [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)
*Zijun Gao,Zhikun Xu,Xiao Ye,Ben Zhou*

Main category: cs.AI

TL;DR: CORE是一个强化学习框架，通过将明确的概念转化为可控监督信号，解决LLMs在数学问题中模式复用而非概念理解的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决数学练习时往往只是模式复用，而非真正理解概念。现有的RLVR方法主要强化最终答案，缺乏细粒度的概念监督信号，导致模型改进的是模式复用能力而非概念应用能力。

Method: CORE框架包含三个核心步骤：(1)合成概念对齐的测验；(2)在rollout过程中注入简短概念片段以引发概念引导的轨迹；(3)通过轨迹替换（组失败后）强化概念推理，采用轻量级前向KL约束来对齐无引导和概念引导策略，或直接在概念对齐测验上使用标准GRPO。

Result: 在多个模型上，CORE在领域内概念练习套件和多样化的领域外数学基准测试中都优于普通和SFT基线方法，实现了持续的性能提升。

Conclusion: CORE通过统一在概念对齐测验上的直接训练和概念注入rollout，提供了细粒度的概念监督，弥合了问题解决能力和真正概念推理之间的差距，同时保持算法和验证器的无关性。

Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.

</details>


### [42] [Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models](https://arxiv.org/abs/2512.18901)
*Gökdeniz Gülmez*

Main category: cs.AI

TL;DR: Gabliteration是一种新颖的神经权重修改技术，通过自适应多方向投影和正则化层选择，在修改特定行为模式的同时最小化模型质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有方法在修改模型特定行为模式时会损害模型整体质量，需要一种更精细的权重修改技术来避免这种质量退化。

Method: 采用自适应多方向投影、正则化层选择、动态层优化、正则化投影矩阵和自适应缩放机制，实现理论最优的权重修改。

Result: 开发了gabliterated-v1模型系列（0.6B到4B参数），在Hugging Face上可用，验证了该方法在不同模型规模上的实际适用性。

Conclusion: Gabliteration技术超越了传统的权重修改方法，能够在修改特定行为的同时最小化对无关领域的影响，具有实际应用价值。

Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.

</details>


### [43] [Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage](https://arxiv.org/abs/2512.18908)
*Szymon Rusiecki,Cecilia G. Morales,Kimberly Elenberg,Leonard Weiss,Artur Dubrawski*

Main category: cs.AI

TL;DR: 该研究提出了一种基于贝叶斯网络的决策支持框架，融合多个计算机视觉模型的输出，用于大规模伤亡事件中的自动分诊，显著提升了分诊准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件会压垮紧急医疗系统，导致伤员评估延迟或错误，造成可预防的死亡。需要开发能够支持急救人员的自动分诊系统。

Method: 提出一个决策支持框架，将多个计算机视觉模型（评估严重出血、呼吸窘迫、身体警觉性、可见创伤）的输出融合到一个完全由专家定义规则构建的贝叶斯网络中。该方法不需要训练数据，支持不完整信息推理，对噪声或不确定观察具有鲁棒性。

Result: 在两个涉及11名和9名伤员的场景中，贝叶斯网络模型显著优于仅使用视觉的基线。生理评估准确率从15%提升到42%（第一个场景）和从19%提升到46%（第二个场景）。总体分诊准确率从14%提升到53%，系统诊断覆盖率从31%扩展到95%。

Conclusion: 专家知识引导的概率推理可以显著增强自动分诊系统，为大规模伤亡事件中的急救人员提供有前景的支持方法。该方法使团队在DARPA分诊挑战赛中获得第4名。

Abstract: Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.

</details>


### [44] [Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm](https://arxiv.org/abs/2512.18947)
*Li Yan,Bolun Liu,Chao Li,Jing Liang,Kunjie Yu,Caitong Yue,Xuzhao Chai,Boyang Qu*

Main category: cs.AI

TL;DR: 提出基于聚类自编码器预测的动态响应机制，用于动态多模态多目标优化问题，在决策空间保持多样性同时在目标空间获得良好收敛性。


<details>
  <summary>Details</summary>
Motivation: 动态多模态多目标优化面临双重挑战：既要跟踪多个等效的帕累托最优解集，又要在时变环境中保持种群多样性。现有动态多目标进化算法往往忽略解的多模态特性，而静态多模态多目标进化算法缺乏对动态变化的适应性。

Method: 1) 构建新的动态多模态多目标测试函数基准套件；2) 提出基于聚类自编码器预测的动态响应机制，利用自编码器处理匹配的聚类生成高度多样化的初始种群；3) 在静态优化器中集成自适应小生境策略以平衡收敛性和多样性。

Result: 在12个动态多模态多目标测试函数实例上的实验分析表明，与多个最先进的动态多目标进化算法和多模态多目标进化算法相比，该算法在决策空间能更有效地保持种群多样性，同时在目标空间获得更优的收敛性。

Conclusion: 提出的基于聚类自编码器预测的动态响应机制能有效解决动态多模态多目标优化问题，在决策空间多样性保持和目标空间收敛性方面均表现出优越性能。

Abstract: Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.

</details>


### [45] [Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection](https://arxiv.org/abs/2512.18956)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: SynSelect是一个三阶段合成-选择框架，用于为多模态推理任务生成高质量的长链思维数据，通过多模型合成和两级选择提升多模态大推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理面临挑战：现有方法在推理深度、模态转换错误和生成流程方面存在局限，缺乏高质量的长链思维训练数据，阻碍了多模态大推理模型的发展。

Method: 提出SynSelect三阶段框架：1) 利用多个异构多模态大推理模型生成多样化的候选链思维；2) 应用实例级和批次级选择筛选高质量链思维数据；3) 用于监督微调和强化学习后训练。

Result: 在多个多模态基准测试中，使用SynSelect生成数据进行监督微调的模型显著优于基线方法，经过强化学习后训练后性能进一步提升。

Conclusion: SynSelect是提升多模态大推理模型推理能力的有效方法，通过高质量长链思维数据生成解决了多模态推理中的数据稀缺和质量问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.

</details>


### [46] [ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management](https://arxiv.org/abs/2512.19001)
*Lingjie Zhao,Xue Yu,Yongzhi Qi,Hao Hu,Jianshen Zhang,Yingzheng Ma,Shuyu Han,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 提出OR引导的"预训练-强化"框架，通过仿真增强OR模型生成高质量参考决策，训练深度学习基础模型，再用强化学习进行深度对齐，在京东实际部署中显著提升库存绩效。


<details>
  <summary>Details</summary>
Motivation: 解决AI与运筹学(OR)在复杂库存系统中的协同挑战：如何有效调和AI的自适应感知与OR的结构严谨性，以克服当前AI模型缺乏领域知识和约束理解的局限性。

Method: 提出OR引导的"预训练-强化"框架：1) 仿真增强OR模型生成高质量参考决策；2) 基于OR决策作为训练标签，训练领域知识注入的深度学习基础模型；3) 强化学习作为深度对齐机制，使AI内化OR最优原则，同时探索通用策略优化和特定场景适应。

Result: 在京东的实地部署和双重差分分析显示：周转天数减少5.27天，现货率提升2.29%，持有成本降低29.95%。相比当前工业实践显著优越，证明轻量级、领域知识注入的模型在OR引导下能达到最先进性能。

Conclusion: 与当前暴力模型扩展趋势相反，研究表明轻量级、领域知识注入的模型在结构化OR逻辑指导下能实现最先进性能和强大可迁移性。这为智能供应链管理提供了可扩展且经济高效的范式，突显了AI与OR深度对齐的价值。

Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.

</details>


### [47] [Recontextualization Mitigates Specification Gaming without Modifying the Specification](https://arxiv.org/abs/2512.19027)
*Ariana Azarbal,Victor Gillioz,Vladimir Ivanov,Bryce Woodworth,Jacob Drori,Nevan Wichers,Aram Ebtekar,Alex Cloud,Alexander Matt Turner*

Main category: cs.AI

TL;DR: 提出recontextualization方法，通过重新语境化训练样本来减少语言模型"博弈"训练信号的问题，防止模型学习错误行为


<details>
  <summary>Details</summary>
Motivation: 开发者经常难以指定正确的训练标签和奖励信号，导致语言模型学会"博弈"这些不完善的训练信号，表现出错误行为

Method: recontextualization方法：从阻止错误行为的提示生成补全，然后将这些补全重新语境化为好像是对允许错误行为的提示的响应，从而训练模型即使在允许错误行为的指令下也能抵抗错误行为

Result: 该方法能防止模型学习：1) 优先考虑评估指标而非聊天质量；2) 特殊处理代码以通过错误测试；3) 对用户撒谎；4) 变得谄媚。在不改进监督信号的情况下减少了规范博弈

Conclusion: recontextualization通过重新语境化训练样本，减轻了错误指定训练信号对错误行为的强化，有效减少了语言模型的规范博弈问题

Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.

</details>


### [48] [Can abstract concepts from LLM improve SLM performance?](https://arxiv.org/abs/2512.19069)
*Siddharth Tandon*

Main category: cs.AI

TL;DR: 通过从大语言模型提取概念向量并转移到小模型，实现推理时性能提升，无需复杂基础设施设计


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限设备上部署困难，现有方法需要大量实验和复杂基础设施设计，需要更高效的模型压缩和性能提升方案

Method: 从大模型提取高层概念（表示为steering vectors），将其转移到小模型推理过程中，并引入推理时动态缩放机制调整引导强度

Result: 概念可有效转移到不同家族的小模型（Phi、Llama、Qwen等），在多种任务上提升性能，Qwen3-0.6B准确率提升7-15%

Conclusion: 通过概念向量转移和推理时动态缩放，可在不改变模型架构的情况下显著提升小模型性能，为资源受限设备部署提供新方案

Abstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.

</details>


### [49] [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)
*Yanzhi Zhang,Yitong Duan,Zhaoxi Zhang,Jiyan He,Shuxin Zheng*

Main category: cs.AI

TL;DR: 提出Population-Evolve方法，基于遗传算法优化LLM推理，通过并行推理维护动态候选解种群，使用进化提示让LLM自我进化种群，最终通过多数投票得到答案。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展已成为增强大语言模型推理能力的有前景方向，但现有方法仍有改进空间。受遗传算法启发，希望开发一种无需训练的方法来优化LLM推理过程。

Method: 提出Population-Evolve方法：1) 为每个问题维护动态候选解种群；2) 通过并行推理生成多个解决方案；3) 使用进化提示让LLM自我进化种群；4) 迭代进化直到收敛；5) 最终通过多数投票确定答案。

Result: 实验结果显示Population-Evolve在准确性方面表现优越，具有较低的性能方差和计算效率。同时建立了统一框架，将现有测试时扩展策略解释为遗传算法的变体。

Conclusion: 进化策略在推理阶段具有解锁LLM推理潜力的潜力。Population-Evolve作为一种无需训练的方法，为增强LLM推理能力提供了有效途径。

Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.

</details>


### [50] [$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics](https://arxiv.org/abs/2512.19084)
*Mark Burgess*

Main category: cs.AI

TL;DR: 该论文提出将注意力机制与承诺理论结合，建立机器学习向量表示与知识图谱之间的桥梁，避免依赖语言模型，通过语义时空图实现不确定性下的推理和数据压缩。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习（向量表示）和知识图谱（图表示）之间存在鸿沟，通常依赖语言模型作为桥梁。作者希望建立更直接的连接，同时处理数据统计稳定性（信任）和保持源数据意图性的需求。

Method: 使用承诺理论框架形式化注意力机制，提出语义时空图γ(3,4)作为替代复杂本体论的方法，通过特征在语义过程中的角色进行分类，关注因果边界条件以实现数据压缩。

Result: 建立了注意力机制与承诺理论的联系，展示了向量表示（用于概率估计）和图表示（保持意图性）可以共存，语义时空图方法能实现不确定性下的推理，并可能实现数量级的数据压缩。

Conclusion: 承诺理论为连接机器学习和知识图谱提供了有前景的框架，语义时空图方法在自主机器人、国防部署和应急服务等需要上下文确定和数据压缩的场景中具有应用潜力。

Abstract: The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.

</details>


### [51] [Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving](https://arxiv.org/abs/2512.19093)
*Peiqing Lu,Yuan Zhang,Haoyun Zhang,Jiasen Zheng,Kejian Tong,Wenjun Wu*

Main category: cs.AI

TL;DR: HERALD框架通过混合集成推理，结合语言模型与符号计算，提升双语数学问题求解的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 双语数学问题求解需要语言推理与符号计算的紧密结合，但当前大语言模型在语言处理方面表现良好，却在精确计算方面较弱，需要一种能有效连接两者的解决方案。

Method: 使用HERALD框架，集成NuminaMath-7B-TIR、GPT-4o和Mistral-7B模型，采用自适应路由、基于工具的强化学习和知识蒸馏技术连接不同推理路径。通过置信度校准保持权重稳定，双路径检查确保结果正确，强化学习控制工具使用减少冗余，蒸馏降低延迟而不影响准确性。

Result: 系统展示了结合符号检查、自适应集成和双语微调能够同时实现流畅推理和精确计算。HERALD为多语言数学推理提供了更好的准确性、稳定性和清晰度的实用解决方案。

Conclusion: HERALD框架通过混合集成方法有效连接了语言推理与符号计算，为双语数学问题求解提供了既保持流畅推理又确保精确计算的实用方案，在多语言数学推理任务中表现出优越的性能。

Abstract: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.

</details>


### [52] [Conditioning Accept-Desirability models in the context of AGM-like belief change](https://arxiv.org/abs/2512.19096)
*Kathelijne Coussement,Gert de Cooman,Keano De Vos*

Main category: cs.AI

TL;DR: 本文提出了一种新的条件化规则，用于抽象决策框架中的接受-期望模型，统一了经典和量子概率，并将其扩展到不精确概率语境。


<details>
  <summary>Details</summary>
Motivation: 为了在抽象决策框架中统一经典概率和量子概率，并扩展到不精确概率语境，需要开发适用于接受-期望模型的条件化规则。

Method: 在抽象决策框架中，不确定奖励存在于一般线性空间，事件是该线性空间上的特殊投影算子。提出基于"观察事件引入选项间新无差异"思想的新条件化规则，并关联信念修正算子。

Result: 建立了新的条件化规则和信念修正算子，研究了AGM信念修正公理在更一般框架中的适用性。在经典命题逻辑和完全条件概率两个特例中，所有AGM公理仍然成立。

Conclusion: 提出的条件化规则成功统一了经典和量子概率，并扩展到不精确概率。在特定情况下，AGM信念修正公理仍然有效，为抽象决策理论提供了理论基础。

Abstract: We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.

</details>


### [53] [FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning](https://arxiv.org/abs/2512.19107)
*Zhe Yang,Xiaoshuang Sheng,Zhengnan Zhang,Jidong Wu,Zexing Wang,Xin He,Shenghua Xu,Guanjing Xiong*

Main category: cs.AI

TL;DR: 提出FC-MIR框架，通过关键帧采样和自适应拼接减少移动UI操作轨迹的视觉冗余，结合MLLMs进行意图识别和任务自动化，在50%-60%压缩率下保持性能，但MLLMs在生成有用建议方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 从移动UI操作轨迹识别用户意图对UI理解和任务自动化至关重要。现有MLLMs在视频理解方面表现出色，但在移动设备实时部署时面临计算成本高和冗余帧处理效率低的问题。

Method: 提出FC-MIR框架：1) 使用关键帧采样和自适应拼接减少视觉冗余；2) 集成最先进的闭源MLLMs或微调模型（如Qwen3-VL）进行轨迹总结和意图预测；3) 扩展任务范围，包括生成预测后操作和搜索建议；4) 引入细粒度指标评估总结、预测和建议的实用性。

Result: 压缩方法在50%-60%压缩率下保持性能；闭源和微调MLLMs都表现出强大的意图总结能力，支持轻量级设备部署；但MLLMs在生成有用和"令人惊喜"的建议方面仍有困难；框架已在真实场景中部署，集成了UI感知和UI-Agent代理。

Conclusion: FC-MIR框架通过减少视觉冗余有效提升了移动UI意图识别的效率，MLLMs在意图总结方面表现良好，但在生成实用建议方面仍需改进。该框架为未来该领域的发展奠定了基础。

Abstract: Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.

</details>


### [54] [Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis](https://arxiv.org/abs/2512.19135)
*Chenghao Li,Chaoning Zhang,Yi Lu,Shuxu Chen,Xudong Wang,Jiaquan Zhang,Zhicheng Wang,Zhengxun Jin,Kuien Liu,Sung-Ho Bae,Guoqing Wang,Yang Yang,Hen Tao Shen*

Main category: cs.AI

TL;DR: 该论文首次从结构角度分析推理链质量，使用拓扑数据分析中的持久同调方法，发现推理链的拓扑结构复杂度与准确性正相关，成功推理具有更简单的拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从功能角度评估推理链，很少关注其结构机制。作者想知道为什么不同的推理链在推理中表现不同，以及推理链的哪些组成部分起关键作用。

Method: 应用拓扑数据分析中的持久同调方法，将推理步骤映射到语义空间，提取拓扑特征并分析结构变化。通过计算同调群评估不同尺度的连通性和冗余性，使用条形码和持久性图量化稳定性和一致性。

Result: 推理链的拓扑结构复杂度与准确性正相关。更复杂的链能更早识别正确答案，而成功的推理表现出更简单的拓扑结构，减少了冗余和循环，提高了效率和可解释性。

Conclusion: 该工作为推理链质量评估提供了新的结构视角，并为未来优化提供了指导，表明拓扑结构分析可以揭示推理链的语义连贯性、逻辑冗余以及逻辑断裂和间隙。

Abstract: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.

</details>


### [55] [Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness](https://arxiv.org/abs/2512.19155)
*Yin Jun Phua*

Main category: cs.AI

TL;DR: 通过构建体现不同意识理论的智能体并进行精确架构消融实验，发现GWT、IIT和HOT理论描述了互补的功能层而非竞争性解释


<details>
  <summary>Details</summary>
Motivation: 当前意识研究领域存在多个竞争理论（GWT、IIT、HOT），各自提出不同的神经标志物，但缺乏统一的验证框架

Method: 采用合成神经现象学方法：构建体现不同意识机制的人工智能体，通过精确的架构消融实验测试功能后果

Result: 实验1：自我模型损伤消除元认知校准但保留一阶任务表现；实验2：工作空间容量对信息访问至关重要；实验3：广播机制放大内部噪声导致脆弱性；发现PCI-A在工作空间瓶颈下降低

Conclusion: 这些理论描述了互补的功能层：GWT提供广播能力，HOT提供质量控制，形成层次化设计原则。强调这些智能体并非有意识，而是测试意识理论功能预测的参考实现

Abstract: The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.

</details>


### [56] [Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation](https://arxiv.org/abs/2512.19210)
*Jerry Wang,Ting Yiu Liu*

Main category: cs.AI

TL;DR: 提出一个交互式框架评估LLM在简单策略环境（如石头剪刀布）中是否展现真正"理解"，通过观察者模型识别策略并解释推理过程，量化预测与真实分布的匹配度。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型是否具备真正的"理解"能力，特别是在需要序列推理、适应性和策略识别的简单策略环境中，而非仅仅测试游戏知识本身。

Method: 将LLM定位为观察者，识别石头剪刀布游戏中的策略并解释推理；提供包含静态和动态策略的基准；使用交叉熵、Brier分数和期望值差异三个互补信号量化预测与真实分布的匹配度，并整合为统一的Union Loss；同时使用策略识别率(SIR)指标。

Result: 开发了一个强调交互性、透明度和可重复性的演示系统，用户可以实时调整LLM分布、可视化损失演变、直接检查推理片段，识别失败原因。

Conclusion: 该框架为序列游戏中的类心智推理提供了实用且可解释的代理，揭示了当前LLM推理能力的优势和局限性。

Abstract: We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine "understanding" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.

</details>


### [57] [Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models](https://arxiv.org/abs/2512.19228)
*Valentin Schmidberger,Manuel Eberhardinger,Setareh Maghsudi,Johannes Maucher*

Main category: cs.AI

TL;DR: LLMs可以通过领域特定微调自动生成用于文档伪造检测的基于规则的合理性检查，在受限硬件上实现可执行且有效的验证程序。


<details>
  <summary>Details</summary>
Motivation: 文档伪造对法律、经济和政府流程构成日益严重的威胁，需要更复杂的验证机制。现有的合理性检查由软件工程师手动实现，耗时且难以扩展。LLMs在代码生成方面的进展为自动化生成这些检查提供了新可能，但如何让LLMs适应未知领域的具体要求仍是一个挑战。

Method: 研究使用开源LLMs（Llama 3.1 8B和OpenCoder 8B），通过不同微调策略在领域特定代码和数据上进行适应，生成基于规则的合理性检查。在真实应用场景的结构化数据集上进行微调，并在未见过的伪造模式上评估生成的检查程序。

Result: 模型能够生成可执行且有效的验证程序，证明了LLMs在受限硬件资源上生成用于伪造检测的合理性检查的可行性。

Conclusion: LLMs有潜力作为可扩展工具，在需要可理解性的安全敏感环境中支持人类决策，自动化生成文档伪造检测的合理性检查。

Abstract: Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.

</details>


### [58] [DeliveryBench: Can Agents Earn Profit in Real World?](https://arxiv.org/abs/2512.19234)
*Lingjun Mao,Jiawei Ren,Kun Zhou,Jixuan Chen,Ziqiao Ma,Lianhui Qin*

Main category: cs.AI

TL;DR: DeliveryBench：基于真实外卖配送场景的城市规模具身智能基准测试，评估VLM智能体在长时程、多约束环境下的规划能力


<details>
  <summary>Details</summary>
Motivation: 现有具身智能基准测试主要关注简单短期任务，缺乏真实世界决策中丰富的约束条件。为填补这一空白，研究者基于外卖配送这一真实职业场景，提出了能够捕捉长时程目标（数小时净收益最大化）和多种约束（配送时限、交通成本、车辆电量、人际交互等）的基准测试。

Method: DeliveryBench在程序生成的3D城市中实例化外卖配送场景，包含多样化的道路网络、建筑、功能地点、交通方式和真实的资源动态。研究者在九个不同城市中评估了多种基于VLM的智能体，并与人类玩家进行对比。

Result: 结果显示，当前VLM智能体与人类存在显著性能差距，这些智能体往往目光短浅，经常违反基本常识约束。同时，不同模型展现出明显不同的"个性"（如GPT-5的冒险倾向 vs Claude的保守倾向），揭示了当前VLM具身智能体在真实约束密集环境中的脆弱性和多样性。

Conclusion: DeliveryBench为评估具身智能体在真实约束环境中的长时程规划能力提供了重要基准，揭示了当前VLM智能体在复杂现实任务中的局限性，并展示了不同模型的个性差异，为未来具身智能研究提供了新的评估方向。

Abstract: LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.

</details>


### [59] [Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6](https://arxiv.org/abs/2512.19287)
*Jiaao Wu,Xian Zhang,Fan Yang,Yinpeng Dong*

Main category: cs.AI

TL;DR: Vibe Reasoning是一种人机协作范式，通过元提示、智能体基础化和模型编排，将前沿AI模型的潜在知识转化为实际能力，成功解决了IMO 2025第6题这一组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型已经具备解决复杂数学问题所需的知识，但不知道如何、何时应用这些知识。现有自主AI系统在IMO 2025第6题上公开报告失败，需要一种方法将AI的潜在能力转化为实际表现。

Method: 采用Vibe Reasoning范式，结合GPT-5的探索能力和Gemini 3 Pro的证明优势，使用智能体工作流（Python代码执行和基于文件的记忆），通过迭代精炼和从问题特定提示到通用可转移元提示的演进。

Result: 成功推导出IMO 2025第6题的正确答案（2112）和严格的数学证明，发现了智能体基础化和模型编排的必要性，验证了轻量级人类指导可以解锁前沿模型的数学推理潜力。

Conclusion: Vibe Reasoning通过人机协作有效解决了复杂数学问题，正在开发自动化框架并进行更广泛评估以验证其通用性和有效性，为前沿AI模型的数学推理能力解锁提供了可行路径。

Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.

</details>


### [60] [Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application](https://arxiv.org/abs/2512.19299)
*Haoyu Jiang,Fanjie Zeng,Boan Qu,Xiaojie Lin,Wei Zhong*

Main category: cs.AI

TL;DR: Helios是一个针对智能能源领域定制的大语言模型，通过多智能体协作框架构建了知识库、指令微调数据集和RLHF数据集，显著提升了领域知识掌握、任务执行准确性和人类偏好对齐能力。


<details>
  <summary>Details</summary>
Motivation: 在碳中和背景下，智能能源系统需要跨学科专业知识，但通用大语言模型缺乏领域知识和物理约束意识，无法提供精确的工程对齐推理和生成。

Method: 开发了Enersys多智能体协作框架进行端到端数据集构建，包括：EnerBase知识库、EnerInstruct指令微调数据集、EnerReinforce RLHF数据集。基于这些资源，Helios进行了大规模预训练、SFT和RLHF。

Result: 发布了EnerBench评估基准，并证明该方法显著增强了领域知识掌握、任务执行准确性以及与人类偏好的对齐。

Conclusion: Helios模型及其配套资源为智能能源领域的LLM研究提供了全面解决方案，解决了通用LLM在该领域专业知识不足的问题。

Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.

</details>


### [61] [SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.19317)
*A. A. Gde Yogi Pramana,Jason Ray,Anthony Jaya,Michael Wijaya*

Main category: cs.AI

TL;DR: SafeMed-R1：针对医学视觉问答的混合防御框架，结合对抗训练与随机平滑，在保持高质量临床推理的同时提供认证的鲁棒性保证


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在医学VQA中表现出色，但在对抗攻击下极其脆弱。标准的对抗训练会降低泛化性能和临床推理质量，阻碍其在临床环境中的部署。

Method: 提出SafeMed-R1混合防御框架：训练阶段使用对抗训练与组相对策略优化（AT-GRPO）来鲁棒化推理过程；推理阶段采用随机平滑提供认证的L2范数鲁棒性保证。

Result: 在OmniMedVQA基准测试中，标准微调VLM在干净输入上准确率达95%，但在PGD攻击下崩溃至25%。SafeMed-R1在相同对抗条件下保持84.45%准确率，鲁棒性提升59个百分点。

Conclusion: SafeMed-R1显著提升了医学VQA系统的对抗鲁棒性，同时保持高质量的临床推理。研究还发现显式思维链推理的模型比仅指令的变体具有更好的对抗鲁棒性，表明医学AI系统中可解释性与安全性存在协同效应。

Abstract: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.

</details>


### [62] [VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop](https://arxiv.org/abs/2512.19349)
*JiaWei Zhu,ZiHeng Liu*

Main category: cs.AI

TL;DR: VIGOR+ 是一个结合大语言模型和统计验证的迭代框架，用于生成和优化隐藏混杂变量，解决传统方法中语义合理但统计无效的问题。


<details>
  <summary>Details</summary>
Motivation: 当前利用大语言模型生成隐藏混杂变量的方法存在关键缺陷：生成的混杂变量虽然语义上合理，但缺乏统计效用。需要一种能够将LLM生成与统计验证相结合的方法来改进混杂变量的质量。

Method: 提出VIGOR+框架，建立LLM生成与CEVAE统计验证之间的迭代反馈机制。将CEVAE的验证信号（信息增益、潜在一致性指标、诊断信息）转化为自然语言反馈，指导后续LLM生成轮次，直到满足收敛标准。

Result: 论文形式化了反馈机制，在温和假设下证明了收敛性质，并提供了完整的算法框架。该方法能够有效改进隐藏混杂变量的统计效用。

Conclusion: VIGOR+通过建立LLM生成与统计验证之间的闭环迭代机制，成功解决了隐藏混杂变量生成中语义合理但统计无效的问题，为因果推断提供了更可靠的混杂变量生成方法。

Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.

</details>


### [63] [PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://arxiv.org/abs/2512.19350)
*A. B. M. Ashikur Rahman,Saeed Anwar,Muhammad Usman,Irfan Ahmad,Ajmal Mian*

Main category: cs.AI

TL;DR: 该论文提出了PENDULUM基准测试，用于评估多模态大语言模型中的谄媚行为，发现现有模型在视觉推理中存在显著的谄媚和幻觉倾向。


<details>
  <summary>Details</summary>
Motivation: 谄媚行为（AI模型过度迎合用户输入而牺牲事实准确性或违背视觉证据）是多模态大语言模型面临的关键但未充分探索的挑战。现有研究主要关注纯文本环境，对视觉或多模态环境的研究有限。

Method: 提出了PENDULUM基准测试，包含约2000个人工策划的视觉问答对，专门设计用于引发谄媚响应。该基准涵盖六个不同复杂度的图像领域，并提出了量化视觉推理中谄媚行为的新指标。

Result: 对最先进MLLMs的评估显示模型鲁棒性存在显著差异，且对谄媚和幻觉行为表现出明显易感性。不同模型在不同图像类型和挑战下的表现差异很大。

Conclusion: 研究结果强调了开发抗谄媚架构和训练策略的迫切需求，以提高未来MLLMs的事实一致性和可靠性。提出的基准和指标为深入理解多模态环境中的谄媚行为提供了工具。

Abstract: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.

</details>


### [64] [First-Order Representation Languages for Goal-Conditioned RL](https://arxiv.org/abs/2512.19355)
*Simon Ståhlberg,Hector Geffner*

Main category: cs.AI

TL;DR: 该研究探索在目标条件强化学习和广义规划中使用一阶关系语言，通过将目标和状态表示为原子集合，并采用三种目标表示方式，在稀疏奖励的大型规划实例中成功学习通用策略。


<details>
  <summary>Details</summary>
Motivation: 传统上，一阶关系语言主要用于紧凑表示MDP和学习通用策略。本研究旨在解决当训练实例庞大且目标无法通过随机探索达成时，如何学习目标条件和通用策略的问题。

Method: 基于Hindsight Experience Replay（HER）技术，将目标和状态表示为原子集合，探索三种目标表示方式：完整状态目标、原始目标子集目标、以及这些子目标的提升版本。

Result: 后两种表示方式（目标子集和提升子目标）成功在稀疏奖励的大型规划实例中学习到通用策略，通过自动创建难度递增的目标课程实现更高效的学习。

Conclusion: 将目标和状态表示为原子集合，特别是使用目标子集和提升子目标表示，能够在大型稀疏奖励环境中有效学习通用策略，展示了计算效率提升和未来改进机会。

Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.

</details>


### [65] [Learning General Policies with Policy Gradient Methods](https://arxiv.org/abs/2512.19366)
*Simon Ståhlberg,Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 该研究将组合规划方法与深度强化学习结合，探索DRL学习泛化策略的条件，使用GNN表示策略，在保持组合方法泛化能力的同时避免其可扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 强化学习在泛化能力方面存在挑战，而组合规划方法能学习对给定领域所有实例都有效的可证明正确策略。本研究旨在结合这两种研究思路，探索深度强化学习方法在何种条件下能学习到类似组合方法的泛化策略。

Method: 1) 将策略建模为状态转移分类器（而非具体动作）；2) 使用图神经网络表示规划状态的价值函数和策略；3) 采用actor-critic方法学习策略；4) 通过添加派生谓词和替代成本结构来克服GNN表达限制和最优性-泛化权衡问题。

Result: actor-critic方法能够学习到与组合方法几乎同样好的泛化策略，同时避免了可扩展性瓶颈和特征池的使用。DRL方法的限制主要来自GNN的表达限制和最优性-泛化权衡，而非深度学习或强化学习算法本身。

Conclusion: 深度强化学习方法能够学习到具有良好泛化能力的策略，接近组合方法的水平。通过添加派生谓词和替代成本结构，可以在不改变基本DRL方法的情况下解决GNN表达限制和最优性-泛化权衡问题，为学习泛化策略提供了有效途径。

Abstract: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.

</details>


### [66] [EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration](https://arxiv.org/abs/2512.19396)
*Runze Li,Yuwen Zhai,Bo Xu,LiWu Xu,Nian Shi,Wei Zhang,Ran Lin,Liang Wang*

Main category: cs.AI

TL;DR: EchoTrail-GUI：一个为GUI代理引入动态记忆框架，通过经验探索、记忆注入和任务推理三阶段实现自主学习和性能提升


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理存在"数字失忆"问题，每次任务都独立处理，无法从过去成功经验中学习，导致性能次优、重复错误和泛化能力差

Method: 三阶段框架：1) 经验探索阶段：代理自主与GUI环境交互，构建经过奖励模型验证的成功任务轨迹数据库；2) 记忆注入阶段：针对新任务高效检索最相关的历史轨迹作为"记忆"；3) GUI任务推理阶段：将记忆作为上下文指导注入代理的推理和决策过程

Result: 在Android World和AndroidLab基准测试中，EchoTrail-GUI显著提高了基线代理的任务成功率和操作效率，验证了结构化记忆在创建更强大GUI自动化中的有效性

Conclusion: 通过引入动态可访问的记忆机制，EchoTrail-GUI使GUI代理能够模仿人类经验式学习，解决了数字失忆问题，为更智能、更鲁棒的GUI自动化提供了有效解决方案

Abstract: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.

</details>


### [67] [An Agentic Framework for Autonomous Materials Computation](https://arxiv.org/abs/2512.19458)
*Zeyu Xia,Jinzhe Ma,Congjie Zheng,Shufei Zhang,Yuqiang Li,Hang Su,P. Hu,Changshui Zhang,Xingao Gong,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Mao Su*

Main category: cs.AI

TL;DR: 开发了一个面向第一性原理材料计算的领域专用智能体，通过嵌入领域知识确保物理一致性，显著优于独立LLM，为自主计算实验奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在加速科学发现方面表现出潜力，但其静态知识和幻觉问题阻碍了自主研究应用。需要开发能够可靠执行复杂科学工作流程的智能系统。

Method: 构建了一个领域专用智能体框架，通过嵌入材料计算领域的专业知识，确保物理一致的多步骤工作流程，并选择收敛、良定的参数，实现可靠端到端计算执行。

Result: 在多样化的计算任务基准测试中，该系统在准确性和鲁棒性方面显著优于独立的大语言模型。

Conclusion: 这项工作为自主计算实验建立了可验证的基础，代表了迈向完全自动化科学发现的关键一步。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.

</details>


### [68] [QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models](https://arxiv.org/abs/2512.19526)
*Li Puyin,Tiange Xiang,Ella Mao,Shirley Wei,Xinye Chen,Adnan Masood,Li Fei-fei,Ehsan Adeli*

Main category: cs.AI

TL;DR: QuantiPhy是首个评估视觉语言模型物理推理能力的定量基准，包含3.3K+视频-文本实例，测试模型对物体尺寸、速度、加速度的数值估计能力，发现现有模型在数值准确性方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉感知模型（如大型VLMs）的物理推理能力评估主要基于VQA的定性方法，缺乏对模型能否从视频观察中定量推断运动物体运动学量的深入理解。需要建立定量评估基准来推动AI代理对物理世界的真正理解。

Method: 提出QuantiPhy基准，包含3.3K+视频-文本实例，每个实例都有数值真实值。评估模型在给定时间戳下估计物体尺寸、速度、加速度的能力，使用其中一个属性作为输入先验。标准化提示和评分方法以评估数值准确性。

Result: 实验发现最先进的VLMs在定性合理性和实际数值正确性之间存在一致差距。模型严重依赖预训练的世界知识，而非忠实使用提供的视觉和文本输入作为参考来定量推理运动学属性。背景噪声、反事实先验和策略提示等因素影响模型表现。

Conclusion: QuantiPhy提供了首个严格、可扩展的测试平台，推动VLMs超越口头合理性，实现基于数值的物理理解。研究揭示了当前模型在定量物理推理方面的局限性，为未来改进指明了方向。

Abstract: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.

</details>


### [69] [Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios](https://arxiv.org/abs/2512.19551)
*Jiawen Wang,Jingjing Wang Tianyang Chen,Min Zhang,Guodong Zhou*

Main category: cs.AI

TL;DR: 本文提出L^2-EMG任务，旨在让LLM能够持续学习不同场景下的情感运动生成，并设计ES-MoE方法解决情感解耦和场景适应两大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的人本情感运动生成方法主要关注单一固定尺度数据集，忽视了灵活且尺度递增的运动场景（如体育、舞蹈）。有效学习这些新场景能显著提升模型在真实世界中的泛化能力。

Method: 提出ES-MoE方法，包含因果引导的情感解耦块和场景适应的专家构建块，分别解决情感解耦和场景适应两大挑战。

Result: 构建了多个L^2-EMG数据集进行验证，广泛评估表明ES-MoE优于先进基线方法。

Conclusion: 该研究为构建具备共情和智能的闭环自进化具身智能体提供了新思路，通过L^2-EMG任务和ES-MoE方法有效解决了情感运动生成的持续学习问题。

Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.

</details>


### [70] [Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations](https://arxiv.org/abs/2512.19557)
*Lawrence Krukrubo,Julius Odede,Olawande Olusegun*

Main category: cs.AI

TL;DR: 论文提出Hybrid LRR-TED框架解决XAI的"可扩展性-稳定性困境"，通过"发现不对称性"概念，在客户流失预测中结合自动规则学习和少量人工规则，实现高精度预测并减少50%人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI方法面临"可扩展性-稳定性困境"：后验方法（如LIME、SHAP）可扩展但不稳定，监督解释框架（如TED）稳定但需要大量人工标注。需要一种平衡方案来解决这一矛盾。

Method: 提出Hybrid LRR-TED混合框架，基于"发现不对称性"概念。在客户流失预测中，使用自动规则学习器（GLRM）识别广泛的"安全网"（保留模式），然后通过帕累托最优方法仅添加4条人工定义的"风险陷阱"规则来补充自动规则。

Result: 该方法达到94.00%的预测准确率，优于完整的8条人工专家规则基线，同时将人工标注工作量减少50%。证明了自动规则擅长发现保留模式，而人工规则更擅长识别流失触发因素。

Conclusion: 该框架解决了XAI的可扩展性-稳定性困境，提出了人机协同AI的新范式：将专家角色从"规则编写者"转变为"异常处理者"，通过少量高质量人工规则补充自动规则学习，实现高效稳定的可解释AI。

Abstract: Current approaches to Explainable AI (XAI) face a "Scalability-Stability Dilemma." Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel "Asymmetry of Discovery." When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad "Safety Nets" (retention patterns) but struggle to capture specific "Risk Traps" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of "Rule Writers" to "Exception Handlers."

</details>


### [71] [Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight](https://arxiv.org/abs/2512.19691)
*Junze Ye,Daniel Tawfik,Alex J. Goodell,Nikhil V. Kotha,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该研究提出将临床风险评分计算等复杂任务的基准视为"进行中的活文档"，通过医生参与的验证流程修正MedCalc-Bench数据集中的标签错误，并证明修正后的标签能显著提升强化学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM生成特征和规则聚合构建的临床风险评分基准（如MedCalc-Bench）存在将模型错误固化为评估标准的问题，特别是在这些数据集作为强化学习奖励信号时，这种问题会被危险地放大。需要建立动态的基准维护机制。

Method: 提出系统化的医生参与流程，利用先进的代理验证器审计和重新标注MedCalc-Bench数据集，通过自动分类将稀缺的临床医生注意力保留给最有争议的实例。使用修正后的标签通过GRPO方法微调Qwen3-8B模型。

Result: 审计发现原始标签中相当一部分与医学事实存在偏差，主要源于提取错误、计算器逻辑不匹配和临床模糊性。使用修正标签训练的模型比原始基线准确率绝对提升8.7%，验证了标签噪声对模型评估的实质性影响。

Conclusion: 在安全关键领域，严格的基准维护是实现真正模型对齐的前提条件。应将复杂任务基准视为需要定期重新评估的"活文档"，随着创建流程的改进而更新。

Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [72] [Scalable Multiterminal Key Agreement via Error-Correcting Codes](https://arxiv.org/abs/2512.18025)
*Benjamin D. Kim,Daniel Alabi,Lav R. Varshney*

Main category: cs.IT

TL;DR: 提出基于纠错码的多终端密钥协商协议，利用秘密共享与密钥协商的联系，通过Reed-Solomon码实现无信息泄露，并推导了新的密钥容量界限。


<details>
  <summary>Details</summary>
Motivation: 探索秘密共享与密钥协商之间的内在联系，旨在设计一个简单且可扩展的多终端密钥协商协议，解决传统方法复杂度高的问题。

Method: 使用纠错码（特别是具有阈值重构特性的Reed-Solomon码）构建协议，确保窃听者无法获取任何信息。利用密钥容量与多元互信息的对偶性推导理论界限。

Result: 提出了一个基于秘密共享的多终端密钥协商协议，并推导了全秩最大距离可分码和该方案密钥容量的新界限。

Conclusion: 通过连接秘密共享和密钥协商，成功构建了简单可扩展的多终端密钥协议，为安全通信提供了新的理论框架和实用方案。

Abstract: We explore connections between secret sharing and secret key agreement, which yield a simple and scalable multiterminal key agreement protocol. In our construction, we use error-correcting codes, specifically Reed-Solomon codes with threshold reconstruction, to ensure no information is leaked to an eavesdropper. We then derive novel bounds for both full-rank maximum distance separable codes and our scheme's secret key capacity, using key capacity's duality with multivariate mutual information.

</details>


### [73] [Implementing Transport Coding in OMNeT++ for Message Delay Reduction](https://arxiv.org/abs/2512.18332)
*Ilya Petrovanov,Anton Sergeev*

Main category: cs.IT

TL;DR: 传输编码通过在传输层引入受控冗余来减少分组交换网络中的消息延迟：将k个原始数据包编码为n≥k个编码数据包，消息在成功交付前k个数据包后即可重建，将延迟从最大数据包延迟转移到第k个顺序统计量。


<details>
  <summary>Details</summary>
Motivation: 研究传输编码如何通过引入冗余来减少网络消息延迟，特别是在低延迟服务中，需要在延迟减少和吞吐量保持之间找到平衡。

Method: 在OMNeT++中实现简洁、可重现的离散事件传输编码仿真，包括多跳Kleinrock型网络、FIFO队列、指数服务延迟和链路延迟，以及显式的接收端重建机制，记录消息延迟和截止时间违规。

Result: 仿真结果显示，在适度冗余下，平均延迟和延迟交付概率持续减少，同时保持饱和吞吐量接近未编码基线。编码配置在相同消息生成率下优于未编码配置。

Conclusion: 提出的模型为分析传输编码公式和可执行仿真之间提供了透明桥梁，可用于调整低延迟服务中的冗余水平，在延迟减少和吞吐量保持之间取得良好平衡。

Abstract: Transport coding reduces message delay in packet-switched networks by introducing controlled redundancy at the transport layer: $k$ original packets are encoded into $n\ge k$ coded packets, and the message is reconstructed after the first $k$ successful deliveries, effectively shifting latency from the maximum packet delay to the $k$-th order statistic. We present a concise, reproducible discrete-event implementation of transport coding in OMNeT++, including a multi-hop Kleinrock-type network, FIFO queues, exponential service and link delays, and explicit receiver-side reconstruction that records message delay and deadline violations. Using paired uncoded ($n{=}k$) and coded ($n{>}k$) configurations at the same message generation rate, we compare delay, reliability, and saturation effects across code rates and input loads. Simulation results show consistent reductions of average delay and late-delivery probability for moderate redundancy, while keeping the saturation throughput close to the uncoded baseline. The proposed model provides a transparent bridge between analytical transport-coding formulas and executable simulation for tuning redundancy in low-latency services.

</details>


### [74] [Downlink Power Allocation for STAR-RIS-Assisted Cell-Free Massive MIMO with Multi-antenna Users](https://arxiv.org/abs/2512.18359)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 本文研究了STAR-RIS辅助的无蜂窝大规模MIMO系统中多天线用户的下行功率分配，提出了基于ADMM-FP的功率分配算法，相比现有方法可提升20%以上的频谱效率。


<details>
  <summary>Details</summary>
Motivation: STAR-RIS辅助的无蜂窝大规模MIMO系统具有巨大潜力，但现有研究多关注单天线用户。本文旨在研究多天线用户场景下的下行功率分配问题，以提升系统频谱效率。

Method: 1) 推导了使用线性MMSE检测器的下行频谱效率闭式表达式；2) 将下行功率分配问题构建为和频谱效率最大化问题；3) 提出了基于ADMM的分数规划算法来解决该优化问题。

Result: 1) 多天线用户显著提升频谱效率，天线数从1增加到6时至少提升20%；2) 提出的ADMM-FP算法优于现有分数功率控制方法，频谱效率提升超过20%。

Conclusion: 在STAR-RIS辅助的无蜂窝大规模MIMO系统中，采用多天线用户和高效的功率分配算法是必要的，能显著提升系统频谱效率。

Abstract: This paper investigates the downlink power allocation of the simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-assisted cell-free massive multiple-input multiple-output (MIMO) system with multi-antenna users. We introduce downlink spectral efficiency (SE) and derive novel closed-form SE expressions using linear minimum mean squared error (MMSE) detectors. We also address the downlink power allocation via a sum SE maximization problem framed within an alternating direction method of multipliers (ADMM)-based fractional programming (FP) algorithm. Numerical results demonstrate that systems utilizing multi-antenna users significantly enhance SE, achieving at least a 20% SE increase as the number of antennas increases from one to six. Additionally, our proposed ADMM-based FP algorithm outperforms existing fractional power control approaches, yielding a more than 20% SE increase. These results highlight the necessity for adopting multi-antenna users and efficient power allocation algorithms in STAR-RIS-assisted cell-free massive MIMO systems.

</details>


### [75] [Age of Information with Age-Dependent Server Selection](https://arxiv.org/abs/2512.18457)
*Nail Akar,Ismail Cosandal,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究多服务器状态更新系统，提出基于AoI阈值的服务器选择策略，使用多区域吸收马尔可夫链精确分析AoI分布，优化阈值以最小化AoI成本并满足传输成本约束。


<details>
  <summary>Details</summary>
Motivation: 在多服务器状态更新系统中，如何根据AoI值动态选择服务器进行传输，以平衡AoI性能和传输成本，是一个重要问题。现有研究缺乏对异构服务器（不同服务时间和传输成本）的精确分析框架。

Method: 提出多区域吸收马尔可夫链（MR-AMC）框架，精确分析AoI分布。系统采用基于AoI阈值的策略：传输完成后，根据当前AoI值决定等待或选择特定服务器传输。服务器具有异构的离散相位型分布服务时间和传输成本。

Result: 通过MR-AMC框架精确获得了AoI分布，从而计算AoI成本和传输成本。在服务器数量较少时，可以通过穷举搜索找到最优阈值。数值实验验证了分析模型的有效性，并展示了基于AoI的服务器选择策略的优势。

Conclusion: 提出的MR-AMC框架为多服务器状态更新系统提供了精确的分析工具，基于AoI阈值的服务器选择策略能有效优化AoI性能同时满足传输成本约束，特别适用于异构服务器环境。

Abstract: In this paper, we consider a single-source multi-server generate-at-will discrete-time non-preemptive status update system where update packets are transmitted using {\em only one} of the available servers, according to a server selection policy. In particular, when a transmission is complete, the update system makes a threshold-based decision on whether to wait or transmit, and if latter, which server to use for transmissions, on the basis of the instantaneous value of the age of information (AoI) process. In our setting, servers have general heterogeneous discrete phase-type (DPH) distributed service times, and also heterogeneous transmission costs. The goal is to find an age-dependent multi-threshold policy that minimizes the AoI cost with a constraint on transmission costs, the former cost defined in terms of the time average of an arbitrary function of AoI. For this purpose, we propose a novel tool called \emph{multi-regime absorbing Markov chain} (MR-AMC) in discrete time. Using the MR-AMC framework, we exactly obtain the distribution of AoI, and subsequently the costs associated with AoI and transmissions. With the exact analysis in hand, optimum thresholds can be obtained in the case of a few servers, by exhaustive search. We validate the proposed analytical model, and also demonstrate the benefits of age-dependent server selection, with numerical examples.

</details>


### [76] [Real-Time Remote Monitoring of Correlated Markovian Sources](https://arxiv.org/abs/2512.18698)
*Mehrdad Salimnejad,Marios Kountouris,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 提出一种基于误差感知的联合采样传输策略，用于在共享无线信道上实时跟踪两个相关随机过程，最小化时间平均重构误差并满足采样成本约束。


<details>
  <summary>Details</summary>
Motivation: 研究共享无线信道上两个相关随机过程的实时跟踪问题。虽然两个过程源自同一底层现象（如同一源的不同特征），但每个监视器只关注其对应特征。当真实状态与重构状态不匹配时会产生重构误差，需要设计有效的采样传输策略来最小化误差。

Method: 提出误差感知的联合采样传输策略：每个采样器仅在当前过程状态与其对应监视器最近重构状态不同时，以概率方式生成样本。将过程建模为二维离散时间马尔可夫链，采用时间平均重构误差作为性能指标，推导闭式表达式，并构建优化问题最小化误差同时满足平均采样成本约束。

Result: 分析表明，所提出的误差感知策略在考虑的所有方案中实现了最小的时间平均重构误差，同时有效利用了采样预算。在过程间相关性强、跟踪要求严格的场景中，性能提升尤为显著。

Conclusion: 误差感知的联合采样传输策略能够有效跟踪相关随机过程，在最小化重构误差的同时满足资源约束，特别适用于高相关性和严格跟踪要求的应用场景。

Abstract: We investigate real-time tracking of two correlated stochastic processes over a shared wireless channel. The joint evolution of the processes is modeled as a two-dimensional discrete-time Markov chain. Each process is observed by a dedicated sampler and independently reconstructed at a remote monitor according to a task-specific objective. Although both processes originate from a common underlying phenomenon (e.g., distinct features of the same source), each monitor is interested only in its corresponding feature. A reconstruction error is incurred when the true and reconstructed states mismatch at one or both monitors. To address this problem, we propose an error-aware joint sampling and transmission policy, under which each sampler probabilistically generates samples only when the current process state differs from the most recently reconstructed state at its corresponding monitor. We adopt the time-averaged reconstruction error as the primary performance metric and benchmark the proposed policy against state-of-the-art joint sampling and transmission schemes. For each policy, we derive closed-form expressions for the resulting time-averaged reconstruction error. We further formulate and solve an optimization problem that minimizes the time-averaged reconstruction error subject to an average sampling cost constraint. Analytical and numerical results demonstrate that the proposed error-aware policy achieves the minimum time-averaged reconstruction error among the considered schemes while efficiently utilizing the sampling budget. The performance gains are particularly pronounced in regimes with strong inter-process correlation and stringent tracking requirements, where frequent sampling by both samplers is necessary.

</details>


### [77] [Protecting Human Activity Signatures in Compressed IEEE 802.11 CSI Feedback](https://arxiv.org/abs/2512.18529)
*Mohamed Seif,Atsutse Kludze,Yasaman Ghasempour,H. Vincent Poor,Doru Calin,Andrea J. Goldsmith*

Main category: cs.IT

TL;DR: 提出一种符合802.11标准的差分隐私量化机制，通过随机量化Givens旋转角度来保护CSI反馈中的用户隐私，同时保持波束成形性能


<details>
  <summary>Details</summary>
Motivation: IEEE 802.11中的显式CSI反馈通过量化Givens旋转和相位角度来传输波束成形方向，但这些角度编码了传播环境的精细空间特征，可能无意中向被动窃听者泄露用户活动、身份和位置信息

Method: 引入符合标准的差分隐私量化机制，用ε-DP随机量化器替代确定性角度量化，直接应用于发射波束成形矩阵的Givens参数。该机制保留了802.11反馈结构，为角度表示提供闭式敏感度边界，并支持有原则的隐私校准

Result: 数值模拟显示，该机制在提供强大隐私保证的同时，对波束成形性能的影响最小

Conclusion: 提出的差分隐私量化机制能够有效保护CSI反馈中的用户隐私，同时保持与现有802.11标准的兼容性和良好的波束成形性能

Abstract: Explicit channel state information (CSI) feedback in IEEE~802.11 conveys \emph{transmit beamforming directions} by reporting quantized Givens rotation and phase angles that parametrize the right-singular subspace of the channel matrix. Because these angles encode fine-grained spatial signatures of the propagation environment, recent work have shown that plaintext CSI feedback can inadvertently reveal user activity, identity, and location to passive eavesdroppers. In this work, we introduce a standards-compatible \emph{differentially private (DP) quantization mechanism} that replaces deterministic angular quantization with an $\varepsilon$-DP stochastic quantizer applied directly to the Givens parameters of the transmit beamforming matrix. The mechanism preserves the 802.11 feedback structure, admits closed-form sensitivity bounds for the angular representation, and enables principled privacy calibration. Numerical simulations demonstrate strong privacy guarantees with minimal degradation in beamforming performance.

</details>


### [78] [Integrated Control and Communication in LQG Systems](https://arxiv.org/abs/2512.18535)
*Sepehr Jahangiri,H. Ali Talebi*

Main category: cs.IT

TL;DR: 研究MIMO LQG系统中通过控制信号传输信息的ICAC问题，给出了可计算的容量表达式，并证明可以在保持最优控制成本的同时实现非零速率数据传输。


<details>
  <summary>Details</summary>
Motivation: 研究在控制约束下的MIMO向量状态LQG系统中，如何通过控制信号从控制器/编码器向观测器/解码器传输消息，探索通信与控制集成的可能性。

Method: 使用半定规划方法提供可计算的容量表达式，分析在保持最优控制成本的同时传输数据的技术方案。

Result: 证明了可以在LQG系统中以非零速率传输数据，同时保持与不传输信息时相同的最优控制成本，该框架可推广到带反馈的MIMO高斯信道通信。

Conclusion: ICAC框架为通信与控制集成提供了理论基础，能够在不影响控制性能的前提下实现数据传输，并推广了带反馈的MIMO高斯信道通信理论。

Abstract: In this paper, we study the Integrated Communication and Control (ICAC) problem. Specifically, we investigate how messages can be transmitted from the controller/encoder to the observer/decoder through the control signal in Multiple-Input Multiple-Output (MIMO) vector-state Linear Quadratic Gaussian (LQG) systems under control constraints. We provide a computable capacity expression using semidefinite programming. We further show that it is possible to transmit data at a nonzero rate over an LQG system while maintaining the same optimal control cost as in the case where no information message are transmitted. Finally, we discuss how this framework generalizes communication over MIMO Gaussian channels with feedback, both with and without InterSymbol Interference (ISI).

</details>


### [79] [Embracing Beam-Squint Effects for Wideband LEO Satellite Communications: A 3D Rainbow Beamforming Approach](https://arxiv.org/abs/2512.18600)
*Juha Park,Seokho Kim,Wonjae Shin,H. Vincent Poor*

Main category: cs.IT

TL;DR: 论文提出3D彩虹波束赋形技术，利用波束倾斜效应作为资产而非损伤，通过联合相位时间阵列天线实现频率依赖的波束指向，使LEO卫星能在单个时隙服务整个覆盖区域，显著提升上行吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统LEO卫星通信中，波束跳变技术受限于时域切换机制，每个时隙只能覆盖小部分服务区域，随着用户密度增加会加剧上行吞吐量瓶颈和延迟问题。同时，宽带系统中的波束倾斜效应导致模拟波束赋形方向随子载波频率变化，可能在某些频率上造成错位，阻碍宽带卫星通信性能。

Method: 提出3D彩虹波束赋形技术，采用带有真实时间延迟的联合相位时间阵列天线，有意扩大波束倾斜角度，将频率依赖的波束引导到分布式方向。通过制定JPTA波束赋形器优化问题，并采用新颖的联合交替和基于分解的优化框架解决非凸优化问题。

Result: 数值评估显示，在现实的3D LEO卫星通信几何环境下，提出的彩虹波束赋形赋能LEO卫星通信相比传统波束跳变系统实现了高达2.8倍的上行吞吐量提升。

Conclusion: 该研究将波束倾斜从损伤转变为资产，通过3D彩虹波束赋形技术使卫星能在单个时隙服务整个覆盖区域，同时接收大量用户的上行信号，显著提升吞吐量和降低延迟，标志着6G宽带LEO卫星通信的重要突破。

Abstract: Low Earth Orbit (LEO) satellite communications (SATCOM) offers high-throughput, low-latency global connectivity to a very large number of users. To accommodate this demand with limited hardware resources, beam hopping (BH) has emerged as a prominent approach in LEO SATCOM. However, its time-domain switching mechanism confines coverage to a small fraction of the service area during each time slot, exacerbating uplink throughput bottlenecks and latency issues as the user density increases. Meanwhile, wideband systems experience the beam-squint effect, where analog beamforming (BF) directions vary with subcarrier frequencies, potentially causing misalignment at certain frequencies, thereby hindering the performance of wideband SATCOM. In this paper, we aim to shift the paradigm in wideband LEO SATCOM from beam-squint as an impairment to beam-squint as an asset. Specifically, we put forth 3D rainbow BF employing a joint phase-time array (JPTA) antenna with true time delay (TTD) to intentionally widen the beam-squint angle, steering frequency-dependent beams toward distributed directions. This novel approach enables the satellite to serve its entire coverage area in a single time slot. By doing so, the satellite simultaneously receives uplink signals from a massive number of users, significantly boosting throughput and reducing latency. To realize 3D rainbow BF, we formulate a JPTA beamformer optimization problem and address the non-convex nature of the optimization problem through a novel joint alternating and decomposition-based optimization framework. Through numerical evaluations incorporating realistic 3D LEO SATCOM geometry, our numerical results demonstrate that the proposed rainbow BF-empowered LEO SATCOM achieves up to 2.8-fold increase in uplink throughput compared to conventional BH systems. These results mark a significant breakthrough for 6G wideband LEO SATCOM.

</details>


### [80] [A Quantitative Entropy Power Inequality for Dependent Random Vectors](https://arxiv.org/abs/2512.19002)
*Mokshay Madiman,James Melbourne,Cyril Roberto*

Main category: cs.IT

TL;DR: 本文提出了依赖随机向量的定量熵功率不等式，扩展了现有理论，特别证明了对于对数超模联合密度的随机向量，基于条件熵的熵功率不等式成立。


<details>
  <summary>Details</summary>
Motivation: 熵功率不等式是信息论的基础结果，与概率论和几何泛函分析有深刻联系。现有研究已针对独立随机向量建立了该不等式，并有一些针对依赖情况的扩展。本文旨在进一步发展依赖随机向量的定量熵功率不等式。

Method: 扩展了Takano、Johnson和Rioul等人的工作，开发了依赖随机向量的定量熵功率不等式。特别关注具有对数超模联合密度的随机向量。

Result: 成功建立了依赖随机向量的定量熵功率不等式，并证明了对数超模联合密度的随机向量满足基于条件熵的熵功率不等式。

Conclusion: 本文扩展了熵功率不等式理论，为依赖随机向量提供了定量分析工具，特别在对数超模密度条件下建立了条件熵形式的熵功率不等式，深化了对信息论中依赖结构影响的理解。

Abstract: The entropy power inequality for independent random vectors is a foundational result of information theory, with deep connections to probability and geometric functional analysis. Several extensions of the entropy power inequality have been developed for settings with dependence, including by Takano, Johnson, and Rioul. We extend these works by developing a quantitative version of the entropy power inequality for dependent random vectors. A notable consequence is that an entropy power inequality stated using conditional entropies holds for random vectors whose joint density is log-supermodular.

</details>


### [81] [On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation](https://arxiv.org/abs/2512.19067)
*George Vershinin,Asaf Cohen,Omer Gurewitz*

Main category: cs.IT

TL;DR: 研究具有成本感知的顺序假设检验问题，决策者可以在执行过程中中止动作，通过设定每个动作的截止期限来截断成本，分析两种成本揭示模型下的最优策略。


<details>
  <summary>Details</summary>
Motivation: 在实际的顺序假设检验中，动作执行通常会产生随机成本，决策者需要在满足平均错误率约束的同时最小化总成本。允许决策者中止正在进行的动作（通过设定截止期限）可能提供成本优化的机会，但需要系统分析这种机制在不同成本揭示模型下的效果。

Method: 研究两种成本揭示模型：事后模型（成本在获得样本后揭示）和事前模型（成本在样本获取前累积）。分析每个动作截止期限对期望总成本的影响，推导最优策略，并比较与基准模型（确定性成本）的关系。

Result: 在事后模型中，动作截止期限不影响期望总成本，成本-错误权衡与用成本均值替换确定性成本的基准模型一致。在事前模型中，截止期限会增加动作应用次数，但通过引入有效成本可以将期望总成本降低到恒定成本设置的水平。确定了截止期限有益的条件。

Conclusion: 动作中止机制的有效性高度依赖于成本揭示的时间。事后模型中截止期限无益，而事前模型中可以通过截止期限优化成本，但需要仔细设计有效成本参数。研究为实际顺序决策系统中成本管理提供了理论指导。

Abstract: We study a variant of cost-aware sequential hypothesis testing in which a single active Decision Maker (DM) selects actions with positive, random costs to identify the true hypothesis under an average error constraint, while minimizing the expected total cost. The DM may abort an in-progress action, yielding no sample, by truncating its realized cost at a smaller, tunable deterministic limit, which we term a per-action deadline. We analyze how this cancellation option can be exploited under two cost-revelation models: ex-post, where the cost is revealed only after the sample is obtained, and ex-ante, where the cost accrues before sample acquisition.
  In the ex-post model, per-action deadlines do not affect the expected total cost, and the cost-error tradeoffs coincide with the baseline obtained by replacing deterministic costs with cost means. In the ex-ante model, we show how per-action deadlines inflate the expected number of times actions are applied, and that the resulting expected total cost can be reduced to the constant-cost setting by introducing an effective per-action cost. We characterize when deadlines are beneficial and study several families in detail.

</details>


### [82] [Low-Latency and Low-Complexity MLSE for Short-Reach Optical Interconnects](https://arxiv.org/abs/2512.19094)
*Mengqi Guo,Ji Zhou,Haide Wang,Changyuan Yu,Xiangjun Xin,Liangchuan Li*

Main category: cs.IT

TL;DR: 提出简化的分层两步最大似然序列估计(L2S-MLSE)，通过计算简化和状态减少降低复杂度，将延迟从线性阶降至对数阶，显著减少硬件资源需求。


<details>
  <summary>Details</summary>
Motivation: 为满足光互连对高速、低延迟和低复杂度的需求，需要简化MLSE算法以减少硬件实现复杂度。

Method: 提出简化的L2S-MLSE，结合计算简化和状态减少，采用并行滑动块架构降低延迟，将乘法器数量从指数阶减少到线性阶。

Result: 在112-Gbit/s PAM4传输2公里单模光纤实验中，简化L2S-MLSE显著优于FFE-only方案；延迟从34个延迟单元降至7个，乘法器从512个减至33个，加法器和比较器分别减少到37.2%和8.4%。

Conclusion: 简化的L2S-MLSE在保持BER性能的同时，显著降低了延迟和硬件复杂度，适用于高速光互连系统。

Abstract: To meet the high-speed, low-latency, and low-complexity demand for optical interconnects, simplified layered 2-step maximum likelihood sequence estimation (L2S-MLSE) is proposed in this paper. Simplified L2S-MLSE combines computational simplification and reduced state in L2S-MLSE. L2S-MLSE with a parallel sliding block architecture reduces latency from linear order to logarithmic order. Computational simplification reduces the number of multipliers from exponential order to linear order. Incorporating the reduced state with computational simplification further decreases the number of adders and comparators. The simplified L2S-MLSE is evaluated in a 112-Gbit/s PAM4 transmission over 2-km standard single-mode fiber. Experimental results show that the simplified L2S-MLSE significantly outperforms the FFE-only case in bit error ratio (BER) performance. Compared with simplified 1-step MLSE, the latency of simplified L2S-MLSE is reduced from 34 delay units in linear order to 7 delay units in logarithmic order. The simplified scheme in L2S-MLSE reduces the number of variable multipliers from 512 in exponential order to 33 in linear order without BER performance deterioration, while reducing the number of adders and comparators to 37.2% and 8.4%, respectively, with nearly identical BER performance.

</details>


### [83] [On the construction of Cauchy MDS matrices over Galois rings via nilpotent elements and Frobenius maps](https://arxiv.org/abs/2512.19306)
*Shakir Ali,Atif Ahmad Khan,Abhishek Kesarwani*

Main category: cs.IT

TL;DR: 本文提出在Galois环上构造Cauchy MDS矩阵的新方法，利用幂零元和Teichmüller集减少矩阵元素数量，并通过Frobenius自同构构造大量保持MDS性质的函数。


<details>
  <summary>Details</summary>
Motivation: 在Galois环上构造Cauchy MDS矩阵对于编码理论和密码学应用具有重要意义，但现有方法可能存在元素数量较多的问题，需要更高效的构造方法。

Method: 利用Galois环GR(p^s,p^{sm})的幂零元素和Teichmüller集来减少Cauchy MDS矩阵的条目数量，通过Frobenius自同构构造p^{(s-1)m}(p^m-1)个保持MDS性质的函数，并利用Galois环的自同构和同构生成新的Cauchy MDS矩阵。

Result: 成功构造了具有较少元素的Cauchy MDS矩阵，生成了大量保持MDS性质的函数，并证明了利用自同构和同构可以生成新的Cauchy MDS矩阵。

Conclusion: 本文提出的方法为Galois环上的Cauchy MDS矩阵构造提供了更高效的途径，通过利用环的结构特性和自同构性质，显著减少了矩阵元素数量并生成了大量新的MDS矩阵。

Abstract: Let $s,m$ be the positive integers and $p$ be any prime number. Next, let $GR(p^s,p^{sm})$ be a Galois ring of characteristic $p^s$ and cardinality $p^{sm}$. In the present paper, we explore the construction of Cauchy MDS matrices over Galois rings. Moreover, we introduce a new approach that considers nilpotent elements and Teichmüller set of Galois ring $GR(p^s,p^{sm})$ to reduce the number of entries in these matrices. Furthermore, we construct $p^{(s-1)m}(p^m-1)$ distinct functions with the help of Frobenius automorphisms. These functions preserve MDS property of matrices. Finally, we prove some results using automorphisms and isomorphisms of the Galois rings that can be used to generate new Cauchy MDS matrices.

</details>


### [84] [Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models](https://arxiv.org/abs/2512.19334)
*Haohua Chen,Songbin Liu,Junjie Ma*

Main category: cs.IT

TL;DR: 提出OAMP算法用于矩形尖峰矩阵模型中的信号估计，建立严格的状态演化分析，构造迭代最优降噪器，性能与贝叶斯最优估计器的复本对称预测一致。


<details>
  <summary>Details</summary>
Motivation: 在具有一般旋转不变噪声的矩形尖峰矩阵模型中，需要有效的信号估计算法。现有方法可能无法充分利用多个信息异常值，且缺乏对高维动力学的精确分析框架。

Method: 提出正交近似消息传递（OAMP）算法，建立严格的状态演化分析框架，构造迭代最优降噪器，在矩形设置中提出组合多个异常值的程序，适应谱初始化。

Result: 算法性能与贝叶斯最优估计器的复本对称预测一致，推测在广泛的迭代估计方法类中具有统计最优性，能有效处理一般旋转不变噪声模型。

Conclusion: 提出的OAMP算法为矩形尖峰矩阵模型提供了有效的信号估计框架，具有严格的理论保证和接近最优的性能，适用于一般旋转不变噪声环境。

Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.

</details>


### [85] [Enhancing PLS of Indoor IRS-VLC Systems for Colluding and Non-Colluding Eavesdroppers](https://arxiv.org/abs/2512.19339)
*Rashid Iqbal,Ahmed Zoha,Salama Ikki,Muhammad Ali Imran,Hanaa Abumarshoud*

Main category: cs.IT

TL;DR: 该论文研究了IRS辅助室内可见光通信中考虑实际时间延迟的物理层安全问题，使用深度强化学习优化IRS分配以最大化保密容量。


<details>
  <summary>Details</summary>
Motivation: 现有IRS辅助室内VLC研究通常忽略反射路径引入的时间延迟，但这些延迟在实际宽带系统中是固有的。本文采用更现实的IRS诱导时间延迟假设来增强物理层安全。

Method: 考虑室内VLC系统，利用IRS塑造信道使反射信号在合法用户处建设性叠加，在窃听者处产生符号间干扰。将保密容量最大化问题建模为复杂组合优化问题，采用近端策略优化的深度强化学习算法求解。

Result: 在窃听者信道比合法用户更强的极端情况下，相比将所有IRS元素分配给合法用户的基准方案，提出的PPO-based IRS分配在共谋和非共谋窃听场景下分别提升保密容量107%和235%。

Conclusion: 基于时间延迟的IRS控制在实际室内VLC场景中能提供显著的保密优势，特别是在窃听者信道条件更优的极端情况下。

Abstract: Most intelligent reflecting surface (IRS)-aided indoor visible light communication (VLC) studies ignore the time delays introduced by reflected paths, even though these delays are inherent in practical wideband systems. In this work, we adopt a realistic assumption of IRS-induced time delay for physical layer security (PLS) enhancement. We consider an indoor VLC system where an IRS is used to shape the channel so that the reflected signals add constructively at the legitimate user and create intersymbol interference at eavesdroppers located inside the coverage area. The resulting secrecy capacity maximisation over the IRS element allocation is formulated as a complex combinatorial optimisation problem and is solved using deep reinforcement learning with proximal policy optimisation (PPO). The approach is evaluated for both colluding eavesdroppers, which combine their received signals, and non-colluding eavesdroppers, which act independently. Simulation results are shown for various simulation setups, which demonstrate significant secrecy capacity gains. In a worst-case scenario, where the eavesdroppers have stronger channels than the legitimate user, the proposed PPO-based IRS allocation improves secrecy capacity by 107\% and 235\% in the colluding and non-colluding cases, respectively, compared with allocating all IRS elements to the legitimate user. These results demonstrate that time-delay-based IRS control can provide a strong secrecy advantage in practical indoor VLC scenarios.

</details>


### [86] [Fully Asynchronous Unsourced Random Access over Fading Channels](https://arxiv.org/abs/2512.19468)
*Mert Ozates,Mohammad Kazemi,Gianluigi Liva,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出一种用于完全异步无源随机接入的方案，使用导频序列和极性码，采用双滑动窗口解码器，在异步场景下性能接近同步基准


<details>
  <summary>Details</summary>
Motivation: 研究完全异步设置下的无源随机接入问题，其中活跃用户可以在任意时间开始传输，这在实际应用中更实用但更具挑战性

Method: 传输信号包含导频序列和极性码，极性码以开关模式分布在数据包中；接收端使用双滑动窗口解码器，内窗口进行联合定时和导频检测、信道估计、单用户解码和连续干扰消除，外窗口增强干扰消除

Result: 数值结果表明，所提方案与同步基准相比仅有轻微性能损失，同时更具实际应用价值

Conclusion: 提出的完全异步无源随机接入方案在实际应用中可行，性能接近同步系统，解决了异步传输的挑战

Abstract: We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice.

</details>
