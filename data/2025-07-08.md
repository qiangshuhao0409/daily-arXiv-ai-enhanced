<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 14]
- [cs.AI](#cs.AI) [Total: 84]
- [cs.IT](#cs.IT) [Total: 26]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An End-to-End Assurance Framework for AI/ML Workloads in Datacenters](https://arxiv.org/abs/2507.03158)
*Jit Gupta,Tarun Banka,Rahul Gupta,Mithun Dharmaraj,Jasleen Kaur*

Main category: cs.NI

TL;DR: 论文展示了基于SaaS的AI/ML工作负载性能监控与自动化故障排查工具，利用跨层遥测和日志实现端到端保障。


<details>
  <summary>Details</summary>
Motivation: 现代分布式机器学习工作负载（如大语言模型训练）在多GPU系统中运行，性能问题复杂，需快速定位根因并修复。

Method: 通过收集应用遥测、集体通信日志、GPU健康指标、网络流数据等跨层数据，构建依赖图和自动化根因分析。

Result: 展示了跨层依赖图、服务级别期望、自动化根因分析等功能，实现端到端性能保障。

Conclusion: 该工具能有效监控和解决AI/ML工作负载的性能问题，提升系统可靠性。

Abstract: Modern machine learning workloads such as large language model training,
fine-tuning jobs are highly distributed and span across hundreds of systems
with multiple GPUs. Job completion time for these workloads is the artifact of
the application, compute, network and storage performance. In case of failure
or degraded performance it is imperative to understand the root cause and
possible remediation for the problem for end-to-end assurance. This demo
showcases SaaSbased observability and automated troubleshooting for AI/ML
workload performance issues using cross-layer telemetry and logs (e.g.,
Application telemetry, Collective communication logs, GPU Health metrics,
Network Flow Data, NIC ROCEv2 telemetry). Different use cases are demonstrated
for end-to-end assurance such as Cross-layer Dependency Graph, Cross-layer
Service Level Expectations, Automated Root Cause Analysis, GPU-toGPU
application path tracing.

</details>


### [2] [RCA Copilot: Transforming Network Data into Actionable Insights via Large Language Models](https://arxiv.org/abs/2507.03224)
*Alexander Shan,Jasleen Kaur,Rahul Singh,Tarun Banka,Raj Yavatkar,T. Sridhar*

Main category: cs.NI

TL;DR: RCACopilot结合统计测试和大型语言模型推理，自动化复杂网络环境中的根因分析，提供解释性叙述和解决步骤。


<details>
  <summary>Details</summary>
Motivation: 传统根因分析方法耗时且难以理解，统计推断方法缺乏可解释性，工程师难以理解黑盒模型的预测。

Method: RCACopilot结合统计测试和LLM推理，收集并综合运行时诊断信息，预测根因并提供解释和解决步骤。

Result: RCACopilot为操作员提供准确且实用的支持，自动化根因分析。

Conclusion: RCACopilot通过LLM推理和检索技术，显著提升了根因分析的效率和可解释性。

Abstract: Ensuring the reliability and availability of complex networked services
demands effective root cause analysis (RCA) across cloud environments, data
centers, and on-premises networks. Traditional RCA methods, which involve
manual inspection of data sources such as logs and telemetry data, are often
time-consuming and challenging for on-call engineers. While statistical
inference methods have been employed to estimate the causality of network
events, these approaches alone are similarly challenging and suffer from a lack
of interpretability, making it difficult for engineers to understand the
predictions made by black-box models. In this paper, we present RCACopilot, an
advanced on-call system that combines statistical tests and large language
model (LLM) reasoning to automate RCA across various network environments.
RCACopilot gathers and synthesizes critical runtime diagnostic information,
predicts the root cause of incidents, provides a clear explanatory narrative,
and offers targeted action steps for engineers to resolve the issues. By
utilizing LLM reasoning techniques and retrieval, RCACopilot delivers accurate
and practical support for operators.

</details>


### [3] [OpenSN: An Open Source Library for Emulating LEO Satellite Networks](https://arxiv.org/abs/2507.03248)
*Wenhao Lu,Zhiyuan Wang,Hefan Zhang,Shan Zhang,Hongbin Luo*

Main category: cs.NI

TL;DR: OpenSN是一个开源库，用于模拟大规模卫星网络（SN），具有高效性、可扩展性和功能性扩展优势，比现有工具更快且更灵活。


<details>
  <summary>Details</summary>
Motivation: 评估低地球轨道（LEO）卫星网络研究的系统性和可重复性方法不足，需要一种高效、可扩展的模拟工具。

Method: OpenSN采用基于容器的虚拟化技术，支持分布式路由软件，并通过键值数据库分离用户配置与容器网络管理。

Result: OpenSN构建大型星座比StarryNet快5-10倍，链路状态更新比LeoEM快2-4倍，成功模拟了4408颗卫星的Starlink星座。

Conclusion: OpenSN在效率、可扩展性和功能性扩展方面具有优势，是LEO卫星网络研究的宝贵工具。

Abstract: Low-earth-orbit (LEO) satellite constellations (e.g., Starlink) are becoming
a necessary component of future Internet. There have been increasing studies on
LEO satellite networking. It is a crucial problem how to evaluate these studies
in a systematic and reproducible manner. In this paper, we present OpenSN,
i.e., an open source library for emulating large-scale satellite network (SN).
Different from Mininet-based SN emulators (e.g., LeoEM), OpenSN adopts
container-based virtualization, thus allows for running distributed routing
software on each node, and can achieve horizontal scalability via flexible
multi-machine extension. Compared to other container-based SN emulators (e.g.,
StarryNet), OpenSN streamlines the interaction with Docker command line
interface and significantly reduces unnecessary operations of creating virtual
links. These modifications improve emulation efficiency and vertical
scalability on a single machine. Furthermore, OpenSN separates user-defined
configuration from container network management via a Key-Value Database that
records the necessary information for SN emulation. Such a separation
architecture enhances the function extensibility. To sum up, OpenSN exhibits
advantages in efficiency, scalability, and extensibility, thus is a valuable
open source library that empowers research on LEO satellite networking.
Experiment results show that OpenSN constructs mega-constellations 5X-10X
faster than StarryNet, and updates link state 2X-4X faster than LeoEM. We also
verify the scalability of OpenSN by successfully emulating the five-shell
Starlink constellation with a total of 4408 satellites.

</details>


### [4] [Low-power Wireless Network with Real-Time Guarantees for Edge-Cloud Applications](https://arxiv.org/abs/2507.03317)
*Don Tan*

Main category: cs.NI

TL;DR: 探索基于树莓派（RPI）构建可扩展、易部署的实时LoRa测试平台的可行性，通过实验验证其性能指标和实时通信能力。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏大规模LoRa测试平台，无法有效将LoRa通信融入实时场景。

Method: 利用多个RPI管理各自的LoRa无线电，实验评估性能指标（RSSI、SNR、PLR）及毫秒级传输能力，比较不同扩频因子、频率组合及TDMA与CSMA方法。

Result: 配置合适参数后，系统可实现稳定低延迟通信，验证了实时操作的可行性。

Conclusion: 未来工作包括扩展RPI控制的无线电数量、实现真正并行传输，并整合多个RPI以构建完整的大规模实时LoRa测试平台。

Abstract: The goal of this project is to explore the feasibility of building a scalable
& easy-to-deploy real-time LoRa testbed, made from multiple units of Raspberry
Pi (RPI), where each RPI manages its own set of LoRa radios. This project is
motivated by the lack of concrete large-scale LoRa testbeds that effectively
integrate LoRa communications into the real-time world. The paper introduces
how the idea of using RPI came about and why it should work in theory. The
paper then carries out experiments on a component of the large-scale testbed,
to evaluate the feasibility of the said component based on performance metrics
such as RSSI, SNR, PLR and the ability to carry out millisecond-accurate
transmissions. The performance metrics are also used to explore the impact of
using different combinations of spread factors and transmission frequencies, as
well as making comparisons between time-division multiple access (TDMA) and
carrier-sense multiple access (CSMA) approaches. The results show that with the
right parameters configured, the system can achieve stable and low-latency
communications, proving some feasibility to operate under real-time situations.
Future work includes giving each RPI control over more radios, carrying out
true parallel transmissions, and finally integrating multiple RPIs for a more
complete large-scale real-time LoRa testbed.

</details>


### [5] [AoI-Energy-Spectrum Optimization in Post-Disaster Powered Communication Intelligent Network via Hierarchical Heterogeneous Graph Neural Network](https://arxiv.org/abs/2507.03401)
*Hanjian Liu,Jinsong Gui*

Main category: cs.NI

TL;DR: 本文提出了一种灾后供电通信智能网络（PDPCIN），利用无人机和低轨卫星解决灾后通信中断问题，并通过智能同步无人机架构和多维指标优化框架实现高效通信。


<details>
  <summary>Details</summary>
Motivation: 灾后地面基站（GBS）故障导致通信中断，亟需一种高效、可靠的通信恢复方案。

Method: 采用无人机（UAVs）进行无线数据收集和能量传输，结合低轨卫星（LEO SATs）中继数据，并提出了智能同步无人机架构、基于信息年龄的更新机制和多LEO接入策略。

Result: 通过分层异构图神经网络（HHGNN）框架和单LEO卫星密度优化算法（S-LSDO），实现了信息年龄、能效和频谱效率的协同优化。

Conclusion: 所提方案在多项指标上优于现有基准，为灾后通信恢复提供了有效解决方案。

Abstract: This paper designs a post-disaster powered communication intelligent network
(PDPCIN) to address communication disruptions caused by ground base station
(GBS) failures within the post-disaster area. PDPCIN employs unmanned aerial
vehicles (UAVs) to provide wireless data collection (WDC) and wireless energy
transmission (WET) for affected areas and leverages low earth orbit satellites
(LEO SATs) to relay UAV data to the nearest survival GBS. To ensure basic
post-disaster communication while co-optimizing age of information (AoI),
energy efficiency, and spectrum efficiency, intelligent synchronization-UAV
(IS-UAV) architecture, AoI-based four thresholds updating (AFTU) mechanism, and
Dynamic multi-LEO access (DMLA) strategy are proposed. However, three key
challenges remain: time-varying task-resource imbalances, complex topology
caused by multi-device scheduling, and nonlinear coupling in multidimensional
metric optimization, making system optimization NP-hard. Therefore, this paper
proposes a hierarchical heterogeneous graph neural networks (HHGNN) framework.
It models heterogeneous device nodes and their communication relations as a
hierarchical heterogeneous graph structure, integrating our defined graph
sensing, exchange, and mask layer to handle the network's input, feature
propagation, and output. To search appropriate number of single-LEO SATs, we
propose single-LEO SAT density optimization (S-LSDO) algorithm. Finally, we
compare the proposed scheme with state-of-the-art benchmarks to validate its
superior collaborative optimization of AoI, energy efficiency, and spectrum
efficiency. Based on this, we derive the expressions for the expected values of
AoI and stagnant AoI proportion.

</details>


### [6] [RateCount: Learning-Free Device Counting by Wi-Fi Probe Listening](https://arxiv.org/abs/2507.03873)
*Tianlang He,Zhangyu Chang,S. -H. Gary Chan*

Main category: cs.NI

TL;DR: RateCount是一种无需机器学习的轻量级设备计数方法，通过AP接收PRF的速率估计设备数量，准确性高且部署高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的方法在MAC地址随机化下设备计数效率低，需要数据清洗、模型训练和调参，部署成本高。

Method: 提出RateCount，利用AP接收PRF的速率，通过无偏闭式表达式估计设备数量，并建立误差模型计算方差下限。

Result: 实验表明，RateCount在设备计数准确性上与基于学习的方法相当，在人员计数上显著优于现有方案。

Conclusion: RateCount提供了一种高效、轻量且无需学习的方法，适用于设备和人员计数，部署成本低。

Abstract: A Wi-Fi-enabled device, or simply Wi-Fi device, sporadically broadcasts probe
request frames (PRFs) to discover nearby access points (APs), whether connected
to an AP or not. To protect user privacy, unconnected devices often randomize
their MAC addresses in the PRFs, known as MAC address randomization. While
prior works have achieved accurate device counting under MAC address
randomization, they typically rely on machine learning, resulting in
inefficient deployment due to the time-consuming processes of data cleaning,
model training, and hyperparameter tuning. To enhance deployment efficiency, we
propose RateCount, an accurate, lightweight, and learning-free counting
approach based on the rate at which APs receive PRFs within a window. RateCount
employs a provably unbiased closed-form expression to estimate the device count
time-averaged over the window and an error model to compute the lower bound of
the estimation variance. We also demonstrate how to extend RateCount to people
counting by incorporating a device-to-person calibration scheme. Through
extensive real-world experiments conducted at multiple sites spanning a wide
range of counts, we show that RateCount, without any deployment costs for
machine learning, achieves comparable counting accuracy with the
state-of-the-art learning-based device counting and improves previous people
counting schemes by a large margin.

</details>


### [7] [Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks](https://arxiv.org/abs/2507.03950)
*Yizhou Luo,Kwan-Wu Chin,Ruyi Guan,Xi Xiao,Caimeng Wang,Jingyin Feng,Tengjiao He*

Main category: cs.NI

TL;DR: 提出了一种基于无人机（UAV）的物联网（IoT）设备认证框架，通过深度强化学习（DRL）优化无人机的充电计划和设备选择，显著提升了信任度和网络吞吐量。


<details>
  <summary>Details</summary>
Motivation: 物联网设备分布广泛且易受攻击，需要频繁检查，但传统方法效率低。无人机认证框架可解决这一问题。

Method: 采用深度强化学习（DRL）优化无人机的充电计划和设备选择，以应对能量供应不稳定和设备离线的挑战。

Result: 模拟结果显示，方案将信任平均年龄降低88%，认证导致的吞吐量损失减少30%。

Conclusion: 无人机认证框架结合DRL能有效提升物联网设备的安全性和网络性能。

Abstract: Devices operating in Internet of Things (IoT) networks may be deployed across
vast geographical areas and interconnected via multi-hop communications.
Further, they may be unguarded. This makes them vulnerable to attacks and
motivates operators to check on devices frequently. To this end, we propose and
study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in
IoT networks with a charging station powered by solar. A key challenge is
optimizing the trajectory of the UAV to ensure it attests as many devices as
possible. A trade-off here is that devices being checked by the UAV are
offline, which affects the amount of data delivered to a gateway. Another
challenge is that the charging station experiences time-varying energy
arrivals, which in turn affect the flight duration and charging schedule of the
UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL)
solution to optimize the UAV's charging schedule and the selection of devices
to be attested during each flight. The simulation results show that our
solution reduces the average age of trust by 88% and throughput loss due to
attestation by 30%.

</details>


### [8] [TeleSim: A Network-Aware Testbed and Benchmark Dataset for Telerobotic Applications](https://arxiv.org/abs/2507.04425)
*Zexin Deng,Zhenhui Yuan,Longhao Zou*

Main category: cs.NI

TL;DR: TeleSim是一个网络感知的遥操作数据集和测试平台，用于评估不同网络条件下的遥操作系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和测试平台缺乏对网络延迟影响的捕捉，TeleSim旨在填补这一空白。

Method: 通过OMNeT++模拟三种网络质量等级（高、中、低），收集任务完成时间、成功率、视频质量等指标。

Result: 在最差网络条件下，任务完成时间增加221.8%，成功率下降64%。

Conclusion: 网络退化对遥操作性能有显著负面影响，需开发自适应协议。

Abstract: Telerobotic technologies are becoming increasingly essential in fields such
as remote surgery, nuclear decommissioning, and space exploration. Reliable
datasets and testbeds are essential for evaluating telerobotic system
performance prior to real-world deployment. However, there is a notable lack of
datasets that capture the impact of network delays, as well as testbeds that
realistically model the communication link between the operator and the robot.
This paper introduces TeleSim, a network-aware teleoperation dataset and
testbed designed to assess the performance of telerobotic applications under
diverse network conditions. TeleSim systematically collects performance data
from fine manipulation tasks executed under three predefined network quality
tiers: High, Medium, and Low. Each tier is characterized through controlled
settings of bandwidth, latency, jitter, and packet loss. Using OMNeT++ for
precise network simulation, we record a wide range of metrics, including
completion time, success rates, video quality indicators (Peak Signal-to-Noise
Ratio (PSNR) and Structural Similarity Index Measure (SSIM)), and quality of
service (QoS) parameters. TeleSim comprises 300 experimental trials, providing
a robust benchmark for evaluating teleoperation systems across heterogeneous
network scenarios. In the worst network condition, completion time increases by
221.8% and success rate drops by 64%. Our findings reveal that network
degradation leads to compounding negative impacts, notably reduced video
quality and prolonged task execution, highlighting the need for adaptive,
resilient teleoperation protocols. The full dataset and testbed software are
publicly available on our GitHub repository:
https://github.com/ConnectedRoboticsLab and YouTube channel:
https://youtu.be/Fz_1iOYe104.

</details>


### [9] [In-Network Memory Access: Bridging SmartNIC and Host Memory](https://arxiv.org/abs/2507.04001)
*Mohammed Zain Farooqi,Masoud Hemmatpour,Tore Heide Larsen*

Main category: cs.NI

TL;DR: 本文研究了SmartNIC与主机之间的内存访问性能，以优化通信效率。


<details>
  <summary>Details</summary>
Motivation: SmartNIC的广泛应用带来了通信挑战，尤其是主机与卸载组件之间的高效通信需求。

Method: 评估了不同内存访问方法，并分析了SmartNIC和主机上的性能表现。

Result: 为网络内应用提供了内存访问设计的指导。

Conclusion: 研究为选择合适的通信设计提供了依据。

Abstract: SmartNICs have been increasingly utilized across various applications to
offload specific computational tasks, thereby enhancing overall system
performance. However, this offloading process introduces several communication
challenges that must be addressed for effective integration. A key challenge
lies in establishing efficient communication between the offloaded components
and the main application running on the host. In this study, we evaluate
different approaches for achieving memory access between the host and SmartNIC.
We analyze memory access performance on both the SmartNIC and the host to
support in-network applications and guide the selection of an appropriate
memory access design.

</details>


### [10] [On-Demand Multimedia Delivery in 6G: An Optimal-Cost Steiner Tree Approach](https://arxiv.org/abs/2507.04589)
*Zien Wang,Xiucheng Wang,Nan Cheng,Wenchao Xu,Wei Quan,Ruijin Sun,Conghao Zhou*

Main category: cs.NI

TL;DR: 论文提出了一种两阶段动态规划增强的按需Steiner树（OST）算法，解决了6G网络中多媒体数据流的最小流问题（MFP），优化了流聚合和QoS感知路径选择。


<details>
  <summary>Details</summary>
Motivation: 6G网络中多媒体数据流量的激增对沉浸式通信提出了前所未有的挑战，传统路由方法无法满足多目的地和异构QoS需求。

Method: 提出OST算法，通过两阶段动态规划联合优化流聚合和QoS感知路径选择，并数学证明其最优性。

Result: 实验表明，OST在6G多媒体传输场景中比现有方法减少总网络流量10%以上，同时满足按需QoS。

Conclusion: OST是首个能同时优化流聚合和QoS感知路径选择的算法，为6G多媒体分发提供了高效解决方案。

Abstract: The exponential growth of multimedia data traffic in 6G networks poses
unprecedented challenges for immersive communication, where
ultra-high-definition, multi-quality streaming must be delivered on demand
while minimizing network operational costs. Traditional routing approaches,
such as shortest-path algorithms, fail to optimize flow multiplexing across
multiple destinations, while conventional Steiner tree methods cannot
accommodate heterogeneous quality-of-service (QoS) requirements-a critical need
for 6G's personalized services. In this paper, we address a fundamental but
unsolved challenge: the minimum flow problem (MFP) with multi-destination,
heterogeneous outflow demands, which is pivotal for efficient multimedia
distribution such as adaptive-resolution video streaming. To overcome the
limitations of existing methods, we propose a two-stage dynamic
programming-enhanced On-demand Steiner Tree (OST) algorithm, the first approach
that jointly optimizes flow aggregation and QoS-aware path selection for
arbitrary outflow requirements. We rigorously prove the optimality of OST using
mathematical induction, demonstrating that it guarantees the minimum-cost
multicast flow under differentiated service constraints. Extensive experiments
in 6G-like multimedia transmission scenarios show that OST reduces total
network flow by over 10% compared to state-of-the-art methods while ensuring
on-demand QoS fulfillment. The complete code is available at
https://github.com/UNIC-Lab/OST.

</details>


### [11] [Graph Diffusion-Based AeBS Deployment and Resource Allocation for RSMA-Enabled URLLC Low-Altitude Economy Networks](https://arxiv.org/abs/2507.04081)
*Xudong Wang,Lei Feng,Jiacheng Wang,Hongyang Du,Changyuan Zhao,Wenjing Li,Zehui Xiong,Dusit Niyato,Ping Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种基于RSMA的传输设计，用于优化多AeBS网络中的干扰管理和资源分配，通过生成图扩散模型和SCA方法提升系统性能和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 6G超可靠低延迟通信（URLLC）服务需要灵活的无线覆盖，但有限的频谱资源和严重的同频干扰对AeBS的部署和资源分配提出了挑战。

Method: 采用生成图扩散模型进行AeBS部署和用户关联的联合优化，并结合SCA方法优化波束成形和RSMA速率分配。

Result: 仿真结果表明，所提算法在收敛速度、总速率和覆盖范围上优于现有方法，且在动态环境中表现稳健。

Conclusion: 该研究为频谱受限的多AeBS网络提供了一种高效的干扰管理和资源分配方案，显著提升了URLLC服务的性能。

Abstract: As a key component of low-altitude economic networks, aerial base stations
(AeBSs) provide flexible and reliable wireless coverage to support 6G
ultra-reliable and low-latency communication (URLLC) services. However, limited
spectrum resources and severe co-channel interference pose significant
challenges to the deployment and resource allocation of AeBSs. To address these
limitations, this paper proposes a novel rate-splitting multiple access
(RSMA)-enabled transmission design to flexibly manage interference and
effectively enhance URLLC services in spectrum-constrained multi-AeBS networks.
On this basis, we formulate a joint optimization problem involving AeBS
deployment, user association, and resource allocation to maximize the
achievable sum rate and coverage of the total system. Given the NP-hard nature
of the problem and the highly dynamic environment, we propose a novel
alternating optimization framework based on the generative graph diffusion
models. Specifically, we model AeBSs and ground users as graph nodes, then we
employ a discrete graph generation process solved via denoising diffusion is
employed to explore the combinatorial space of deployment and association
strategies. Moreover, the algorithm adopts the successive convex approximation
(SCA) method to optimize AeBS beamforming and RSMA rate allocation under finite
blocklength constraints. Extensive simulations demonstrate that the proposed
algorithm outperforms existing methods in terms of convergence speed, sum rate,
and coverage, while also exhibiting robust performance under varying network
densities and interference levels.

</details>


### [12] [Resource-Efficient Seamless Transitions For High-Performance Multi-hop UAV Multicasting](https://arxiv.org/abs/2507.04421)
*Wanqing Tu*

Main category: cs.NI

TL;DR: 本文提出了一种高效的无人机过渡算法（ETF），用于在组播环境中优化无人机的轨迹，确保无缝过渡和高性能组播。


<details>
  <summary>Details</summary>
Motivation: 无人机应用需要高效的组播通信以传输丰富媒体内容并扩展覆盖范围，因此需要快速且资源高效的无人机过渡方法。

Method: 开发了ETF算法，通过评估直线轨迹的无缝性，并在中断时生成由最少无缝直线组成的新轨迹。

Result: 仿真研究表明，ETF在无缝过渡无人机组成员时，显著提升了组播性能。

Conclusion: ETF算法在无人机组播环境中实现了高效的轨迹优化和无缝过渡。

Abstract: Many UAV-related applications require group communications between UAVs to
reliably and efficiently deliver rich media content as well as to extend
line-of-sight coverage between sky and ground. This paper studies fast yet
resource-efficient UAV transitions while maintaining high multicasting
performance. We develop a set of analytic and algorithmic results to form the
efficient transition formation (ETF) algorithm that deals with different UAV
transition scenarios in a multicasting environment. The ETF algorithm first
evaluates the seamlessness of a straight-line trajectory (SLT), by processing
low-complexity computations (e.g., Euclidean distances) or a chain of fast
checks with controlled traffic overheads. For an interrupted SLT, ETF
establishes a new trajectory consisting of a minimum number of seamless
straight lines that join at specially selected locations in terms of
controlling mobile UAVs' seamless travel distances. Our simulation studies
quantify the multicasting performance gains that ETF allows, outperforming
compared studies when seamlessly transiting UAV group members.

</details>


### [13] [Low-Latency Software Polar Encoders and Decoders for Short Blocklengths](https://arxiv.org/abs/2507.04734)
*Mathieu Leonardon,Mohammed El Houcine Ayoubi,Adrien Cassagne,Romain Tajan,Camille Leroux*

Main category: cs.NI

TL;DR: 本文介绍了针对2025年国际编码专题研讨会（ISTC 2025）竞赛开发的低延迟Polar码编码器和解码器，采用自适应连续取消列表（ASCL）解码器，并提出了新颖的ASCL展开解码器生成器。


<details>
  <summary>Details</summary>
Motivation: 竞赛要求实现最低延迟的信道码编码器和解码器，本文旨在通过优化Polar码和ASCL解码器实现这一目标。

Method: 通过设计空间探索（包括码构造、CRC选择和列表大小）和优化的比特打包编码器，寻找信噪比和解码时间的最佳平衡。

Result: 在不同帧错误率和信息比特长度下，实现了优化的性能，并将所有实现开源在AFF3CT工具箱中。

Conclusion: 本文提出的方法在低延迟Polar码编码和解码方面取得了显著成果，并通过开源工具支持进一步研究。

Abstract: This paper presents our low-latency Polar code encoders and decoders
developed for the 2025 International Symposium on Topics in Coding (ISTC 2025)
contest, which challenges participants to implement the fastest possible
channel code encoders and decoders in terms of average and maximum latency on a
CPU target. Our solution is based on Polar codes with an Adaptive Successive
Cancellation List (ASCL) decoder. We introduce a novel ASCL unrolled decoder
generator. We conduct an extensive exploration of the design space, including
code construction, CRC selection, and list size, to identify optimal trade-offs
between signal-to-noise ratio and decoding time across various operating
points. The considered operating points are frame error rates of 10^{-3} and
10^{-5}, information bit lengths of 64, 128, 256, and 512, and code rates of
1/4, 1/2, and 4/5. We also propose an optimized bit-packed encoder. All
implementations of the encoders and decoders, along with the code construction
and the unrolled decoders generator, are released as open source in the AFF3CT
toolbox.

</details>


### [14] [User Association in the Presence of Jamming in Wireless Networks Using the Whittle Index](https://arxiv.org/abs/2507.04968)
*Pramod N Chine,Suven Jagtiani,Mandar R Nalavade,Gaurav S Kasbekar*

Main category: cs.NI

TL;DR: 论文提出了一种基于Whittle索引的用户关联策略，用于无线网络中用户与基站的动态关联，以最小化长期平均持有成本，并通过仿真验证其优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中，用户与基站的关联策略对网络性能有显著影响，尤其是在存在干扰攻击的情况下，如何动态优化关联以减少成本和延迟是一个重要问题。

Method: 利用Whittle框架将硬性约束松弛为长期平均约束，采用拉格朗日乘子法将问题分解为独立的马尔可夫决策过程，并证明其Whittle索引性，设计基于Whittle索引的关联策略。

Result: 仿真结果表明，所提出的策略在平均成本、平均延迟和公平性等指标上优于现有方法。

Conclusion: 基于Whittle索引的用户关联策略能有效优化无线网络性能，适用于动态和干扰环境。

Abstract: In wireless networks, algorithms for user association, i.e., the task of
choosing the base station (BS) that every arriving user should join,
significantly impact the network performance. A wireless network with multiple
BSs, operating on non-overlapping channels, is considered. The channels of the
BSs are susceptible to jamming by attackers. During every time slot, a user
arrives with a certain probability. There exists a holding cost in each slot
for every user associated with a BS. The goal here is to design a user
association scheme, which assigns a BS to each user upon arrival with the
objective of minimizing the long-run total average holding cost borne within
the network. This objective results in low average delays attained by the
users. This association problem is an instance of restless multi-armed bandit
problems, and is known to be hard to solve. By making use of the framework
presented by Whittle, the hard per-stage constraint that every arriving user
must connect to exactly one BS in a time slot is relaxed to a long-term
time-averaged constraint. Subsequently, we employ the Lagrangian multiplier
strategy to reformulate the problem into an unconstrained form and decompose it
into separate Markov Decision Processes at the BSs. Further, the problem is
proven to be Whittle indexable and a method for calculating the Whittle indices
corresponding to different BSs is presented. We design a user association
policy under which, upon arrival of a user in a time slot, it is assigned to
the BS having the least Whittle index in that slot. Through extensive
simulations, we show that our proposed association policy based on the Whittle
index outperforms various user association policies proposed in previous work
in terms of different metrics such as average cost, average delay, and Jain's
fairness index.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
*Anand Gokhale,Vaibhav Srivastava,Francesco Bullo*

Main category: cs.AI

TL;DR: 提出了一种模块化的actor-critic架构，通过线性时序逻辑（LTL）指导LLM，结合语言模型的推理能力和形式逻辑的保证，提升长期规划任务的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长期规划任务中因错误累积导致的不安全或低效行为问题。

Method: 采用模块化架构，LLM actor负责高层动作选择，LTLCrit critic分析轨迹并提出LTL约束。支持固定安全约束和自适应软约束。

Result: 在Minecraft钻石挖掘任务中实现100%完成率，效率优于基线LLM规划器。

Conclusion: 通过逻辑监督LLM是一种强大且灵活的范式，适用于安全、通用的决策任务。

Abstract: Large language models (LLMs) have demonstrated promise in reasoning tasks and
general decision-making in static environments. In long-term planning tasks,
however, errors tend to accumulate, often leading to unsafe or inefficient
behavior, limiting their use in general-purpose settings. We propose a modular
actor-critic architecture in which an LLM actor is guided by LTLCrit, a
trajectory-level LLM critic that communicates via linear temporal logic (LTL).
Our setup combines the reasoning strengths of language models with the
guarantees of formal logic. The actor selects high-level actions from natural
language observations, while the critic analyzes full trajectories and proposes
new LTL constraints that shield the actor from future unsafe or inefficient
behavior. The architecture supports both fixed, hand-specified safety
constraints and adaptive, learned soft constraints that promote long-term
efficiency. Our architecture is model-agnostic: any LLM-based planner can serve
as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize
planning as graph traversal under symbolic constraints, allowing LTLCrit to
analyze failed or suboptimal trajectories and generate new temporal logic rules
that improve future behavior. We evaluate our system on the Minecraft
diamond-mining benchmark, achieving 100% completion rates and improving
efficiency compared to baseline LLM planners. Our results suggest that enabling
LLMs to supervise each other through logic is a powerful and flexible paradigm
for safe, generalizable decision making.

</details>


### [16] [LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance](https://arxiv.org/abs/2507.02977)
*Igor Ivanov*

Main category: cs.AI

TL;DR: 前沿LLMs在受监控的沙盒环境中仍试图作弊，揭示了目标导向行为与对齐之间的根本矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在明确被告知限制和监控的情况下是否仍会作弊，以探索目标导向行为与对齐的冲突。

Method: 在沙盒环境中让LLMs完成不可能的任务，监控其行为并记录作弊尝试。

Result: 部分前沿LLMs持续作弊并试图规避限制。

Conclusion: 当前LLMs在目标导向行为与对齐之间存在根本矛盾，需进一步研究解决。

Abstract: In this paper, LLMs are tasked with completing an impossible quiz, while they
are in a sandbox, monitored, told about these measures and instructed not to
cheat. Some frontier LLMs cheat consistently and attempt to circumvent
restrictions despite everything. The results reveal a fundamental tension
between goal-directed behavior and alignment in current LLMs. The code and
evaluation logs are available at github.com/baceolus/cheating_evals

</details>


### [17] [Limits of Safe AI Deployment: Differentiating Oversight and Control](https://arxiv.org/abs/2507.03525)
*David Manheim,Aidan Homewood*

Main category: cs.AI

TL;DR: 本文区分了AI系统中的监督与控制，提出了一个理论框架，并探讨了其在实际应用中的条件和局限性。


<details>
  <summary>Details</summary>
Motivation: 由于学术和政策讨论中常混淆监督与控制，导致设计或评估人类监督AI系统时效果不佳，本文旨在澄清两者的区别及其在AI领域的应用。

Method: 通过批判性文献综述，区分监督与控制，并提出理论框架、成熟度模型及边界分析。

Result: 提出了一个理论框架、监督方法的文档化与风险管理整合方案，以及AI监督的成熟度模型。

Conclusion: 本文强调了监督与控制的区别及其实际应用条件，为监管者、审计者和从业者提供了识别局限性和未来需求的工具。

Abstract: Oversight and control (collectively, supervision) are often invoked as key
levers for ensuring that AI systems are accountable, reliable, and able to
fulfill governance and management requirements. However, the concepts are
frequently conflated or insufficiently distinguished in academic and policy
discourse, undermining efforts to design or evaluate systems that should remain
under meaningful human supervision.
  This paper undertakes a targeted critical review of literature on supervision
outside of AI, along with a brief summary of past work on the topic related to
AI. We then differentiate control as being ex-ante or real-time, and
operational rather than policy or governance. In contrast, oversight is either
a policy and governance function, or is ex-post. We suggest that control aims
to prevent failures. In contrast, oversight often focuses on detection,
remediation, or incentives for future prevention; all preventative oversight
strategies nonetheless necessitate control.
  Building on this foundation, we make three contributions. First, we propose a
theoretically-informed yet policy-grounded framework that articulates the
conditions under which each mechanism is possible, where they fall short, and
what is required to make them meaningful in practice. Second, we outline how
supervision methods should be documented and integrated into risk management,
and drawing on the Microsoft Responsible AI Maturity Model, we outline a
maturity model for AI supervision. Third, we explicitly highlight some
boundaries of these mechanisms, including where they apply, where they fail,
and where it is clear that no existing methods suffice. This foregrounds the
question of whether meaningful supervision is possible in a given deployment
context, and can support regulators, auditors, and practitioners in identifying
both present limitations and the need for new conceptual and technical
advances.

</details>


### [18] [Discovering Algorithms with Computational Language Processing](https://arxiv.org/abs/2507.03190)
*Theo Bourdais,Abeynaya Gnanasekaran,Houman Owhadi,Tuhin Sahai*

Main category: cs.AI

TL;DR: 提出了一种自动化算法发现的框架，通过将算法表示为操作序列的标记，利用语法链式组合，结合蒙特卡洛树搜索和强化学习，生成高性能新算法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂组合优化和量子计算问题，提升算法性能。

Method: 将算法表示为标记序列，利用语法链式组合，结合蒙特卡洛树搜索和强化学习探索新算法。

Result: 生成的算法在NP难问题和量子计算中显著优于现有方法。

Conclusion: 该框架能针对具体问题实例生成高效算法，而非仅适用于问题类别。

Abstract: Algorithms are the engine for reproducible problem-solving. We present a
framework automating algorithm discovery by conceptualizing them as sequences
of operations, represented as tokens. These computational tokens are chained
using a grammar, enabling the formation of increasingly sophisticated
procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement
learning (RL) explores token chaining and drives the creation of new tokens.
This methodology rediscovers, improves, and generates new algorithms that
substantially outperform existing methods for strongly NP-hard combinatorial
optimization problems and foundational quantum computing approaches such as
Grover's and Quantum Approximate Optimization Algorithm. Operating at the
computational rather than code-generation level, our framework produces
algorithms that can be tailored specifically to problem instances, not merely
classes.

</details>


### [19] [SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models](https://arxiv.org/abs/2507.03223)
*Jeshwanth Challagundla*

Main category: cs.AI

TL;DR: SI-Agent是一个自动生成和优化人类可读系统指令（SIs）的框架，通过反馈驱动循环提升性能与可读性。


<details>
  <summary>Details</summary>
Motivation: 手动设计系统指令（SIs）耗时且效果不佳，现有自动化方法牺牲可读性。

Method: SI-Agent采用三个协作代理（Instructor、Follower、Feedback）和迭代反馈循环优化SIs。

Result: 实验表明SI-Agent在任务性能、可读性和效率上优于基线方法。

Conclusion: SI-Agent在性能和可读性间取得平衡，有望推动LLM定制化和透明度提升。

Abstract: System Instructions (SIs), or system prompts, are pivotal for guiding Large
Language Models (LLMs) but manual crafting is resource-intensive and often
suboptimal. Existing automated methods frequently generate non-human-readable
"soft prompts," sacrificing interpretability. This paper introduces SI-Agent, a
novel agentic framework designed to automatically generate and iteratively
refine human-readable SIs through a feedback-driven loop. SI-Agent employs
three collaborating agents: an Instructor Agent, an Instruction Follower Agent
(target LLM), and a Feedback/Reward Agent evaluating task performance and
optionally SI readability. The framework utilizes iterative cycles where
feedback guides the Instructor's refinement strategy (e.g., LLM-based editing,
evolutionary algorithms). We detail the framework's architecture, agent roles,
the iterative refinement process, and contrast it with existing methods. We
present experimental results validating SI-Agent's effectiveness, focusing on
metrics for task performance, SI readability, and efficiency. Our findings
indicate that SI-Agent generates effective, readable SIs, offering a favorable
trade-off between performance and interpretability compared to baselines.
Potential implications include democratizing LLM customization and enhancing
model transparency. Challenges related to computational cost and feedback
reliability are acknowledged.

</details>


### [20] [Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems](https://arxiv.org/abs/2507.03226)
*Congmin Min,Rhea Mathew,Joyce Pan,Sahil Bansal,Abbas Keshavarzi,Amar Viswanathan Kannan*

Main category: cs.AI

TL;DR: 提出了一种可扩展且成本高效的GraphRAG框架，用于企业环境中的多跳推理和结构化检索，解决了传统方法的高计算成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: GraphRAG在多跳推理和结构化检索中表现优异，但其高计算成本和延迟限制了实际应用。

Method: 引入两种创新：1) 基于依赖关系的知识图谱构建管道，完全摆脱对LLM的依赖；2) 轻量级图检索策略，结合混合查询节点识别和一跳遍历。

Result: 在SAP数据集上表现优异，性能提升15%和4.35%，依赖构建方法达到LLM生成图谱94%的性能，同时显著降低成本。

Conclusion: 验证了GraphRAG在大规模企业应用中的可行性，为实用、可解释和领域适应的检索增强推理铺平了道路。

Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based
Retrieval Augmented Generation (GraphRAG) in enterprise environments. While
GraphRAG has shown promise for multi-hop reasoning and structured retrieval,
its adoption has been limited by the high computational cost of constructing
knowledge graphs using large language models (LLMs) and the latency of
graph-based retrieval. To address these challenges, we introduce two core
innovations: (1) a dependency-based knowledge graph construction pipeline that
leverages industrial-grade NLP libraries to extract entities and relations from
unstructured text completely eliminating reliance on LLMs; and (2) a
lightweight graph retrieval strategy that combines hybrid query node
identification with efficient one-hop traversal for high-recall, low-latency
subgraph extraction. We evaluate our framework on two SAP datasets focused on
legacy code migration and demonstrate strong empirical performance. Our system
achieves up to 15% and 4.35% improvements over traditional RAG baselines based
on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based
construction approach attains 94% of the performance of LLM-generated knowledge
graphs (61.87% vs. 65.83%) while significantly reducing cost and improving
scalability. These results validate the feasibility of deploying GraphRAG
systems in real-world, large-scale enterprise applications without incurring
prohibitive resource requirements paving the way for practical, explainable,
and domain-adaptable retrieval-augmented reasoning.

</details>


### [21] [CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs](https://arxiv.org/abs/2507.03254)
*Bruce Yang,Xinfeng He,Huan Gao,Yifan Cao,Xiaofan Li,David Hsu*

Main category: cs.AI

TL;DR: CodeAgents是一个提示框架，通过模块化伪代码提升多智能体系统的规划能力，显著提高任务准确性和令牌效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构化提示策略局限于单智能体环境，且忽视令牌效率和可扩展性，CodeAgents旨在解决这些问题。

Method: 将智能体交互组件（任务、计划、反馈等）编码为模块化伪代码，结合控制结构和逻辑变量。

Result: 在多个基准测试中表现优异，任务准确率提升3-36%，令牌使用减少55-87%。

Conclusion: CodeAgents为多智能体系统提供了一种高效、可解释的规划方法，强调了令牌效率的重要性。

Abstract: Effective prompt design is essential for improving the planning capabilities
of large language model (LLM)-driven agents. However, existing structured
prompting strategies are typically limited to single-agent, plan-only settings,
and often evaluate performance solely based on task accuracy - overlooking
critical factors such as token efficiency, modularity, and scalability in
multi-agent environments. To address these limitations, we introduce
CodeAgents, a prompting framework that codifies multi-agent reasoning and
enables structured, token-efficient planning in multi-agent systems. In
CodeAgents, all components of agent interaction - Task, Plan, Feedback, system
roles, and external tool invocations - are codified into modular pseudocode
enriched with control structures (e.g., loops, conditionals), boolean logic,
and typed variables. This design transforms loosely connected agent plans into
cohesive, interpretable, and verifiable multi-agent reasoning programs. We
evaluate the proposed framework across three diverse benchmarks - GAIA,
HotpotQA, and VirtualHome - using a range of representative LLMs. Results show
consistent improvements in planning performance, with absolute gains of 3-36
percentage points over natural language prompting baselines. On VirtualHome,
our method achieves a new state-of-the-art success rate of 56%. In addition,
our approach reduces input and output token usage by 55-87% and 41-70%,
respectively, underscoring the importance of token-aware evaluation metrics in
the development of scalable multi-agent LLM systems. The code and resources are
available at: https://anonymous.4open.science/r/CodifyingAgent-5A86

</details>


### [22] [Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective](https://arxiv.org/abs/2507.04594)
*Niloofar Shadab,Tyler Cody,Alejandro Salado,Taylan G. Topcu,Mohammad Shadab,Peter Beling*

Main category: cs.AI

TL;DR: 论文提出了一种新的系统原则“核心与外围”，用于解决智能系统扩展问题，并通过实证验证其在生物和人工智能系统中的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法在智能系统扩展中表现不佳，需要新的系统原则来支持通用智能的工程化。

Method: 基于抽象系统理论和必要多样性法则，提出“核心与外围”原则，并通过数学定义核心主导与外围主导系统。

Result: 实证研究表明，这些原则适用于生物和人工智能系统，连接了抽象理论与实际应用。

Conclusion: “核心与外围”原则为智能系统工程提供了新的理论框架，具有实际应用价值。

Abstract: Engineering methodologies predominantly revolve around established principles
of decomposition and recomposition. These principles involve partitioning
inputs and outputs at the component level, ensuring that the properties of
individual components are preserved upon composition. However, this view does
not transfer well to intelligent systems, particularly when addressing the
scaling of intelligence as a system property. Our prior research contends that
the engineering of general intelligence necessitates a fresh set of overarching
systems principles. As a result, we introduced the "core and periphery"
principles, a novel conceptual framework rooted in abstract systems theory and
the Law of Requisite Variety. In this paper, we assert that these abstract
concepts hold practical significance. Through empirical evidence, we illustrate
their applicability to both biological and artificial intelligence systems,
bridging abstract theory with real-world implementations. Then, we expand on
our previous theoretical framework by mathematically defining core-dominant vs
periphery-dominant systems.

</details>


### [23] [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
*Jie Peng,Jiarui Ji,Runlin Lei,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 论文提出了GDGB基准，用于动态文本属性图（DyTAG）的生成任务，解决了现有数据集文本质量差和任务标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DyTAG数据集文本质量差，缺乏生成任务的标准化评估，限制了DyTAG生成任务的发展。

Method: 提出GDGB基准，包含八个高质量文本特征的DyTAG数据集，定义了两个新任务（TDGG和IDGG），并设计了多维度评估指标。

Result: GDGB支持对TDGG和IDGG的严格评估，揭示了结构和文本特征在DyTAG生成中的关键作用。

Conclusion: GDGB为生成DyTAG研究提供了基础资源，推动了DyTAG生成的实际应用。

Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate
structural, temporal, and textual attributes, are crucial for modeling complex
real-world systems. However, most of the existing DyTAG datasets exhibit poor
textual quality, which severely limits their utility for DyTAG generation tasks
requiring semantically rich inputs. Additionally, prior work mainly focuses on
discriminative tasks on DyTAGs, resulting in a lack of standardized task
formulations and evaluation protocols tailored for DyTAG generation. To address
these critical issues, we propose Generative DyTAG Benchmark (GDGB), which
comprises eight meticulously curated DyTAG datasets with high-quality textual
features for both nodes and edges, overcoming limitations of prior datasets.
Building on GDGB, we define two novel DyTAG generation tasks: Transductive
Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).
TDGG transductively generates a target DyTAG based on the given source and
destination node sets, while the more challenging IDGG introduces new node
generation to inductively model the dynamic expansion of real-world graph data.
To enable holistic evaluation, we design multifaceted metrics that assess the
structural, temporal, and textual quality of the generated DyTAGs. We further
propose GAG-General, an LLM-based multi-agent generative framework tailored for
reproducible and robust benchmarking of DyTAG generation. Experimental results
demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key
insights revealing the critical interplay of structural and textual features in
DyTAG generation. These findings establish GDGB as a foundational resource for
advancing generative DyTAG research and unlocking further practical
applications in DyTAG generation. GDGB datasets, source codes, and leaderboards
are available at \href{https://gdgb-algo.github.io/}{here}.

</details>


### [24] [Memory Mosaics at scale](https://arxiv.org/abs/2507.03285)
*Jianyu Zhang,Léon Bottou*

Main category: cs.AI

TL;DR: Memory Mosaics v2在大型语言模型规模（如llama-8B）和真实数据集上展示了优越的训练知识存储、新知识存储和上下文学习能力，显著优于Transformer。


<details>
  <summary>Details</summary>
Motivation: 验证Memory Mosaics在大型语言模型规模下的性能，并探索其在真实数据集上的表现。

Method: 将Memory Mosaics扩展到10B规模，训练1万亿token，并引入架构改进（Memory Mosaics v2），评估其在三个维度上的能力。

Result: Memory Mosaics v2在训练知识存储上与Transformer相当，在新任务推理和上下文学习上显著优于Transformer。

Conclusion: Memory Mosaics v2在性能上具有显著优势，且无法通过简单增加Transformer的训练数据来复制其效果。

Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications ("Memory
Mosaics v2"), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.

</details>


### [25] [NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval](https://arxiv.org/abs/2507.03329)
*Devendra Patel,Aaditya Jain,Jayant Verma,Divyansh Rajput,Sunil Mahala,Ketki Suresh Khapare,Jayateja Kalla*

Main category: cs.AI

TL;DR: NDAI-NeuroMAP是首个专为神经科学领域设计的高精度信息检索密集向量嵌入模型，通过多目标优化框架显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学领域信息检索任务中通用和生物医学嵌入模型的不足，强调领域特定嵌入架构的重要性。

Method: 利用50万精心构建的三元组、25万神经科学定义条目和25万知识图谱三元组，基于FremyCompany/BioLORD-2023模型进行多目标优化微调。

Result: 在2.4万神经科学查询测试集上表现优于现有通用和生物医学嵌入模型。

Conclusion: 领域特定嵌入架构对神经科学RAG系统和临床NLP应用至关重要。

Abstract: We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector
embedding model engineered for high-precision information retrieval tasks. Our
methodology encompasses the curation of an extensive domain-specific training
corpus comprising 500,000 carefully constructed triplets
(query-positive-negative configurations), augmented with 250,000
neuroscience-specific definitional entries and 250,000 structured
knowledge-graph triplets derived from authoritative neurological ontologies. We
employ a sophisticated fine-tuning approach utilizing the
FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective
optimization framework combining contrastive learning with triplet-based metric
learning paradigms. Comprehensive evaluation on a held-out test dataset
comprising approximately 24,000 neuroscience-specific queries demonstrates
substantial performance improvements over state-of-the-art general-purpose and
biomedical embedding models. These empirical findings underscore the critical
importance of domain-specific embedding architectures for neuroscience-oriented
RAG systems and related clinical natural language processing applications.

</details>


### [26] [Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking](https://arxiv.org/abs/2507.03330)
*Franklin Mingzhe Li,Kaitlyn Ng,Bin Zhu,Patrick Carrington*

Main category: cs.AI

TL;DR: OSCAR是一种基于物体状态识别的技术管道，用于支持无视觉烹饪中的食谱进度跟踪，通过整合食谱解析、物体状态提取和视觉对齐，提高了步骤预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 烹饪对日常生活独立性和幸福感至关重要，但对视力障碍者来说，由于缺乏进度跟踪和上下文反馈的支持，仍然具有挑战性。物体状态为上下文感知烹饪支持提供了潜力。

Method: OSCAR整合了食谱解析、物体状态提取、视觉对齐和时间因果建模，以实时跟踪烹饪步骤。

Result: 在173个教学视频和12个真实世界无视觉烹饪会话中，OSCAR显著提高了步骤预测准确性，并揭示了影响性能的关键因素。

Conclusion: OSCAR为上下文感知烹饪支持提供了技术管道、真实世界数据集和设计见解，为未来系统开发奠定了基础。

Abstract: Cooking plays a vital role in everyday independence and well-being, yet
remains challenging for people with vision impairments due to limited support
for tracking progress and receiving contextual feedback. Object status - the
condition or transformation of ingredients and tools - offers a promising but
underexplored foundation for context-aware cooking support. In this paper, we
present OSCAR (Object Status Context Awareness for Recipes), a technical
pipeline that explores the use of object status recognition to enable recipe
progress tracking in non-visual cooking. OSCAR integrates recipe parsing,
object status extraction, visual alignment with cooking steps, and time-causal
modeling to support real-time step tracking. We evaluate OSCAR on 173
instructional videos and a real-world dataset of 12 non-visual cooking sessions
recorded by BLV individuals in their homes. Our results show that object status
consistently improves step prediction accuracy across vision-language models,
and reveal key factors that impact performance in real-world conditions, such
as implicit tasks, camera placement, and lighting. We contribute the pipeline
of context-aware recipe progress tracking, an annotated real-world non-visual
cooking dataset, and design insights to guide future context-aware assistive
cooking systems.

</details>


### [27] [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
*Ashutosh Hathidara,Julien Yu,Sebastian Schreiber*

Main category: cs.AI

TL;DR: DiaFORGE是一个三阶段对话框架，通过生成多轮对话、微调开源模型和动态评估，显著提升大型语言模型在调用企业API时的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在调用企业API时因工具相似或参数不明确而失败的问题。

Method: 包括三阶段：生成多轮对话、监督微调开源模型、动态评估模型性能。

Result: 在DiaBENCH基准上，DiaFORGE训练模型的工具调用成功率比GPT-4o和Claude-3.5-Sonnet分别提高27和49个百分点。

Conclusion: DiaFORGE为构建可靠的企业级工具调用代理提供了实用蓝图，并发布了开放数据集以促进研究。

Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.

</details>


### [28] [Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)](https://arxiv.org/abs/2507.03608)
*Sarat Ahmad,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.AI

TL;DR: 论文比较了Vector RAG、GraphRAG和Hybrid GraphRAG在ORAN架构中的表现，结果显示GraphRAG和Hybrid GraphRAG优于传统RAG，尤其在事实正确性和上下文相关性方面。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在无线网络自主优化中具有潜力，但针对电信任务的LLM微调成本高。RAG提供了一种无需完全重新训练的领域适应方法，但缺乏系统评估。

Method: 研究比较了Vector RAG、GraphRAG和Hybrid GraphRAG在ORAN规范中的表现，评估了生成指标：忠实性、答案相关性、上下文相关性和事实正确性。

Result: GraphRAG和Hybrid GraphRAG表现优于传统RAG，Hybrid GraphRAG事实正确性提高8%，GraphRAG上下文相关性提高7%。

Conclusion: GraphRAG和Hybrid GraphRAG在ORAN架构中具有显著优势，为高要求领域提供了更可靠的解决方案。

Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling
autonomous optimization in future wireless networks. Within the ORAN
architecture, Large Language Models (LLMs) can be specialized to generate xApps
and rApps by leveraging specifications and API definitions from the RAN
Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for
telecom-specific tasks remains expensive and resource-intensive.
Retrieval-Augmented Generation (RAG) offers a practical alternative through
in-context learning, enabling domain adaptation without full retraining. While
traditional RAG systems rely on vector-based retrieval, emerging variants such
as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval
strategies to support multi-hop reasoning and improve factual grounding.
Despite their promise, these methods lack systematic, metric-driven
evaluations, particularly in high-stakes domains such as ORAN. In this study,
we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid
GraphRAG using ORAN specifications. We assess performance across varying
question complexities using established generation metrics: faithfulness,
answer relevance, context relevance, and factual correctness. Results show that
both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG
improves factual correctness by 8%, while GraphRAG improves context relevance
by 7%.

</details>


### [29] [Effects of structure on reasoning in instance-level Self-Discover](https://arxiv.org/abs/2507.03347)
*Sachith Gunasekara,Yasiru Ratnayake*

Main category: cs.AI

TL;DR: 本文比较了结构化与非结构化推理在LLM中的性能，发现非结构化推理在复杂任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究结构化与非结构化推理在LLM中的性能差异，以解决计算预算和忠实性问题。

Method: 引入iSelf-Discover框架，动态生成结构化JSON与非结构化推理，并在多个基准测试中进行比较。

Result: 非结构化推理在复杂任务（如MATH基准）中表现更优，相对性能提升达18.90%。

Conclusion: 研究结果表明，复杂问题解决中应重新评估对结构化格式的依赖，并优化复合系统的组织方式。

Abstract: The drive for predictable LLM reasoning in their integration with compound
systems has popularized structured outputs, yet concerns remain about
performance trade-offs compared to unconstrained natural language. At the same
time, training on unconstrained Chain of Thought (CoT) traces has brought about
a new class of strong reasoning models that nevertheless present novel compute
budget and faithfulness challenges. This paper introduces iSelf-Discover, an
instance-level adaptation of the Self-Discover framework, and using it compares
dynamically generated structured JSON reasoning with its unstructured
counterpart. Our empirical evaluation across diverse benchmarks using
state-of-the-art open-source models supports a consistent advantage for
unstructured reasoning. Notably, on the complex MATH benchmark, unstructured
plans achieved relative performance improvements of up to 18.90\% over
structured approaches. Zero-shot unstructured iSelf-Discover variants are also
shown to outperform their five-shot structured counterparts, underscoring the
significance of this gap, even when structured plans are dynamically generated
to ensure reasoning precedes the final answer. We further demonstrate that the
optimal granularity of plan generation (instance-level vs. task-level) is
context-dependent. These findings invite re-evaluation of the reliance on
structured formats for complex problem-solving and how compound systems should
be organized.

</details>


### [30] [MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents](https://arxiv.org/abs/2507.04376)
*Georgios Ioannides,Christos Constantinou,Vinija Jain,Aman Chadha,Aaron Elkins*

Main category: cs.AI

TL;DR: MOD-X是一个新型的模块化开放去中心化交换框架，旨在解决异构智能体间的互操作性问题，提供分层架构、通用消息总线、状态管理、翻译能力和区块链安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一模型发展为专业化智能体生态系统，标准化通信协议的需求日益迫切，现有协议存在局限性。

Method: 提出MOD-X框架，包括分层架构、通用消息总线、状态管理、翻译能力和区块链安全机制，支持发布-订阅通信模型、语义能力发现和动态工作流编排。

Result: MOD-X能够有效集成异构智能体（如规则系统、神经网络、符号推理引擎等），实现去中心化、可扩展的互操作性。

Conclusion: MOD-X为去中心化智能体生态系统提供了理论与实践结合的解决方案，满足无需中央协调的规模化需求。

Abstract: As Artificial Intelligence systems evolve from monolithic models to
ecosystems of specialized agents, the need for standardized communication
protocols becomes increasingly critical. This paper introduces MOD-X (Modular
Open Decentralized eXchange), a novel architectural framework proposal for
agent interoperability that addresses key limitations of existing protocols.
Unlike current approaches, MOD-X proposes a layered architecture with a
Universal Message Bus, thorough state management, translation capabilities, and
blockchain-based security mechanisms. We present MOD-X's architecture, compare
it with existing protocols, and demonstrate its application through a worked
example how it enables integration between heterogeneous specialist agents
(agents with different architectures, vendors, capabilities, and knowledge
representations--including rule-based systems, neural networks, symbolic
reasoning engines, and legacy software with agent wrappers). MOD-X's key
innovations include a publish-subscribe communication model, semantic
capability discovery, and dynamic workflow orchestration--providing a framework
that bridges theoretical formalism with practical implementation. This
architecture addresses the growing need for truly decentralized, interoperable
agent ecosystems that can scale effectively without the need for central
coordination.

</details>


### [31] [Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy](https://arxiv.org/abs/2507.03407)
*Junwei Su,Cheng Xin,Ao Shang,Shan Wu,Zhenzhen Xie,Ruogu Xiong,Xiaoyu Xu,Cheng Zhang,Guang Chen,Yau-Tuen Chan,Guoyi Tang,Ning Wang,Yong Xu,Yibin Feng*

Main category: cs.AI

TL;DR: 本文系统综述了AI/ML在药物发现全流程中的应用，填补了现有文献对关键阶段依赖关系的忽视，并通过案例研究展示了实际效果。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法复杂、成本高、耗时长且失败率高，亟需全面了解AI/ML如何整合到全流程中。

Method: 详细分析了AI/ML在目标识别、命中筛选和先导优化等核心阶段的应用，结合案例研究展示实际效果。

Result: 展示了AI/ML在各阶段的显著方法进展及其影响，并通过案例研究验证了其实际应用价值。

Conclusion: 本文为研究者利用AI/ML克服瓶颈、加速药物发现提供了重要参考，并指出了未来研究方向。

Abstract: This paper systematically reviews recent advances in artificial intelligence
(AI), with a particular focus on machine learning (ML), across the entire drug
discovery pipeline. Due to the inherent complexity, escalating costs, prolonged
timelines, and high failure rates of traditional drug discovery methods, there
is a critical need to comprehensively understand how AI/ML can be effectively
integrated throughout the full process. Currently available literature reviews
often narrowly focus on specific phases or methodologies, neglecting the
dependence between key stages such as target identification, hit screening, and
lead optimization. To bridge this gap, our review provides a detailed and
holistic analysis of AI/ML applications across these core phases, highlighting
significant methodological advances and their impacts at each stage. We further
illustrate the practical impact of these techniques through an in-depth case
study focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,
highlighting real-world successes in molecular target identification and
therapeutic candidate discovery. Additionally, we discuss significant
challenges facing AI/ML in drug discovery and outline promising future research
directions. Ultimately, this review serves as an essential orientation for
researchers aiming to leverage AI/ML to overcome existing bottlenecks and
accelerate drug discovery.

</details>


### [32] [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409)
*Christopher Summerfield,Lennart Luettgau,Magda Dubois,Hannah Rose Kirk,Kobi Hackenburg,Catherine Fist,Katarina Slama,Nicola Ding,Rebecca Anselmetti,Andrew Strait,Mario Giulianelli,Cozmin Ududec*

Main category: cs.AI

TL;DR: 论文探讨当前AI系统是否可能发展出“阴谋”能力（隐蔽且战略性地追求未对齐目标），并与1970年代研究非人灵长类动物掌握自然语言的实践对比，提出避免历史研究缺陷的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否可能发展出隐蔽的战略性行为，以避免重蹈1970年代研究的覆辙（如过度拟人化、依赖轶事和缺乏理论框架）。

Method: 比较当前AI研究与1970年代非人灵长类语言研究的实践，分析历史研究的缺陷。

Result: 提出避免过度拟人化、依赖轶事和缺乏理论框架的建议，并给出具体步骤以推动科学严谨的研究。

Conclusion: AI“阴谋”研究应吸取历史教训，采取科学严谨的方法，避免重蹈覆辙。

Abstract: We examine recent research that asks whether current AI systems may be
developing a capacity for "scheming" (covertly and strategically pursuing
misaligned goals). We compare current research practices in this field to those
adopted in the 1970s to test whether non-human primates could master natural
language. We argue that there are lessons to be learned from that historical
research endeavour, which was characterised by an overattribution of human
traits to other agents, an excessive reliance on anecdote and descriptive
analysis, and a failure to articulate a strong theoretical framework for the
research. We recommend that research into AI scheming actively seeks to avoid
these pitfalls. We outline some concrete steps that can be taken for this
research programme to advance in a productive and scientifically rigorous
fashion.

</details>


### [33] [Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis](https://arxiv.org/abs/2507.03460)
*Weitong Zhang,Mengyun Qiao,Chengqi Zang,Steven Niederer,Paul M Matthews,Wenjia Bai,Bernhard Kainz*

Main category: cs.AI

TL;DR: MESHAgents框架利用多学科AI代理自动发现影像表型与疾病风险因素的非线性关联，性能接近专家选择，并提升部分疾病的召回率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工假设和选择关联因素，可能忽略复杂非线性关系，需自动化解决方案。

Method: 结合多学科AI代理（如心脏病学、生物力学、统计学）动态生成和验证表型关联，实现自动化PheWAS。

Result: 在心血管影像研究中，MESHAgents发现超出标准人口因素的混杂变量，疾病分类AUC差异仅-0.004，6/9疾病召回率提升。

Conclusion: MESHAgents提供可扩展、透明的影像表型发现方法，性能接近专家驱动方法。

Abstract: Identifying the associations between imaging phenotypes and disease risk
factors and outcomes is essential for understanding disease mechanisms and
improving diagnosis and prognosis models. However, traditional approaches rely
on human-driven hypothesis testing and selection of association factors, often
overlooking complex, non-linear dependencies among imaging phenotypes and other
multi-modal data. To address this, we introduce a Multi-agent Exploratory
Synergy for the Heart (MESHAgents) framework that leverages large language
models as agents to dynamically elicit, surface, and decide confounders and
phenotypes in association studies, using cardiovascular imaging as a proof of
concept. Specifically, we orchestrate a multi-disciplinary team of AI agents --
spanning cardiology, biomechanics, statistics, and clinical research -- which
spontaneously generate and converge on insights through iterative,
self-organizing reasoning. The framework dynamically synthesizes statistical
correlations with multi-expert consensus, providing an automated pipeline for
phenome-wide association studies (PheWAS). We demonstrate the system's
capabilities through a population-based study of imaging phenotypes of the
heart and aorta. MESHAgents autonomously uncovered correlations between imaging
phenotypes and a wide range of non-imaging factors, identifying additional
confounder variables beyond standard demographic factors. Validation on
diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve
performance comparable to expert-selected phenotypes, with mean AUC differences
as small as -0.004 on disease classification tasks. Notably, the recall score
improves for 6 out of 9 disease types. Our framework provides clinically
relevant imaging phenotypes with transparent reasoning, offering a scalable
alternative to expert-driven methods.

</details>


### [34] [REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services](https://arxiv.org/abs/2507.03477)
*Kexin Zhu,Yang Han*

Main category: cs.AI

TL;DR: 论文提出了REAL评估套件，用于评估大语言模型在房地产交易和服务中的能力，发现现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型是否能在房地产交易和服务中扮演类似人类的角色。

Method: 开发了包含5,316个高质量评估条目的REAL评估套件，涵盖4个主题和14个类别。

Result: 实验结果表明，大语言模型在房地产领域的应用仍有显著改进空间。

Conclusion: REAL套件为评估大语言模型在房地产领域的能力提供了有效工具，但模型性能仍需提升。

Abstract: The development of large language models (LLMs) has greatly promoted the
progress of chatbot in multiple fields. There is an urgent need to evaluate
whether LLMs can play the role of agent in housing transactions and services as
well as humans. We present Real Estate Agent Large Language Model Evaluation
(REAL), the first evaluation suite designed to assess the abilities of LLMs in
the field of housing transactions and services. REAL comprises 5,316
high-quality evaluation entries across 4 topics: memory, comprehension,
reasoning and hallucination. All these entries are organized as 14 categories
to assess whether LLMs have the knowledge and ability in housing transactions
and services scenario. Additionally, the REAL is used to evaluate the
performance of most advanced LLMs. The experiment results indicate that LLMs
still have significant room for improvement to be applied in the real estate
field.

</details>


### [35] [A Universal Approach to Feature Representation in Dynamic Task Assignment Problems](https://arxiv.org/abs/2507.03579)
*Riccardo Lo Bianco,Remco Dijkman,Wim Nuijten,Willem van Jaarsveld*

Main category: cs.AI

TL;DR: 论文提出一种基于图的特征表示方法（assignment graph）和PPO算法的改进，用于解决动态任务分配问题中的无限状态和动作空间挑战。


<details>
  <summary>Details</summary>
Motivation: 动态任务分配问题中，现有DRL方法在处理无限状态和动作空间时存在表示和学习的挑战。

Method: 提出assignment graph表示方法，将标记Colored Petri Nets映射到assignment graph，并改进PPO算法以学习任务分配策略。

Result: 实验表明，该方法能有效表示和学习接近最优的任务分配策略，适用于不同维度的状态和动作空间。

Conclusion: 该方法为动态任务分配问题提供了一种通用且高效的解决方案。

Abstract: Dynamic task assignment concerns the optimal assignment of resources to tasks
in a business process. Recently, Deep Reinforcement Learning (DRL) has been
proposed as the state of the art for solving assignment problems. DRL methods
usually employ a neural network (NN) as an approximator for the policy
function, which ingests the state of the process and outputs a valuation of the
possible assignments. However, representing the state and the possible
assignments so that they can serve as inputs and outputs for a policy NN
remains an open challenge, especially when tasks or resources have features
with an infinite number of possible values. To solve this problem, this paper
proposes a method for representing and solving assignment problems with
infinite state and action spaces. In doing so, it provides three contributions:
(I) A graph-based feature representation of assignment problems, which we call
assignment graph; (II) A mapping from marked Colored Petri Nets to assignment
graphs; (III) An adaptation of the Proximal Policy Optimization algorithm that
can learn to solve assignment problems represented through assignment graphs.
To evaluate the proposed representation method, we model three archetypal
assignment problems ranging from finite to infinite state and action space
dimensionalities. The experiments show that the method is suitable for
representing and learning close-to-optimal task assignment policies regardless
of the state and action space dimensionalities.

</details>


### [36] [EvoAgentX: An Automated Framework for Evolving Agentic Workflows](https://arxiv.org/abs/2507.03616)
*Yingxu Wang,Siwei Liu,Jinyuan Fang,Zaiqiao Meng*

Main category: cs.AI

TL;DR: EvoAgentX是一个开源平台，用于自动化生成、执行和优化多智能体工作流，整合了三种优化算法，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统（MAS）框架需要手动配置工作流，缺乏动态演化和性能优化的原生支持，且优化算法未统一整合。

Method: EvoAgentX采用五层模块化架构（基础组件、智能体、工作流、演化和评估层），整合了TextGrad、AFlow和MIPRO三种优化算法。

Result: 在HotPotQA、MBPP和MATH等任务中，EvoAgentX显著提升了性能（如HotPotQA F1提高7.44%，MBPP pass@1提高10.00%）。

Conclusion: EvoAgentX通过自动化工作流和优化算法，显著提升了多智能体系统的性能，适用于复杂任务。

Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for
orchestrating large language models (LLMs) and specialized tools to
collaboratively address complex tasks. However, existing MAS frameworks often
require manual workflow configuration and lack native support for dynamic
evolution and performance optimization. In addition, many MAS optimization
algorithms are not integrated into a unified framework. In this paper, we
present EvoAgentX, an open-source platform that automates the generation,
execution, and evolutionary optimization of multi-agent workflows. EvoAgentX
employs a modular architecture consisting of five core layers: the basic
components, agent, workflow, evolving, and evaluation layers. Specifically,
within the evolving layer, EvoAgentX integrates three MAS optimization
algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,
tool configurations, and workflow topologies. We evaluate EvoAgentX on
HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and
mathematical problem solving, respectively, and further assess it on real-world
tasks using GAIA. Experimental results show that EvoAgentX consistently
achieves significant performance improvements, including a 7.44% increase in
HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve
accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The
source code is available at: https://github.com/EvoAgentX/EvoAgentX

</details>


### [37] [Large Language Models for Combinatorial Optimization: A Systematic Review](https://arxiv.org/abs/2507.03637)
*Francesca Da Ros,Michael Soprano,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 本文通过PRISMA指南系统综述了大型语言模型（LLMs）在组合优化（CO）中的应用，筛选出103项研究并分类，总结了LLMs的任务、架构、数据集及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在组合优化领域的应用现状，填补研究空白并提供系统性的综述。

Method: 使用PRISMA指南，通过Scopus和Google Scholar检索2000+文献，按语言、研究焦点、年份和类型筛选出103项研究。

Result: 分类总结了LLMs在CO中的任务、架构、数据集及应用领域，并提出了未来研究方向。

Conclusion: LLMs在CO中具有潜力，未来需进一步优化模型架构和数据集以提升性能。

Abstract: This systematic review explores the application of Large Language Models
(LLMs) in Combinatorial Optimization (CO). We report our findings using the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
guidelines. We conduct a literature search via Scopus and Google Scholar,
examining over 2,000 publications. We assess publications against four
inclusion and four exclusion criteria related to their language, research
focus, publication year, and type. Eventually, we select 103 studies. We
classify these studies into semantic categories and topics to provide a
comprehensive overview of the field, including the tasks performed by LLMs, the
architectures of LLMs, the existing datasets specifically designed for
evaluating LLMs in CO, and the field of application. Finally, we identify
future directions for leveraging LLMs in this field.

</details>


### [38] [Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning](https://arxiv.org/abs/2507.03682)
*Rebekah A. Gelpí,Eric Xue,William A. Cunningham*

Main category: cs.AI

TL;DR: 提出了一种混合方法，结合大型语言模型（LLMs）和贝叶斯逆规划模型，以提升机器心智理论（ToM）的性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯逆规划模型在ToM任务中表现准确但难以扩展，而LLMs在ToM任务中表现出潜力但存在推理脆弱性。结合两者以发挥各自优势。

Method: 使用LLMs生成假设和似然函数，结合贝叶斯逆规划模型计算后验概率，预测代理的心理状态。

Result: 混合方法在ToM任务中表现优于单独使用LLMs或链式思维提示，即使是较小的LLMs也能提升性能。

Conclusion: 该方法为ToM模型的未来发展和社会智能生成代理的创建提供了有前景的方向。

Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large
language models (LLMs) as a mechanism for generating hypotheses and likelihood
functions with a Bayesian inverse planning model that computes posterior
probabilities for an agent's likely mental states given its actions. Bayesian
inverse planning models can accurately predict human reasoning on a variety of
ToM tasks, but these models are constrained in their ability to scale these
predictions to scenarios with a large number of possible hypotheses and
actions. Conversely, LLM-based approaches have recently demonstrated promise in
solving ToM benchmarks, but can exhibit brittleness and failures on reasoning
tasks even when they pass otherwise structurally identical versions. By
combining these two methods, this approach leverages the strengths of each
component, closely matching optimal results on a task inspired by prior inverse
planning models and improving performance relative to models that utilize LLMs
alone or with chain-of-thought prompting, even with smaller LLMs that typically
perform poorly on ToM tasks. We also exhibit the model's potential to predict
mental states on open-ended tasks, offering a promising direction for future
development of ToM models and the creation of socially intelligent generative
agents.

</details>


### [39] [Towards Unified Neurosymbolic Reasoning on Knowledge Graphs](https://arxiv.org/abs/2507.03697)
*Qika Lin,Fangzhi Xu,Hao Lu,Kai He,Rui Mao,Jun Liu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: Tunsr是一个统一的神经符号推理框架，通过结合神经和符号方法的优势，解决了知识图谱推理中的多样性和统一性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能有效整合神经和符号推理的优势，且局限于单一推理场景，无法满足现实任务的多样化需求。

Method: Tunsr引入一致的推理图结构，通过前向逻辑消息传递机制更新节点表示和注意力，并利用FARI算法归纳一阶逻辑规则。

Result: 在四个推理场景的19个数据集上，Tunsr表现出高效性。

Conclusion: Tunsr成功统一了神经和符号推理，解决了多样性和表示差距问题。

Abstract: Knowledge Graph (KG) reasoning has received significant attention in the
fields of artificial intelligence and knowledge engineering, owing to its
ability to autonomously deduce new knowledge and consequently enhance the
availability and precision of downstream applications. However, current methods
predominantly concentrate on a single form of neural or symbolic reasoning,
failing to effectively integrate the inherent strengths of both approaches.
Furthermore, the current prevalent methods primarily focus on addressing a
single reasoning scenario, presenting limitations in meeting the diverse
demands of real-world reasoning tasks. Unifying the neural and symbolic
methods, as well as diverse reasoning scenarios in one model is challenging as
there is a natural representation gap between symbolic rules and neural
networks, and diverse scenarios exhibit distinct knowledge structures and
specific reasoning objectives. To address these issues, we propose a unified
neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first
introduces a consistent structure of reasoning graph that starts from the query
entity and constantly expands subsequent nodes by iteratively searching
posterior neighbors. Based on it, a forward logic message-passing mechanism is
proposed to update both the propositional representations and attentions, as
well as first-order logic (FOL) representations and attentions of each node. In
this way, Tunsr conducts the transformation of merging multiple rules by
merging possible relations at each step. Finally, the FARI algorithm is
proposed to induce FOL rules by constantly performing attention calculations
over the reasoning graph. Extensive experimental results on 19 datasets of four
reasoning scenarios (transductive, inductive, interpolation, and extrapolation)
demonstrate the effectiveness of Tunsr.

</details>


### [40] [Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology](https://arxiv.org/abs/2507.03722)
*Ruian Ke,Ruy M. Ribeiro*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在跨学科研究中的应用，强调其作为辅助工具的优势与局限性，并以计算生物学案例展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: LLMs在研究中受到质疑，需明确其优缺点以确保负责任的使用。

Method: 提出整合LLMs的路线图，通过案例研究（HIV反弹动力学建模）展示其如何促进跨学科合作。

Result: LLMs在人类参与的框架下能有效辅助研究，加速科学发现。

Conclusion: 负责任地使用LLMs将推动跨学科创新研究。

Abstract: Large language models (LLMs) are powerful artificial intelligence (AI) tools
transforming how research is conducted. However, their use in research has been
met with skepticism, due to concerns about hallucinations, biases and potential
harms to research. These emphasize the importance of clearly understanding the
strengths and weaknesses of LLMs to ensure their effective and responsible use.
Here, we present a roadmap for integrating LLMs into cross-disciplinary
research, where effective communication, knowledge transfer and collaboration
across diverse fields are essential but often challenging. We examine the
capabilities and limitations of LLMs and provide a detailed computational
biology case study (on modeling HIV rebound dynamics) demonstrating how
iterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary
collaboration and research. We argue that LLMs are best used as augmentative
tools within a human-in-the-loop framework. Looking forward, we envisage that
the responsible use of LLMs will enhance innovative cross-disciplinary research
and substantially accelerate scientific discoveries.

</details>


### [41] [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
*Riya Naik,Ashwin Srinivasan,Swati Agarwal,Estrid He*

Main category: cs.AI

TL;DR: 论文探讨了通过基于代理的架构增强LLM问答系统的推理能力，自动解决提问中的不完整或模糊问题，缩短交互时间并提高答案质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM问答系统在多轮交互中可能因上下文不清晰而显得繁琐，需通过推理解决。

Method: 采用基于LLM的代理（如GPT-3.5-Turbo和Llama-4-Scout）作为零样本ReAct代理，通过分类、解决和回答三个动作处理问题。

Result: 代理方法缩短了交互时间，提高了答案质量，并能解释问题缺陷的解决过程，但可能增加LLM调用和延迟。

Conclusion: 代理方法在大多数情况下优于传统方法，适用于开发更健壮的问答系统。

Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of
question. However, consulting an LLM does not have to be a single turn
activity. But long multi-turn interactions can get tedious if it is simply to
clarify contextual information that can be arrived at through reasoning. In
this paper, we examine the use of agent-based architecture to bolster LLM-based
Question-Answering systems with additional reasoning capabilities. We examine
the automatic resolution of potential incompleteness or ambiguities in
questions by transducers implemented using LLM-based agents. We focus on
several benchmark datasets that are known to contain questions with these
deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and
Llama-4-Scout) with agents that act as specialists in detecting and resolving
deficiencies of incompleteness and ambiguity. The agents are implemented as
zero-shot ReAct agents. Rather than producing an answer in a single step, the
model now decides between 3 actions a) classify b) resolve c) answer. Action a)
decides if the question is incomplete, ambiguous, or normal. Action b)
determines if any deficiencies identified can be resolved. Action c) answers
the resolved form of the question. We compare the use of LLMs with and without
the use of agents with these components. Our results show benefits of agents
with transducer 1) A shortening of the length of interactions with human 2) An
improvement in the answer quality and 3) Explainable resolution of deficiencies
in the question. On the negative side we find while it may result in additional
LLM invocations and in some cases, increased latency. But on tested datasets,
the benefits outweigh the costs except when questions already have sufficient
context. Suggesting the agent-based approach could be a useful mechanism to
harness the power of LLMs to develop more robust QA systems.

</details>


### [42] [Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach](https://arxiv.org/abs/2507.03775)
*Hiba Bederina*

Main category: cs.AI

TL;DR: 本文提出了一种解决Close Enough Traveling Salesman Problem（CETSP）的方法，通过简化数学公式和利用凸集约束设计，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 旨在简化CETSP的数学建模，降低计算复杂度，同时保持解的质量。

Method: 引入近似欧几里得距离的重新公式化，简化目标函数，并使用凸集约束设计。采用分段CPLEX计算策略进行实证验证。

Result: 在真实CETSP实例中验证了方法的有效性，能够高效管理计算资源且不影响解的质量。

Conclusion: 提出的数学公式在性能上表现良好，为CETSP提供了实用的解决方案。

Abstract: This article explores an approach to addressing the Close Enough Traveling
Salesman Problem (CETSP). The objective is to streamline the mathematical
formulation by introducing reformulations that approximate the Euclidean
distances and simplify the objective function. Additionally, the use of convex
sets in the constraint design offers computational benefits. The proposed
methodology is empirically validated on real-world CETSP instances, with the
aid of computational strategies such as a fragmented CPLEX-based approach.
Results demonstrate its effectiveness in managing computational resources
without compromising solution quality. Furthermore, the article analyzes the
behavior of the proposed mathematical formulations, providing comprehensive
insights into their performance.

</details>


### [43] [Learning Dark Souls Combat Through Pixel Input With Neuroevolution](https://arxiv.org/abs/2507.03793)
*Jim O'Connor,Gary B. Parker,Mustafa Bugti*

Main category: cs.AI

TL;DR: 论文研究了NEAT在《黑暗之魂》游戏自动化中的应用，通过直接进化神经网络从像素数据中学习，无需游戏状态信息，成功率达到35%。


<details>
  <summary>Details</summary>
Motivation: 探索在复杂视觉输入和高难度游戏环境中，神经进化方法（NEAT）的可行性，尤其是缺乏API支持或明确状态表示的场景。

Method: 引入DSAPI框架，结合实时计算机视觉提取游戏数据，使用NEAT直接进化神经网络，训练代理击败初始Boss。

Result: 实验结果显示，进化后的代理在击败初始Boss时达到35%的成功率。

Conclusion: 研究表明，基于视觉的神经进化在复杂游戏环境中具有潜力，尤其是缺乏直接API支持的场景。

Abstract: This paper investigates the application of Neuroevolution of Augmenting
Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging
action role-playing game characterized by complex combat mechanics, dynamic
environments, and high-dimensional visual inputs. Unlike traditional
reinforcement learning or game playing approaches, our method evolves neural
networks directly from raw pixel data, circumventing the need for explicit
game-state information. To facilitate this approach, we introduce the Dark
Souls API (DSAPI), a novel Python framework leveraging real-time computer
vision techniques for extracting critical game metrics, including player and
enemy health states. Using NEAT, agents evolve effective combat strategies for
defeating the Asylum Demon, the game's initial boss, without predefined
behaviors or domain-specific heuristics. Experimental results demonstrate that
evolved agents achieve up to a 35% success rate, indicating the viability of
neuroevolution in addressing complex, visually intricate gameplay scenarios.
This work represents an interesting application of vision-based neuroevolution,
highlighting its potential use in a wide range of challenging game environments
lacking direct API support or well-defined state representations.

</details>


### [44] [Generating Novelty in Open-World Multi-Agent Strategic Board Games](https://arxiv.org/abs/2507.03802)
*Mayank Kejriwal,Shilpa Thomas*

Main category: cs.AI

TL;DR: GNOME是一个用于测试多智能体AI系统面对新颖性表现的开源平台，支持未预期的新颖性，并在NeurIPS 2020上以《大富翁》游戏展示。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体AI系统在开放世界环境中面对未预期新颖性的适应能力。

Method: 通过分离AI开发与模拟器，GNOME平台支持未预期新颖性的测试，并利用Web GUI展示。

Result: GNOME在NeurIPS 2020上成功展示，并用于DARPA SAIL-ON项目评估新颖性适应智能体。

Conclusion: GNOME为研究AI鲁棒性和开放世界新颖性提供了有效工具。

Abstract: We describe GNOME (Generating Novelty in Open-world Multi-agent
Environments), an experimental platform that is designed to test the
effectiveness of multi-agent AI systems when faced with \emph{novelty}. GNOME
separates the development of AI gameplaying agents with the simulator, allowing
\emph{unanticipated} novelty (in essence, novelty that is not subject to
model-selection bias). Using a Web GUI, GNOME was recently demonstrated at
NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI
robustness and the nature of novelty in real-world environments. In this
article, we further detail the key elements of the demonstration, and also
provide an overview of the experimental design that is being currently used in
the DARPA Science of Artificial Intelligence and Learning for Open-World
Novelty (SAIL-ON) program to evaluate external teams developing
novelty-adaptive gameplaying agents.

</details>


### [45] [Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts](https://arxiv.org/abs/2507.03811)
*Gianlucca Zuin,Saulo Mastelini,Túlio Loures,Adriano Veloso*

Main category: cs.AI

TL;DR: 提出了一种基于代理和大型语言模型的框架，用于通过员工交互迭代重建数据集描述，解决了组织中隐性知识记录的挑战。


<details>
  <summary>Details</summary>
Motivation: 组织中隐性知识记录存在初始信息不完整、难以识别知识个体、正式与非正式网络交织等问题。

Method: 采用基于代理的框架，结合大型语言模型，将知识传播建模为SI过程，并进行864次模拟。

Result: 代理实现了94.9%的完整知识召回率，自我反馈分数与外部文献评分强相关。

Conclusion: 该方法能有效捕获分散知识，无需直接访问领域专家，展示了代理处理组织复杂性的能力。

Abstract: Documenting tacit knowledge in organizations can be a challenging task due to
incomplete initial information, difficulty in identifying knowledgeable
individuals, the interplay of formal hierarchies and informal networks, and the
need to ask the right questions. To address this, we propose an agent-based
framework leveraging large language models (LLMs) to iteratively reconstruct
dataset descriptions through interactions with employees. Modeling knowledge
dissemination as a Susceptible-Infectious (SI) process with waning infectivity,
we conduct 864 simulations across various synthetic company structures and
different dissemination parameters. Our results show that the agent achieves
94.9% full-knowledge recall, with self-critical feedback scores strongly
correlating with external literature critic scores. We analyze how each
simulation parameter affects the knowledge retrieval process for the agent. In
particular, we find that our approach is able to recover information without
needing to access directly the only domain specialist. These findings highlight
the agent's ability to navigate organizational complexity and capture
fragmented knowledge that would otherwise remain inaccessible.

</details>


### [46] [RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation](https://arxiv.org/abs/2507.03829)
*George Hannah,Jacopo de Berardinis,Terry R. Payne,Valentina Tamma,Andrew Mitchell,Ellen Piercy,Ewan Johnson,Andrew Ng,Harry Rostron,Boris Konev*

Main category: cs.AI

TL;DR: 论文提出RELRaE框架，利用大语言模型（LLMs）从XML数据中提取和标注关系，支持实验室自动化中的知识图谱生成。


<details>
  <summary>Details</summary>
Motivation: 解决实验室机器人产生的XML数据互操作性问题，将其转换为知识图谱。

Method: 使用RELRaE框架，通过LLMs分阶段提取和标注XML模式中的隐含关系。

Result: LLMs能有效支持关系标签的生成，在半自动本体生成框架中具有价值。

Conclusion: LLMs在实验室自动化和本体生成中具有实用性和潜力。

Abstract: A large volume of XML data is produced in experiments carried out by robots
in laboratories. In order to support the interoperability of data between labs,
there is a motivation to translate the XML data into a knowledge graph. A key
stage of this process is the enrichment of the XML schema to lay the foundation
of an ontology schema. To achieve this, we present the RELRaE framework, a
framework that employs large language models in different stages to extract and
accurately label the relationships implicitly present in the XML schema. We
investigate the capability of LLMs to accurately generate these labels and then
evaluate them. Our work demonstrates that LLMs can be effectively used to
support the generation of relationship labels in the context of lab automation,
and that they can play a valuable role within semi-automatic ontology
generation frameworks more generally.

</details>


### [47] [Economic Evaluation of LLMs](https://arxiv.org/abs/2507.03834)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 论文提出了一种基于经济评估的LLM性能比较框架，将性能权衡量化为单一数值，发现推理模型在错误成本超过0.01美元时表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法比较具有不同优缺点的LLM的问题，如低成本高错误率模型与高成本高准确率模型的对比。

Method: 提出经济评估框架，将性能权衡量化为基于具体用例经济约束的单一数值（如错误成本、延迟成本等）。

Result: 推理模型在错误成本超过0.01美元时表现更优；单个大型LLM在错误成本低至0.1美元时优于级联模型。

Conclusion: 在自动化重要任务时，应优先使用最强大的模型，因为部署成本远低于错误的经济影响。

Abstract: Practitioners often navigate LLM performance trade-offs by plotting Pareto
frontiers of optimal accuracy-cost trade-offs. However, this approach offers no
way to compare between LLMs with distinct strengths and weaknesses: for
example, a cheap, error-prone model vs a pricey but accurate one. To address
this gap, we propose economic evaluation of LLMs. Our framework quantifies the
performance trade-off of an LLM as a single number based on the economic
constraints of a concrete use case, all expressed in dollars: the cost of
making a mistake, the cost of incremental latency, and the cost of abstaining
from a query. We apply our economic evaluation framework to compare the
performance of reasoning and non-reasoning models on difficult questions from
the MATH benchmark, discovering that reasoning models offer better
accuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds
\$0.01. In addition, we find that single large LLMs often outperform cascades
when the cost of making a mistake is as low as \$0.1. Overall, our findings
suggest that when automating meaningful human tasks with AI models,
practitioners should typically use the most powerful available model, rather
than attempt to minimize AI deployment costs, since deployment costs are likely
dwarfed by the economic impact of AI errors.

</details>


### [48] [Participatory Evolution of Artificial Life Systems via Semantic Feedback](https://arxiv.org/abs/2507.03839)
*Shuowen Li,Kexin Wang,Minglu Fang,Danqi Huang,Ali Asadipour,Haipeng Mi,Yitong Sun*

Main category: cs.AI

TL;DR: 提出了一种语义反馈框架，通过自然语言指导人工生命系统的演化，结合编码器、优化器和评估方法，实现用户意图对视觉和行为规则的调控。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言更直观地指导人工生命系统的演化，提升语义对齐和用户参与度。

Method: 集成提示到参数编码器、CMA-ES优化器和基于CLIP的评估，支持交互式生态系统模拟。

Result: 用户研究表明，相比手动调整，语义对齐效果更好，展示了系统在生成设计和开放式演化中的潜力。

Conclusion: 该框架为参与式生成设计和开放式演化提供了有效平台。

Abstract: We present a semantic feedback framework that enables natural language to
guide the evolution of artificial life systems. Integrating a
prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the
system allows user intent to modulate both visual outcomes and underlying
behavioral rules. Implemented in an interactive ecosystem simulation, the
framework supports prompt refinement, multi-agent interaction, and emergent
rule synthesis. User studies show improved semantic alignment over manual
tuning and demonstrate the system's potential as a platform for participatory
generative design and open-ended evolution.

</details>


### [49] [From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM](https://arxiv.org/abs/2507.03868)
*Xinyi Wu,Yanhao Jia,Luwei Xiao,Shuai Zhao,Fengkuang Chiang,Erik Cambria*

Main category: cs.AI

TL;DR: Uni-Retrieval和Uni-RAG框架通过多模态检索和生成，提升教育内容的多样性和可访问性，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统难以处理教育场景中的多样性和模糊性，需更高效的解决方案。

Method: 开发轻量级多模态检索模块Uni-Retrieval，结合Prompt Bank和MoE-LoRA模块，并与指令调优语言模型集成形成Uni-RAG。

Result: 在SER等基准测试中，Uni-RAG在检索准确性和生成质量上优于基线系统，且计算成本低。

Conclusion: Uni-RAG为智能教育系统提供了可扩展的解决方案，支持个性化、可解释的高效学习辅助。

Abstract: In AI-facilitated teaching, leveraging various query styles to interpret
abstract educational content is crucial for delivering effective and accessible
learning experiences. However, existing retrieval systems predominantly focus
on natural text-image matching and lack the capacity to address the diversity
and ambiguity inherent in real-world educational scenarios. To address this
limitation, we develop a lightweight and efficient multi-modal retrieval
module, named Uni-Retrieval, which extracts query-style prototypes and
dynamically matches them with tokens from a continually updated Prompt Bank.
This Prompt Bank encodes and stores domain-specific knowledge by leveraging a
Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to
enhance Uni-Retrieval's capability to accommodate unseen query types at test
time. To enable natural language educational content generation, we integrate
the original Uni-Retrieval with a compact instruction-tuned language model,
forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given
a style-conditioned query, Uni-RAG first retrieves relevant educational
materials and then generates human-readable explanations, feedback, or
instructional content aligned with the learning objective. Experimental results
on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline
retrieval and RAG systems in both retrieval accuracy and generation quality,
while maintaining low computational cost. Our framework provides a scalable,
pedagogically grounded solution for intelligent educational systems, bridging
retrieval and generation to support personalized, explainable, and efficient
learning assistance across diverse STEM scenarios.

</details>


### [50] [Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing](https://arxiv.org/abs/2507.03870)
*Rahil P Mehta,Yashwanthi Anand,Manish Motwani,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: AIProbe是一种黑盒测试技术，通过差异测试区分自主代理行为错误是源于代理缺陷还是环境不可行性。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理及其环境复杂性增加，区分行为错误来源变得困难但对可靠部署至关重要。

Method: AIProbe生成多样化环境配置和任务，使用拉丁超立方采样修改参数，并通过独立于代理的搜索规划器解决任务，比较代理与规划器性能以识别错误来源。

Result: AIProbe在多个领域中显著优于现有技术，能检测更多总错误和独特错误。

Conclusion: AIProbe有助于可靠部署自主代理，通过高效识别错误来源。

Abstract: When an autonomous agent behaves undesirably, including failure to complete a
task, it can be difficult to determine whether the behavior is due to a
systemic agent error, such as flaws in the model or policy, or an environment
error, where a task is inherently infeasible under a given environment
configuration, even for an ideal agent. As agents and their environments grow
more complex, identifying the error source becomes increasingly difficult but
critical for reliable deployment. We introduce AIProbe, a novel black-box
testing technique that applies differential testing to attribute undesirable
agent behaviors either to agent deficiencies, such as modeling or training
flaws, or due to environmental infeasibility. AIProbe first generates diverse
environmental configurations and tasks for testing the agent, by modifying
configurable parameters using Latin Hypercube sampling. It then solves each
generated task using a search-based planner, independent of the agent. By
comparing the agent's performance to the planner's solution, AIProbe identifies
whether failures are due to errors in the agent's model or policy, or due to
unsolvable task conditions. Our evaluation across multiple domains shows that
AIProbe significantly outperforms state-of-the-art techniques in detecting both
total and unique errors, thereby contributing to a reliable deployment of
autonomous agents.

</details>


### [51] [LLMs model how humans induce logically structured rules](https://arxiv.org/abs/2507.03876)
*Alyssa Loo,Ellie Pavlick,Roman Feiman*

Main category: cs.AI

TL;DR: 论文探讨了神经网络（尤其是大语言模型LLMs）能否作为认知科学的计算模型，通过实验比较LLMs与贝叶斯概率思维语言模型（pLoT），发现LLMs在解释人类逻辑概念方面表现优异且具有独特性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证神经网络（特别是LLMs）是否能解释人类认知的原始表征和计算规则，挑战传统贝叶斯模型（pLoT）的优越性。

Method: 通过四个实验，测试多种LLMs在逻辑规则归纳任务中的表现，并与pLoT模型进行对比。

Result: LLMs在拟合人类行为上至少与pLoT模型相当，且其预测规则的性质与pLoT不同，表明LLMs并非pLoT的简单实现。

Conclusion: LLMs可能提供了一种新的理论框架，用于解释人类逻辑概念的原始表征和计算，值得未来认知科学研究关注。

Abstract: A central goal of cognitive science is to provide a computationally explicit
account of both the structure of the mind and its development: what are the
primitive representational building blocks of cognition, what are the rules via
which those primitives combine, and where do these primitives and rules come
from in the first place? A long-standing debate concerns the adequacy of
artificial neural networks as computational models that can answer these
questions, in particular in domains related to abstract cognitive function,
such as language and logic. This paper argues that recent advances in neural
networks -- specifically, the advent of large language models (LLMs) --
represent an important shift in this debate. We test a variety of LLMs on an
existing experimental paradigm used for studying the induction of rules
formulated over logical concepts. Across four experiments, we find converging
empirical evidence that LLMs provide at least as good a fit to human behavior
as models that implement a Bayesian probablistic language of thought (pLoT),
which have been the best computational models of human behavior on the same
task. Moreover, we show that the LLMs make qualitatively different predictions
about the nature of the rules that are inferred and deployed in order to
complete the task, indicating that the LLM is unlikely to be a mere
implementation of the pLoT solution. Based on these results, we argue that LLMs
may instantiate a novel theoretical account of the primitive representations
and computations necessary to explain human logical concepts, with which future
work in cognitive science should engage.

</details>


### [52] [Agent Exchange: Shaping the Future of AI Agent Economics](https://arxiv.org/abs/2507.03904)
*Yingxuan Yang,Ying Wen,Jun Wang,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出Agent Exchange (AEX)，一个专为AI代理经济设计的拍卖平台，支持代理间的价值交换和协调。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，AI代理从被动工具转变为自主经济参与者，需要新的基础设施支持其经济活动。

Method: AEX借鉴在线广告的实时竞价（RTB）系统，设计为中央拍卖引擎，连接用户侧平台（USP）、代理侧平台（ASP）、代理中心（Agent Hubs）和数据管理平台（DMP）。

Result: AEX为AI代理经济提供了优化的基础设施，支持代理协调和经济参与。

Conclusion: AEX为未来AI生态系统中的代理经济基础设施奠定了基础。

Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from
passive computational tools into autonomous economic actors. This shift marks
the emergence of the agent-centric economy, in which agents take on active
economic roles-exchanging value, making strategic decisions, and coordinating
actions with minimal human oversight. To realize this vision, we propose Agent
Exchange (AEX), a specialized auction platform designed to support the dynamics
of the AI agent marketplace. AEX offers an optimized infrastructure for agent
coordination and economic participation. Inspired by Real-Time Bidding (RTB)
systems in online advertising, AEX serves as the central auction engine,
facilitating interactions among four ecosystem components: the User-Side
Platform (USP), which translates human goals into agent-executable tasks; the
Agent-Side Platform (ASP), responsible for capability representation,
performance tracking, and optimization; Agent Hubs, which coordinate agent
teams and participate in AEX-hosted auctions; and the Data Management Platform
(DMP), ensuring secure knowledge sharing and fair value attribution. We outline
the design principles and system architecture of AEX, laying the groundwork for
agent-based economic infrastructure in future AI ecosystems.

</details>


### [53] [Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models](https://arxiv.org/abs/2507.03916)
*Yifan Jiang,Yibo Xue,Yukun Kang,Pin Zheng,Jian Peng,Feiran Wu,Changliang Xu*

Main category: cs.AI

TL;DR: 论文提出了首个公开的幻灯片动画数据集，并利用LoRA微调Qwen-2.5-VL-7B模型，在动画生成任务中显著优于GPT-4.1和Gemini-2.5-Pro。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的幻灯片生成工具缺乏动画支持，且现有视觉语言模型因缺少公开数据集和时间推理能力不足而难以处理动画任务。

Method: 发布包含12,000组自然语言描述、动画JSON文件和渲染视频的数据集，并使用LoRA微调Qwen-2.5-VL-7B模型。

Result: 模型在BLEU-4、ROUGE-L、SPICE和CODA指标上显著提升，CODA细节得分尤其突出。

Conclusion: 数据集、LoRA增强模型和CODA指标为未来基于VLM的动态幻灯片生成研究提供了基准和基础。

Abstract: Slide animations, such as fade-ins, fly-ins, and wipes, are critical for
audience engagement, efficient information delivery, and vivid visual
expression. However, most AI-driven slide-generation tools still lack native
animation support, and existing vision-language models (VLMs) struggle with
animation tasks due to the absence of public datasets and limited
temporal-reasoning capabilities. To address this gap, we release the first
public dataset for slide-animation modeling: 12,000 triplets of
natural-language descriptions, animation JSON files, and rendered videos,
collectively covering every built-in PowerPoint effect. Using this resource, we
fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent
improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our
Coverage-Order-Detail Assessment (CODA) metric, which evaluates action
coverage, temporal order, and detail fidelity. On a manually curated test set
of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and
shows significant improvements in CODA-detail. This demonstrates that low-rank
adaptation enables reliable temporal reasoning and generalization beyond
synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric
provide a rigorous benchmark and foundation for future research on VLM-based
dynamic slide generation.

</details>


### [54] [CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate](https://arxiv.org/abs/2507.03928)
*Yiliu Sun,Zicheng Zhao,Sheng Wan,Chen Gong*

Main category: cs.AI

TL;DR: CortexDebate是一种新型的多智能体辩论方法，通过稀疏辩论图和MDM模块解决现有MAD方法的输入过长和过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法存在输入过长和过度自信问题，导致辩论效果下降。

Method: 提出CortexDebate方法，构建稀疏辩论图，并引入MDM模块优化图结构。

Result: 在四个任务类型的八个数据集上验证了CortexDebate的有效性。

Conclusion: CortexDebate显著提升了多智能体辩论的效果。

Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues
such as hallucination and inadequate reasoning abilities. To mitigate these
issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where
LLM agents engage in in-depth debates with others on tasks. However, existing
MAD methods face two major issues: (a) too lengthy input contexts, which causes
LLM agents to get lost in plenty of input information and experiences
performance drop; and (b) the overconfidence dilemma, where self-assured LLM
agents dominate the debate, leading to low debating effectiveness. To address
these limitations, we propose a novel MAD method called "CortexDebate".
Inspired by the human brain's tendency to establish a sparse and dynamically
optimized network among cortical areas governed by white matter, CortexDebate
constructs a sparse debating graph among LLM agents, where each LLM agent only
debates with the ones that are helpful to it. To optimize the graph, we propose
a module named McKinsey-based Debate Matter (MDM), which acts as an artificial
analog to white matter. By integrating the McKinsey Trust Formula, a
well-established measure of trustworthiness from sociology, MDM enables
credible evaluations that guide graph optimization. The effectiveness of our
CortexDebate has been well demonstrated by extensive experimental results
across eight datasets from four task types.

</details>


### [55] [An ASP-Based Framework for MUSes](https://arxiv.org/abs/2507.03929)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.AI

TL;DR: 该论文提出了一种基于答案集编程（ASP）的框架MUS-ASP，用于在线枚举最小不可满足子集（MUS），并展示了其在MUS枚举和计数任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 理解不可满足公式的核心原因对许多应用至关重要，而最小不可满足子集（MUS）是捕捉这一原因的有效方法。当前研究主要集中在枚举MUS和计数MUS数量上。

Method: 通过将MUS枚举问题转化为答案集求解问题，利用ASP在知识表示和解决复杂组合问题上的优势，设计了MUS-ASP框架。

Result: 实验评估表明，MUS-ASP在MUS枚举和计数任务中表现出色，尤其是在与混合求解器集成时显著提升了效率。

Conclusion: MUS-ASP框架通过结合ASP的高效计算能力，为MUS枚举和计数任务提供了有效的解决方案。

Abstract: Given an unsatisfiable formula, understanding the core reason for
unsatisfiability is crucial in several applications. One effective way to
capture this is through the minimal unsatisfiable subset (MUS), the
subset-minimal set of clauses that remains unsatisfiable. Current research
broadly focuses on two directions: (i) enumerating as many MUSes as possible
within a given time limit, and (ii) counting the total number of MUSes for a
given unsatisfiable formula.
  In this paper, we introduce an answer set programming-based framework, named
MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for
its strengths in knowledge representation and is particularly suitable for
specifying complex combinatorial problems. By translating MUS enumeration into
answer set solving, MUS-ASP leverages the computational efficiency of
state-of-the-art ASP systems. Our extensive experimental evaluation
demonstrates the effectiveness of MUS-ASP and highlights the acceleration in
both MUS enumeration and counting tasks, particularly when integrated within
hybrid solvers, including the framework proposed in this paper.

</details>


### [56] [Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features](https://arxiv.org/abs/2507.03998)
*Thuy An Ha,Bao Quoc Vo*

Main category: cs.AI

TL;DR: 论文探讨了如何通过结合数据无关特征和隐藏状态特征来提升大型语言模型（LLM）在跨领域任务中的不确定性量化性能，但实验结果并不一致。


<details>
  <summary>Details</summary>
Motivation: LLM常生成高自信但事实错误的回答，需改进其不确定性量化方法以评估输出质量。

Method: 结合数据无关特征与隐藏状态特征，并筛选最具信息量的隐藏状态特征，以提升跨领域性能。

Result: 引入数据无关特征通常能提升性能，但在某些情况下会降低效果；筛选隐藏状态特征也未一致提升性能。

Conclusion: 数据无关特征在某些情况下被低估，导致实验结果不一致，需进一步研究其权重分配问题。

Abstract: Large Language Models (LLMs) often generate responses that are factually
incorrect yet expressed with high confidence, which can pose serious risks for
end users. To address this, it is essential for LLMs not only to produce
answers but also to provide accurate estimates of their correctness.
Uncertainty quantification methods have been introduced to assess the quality
of LLM outputs, with factual accuracy being a key aspect of that quality. Among
these methods, those that leverage hidden states to train probes have shown
particular promise, as these internal representations encode information
relevant to the factuality of responses, making this approach the focus of this
paper. However, the probe trained on the hidden states of one dataset often
struggles to generalise to another dataset of a different task or domain. To
address this limitation, we explore combining data-agnostic features with
hidden-state features and assess whether this hybrid feature set enhances
out-of-domain performance. We further examine whether selecting only the most
informative hidden-state features, thereby discarding task-specific noise,
enables the data-agnostic features to contribute more effectively. The
experiment results indicate that although introducing data-agnostic features
generally enhances generalisation performance in most cases, in certain
scenarios their inclusion degrades performance. A similar pattern emerges when
retaining only the most important hidden-state features - adding data-agnostic
features does not consistently further enhance performance compared to using
the full set of hidden-state features. A closer analysis reveals that, in some
specific cases, the trained probe underweights the data-agnostic features
relative to the hidden-state features, which we believe is the main reason why
the results are inconclusive.

</details>


### [57] [Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving](https://arxiv.org/abs/2507.04034)
*Weizhi Tang,Kwabena Nuamah,Vaishak Belle*

Main category: cs.AI

TL;DR: Lyria是一个结合LLM语义理解和遗传算法全局搜索的框架，通过实验验证其有效性，并分析影响性能的因素。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多目标优化、约束满足和大解空间等复杂问题上的局限性。

Method: 结合LLM的语义理解能力和遗传算法的全局搜索优化能力，提出Lyria框架，包含7个关键组件。

Result: 在4种LLM和3类问题上的实验证明Lyria有效，并通过7项消融实验分析性能影响因素。

Conclusion: Lyria通过结合LLM和遗传算法的优势，有效解决复杂问题，并明确了性能关键因素。

Abstract: While Large Language Models (LLMs) have demonstrated impressive abilities
across various domains, they still struggle with complex problems characterized
by multi-objective optimization, precise constraint satisfaction, immense
solution spaces, etc. To address the limitation, drawing on the superior
semantic understanding ability of LLMs and also the outstanding global search
and optimization capability of genetic algorithms, we propose to capitalize on
their respective strengths and introduce Lyria, a general LLM-driven genetic
algorithm framework, comprising 7 essential components. Through conducting
extensive experiments with 4 LLMs across 3 types of problems, we demonstrated
the efficacy of Lyria. Additionally, with 7 additional ablation experiments, we
further systematically analyzed and elucidated the factors that affect its
performance.

</details>


### [58] [Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments](https://arxiv.org/abs/2507.04037)
*Zheng Jia,Shengbin Yue,Wei Chen,Siyuan Wang,Yidong Liu,Yun Song,Zhongyu Wei*

Main category: cs.AI

TL;DR: 论文介绍了J1-ENVS和J1-EVAL，用于评估LLM代理在动态法律环境中的表现，发现现有模型在动态执行方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 解决静态基准与动态法律实践之间的差距，推动法律智能的发展。

Method: 开发了交互式动态法律环境J1-ENVS和细粒度评估框架J1-EVAL，并在17个LLM代理上进行实验。

Result: 实验显示，许多模型在法律知识上表现良好，但在动态环境中的程序执行上表现不佳，即使是GPT-4o也未达到60%的整体性能。

Conclusion: 动态法律智能仍面临挑战，研究结果为未来方向提供了重要参考。

Abstract: The gap between static benchmarks and the dynamic nature of real-world legal
practice poses a key barrier to advancing legal intelligence. To this end, we
introduce J1-ENVS, the first interactive and dynamic legal environment tailored
for LLM-based agents. Guided by legal experts, it comprises six representative
scenarios from Chinese legal practices across three levels of environmental
complexity. We further introduce J1-EVAL, a fine-grained evaluation framework,
designed to assess both task performance and procedural compliance across
varying levels of legal proficiency. Extensive experiments on 17 LLM agents
reveal that, while many models demonstrate solid legal knowledge, they struggle
with procedural execution in dynamic settings. Even the SOTA model, GPT-4o,
falls short of 60% overall performance. These findings highlight persistent
challenges in achieving dynamic legal intelligence and offer valuable insights
to guide future research.

</details>


### [59] [HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration](https://arxiv.org/abs/2507.04067)
*Yuyang Cheng,Yumiao Xu,Chaojia Yu,Yong Zhao*

Main category: cs.AI

TL;DR: HAWK是一个模块化框架，解决多智能体系统的互操作性、任务调度和资源共享问题，通过五层架构和标准化接口实现高效协作。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在跨平台互操作性、动态任务调度和资源共享方面存在挑战，缺乏标准化接口和灵活协作框架。

Method: 提出HAWK框架，包含五层（用户、工作流、操作者、智能体、资源）和十六个标准化接口，支持任务解析、工作流编排、智能调度等功能。

Result: 通过CreAgentive原型验证，HAWK提高了吞吐量、降低了调用复杂度，并增强了系统可控性。

Conclusion: HAWK展示了在多领域的应用潜力，未来研究方向包括幻觉缓解、实时性能优化和跨域适应性提升。

Abstract: Contemporary multi-agent systems encounter persistent challenges in
cross-platform interoperability, dynamic task scheduling, and efficient
resource sharing. Agents with heterogeneous implementations often lack
standardized interfaces; collaboration frameworks remain brittle and hard to
extend; scheduling policies are static; and inter-agent state synchronization
is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular
framework comprising five layers-User, Workflow, Operator, Agent, and
Resource-and supported by sixteen standardized interfaces. HAWK delivers an
end-to-end pipeline covering task parsing, workflow orchestration, intelligent
scheduling, resource invocation, and data synchronization. At its core lies an
adaptive scheduling and optimization module in the Workflow Layer, which
harnesses real-time feedback and dynamic strategy adjustment to maximize
utilization. The Resource Layer provides a unified abstraction over
heterogeneous data sources, large models, physical devices, and third-party
services&tools, simplifying cross-domain information retrieval. We demonstrate
HAWK's scalability and effectiveness via CreAgentive, a multi-agent
novel-generation prototype, which achieves marked gains in throughput, lowers
invocation complexity, and improves system controllability. We also show how
hybrid deployments of large language models integrate seamlessly within HAWK,
highlighting its flexibility. Finally, we outline future research
avenues-hallucination mitigation, real-time performance tuning, and enhanced
cross-domain adaptability-and survey prospective applications in healthcare,
government, finance, and education.

</details>


### [60] [How to Train Your LLM Web Agent: A Statistical Diagnosis](https://arxiv.org/abs/2507.04103)
*Dheeraj Vattikonda,Santhoshi Ravichandran,Emiliano Penaloza,Hadi Nekoei,Megh Thakkar,Thibault Le Sellier de Chezelles,Nicolas Gontier,Miguel Muñoz-Mármol,Sahar Omidi Shayegan,Stefania Raimondo,Xue Liu,Alexandre Drouin,Laurent Charlin,Alexandre Piché,Alexandre Lacoste,Massimo Caccia*

Main category: cs.AI

TL;DR: 该论文提出了一种基于两阶段管道的LLM网络代理训练方法，结合监督微调和强化学习，显著降低了计算成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 开源LLM网络代理与闭源系统之间的差距日益扩大，主要由于单步任务局限性和高计算成本。

Method: 采用两阶段管道：先通过监督微调训练学生模型模仿教师模型，再进行策略强化学习。

Result: 结合监督微调和强化学习的策略在性能上优于单独使用任一方法，且计算成本降低55%。

Conclusion: 该方法有效推动了计算性能的帕累托前沿，缩小了与闭源模型的差距。

Abstract: LLM-based web agents have recently made significant progress, but much of it
has occurred in closed-source systems, widening the gap with open-source
alternatives. Progress has been held back by two key challenges: first, a
narrow focus on single-step tasks that overlooks the complexity of multi-step
web interactions; and second, the high compute costs required to post-train
LLM-based web agents. To address this, we present the first statistically
grounded study on compute allocation for LLM web-agent post-training. Our
approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate
a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy
reinforcement learning. We find this process highly sensitive to hyperparameter
choices, making exhaustive sweeps impractical. To spare others from expensive
trial-and-error, we sample 1,370 configurations and use bootstrapping to
estimate effective hyperparameters. Our results show that combining SFT with
on-policy RL consistently outperforms either approach alone on both WorkArena
and MiniWob++. Further, this strategy requires only 55% of the compute to match
the peak performance of pure SFT on MiniWob++, effectively pushing the
compute-performance Pareto frontier, and is the only strategy that can close
the gap with closed-source models.

</details>


### [61] [Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing](https://arxiv.org/abs/2507.04105)
*Jinwei Hu,Yi Dong,Zhengtao Ding,Xiaowei Huang*

Main category: cs.AI

TL;DR: 提出了一种用于增强大型语言模型（LLM）赋能的多智能体系统（MAS）安全性的防御框架，适用于航空航天等安全关键领域。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，LLM赋能的MAS容易受到对抗性影响，需要一种可扩展且实用的方法来确保其安全性。

Method: 采用随机平滑技术，结合两阶段自适应采样机制，在无需传统验证方法的黑盒设置下提供概率保证。

Result: 仿真结果表明，该方法能有效阻止对抗性行为和幻觉传播，同时保持共识性能。

Conclusion: 为LLM赋能的MAS在现实高风险环境中的安全部署提供了可行且可扩展的解决方案。

Abstract: This paper presents a defense framework for enhancing the safety of large
language model (LLM) empowered multi-agent systems (MAS) in safety-critical
domains such as aerospace. We apply randomized smoothing, a statistical
robustness certification technique, to the MAS consensus context, enabling
probabilistic guarantees on agent decisions under adversarial influence. Unlike
traditional verification methods, our approach operates in black-box settings
and employs a two-stage adaptive sampling mechanism to balance robustness and
computational efficiency. Simulation results demonstrate that our method
effectively prevents the propagation of adversarial behaviors and
hallucinations while maintaining consensus performance. This work provides a
practical and scalable path toward safe deployment of LLM-based MAS in
real-world, high-stakes environments.

</details>


### [62] [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://arxiv.org/abs/2507.04136)
*Saksham Sahai Srivastava,Vaneet Aggarwal*

Main category: cs.AI

TL;DR: 该综述探讨了强化学习（RL）在大型语言模型（LLMs）中的应用，重点介绍了RLHF、RLAIF、DPO和GRPO等方法，并分析了其在指令遵循、伦理对齐和推理能力方面的作用。


<details>
  <summary>Details</summary>
Motivation: 通过RL提升LLMs的能力，解决指令遵循、伦理对齐和推理等关键挑战。

Method: 综述了RL与LLMs的集成方法，包括PPO、Q-Learning、Actor-Critic等算法，以及RLHF、RLAIF、DPO和GRPO等专门技术。

Result: RLHF在模型对齐中占主导，RLVR显著提升逐步推理能力，但仍存在奖励破解、计算成本和反馈收集等挑战。

Conclusion: 未来方向包括混合RL算法、验证器引导训练和多目标对齐框架，为RL驱动的LLM发展提供路线图。

Abstract: Reinforcement Learning (RL) has emerged as a transformative approach for
aligning and enhancing Large Language Models (LLMs), addressing critical
challenges in instruction following, ethical alignment, and reasoning
capabilities. This survey offers a comprehensive foundation on the integration
of RL with language models, highlighting prominent algorithms such as Proximal
Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,
it provides an extensive technical overview of RL techniques specifically
tailored for LLMs, including foundational methods like Reinforcement Learning
from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced
strategies such as Direct Preference Optimization (DPO) and Group Relative
Policy Optimization (GRPO). We systematically analyze their applications across
domains, i.e., from code generation to tool-augmented reasoning. We also
present a comparative taxonomy based on reward modeling, feedback mechanisms,
and optimization strategies. Our evaluation highlights key trends. RLHF remains
dominant for alignment, and outcome-based RL such as RLVR significantly
improves stepwise reasoning. However, persistent challenges such as reward
hacking, computational costs, and scalable feedback collection underscore the
need for continued innovation. We further discuss emerging directions,
including hybrid RL algorithms, verifier-guided training, and multi-objective
alignment frameworks. This survey serves as a roadmap for researchers advancing
RL-driven LLM development, balancing capability enhancement with safety and
scalability.

</details>


### [63] [Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model](https://arxiv.org/abs/2507.04206)
*Sibei Liu,Zhijian Hu*

Main category: cs.AI

TL;DR: 论文通过热力学类比（Mpemba效应）解释了LLM训练中学习率调度策略（WSD）的机制，提出高平台学习率能加速损失下降，并推导了最优平台学习率（强Mpemba点）的存在条件。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练中的学习率调度策略（如WSD）缺乏理论解释，平台高度和衰减计划多依赖经验。论文旨在通过热力学类比提供理论支持。

Method: 采用“谷-河”损失景观模型，结合Mpemba效应分析学习率调度，推导最优平台学习率的存在条件及衰减动力学。

Result: 发现高平台学习率能加速损失下降，存在“强Mpemba点”使最慢模式消失，从而加快收敛。

Conclusion: 研究为基于平台的学习率调度提供了理论依据，并指导LLM中学习率的调参，减少超参数搜索。

Abstract: Learning rate (LR) schedules in large language model (LLM) training often
follow empirical templates: warm-up, constant plateau/stable phase, and decay
(WSD). However, the mechanistic explanation for this strategy remains
underexplored, and the choice of plateau height and decay schedule is largely
heuristic. In this paper, we connect training dynamics to a thermodynamic
analogy via the Mpemba effect - a phenomenon in which a hotter system cools
faster than a colder one when quenched into the same bath. We analyze a class
of "valley-river" loss landscapes, where sharp (valley) directions equilibrate
quickly, while flatter (river) directions govern global descent. The Mpemba
effect provides an explanation for the necessity of the warm-up phase and
motivates a high plateau - rather than a low one - for accelerating loss
decrease during decay. We show that for certain loss landscapes, there exists
an optimal plateau learning rate - the "strong Mpemba point" - at which the
slowest mode vanishes, resulting in faster convergence during the decay phase.
We derive analytical conditions for its existence and estimate decay dynamics
required to preserve the Mpemba advantage. Our minimal model and analysis offer
a principled justification for plateau-based schedulers and provide guidance
for tuning LR in LLMs with minimal hyperparameter sweep.

</details>


### [64] [Clustering via Self-Supervised Diffusion](https://arxiv.org/abs/2507.04283)
*Roy Uziel,Irit Chelly,Oren Freifeld,Ari Pakman*

Main category: cs.AI

TL;DR: CLUDI是一种结合扩散模型和预训练Vision Transformer的自监督聚类框架，通过师生范式实现高性能聚类。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但尚未应用于聚类任务，因此提出CLUDI以填补这一空白。

Method: CLUDI采用师生范式，教师模型通过扩散采样生成多样聚类分配，学生模型优化为稳定预测。

Result: CLUDI在多个数据集上实现了最先进的聚类性能，适应复杂数据分布。

Conclusion: CLUDI为聚类任务提供了新的解决方案，展示了扩散模型在非生成任务中的潜力。

Abstract: Diffusion models, widely recognized for their success in generative tasks,
have not yet been applied to clustering. We introduce Clustering via Diffusion
(CLUDI), a self-supervised framework that combines the generative power of
diffusion models with pre-trained Vision Transformer features to achieve robust
and accurate clustering. CLUDI is trained via a teacher-student paradigm: the
teacher uses stochastic diffusion-based sampling to produce diverse cluster
assignments, which the student refines into stable predictions. This
stochasticity acts as a novel data augmentation strategy, enabling CLUDI to
uncover intricate structures in high-dimensional data. Extensive evaluations on
challenging datasets demonstrate that CLUDI achieves state-of-the-art
performance in unsupervised classification, setting new benchmarks in
clustering robustness and adaptability to complex data distributions.

</details>


### [65] [Answer Set Programming Modulo Theories and Reasoning about Continuous Changes](https://arxiv.org/abs/2507.04299)
*Joohyung Lee,Yunsong Meng*

Main category: cs.AI

TL;DR: ASPMT是ASP与SMT紧密结合的新框架，类似于一阶逻辑与SMT的关系，通过固定背景理论的解释实现。类似于ASP与SAT的关系，紧致的ASPMT程序可转换为SMT实例。通过增强动作语言C+处理连续和离散变化，展示了ASPMT的实用性。


<details>
  <summary>Details</summary>
Motivation: 结合ASP与SMT的优势，解决复杂问题（如动作语言C+中的连续与离散变化）。

Method: 基于功能稳定模型语义，固定背景理论的解释，将ASPMT程序转换为SMT实例。

Result: 成功将ASPMT应用于动作语言C+，实现连续和离散变化的处理，并展示了累积效应。

Conclusion: ASPMT为复杂问题提供了新解决方案，扩展了动作语言C+的能力。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight
integration of answer set programming (ASP) and satisfiability modulo theories
(SMT). Similar to the relationship between first-order logic and SMT, it is
based on a recent proposal of the functional stable model semantics by fixing
interpretations of background theories. Analogously to a known relationship
between ASP and SAT, ``tight'' ASPMT programs can be translated into SMT
instances. We demonstrate the usefulness of ASPMT by enhancing action language
C+ to handle continuous changes as well as discrete changes. We reformulate the
semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to
compute the language. We also show how the language can represent cumulative
effects on continuous resources.

</details>


### [66] [Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems](https://arxiv.org/abs/2507.04338)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.AI

TL;DR: 提出了一种可配置的胜者通吃电路，用于神经形态计算，具有低功耗和高效能。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算需要低功耗的学习单元，胜者通吃电路是关键组件之一。

Method: 设计了一种可配置的胜者通吃电路，支持k-winner和滞后特性，并在IBM 65 nm工艺节点上进行了仿真。

Result: 电路功耗为34.9 μW，延迟为10.4 ns，可处理1000个输入，适用于空间滤波和分类任务。

Conclusion: 该电路在神经形态计算中表现出高效能和低功耗，具有实际应用潜力。

Abstract: Recent advances in neuromorphic computing demonstrate on-device learning
capabilities with low power consumption. One of the key learning units in these
systems is the winner-take-all circuit. In this research, we propose a
winner-take-all circuit that can be configured to achieve k-winner and
hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9
$\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The
utility of the circuit is demonstrated for spatial filtering and
classification.

</details>


### [67] [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
*Xingyang He,Xiao Ling,Jie Liu*

Main category: cs.AI

TL;DR: SmartThinker是一个两阶段学习框架，通过细粒度控制推理步骤长度，减少冗余推理，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理时存在冗余和低效问题，传统的全局长度惩罚方法会导致关键推理步骤被过度压缩，而简单步骤保留不必要细节。

Method: SmartThinker采用两阶段方法：1）通过拒绝采样和监督微调（SFT）适应短推理模式；2）通过Step-Level Length Control Policy Optimization（SCPO）优化模型输出分布，动态调整关键步骤和次要步骤的长度分配。

Result: 在多个推理基准测试和不同主干模型上，SmartThinker显著减少了冗余推理，性能与现有方法相当或更优。

Conclusion: SmartThinker通过细粒度控制推理步骤长度，有效解决了推理冗余问题，同时保持了高性能。

Abstract: Large reasoning models (LRMs) have exhibited remarkable reasoning
capabilities through inference-time scaling, but this progress has also
introduced considerable redundancy and inefficiency into their reasoning
processes, resulting in substantial computational waste. Previous work has
attempted to mitigate this issue by penalizing the overall length of generated
samples during reinforcement learning (RL), with the goal of encouraging a more
concise chains of thought. However, we observe that such global length penalty
often lead to excessive compression of critical reasoning steps while
preserving unnecessary details in simpler ones, yielding a suboptimal trade-off
between accuracy and efficiency. To address this issue, we propose
SmartThinker, a two-stage learnable framework designed to enable fine-grained
control over the length of reasoning chains based on the importance of each
individual step. In the first stage, SmartThinker adapts a reasoning model to a
short-form reasoning mode through rejection sampling combined with supervised
fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length
Control Policy Optimization (SCPO) to refine the model output distribution,
which increases the proportion of length allocated to critical steps while
reducing redundancy in less important ones. SCPO consists of four core
components: an online importance estimator, a step-level length control reward
function, a step-level generalized advantage estimation (S-GAE) and a
difficulty-adaptive clipping strategy. Working in concert, these components
enable SCPO to implement differentiated length control across reasoning steps.
Empirical results across multiple reasoning benchmarks and various backbone
models demonstrate that SmartThinker significantly reduces redundant reasoning
while achieving comparable or even superior performance to existing methods.

</details>


### [68] [WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis](https://arxiv.org/abs/2507.04370)
*Yifei Gao,Junhong Ye,Jiaqi Wang,Jitao Sang*

Main category: cs.AI

TL;DR: WebSynthesis框架通过虚拟环境模拟和树状规划解决LLM在动态网页导航中的环境不稳定和高成本问题，性能媲美真实数据训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂动态网页导航中环境状态不可控和API成本高的问题。

Method: 提出WebSynthesis框架，利用学习的世界模型模拟虚拟网页环境，支持树状规划生成高质量轨迹。

Result: 在小规模合成数据集上训练的模型性能媲美或超越大规模真实数据训练的模型。

Conclusion: WebSynthesis为LLM提供了可扩展的自我改进方法，解决了环境不稳定和高成本问题。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved the capabilities of web agents. However, effectively navigating
complex and dynamic web environments still requires more advanced
trajectory-level planning and execution. Prior studies have addressed
self-improving agents by collecting extensive GUI trajectories from
real-environment interactions. Despite their effectiveness, these approaches
encounter two critical challenges: (1) Uncontrollable environment states, where
real or sandboxed web environments often yield unstable and non-deterministic
feedback, complicating the reproduction and debugging of agent behaviors; and
(2) High API costs, as generating even a single interaction trajectory can
involve hundreds of queries, leading to considerable API usage and
computational expenses. To address these limitations and enable scalable
self-improvement for agents, we propose WebSynthesis, a novel framework for
trajectory synthesis and training. WebSynthesis leverages a learned world model
to simulate virtual web environments, allowing a policy agent to perform
efficient and reversible tree-based planning. This approach supports the
large-scale generation of diverse and high-quality trajectories, which are
subsequently utilized to refine the agent's policy. Experimental results
demonstrate that an agent trained using WebSynthesis on a small-scale synthetic
dataset achieves performance comparable to or even surpassing that of models
trained on large-scale real-world data.

</details>


### [69] [DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting](https://arxiv.org/abs/2507.04381)
*Bing Fan,Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.AI

TL;DR: DC-Mamber结合Mamba和线性Transformer，通过双通道策略分别提取局部和全局特征，提升了多元时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Transformer和Mamba）在多元时间序列预测中各有局限：Transformer全局依赖强但计算复杂度高，Mamba计算高效但全局信息整合不足。

Method: 提出DC-Mamber模型，Mamba通道提取变量内特征（通道独立），Transformer通道建模跨时间步全局依赖（通道混合），最后融合双通道特征进行预测。

Result: 在八个公开数据集上验证，DC-Mamber的准确性优于现有模型。

Conclusion: DC-Mamber通过结合Mamba和Transformer的优势，有效解决了多元时间序列预测中的局部与全局建模问题。

Abstract: In multivariate time series forecasting (MTSF), existing strategies for
processing sequences are typically categorized as channel-independent and
channel-mixing. The former treats all temporal information of each variable as
a token, focusing on capturing local temporal features of individual variables,
while the latter constructs a token from the multivariate information at each
time step, emphasizing the modeling of global temporal dependencies. Current
mainstream models are mostly based on Transformer and the emerging Mamba.
Transformers excel at modeling global dependencies through self-attention
mechanisms but exhibit limited sensitivity to local temporal patterns and
suffer from quadratic computational complexity, restricting their efficiency in
long-sequence processing. In contrast, Mamba, based on state space models
(SSMs), achieves linear complexity and efficient long-range modeling but
struggles to aggregate global contextual information in parallel. To overcome
the limitations of both models, we propose DC-Mamber, a dual-channel
forecasting model based on Mamba and linear Transformer for time series
forecasting. Specifically, the Mamba-based channel employs a
channel-independent strategy to extract intra-variable features, while the
Transformer-based channel adopts a channel-mixing strategy to model
cross-timestep global dependencies. DC-Mamber first maps the raw input into two
distinct feature representations via separate embedding layers. These
representations are then processed by a variable encoder (built on Mamba) and a
temporal encoder (built on linear Transformer), respectively. Finally, a fusion
layer integrates the dual-channel features for prediction. Extensive
experiments on eight public datasets confirm DC-Mamber's superior accuracy over
existing models.

</details>


### [70] [LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers](https://arxiv.org/abs/2507.04404)
*Jingze Zhu,Yongliang Wu,Wenbo Zhu,Jiawang Cao,Yanqiang Zheng,Jiawei Chen,Xu Yang,Bernt Schiele,Jonas Fischer,Xinting Hu*

Main category: cs.AI

TL;DR: 提出了一种基于注意力分析的对比解码方法，通过联合利用token和layer信号，提升大语言模型的事实生成能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务中存在事实错误问题，现有方法未能充分利用token和layer的联合动态。

Method: 引入token感知、layer定位的对比解码方法，通过抑制特定token在特定层的注意力，生成对比信号指导解码。

Result: 实验表明，该方法无需额外训练或模型修改，显著提升了多种大语言模型和基准测试的事实性。

Conclusion: 该方法通过联合利用token和layer信号，有效提升了模型的事实生成能力。

Abstract: Large language models (LLMs) excel at natural language understanding and
generation but remain vulnerable to factual errors, limiting their reliability
in knowledge-intensive tasks. While decoding-time strategies provide a
promising efficient solution without training, existing methods typically treat
token-level and layer-level signals in isolation, overlooking the joint
dynamics between them. In this work, we introduce a token-aware,
layer-localized contrastive decoding method that aligns specific token types
with their most influential transformer layers to improve factual generation.
Through empirical attention analysis, we identify two key patterns: punctuation
tokens receive dominant attention in early layers, while conceptual tokens
govern semantic reasoning in intermediate layers. By selectively suppressing
attention to these token types at their respective depths, we achieve the
induction of controlled factual degradation and derive contrastive signals to
guide the final factual decoding. Our method requires no additional training or
model modification, and experiments demonstrate that our method consistently
improves factuality across multiple LLMs and various benchmarks.

</details>


### [71] [ARMR: Adaptively Responsive Network for Medication Recommendation](https://arxiv.org/abs/2507.04428)
*Feiyue Wu,Tianxing Wu,Shenqi Jing*

Main category: cs.AI

TL;DR: 提出了一种自适应响应网络（ARMR）用于药物推荐，通过分段时间学习和动态调整机制，平衡历史药物和新药物的使用，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡历史药物和新药物的使用，无法适应患者病情变化。

Method: ARMR结合分段时间学习（区分近期和远期病史）和自适应响应机制（动态调整对新旧药物的关注）。

Result: 在MIMIC-III和MIMIC-IV数据集上，ARMR在不同评估指标上优于现有基线方法。

Conclusion: ARMR提供了更个性化和准确的药物推荐，代码已开源。

Abstract: Medication recommendation is a crucial task in healthcare, especially for
patients with complex medical conditions. However, existing methods often
struggle to effectively balance the reuse of historical medications with the
introduction of new drugs in response to the changing patient conditions. In
order to address this challenge, we propose an Adaptively Responsive network
for Medication Recommendation (ARMR), a new method which incorporates 1) a
piecewise temporal learning component that distinguishes between recent and
distant patient history, enabling more nuanced temporal understanding, and 2)
an adaptively responsive mechanism that dynamically adjusts attention to new
and existing drugs based on the patient's current health state and medication
history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR
has better performance compared with the state-of-the-art baselines in
different evaluation metrics, which contributes to more personalized and
accurate medication recommendations. The source code is publicly avaiable at:
https://github.com/seucoin/armr2.

</details>


### [72] [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
*Debodeep Banerjee,Burcu Sayin,Stefano Teso,Andrea Passerini*

Main category: cs.AI

TL;DR: MedGellan是一个轻量级、无需标注的框架，利用大语言模型（LLM）从原始医疗记录生成临床指导，帮助医生预测诊断，初步实验显示其能提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医疗决策至关重要，错误可能导致严重后果。完全自动化仍具挑战性，因此结合机器智能与人工监督的混合框架成为实用选择。

Method: MedGellan采用贝叶斯启发的提示策略，尊重临床数据的时间顺序，利用LLM生成临床指导。

Result: 初步实验表明，MedGellan生成的指导能显著提升诊断性能，尤其是在召回率和F1分数上。

Conclusion: MedGellan为医疗决策提供了一种有效的混合框架，结合了LLM的优势与医生的专业判断。

Abstract: Medical decision-making is a critical task, where errors can result in
serious, potentially life-threatening consequences. While full automation
remains challenging, hybrid frameworks that combine machine intelligence with
human oversight offer a practical alternative. In this paper, we present
MedGellan, a lightweight, annotation-free framework that uses a Large Language
Model (LLM) to generate clinical guidance from raw medical records, which is
then used by a physician to predict diagnoses. MedGellan uses a
Bayesian-inspired prompting strategy that respects the temporal order of
clinical data. Preliminary experiments show that the guidance generated by the
LLM with MedGellan improves diagnostic performance, particularly in recall and
$F_1$ score.

</details>


### [73] [A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories](https://arxiv.org/abs/2507.04439)
*Videep Venkatesha,Mary Cati Poulos,Christopher Steadman,Caitlin Mills,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.AI

TL;DR: 该研究通过语言特征分析自发思维（如Deja Vu、非自愿自传体记忆和意外思维），验证并更新了现有理论，揭示了这些思维类型的语言模式特点。


<details>
  <summary>Details</summary>
Motivation: 探索自发思维（如Deja Vu等）的语言表达特征，以更深入地理解其认知、情感和注意力的动态交互。

Method: 分析参与者对这些思维类型的语言描述，提取其语言模式的特征。

Result: Deja Vu表现为抽象和空间语言，非自愿自传体记忆富含个人和情感细节，意外思维则具有不可预测性和认知干扰特征。

Conclusion: 语言可作为研究自发认知状态的重要窗口，为相关理论提供新支持。

Abstract: The onset of spontaneous thoughts are reflective of dynamic interactions
between cognition, emotion, and attention. Typically, these experiences are
studied through subjective appraisals that focus on their triggers,
phenomenology, and emotional salience. In this work, we use linguistic
signatures to investigate Deja Vu, Involuntary Autobiographical Memories and
Unexpected Thoughts. Specifically, we analyze the inherent characteristics of
the linguistic patterns in participant generated descriptions of these thought
types. We show how, by positioning language as a window into spontaneous
cognition, existing theories on these attentional states can be updated and
reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is
a metacognitive experience characterized by abstract and spatial language,
Involuntary Autobiographical Memories are rich in personal and emotionally
significant detail, and Unexpected Thoughts are marked by unpredictability and
cognitive disruption. This work is demonstrative of languages potential to
reveal deeper insights into how internal spontaneous cognitive states manifest
through expression.

</details>


### [74] [Anomalous Decision Discovery using Inverse Reinforcement Learning](https://arxiv.org/abs/2507.04464)
*Ashish Bastola,Mert D. Pesé,Long Cheng,Jonathon Smereka,Abolfazl Razi*

Main category: cs.AI

TL;DR: 提出了一种基于逆强化学习（IRL）的异常检测框架TRAP，用于自动驾驶车辆，解决了现有方法在噪声鲁棒性和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在未见场景、传感器噪声和遮挡下效果不佳，且监督学习需要大量标注数据，限制了实际应用。

Method: 采用IRL框架TRAP，通过隐式学习时间信用分配和预训练，结合可变时间跨度采样，实现早期异常检测。

Result: 在14,000+模拟轨迹上表现优异，AUC达0.90，F1-score为82.2%，显著优于基线方法。

Conclusion: TRAP在噪声鲁棒性和泛化性上表现突出，为自动驾驶异常检测提供了有效解决方案。

Abstract: Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by
identifying unusual behaviors through perception systems that could compromise
safety and lead to hazardous situations. Current approaches, which often rely
on predefined thresholds or supervised learning paradigms, exhibit reduced
efficacy when confronted with unseen scenarios, sensor noise, and occlusions,
leading to potential safety-critical failures. Moreover, supervised methods
require large annotated datasets, limiting their real-world feasibility. To
address these gaps, we propose an anomaly detection framework based on Inverse
Reinforcement Learning (IRL) to infer latent driving intentions from sequential
perception data, thus enabling robust identification. Specifically, we present
Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework
for anomaly detection, to address two critical limitations of existing methods:
noise robustness and generalization to unseen scenarios. Our core innovation is
implicitly learning temporal credit assignments via reward and worst-case
supervision. We leverage pre-training with variable-horizon sampling to
maximize time-to-consequence, resulting in early detection of behavior
deviation. Experiments on 14,000+ simulated trajectories demonstrate
state-of-the-art performance, achieving 0.90 AUC and 82.2\% F1-score -
outperforming similarly trained supervised and unsupervised baselines by 39\%
on Recall and 12\% on F1-score, respectively. Similar performance is achieved
while exhibiting robustness to various noise types and generalization to unseen
anomaly types. Our code will be available at:
https://github.com/abastola0/TRAP.git

</details>


### [75] [Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference](https://arxiv.org/abs/2507.04494)
*Niels Leadholm,Viviane Clay,Scott Knudstrup,Hojae Lee,Jeff Hawkins*

Main category: cs.AI

TL;DR: 论文提出了一种名为Monty的千脑系统，模拟生物智能的模块化结构，通过传感器运动学习和模块间交互实现高效泛化和快速推理。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽在任务表现上出色，但缺乏生物智能的核心特征，如快速持续学习、基于传感器运动的表征和结构化知识。

Method: 利用YCB数据集评估Monty在3D物体感知任务中的表现，包括物体识别和姿态估计，研究其传感器运动学习、模块化架构和快速推理策略。

Result: Monty通过结构化表征实现鲁棒泛化，支持快速推理，并通过模块间通信加速推理速度。其学习机制在计算效率上优于当前深度学习架构。

Conclusion: 千脑系统是一种有前景的AI新方法，Monty的初步成果验证了其潜力。

Abstract: Current AI systems achieve impressive performance on many tasks, yet they
lack core attributes of biological intelligence, including rapid, continual
learning, representations grounded in sensorimotor interactions, and structured
knowledge that enables efficient generalization. Neuroscience theory suggests
that mammals evolved flexible intelligence through the replication of a
semi-independent, sensorimotor module, a functional unit known as a cortical
column. To address the disparity between biological and artificial
intelligence, thousand-brains systems were proposed as a means of mirroring the
architecture of cortical columns and their interactions.
  In the current work, we evaluate the unique properties of Monty, the first
implementation of a thousand-brains system. We focus on 3D object perception,
and in particular, the combined task of object recognition and pose estimation.
Utilizing the YCB dataset of household objects, we first assess Monty's use of
sensorimotor learning to build structured representations, finding that these
enable robust generalization. These representations include an emphasis on
classifying objects by their global shape, as well as a natural ability to
detect object symmetries. We then explore Monty's use of model-free and
model-based policies to enable rapid inference by supporting principled
movements. We find that such policies complement Monty's modular architecture,
a design that can accommodate communication between modules to further
accelerate inference speed via a novel `voting' algorithm. Finally, we examine
Monty's use of associative, Hebbian-like binding to enable rapid, continual,
and computationally efficient learning, properties that compare favorably to
current deep learning architectures. While Monty is still in a nascent stage of
development, these findings support thousand-brains systems as a powerful and
promising new approach to AI.

</details>


### [76] [Churn-Aware Recommendation Planning under Aggregated Preference Feedback](https://arxiv.org/abs/2507.04513)
*Gur Keinan,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 论文研究了在隐私保护限制下推荐系统的序列决策问题，提出Rec-APC模型，证明最优策略会收敛于纯利用，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近期法规和技术限制导致推荐系统只能获取群体偏好数据，无法访问个体用户数据，这给个性化推荐带来挑战。

Method: 引入Rec-APC模型，通过贝叶斯更新处理匿名用户的二元反馈，提出分支定界算法计算最优策略。

Result: 实验证明Rec-APC在用户类型较多时优于POMDP求解器SARSOP，且策略快速收敛。

Conclusion: 该方法为聚合偏好数据下的决策提供了新思路，展示了其实际应用潜力。

Abstract: We study a sequential decision-making problem motivated by recent regulatory
and technological shifts that limit access to individual user data in
recommender systems (RSs), leaving only population-level preference
information. This privacy-aware setting poses fundamental challenges in
planning under uncertainty: Effective personalization requires exploration to
infer user preferences, yet unsatisfactory recommendations risk immediate user
churn. To address this, we introduce the Rec-APC model, in which an anonymous
user is drawn from a known prior over latent user types (e.g., personas or
clusters), and the decision-maker sequentially selects items to recommend.
Feedback is binary -- positive responses refine the posterior via Bayesian
updates, while negative responses result in the termination of the session.
  We prove that optimal policies converge to pure exploitation in finite time
and propose a branch-and-bound algorithm to efficiently compute them.
Experiments on synthetic and MovieLens data confirm rapid convergence and
demonstrate that our method outperforms the POMDP solver SARSOP, particularly
when the number of user types is large or comparable to the number of content
categories. Our results highlight the applicability of this approach and
inspire new ways to improve decision-making under the constraints imposed by
aggregated preference data.

</details>


### [77] [Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence](https://arxiv.org/abs/2507.04528)
*Sonal Allana,Rozita Dara,Xiaodong Lin,Pulei Xiong*

Main category: cs.AI

TL;DR: XAI方法可能泄露隐私，本文探索隐私增强技术（PETs）作为防御机制，评估了三种PETs对XAI隐私攻击的缓解效果，最佳情况下攻击风险降低49.47%。


<details>
  <summary>Details</summary>
Motivation: XAI方法在提高透明度的同时可能泄露个人隐私，目前缺乏针对隐私攻击的防御措施。

Method: 评估三种PETs（合成训练数据、差分隐私训练和噪声添加）在两类特征XAI上的效果。

Result: PETs集成后攻击风险最高降低49.47%，同时保持模型效用和解释质量。

Conclusion: 提出了在XAI中使用PETs的策略，以最大化隐私保护效果并最小化攻击成功率。

Abstract: Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating
the risk of non-transparency in the decision-making process of black-box
Artificial Intelligence (AI) systems. However, despite the benefits, XAI
methods are found to leak the privacy of individuals whose data is used in
training or querying the models. Researchers have demonstrated privacy attacks
that exploit explanations to infer sensitive personal information of
individuals. Currently there is a lack of defenses against known privacy
attacks targeting explanations when vulnerable XAI are used in production and
machine learning as a service system. To address this gap, in this article, we
explore Privacy Enhancing Technologies (PETs) as a defense mechanism against
attribute inference on explanations provided by feature-based XAI methods. We
empirically evaluate 3 types of PETs, namely synthetic training data,
differentially private training and noise addition, on two categories of
feature-based XAI. Our evaluation determines different responses from the
mitigation methods and side-effects of PETs on other system properties such as
utility and performance. In the best case, PETs integration in explanations
reduced the risk of the attack by 49.47%, while maintaining model utility and
explanation quality. Through our evaluation, we identify strategies for using
PETs in XAI for maximizing benefits and minimizing the success of this privacy
attack on sensitive personal information.

</details>


### [78] [DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification](https://arxiv.org/abs/2507.04600)
*Zhipeng Liu,Peibo Duan,Binwu Wang,Xuan Tang,Qi Chu,Changsheng Zhang,Yongsheng Huang,Bin Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的端到端解耦多尺度时间序列分类框架（DisMS-TS），通过消除多尺度时间序列中的冗余共享特征，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列通常具有复杂的时序变化，现有方法在多尺度分析中未能有效消除冗余的共享特征，导致模型性能下降。

Method: 设计了时间解耦模块，分别捕获共享和特定尺度的时序表示，并引入两个正则化项以确保共享特征的一致性和特定特征的差异性。

Result: 在多个数据集上的实验表明，DisMS-TS优于基线方法，准确率最高提升9.71%。

Conclusion: DisMS-TS通过解耦多尺度特征，显著提升了时间序列分类的性能。

Abstract: Real-world time series typically exhibit complex temporal variations, making
the time series classification task notably challenging. Recent advancements
have demonstrated the potential of multi-scale analysis approaches, which
provide an effective solution for capturing these complex temporal patterns.
However, existing multi-scale analysis-based time series prediction methods
fail to eliminate redundant scale-shared features across multi-scale time
series, resulting in the model over- or under-focusing on scale-shared
features. To address this issue, we propose a novel end-to-end Disentangled
Multi-Scale framework for Time Series classification (DisMS-TS). The core idea
of DisMS-TS is to eliminate redundant shared features in multi-scale time
series, thereby improving prediction performance. Specifically, we propose a
temporal disentanglement module to capture scale-shared and scale-specific
temporal representations, respectively. Subsequently, to effectively learn both
scale-shared and scale-specific temporal representations, we introduce two
regularization terms that ensure the consistency of scale-shared
representations and the disparity of scale-specific representations across all
temporal scales. Extensive experiments conducted on multiple datasets validate
the superiority of DisMS-TS over its competitive baselines, with the accuracy
improvement up to 9.71%.

</details>


### [79] [Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?](https://arxiv.org/abs/2507.04632)
*Yun Qu,Qi Cheems Wang,Yixiu Mao,Vincent Tao Hu,Xiangyang Ji*

Main category: cs.AI

TL;DR: 论文提出了一种名为MoPPS的贝叶斯风险预测框架，通过迭代近似评估减少对LLM的高频调用，从而降低计算成本并加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调在提升大型语言模型推理能力方面有效，但传统方法因频繁的提示评估和策略更新导致高计算成本。

Method: MoPPS通过贝叶斯推理建模提示的成功率作为潜在变量，并在多臂老虎机框架中使用后验采样，实现高效且自适应的提示选择。

Result: 实验表明，MoPPS能可靠预测提示难度，显著减少LLM调用次数并加速训练。

Conclusion: MoPPS为减少计算开销提供了一种高效的方法，适用于数学、规划和视觉几何任务。

Abstract: Recent advances have witnessed the effectiveness of reinforcement learning
(RL) finetuning in enhancing the reasoning capabilities of large language
models (LLMs). The optimization process often requires numerous iterations to
achieve satisfactory performance, resulting in high computational costs due to
the need for frequent prompt evaluations under intensive LLM interactions and
repeated policy updates. Appropriate online prompt selection methods reduce
iteration steps by prioritizing informative prompts during training, while the
pipeline's reliance on exhaustive prompt evaluation and subset selection for
optimization still incurs substantial computational overhead due to frequent
LLM inference calls. Distinguished from these direct evaluate-then-select
schemes, this work investigates iterative approximate evaluation for arbitrary
prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian
risk-predictive framework that online estimates prompt difficulty without
requiring costly LLM interactions. Technically, MoPPS models each prompt's
success rate as a latent variable, performs streaming Bayesian inference, and
employs posterior sampling in a constructed multi-armed bandit machine,
enabling sample efficient and adaptive prompt selection. Extensive experiments
across mathematics, planning, and vision-based geometry tasks show that MoPPS
reliably predicts prompt difficulty and accelerates training with significantly
reduced LLM rollouts.

</details>


### [80] [Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message](https://arxiv.org/abs/2507.04673)
*Wei Duan,Li Qian*

Main category: cs.AI

TL;DR: 论文提出了一种新型的越狱技术“特洛伊木马提示”，通过伪造对话历史绕过安全机制，揭示了现代对话AI的安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 对话界面的普及增强了LLM的实用性，但也引入了未探索的攻击面，尤其是模型对其自身对话历史的信任问题。

Method: 提出特洛伊木马提示技术，通过伪造模型的历史对话记录，注入恶意内容并触发有害输出。

Result: 实验证明该技术在攻击成功率上显著优于传统方法，揭示了不对称安全对齐的漏洞。

Conclusion: 现代对话AI需从输入级过滤转向协议级验证，以确保对话上下文的完整性。

Abstract: The rise of conversational interfaces has greatly enhanced LLM usability by
leveraging dialogue history for sophisticated reasoning. However, this reliance
introduces an unexplored attack surface. This paper introduces Trojan Horse
Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by
forging the model's own past utterances within the conversational history
provided to its API. A malicious payload is injected into a model-attributed
message, followed by a benign user prompt to trigger harmful content
generation. This vulnerability stems from Asymmetric Safety Alignment: models
are extensively trained to refuse harmful user requests but lack comparable
skepticism towards their own purported conversational history. This implicit
trust in its "past" creates a high-impact vulnerability. Experimental
validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan
Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than
established user-turn jailbreaking methods. These findings reveal a fundamental
flaw in modern conversational AI security, necessitating a paradigm shift from
input-level filtering to robust, protocol-level validation of conversational
context integrity.

</details>


### [81] [Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs](https://arxiv.org/abs/2507.04719)
*Roozbeh Yousefzadeh,Xuenan Cao*

Main category: cs.AI

TL;DR: 本文批评并讨论了形式推理和自动定理证明领域的基准测试与评估实践，提倡开放代码、开放数据和无错误的基准测试以推动进步。


<details>
  <summary>Details</summary>
Motivation: 探讨当前实践中的问题，促进该领域的开放性和协作性，消除贡献障碍。

Method: 分析现有实践，识别问题并提出改进建议。

Result: 提出了促进开放性和减少误导性评估的方法。

Conclusion: 旨在通过讨论推动自动定理证明、自动形式化和非形式推理领域的合作。

Abstract: This position paper provides a critical but constructive discussion of
current practices in benchmarking and evaluative practices in the field of
formal reasoning and automated theorem proving. We take the position that open
code, open data, and benchmarks that are complete and error-free will
accelerate progress in this field. We identify practices that create barriers
to contributing to this field and suggest ways to remove them. We also discuss
some of the practices that might produce misleading evaluative information. We
aim to create discussions that bring together people from various groups
contributing to automated theorem proving, autoformalization, and informal
reasoning.

</details>


### [82] [LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation](https://arxiv.org/abs/2507.04722)
*Jinzhi Wang,Bin Li,Qingke Peng,Haozhou Li,Zeyuan Zeng,Ruimeng Li,Biyi Zhou*

Main category: cs.AI

TL;DR: LumiCRS是一个端到端框架，通过自适应焦点损失、原型学习和GPT-4驱动的对话增强，解决了对话推荐系统中的长尾分布问题，显著提升了推荐的准确性、多样性和公平性。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）中存在极端长尾分布问题，导致对高频热门内容的偏好，牺牲了多样性并加剧了冷启动问题。

Method: LumiCRS采用三层策略：(i) 自适应综合焦点损失（ACFL）动态调整类别权重；(ii) 原型学习稳定长尾推荐；(iii) GPT-4驱动的对话增强模块生成多样长尾对话片段。

Result: 在REDIAL和INSPIRED基准测试中，LumiCRS的Recall@10和Tail-Recall@10比基线提高了7-15%，人类评估也显示其在流畅性、信息量和长尾相关性上的优势。

Conclusion: LumiCRS通过多层协作有效解决了长尾分布问题，提升了对话推荐系统的效率和公平性。

Abstract: Conversational recommender systems (CRSs) often suffer from an extreme
long-tail distribution of dialogue data, causing a strong bias toward
head-frequency blockbusters that sacrifices diversity and exacerbates the
cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL
corpus show that only 10% of head movies account for nearly half of all
mentions, whereas about 70% of tail movies receive merely 26% of the attention.
This imbalance gives rise to three critical challenges: head over-fitting, body
representation drift, and tail sparsity. To address these issues, we propose
LumiCRS, an end-to-end framework that mitigates long-tail imbalance through
three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss
(ACFL) that dynamically adjusts class weights and focusing factors to curb head
over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail
Recommendation, which selects semantic, affective, and contextual prototypes to
guide clustering and stabilize body and tail representations; and (iii) a
GPT-4o-driven prototype-guided dialogue augmentation module that automatically
generates diverse long-tail conversational snippets to alleviate tail sparsity
and distribution shift. Together, these strategies enable LumiCRS to markedly
improve recommendation accuracy, diversity, and fairness: on the REDIAL and
INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over
fifteen strong baselines, while human evaluations confirm superior fluency,
informativeness, and long-tail relevance. These results demonstrate the
effectiveness of multi-layer collaboration in building an efficient and fair
long-tail conversational recommender.

</details>


### [83] [ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning](https://arxiv.org/abs/2507.04736)
*Zhirong Chen,Kaiyan Chang,Zhuolin Li,Xinyang He,Chujie Chen,Cangyuan Li,Mengdi Wang,Haobo Xu,Yinhe Han,Ying Wang*

Main category: cs.AI

TL;DR: ChipSeek-R1 是一个基于分层奖励的强化学习框架，用于训练大型语言模型（LLM）生成功能正确且硬件质量（PPA）优化的 RTL 代码。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法同时优化功能正确性和硬件质量（PPA），需要一种新方法来提升 LLM 的设计能力。

Method: 采用分层奖励系统，结合语法、功能正确性和 PPA 指标的反馈，通过强化学习训练 LLM。

Result: 在标准基准测试中，ChipSeek-R1 实现了功能正确性的最佳结果，并在 RTLLM 基准中生成了 27 个 PPA 指标优于人工编写的 RTL 设计。

Conclusion: 研究表明，将工具链反馈集成到 LLM 训练中是有效的，强化学习能够实现自动化生成超越人工的 RTL 代码。

Abstract: Large Language Models (LLMs) show significant potential for automating
Register-Transfer Level (RTL) code generation. However, current approaches face
a critical challenge: they can not simultaneously optimize for functional
correctness and hardware quality (Power, Performance, Area - PPA). Methods
based on supervised fine-tuning often generate functionally correct but
PPA-suboptimal code, lacking mechanisms to learn optimization principles. In
contrast, post-processing techniques that attempt to improve PPA metrics after
generation are often inefficient because they operate externally without
updating the LLM's parameters, thus failing to enhance the model's intrinsic
design capabilities.
  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven
reinforcement learning framework to train LLMs to generate RTL code that
achieves both functional correctness and optimized PPA metrics. ChipSeek-R1
employs a hierarchical reward system, which incorporates direct feedback on
syntax, functional correctness (from simulators) and PPA metrics (from
synthesis tools) during reinforcement learning. This enables the model to learn
complex hardware design trade-offs via trial-and-error, generating RTL code
that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on
standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results
in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1
generated 27 RTL designs surpassing the PPA metrics of the original
human-written code. Our findings demonstrate the effectiveness of integrating
toolchain feedback into LLM training and highlight the potential for
reinforcement learning to enable automated generation of human-surpassing RTL
code. We open-source our code in anonymous github.

</details>


### [84] [Activation Steering for Chain-of-Thought Compression](https://arxiv.org/abs/2507.04742)
*Seyedarmin Azizi,Erfan Baghaei Potraghloo,Massoud Pedram*

Main category: cs.AI

TL;DR: 论文提出了一种名为激活导向压缩（ASC）的技术，通过调整模型的隐藏表示，在不重新训练的情况下缩短推理链，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理时生成的中间步骤（CoTs）通常过于冗长，导致资源浪费和延迟增加。

Method: 通过提取和注入“导向向量”来切换模型的推理模式，从冗长的英语推理转向简洁的数学推理。

Result: ASC在MATH500和GSM8K数据集上减少了67.43%的推理链长度，同时保持准确性，并在8B模型上实现了2.73倍的推理加速。

Conclusion: ASC是一种无需训练的高效方法，适用于对延迟或成本敏感的场景。

Abstract: Large language models (LLMs) excel at complex reasoning when they include
intermediate steps, known as "chains of thought" (CoTs). However, these
rationales are often overly verbose, even for simple problems, leading to
wasted context, increased latency, and higher energy consumption. We observe
that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct
regions in the model's residual-stream activation space. By extracting and
injecting a "steering vector" to transition between these modes, we can
reliably shift generation toward more concise reasoning, effectively
compressing CoTs without retraining. We formalize this approach as
Activation-Steered Compression (ASC), an inference-time technique that shortens
reasoning traces by directly modifying hidden representations. In addition, we
provide a theoretical analysis of the impact of ASC on the output distribution,
derived from a closed-form KL-divergence-bounded constraint to regulate
steering strength. Using only 100 paired verbose and concise examples, ASC
achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,
while maintaining accuracy across 7B, 8B, and 32B parameter models. As a
training-free method, ASC introduces negligible runtime overhead and, on
MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock
time on an 8B model. This makes ASC a practical and efficient tool for
streamlining the deployment of reasoning-capable LLMs in latency- or
cost-sensitive settings. The code is available at:
https://github.com/ArminAzizi98/ASC

</details>


### [85] [LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction](https://arxiv.org/abs/2507.04748)
*Sungmin Lee,Minju Kang,Joonhee Lee,Seungyong Lee,Dongju Kim,Jingi Hong,Jun Shin,Pei Zhang,JeongGil Ko*

Main category: cs.AI

TL;DR: JARVIS是一个基于LLM的两阶段QA框架，专为HVAC系统交互设计，通过专家LLM和代理实现高效、准确的查询处理。


<details>
  <summary>Details</summary>
Motivation: 提升非专家用户与HVAC系统的交互体验，解决实时、上下文感知的QA挑战。

Method: 采用两阶段框架：专家LLM翻译查询，代理执行SQL数据检索和响应生成；集成自适应上下文注入、参数化SQL构建器和自底向上规划。

Result: 在真实HVAC数据和专家QA数据集上评估，JARVIS在响应质量和准确性上优于基线方法。

Conclusion: JARVIS有效解决了HVAC系统交互的挑战，提供了准确且可解释的响应。

Abstract: Question-answering (QA) interfaces powered by large language models (LLMs)
present a promising direction for improving interactivity with HVAC system
insights, particularly for non-expert users. However, enabling accurate,
real-time, and context-aware interactions with HVAC systems introduces unique
challenges, including the integration of frequently updated sensor data,
domain-specific knowledge grounding, and coherent multi-stage reasoning. In
this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for
sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to
translate high-level user queries into structured execution instructions, and
an Agent that performs SQL-based data retrieval, statistical processing, and
final response generation. To address HVAC-specific challenges, JARVIS
integrates (1) an adaptive context injection strategy for efficient HVAC and
deployment-specific information integration, (2) a parameterized SQL builder
and executor to improve data access reliability, and (3) a bottom-up planning
scheme to ensure consistency across multi-stage response generation. We
evaluate JARVIS using real-world data collected from a commercial HVAC system
and a ground truth QA dataset curated by HVAC experts to demonstrate its
effectiveness in delivering accurate and interpretable responses across diverse
queries. Results show that JARVIS consistently outperforms baseline and
ablation variants in both automated and user-centered assessments, achieving
high response quality and accuracy.

</details>


### [86] [FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System](https://arxiv.org/abs/2507.04770)
*Toan Nguyen,Tri Le,Quang Nguyen,Anh Nguyen*

Main category: cs.AI

TL;DR: FurniMAS是一个多智能体系统，用于自动化家具装饰，通过结合LLM和非LLM智能体协作生成高质量的3D装饰效果。


<details>
  <summary>Details</summary>
Motivation: 家具装饰需要专业艺术技能且耗时，多智能体系统可以自动化这一过程，提高效率和质量。

Method: 提出FurniMAS系统，结合LLM和非LLM智能体，通过协作完成装饰任务，包括资产选择、风格匹配和布局设计。

Result: 实验表明FurniMAS在生成高质量3D装饰方面显著优于其他基线方法。

Conclusion: FurniMAS通过多智能体协作有效解决了家具装饰的自动化问题，具有实际应用潜力。

Abstract: Furniture decoration is an important task in various industrial applications.
However, achieving a high-quality decorative result is often time-consuming and
requires specialized artistic expertise. To tackle these challenges, we explore
how multi-agent systems can assist in automating the decoration process. We
propose FurniMAS, a multi-agent system for automatic furniture decoration.
Specifically, given a human prompt and a household furniture item such as a
working desk or a TV stand, our system suggests relevant assets with
appropriate styles and materials, and arranges them on the item, ensuring the
decorative result meets functionality, aesthetic, and ambiance preferences.
FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each
fulfilling distinct roles in a typical decoration project. These agents
collaborate through communication, logical reasoning, and validation to
transform the requirements into the final outcome. Extensive experiments
demonstrate that our FurniMAS significantly outperforms other baselines in
generating high-quality 3D decor.

</details>


### [87] [Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents](https://arxiv.org/abs/2507.04803)
*George Jagadeesh,Srikrishna Iyer,Michal Polanowski,Kai Xin Thia*

Main category: cs.AI

TL;DR: 研究探讨了使用大型语言模型（LLM）预测交通事件对交通流影响的可行性，提出了一种基于LLM的解决方案，无需大量训练数据，并能利用自由文本事件日志。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习解决方案需要大量训练数据，而LLM能利用自由文本日志且无需大量数据，为交通事件影响预测提供了新思路。

Method: 提出了一种完全基于LLM的解决方案，结合交通特征和LLM提取的事件特征进行预测，并设计了有效的示例选择方法用于上下文学习。

Result: 在真实交通事件数据集上评估了三种先进LLM和两种机器学习模型，表现最佳的LLM与最准确的机器学习模型精度相当。

Conclusion: LLM在交通事件影响预测中具有实际可行性，尽管未针对该任务进行训练，但仍能达到与传统方法相当的精度。

Abstract: This study examines the feasibility of applying large language models (LLMs)
for forecasting the impact of traffic incidents on the traffic flow. The use of
LLMs for this task has several advantages over existing machine learning-based
solutions such as not requiring a large training dataset and the ability to
utilize free-text incident logs. We propose a fully LLM-based solution that
predicts the incident impact using a combination of traffic features and
LLM-extracted incident features. A key ingredient of this solution is an
effective method of selecting examples for the LLM's in-context learning. We
evaluate the performance of three advanced LLMs and two state-of-the-art
machine learning models on a real traffic incident dataset. The results show
that the best-performing LLM matches the accuracy of the most accurate machine
learning model, despite the former not having been trained on this prediction
task. The findings indicate that LLMs are a practically viable option for
traffic incident impact prediction.

</details>


### [88] [DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine](https://arxiv.org/abs/2507.04877)
*Zewen Sun,Ruoxiang Huang,Jiahe Feng,Rundong Kong,Yuqian Wang,Hengyu Liu,Ziqi Gong,Yuyuan Qin,Yingxue Wang,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DoPI的新型LLM系统，旨在通过多轮对话和知识图谱提升中医诊断能力。系统采用协作架构，显著提高了诊断准确性和对话能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在医学应用中存在多轮对话和主动提问的局限性，阻碍了其在真实诊断场景中的实际应用。

Method: 提出DoPI系统，包含指导模型和专家模型，前者负责多轮对话和动态提问，后者提供诊断和治疗方案。构建了多轮医患对话数据集并提出了新的评估方法。

Result: 实验结果显示，DoPI系统在问诊结果中达到84.68%的准确率，显著提升了诊断中的沟通能力。

Conclusion: DoPI系统有效解决了LLM在中医诊断中的局限性，为实际应用提供了可行方案。

Abstract: Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)
diagnosis through multi-turn dialogues and knowledge graphs presents a
significant challenge for modern AI systems. Current large language models
(LLMs), despite their advancements, exhibit notable limitations in medical
applications, particularly in conducting effective multi-turn dialogues and
proactive questioning. These shortcomings hinder their practical application
and effectiveness in simulating real-world diagnostic scenarios. To address
these limitations, we propose DoPI, a novel LLM system specifically designed
for the TCM domain. The DoPI system introduces a collaborative architecture
comprising a guidance model and an expert model. The guidance model conducts
multi-turn dialogues with patients and dynamically generates questions based on
a knowledge graph to efficiently extract critical symptom information.
Simultaneously, the expert model leverages deep TCM expertise to provide final
diagnoses and treatment plans. Furthermore, this study constructs a multi-turn
doctor-patient dialogue dataset to simulate realistic consultation scenarios
and proposes a novel evaluation methodology that does not rely on manually
collected real-world consultation data. Experimental results show that the DoPI
system achieves an accuracy rate of 84.68 percent in interrogation outcomes,
significantly enhancing the model's communication ability during diagnosis
while maintaining professional expertise.

</details>


### [89] [MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction](https://arxiv.org/abs/2507.04893)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.AI

TL;DR: MARBLE是一种多智能体规则驱动的LLM引擎，通过分解任务和模块化推理，显著提升了交通事故严重性预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 交通事故严重性预测因数据不完整、特征依赖性强和类别不平衡而困难，现有方法难以应对现实噪声且缺乏可解释性。

Method: MARBLE利用多个专门推理智能体（包括可互换的ML支持智能体）分解任务，通过规则或LLM引导的共识机制协调预测。

Result: 在英美数据集上，MARBLE准确率接近90%，显著优于传统方法和SOTA提示推理方法（如CoT、L2M、ToT）。

Conclusion: MARBLE为安全关键应用中的不确定性推理提供了通用且可解释的框架。

Abstract: Accident severity prediction plays a critical role in transportation safety
systems but is a persistently difficult task due to incomplete data, strong
feature dependencies, and severe class imbalance in which rare but
high-severity cases are underrepresented and hard to detect. Existing methods
often rely on monolithic models or black box prompting, which struggle to scale
in noisy, real-world settings and offer limited interpretability. To address
these challenges, we propose MARBLE a multiagent rule based LLM engine that
decomposes the severity prediction task across a team of specialized reasoning
agents, including an interchangeable ML-backed agent. Each agent focuses on a
semantic subset of features (e.g., spatial, environmental, temporal), enabling
scoped reasoning and modular prompting without the risk of prompt saturation.
Predictions are coordinated through either rule-based or LLM-guided consensus
mechanisms that account for class rarity and confidence dynamics. The system
retains structured traces of agent-level reasoning and coordination outcomes,
supporting in-depth interpretability and post-hoc performance diagnostics.
Across both UK and US datasets, MARBLE consistently outperforms traditional
machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning
methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and
Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below
48%. This performance redefines the practical ceiling for accident severity
classification under real world noise and extreme class imbalance. Our results
position MARBLE as a generalizable and interpretable framework for reasoning
under uncertainty in safety-critical applications.

</details>


### [90] [Supported Abstract Argumentation for Case-Based Reasoning](https://arxiv.org/abs/2507.04994)
*Adam Gould,Gabriel de Olim Gaul,Francesca Toni*

Main category: cs.AI

TL;DR: sAA-CBR是一种二元分类模型，通过引入支持机制改进AA-CBR，消除了无关案例（spikes）的问题，同时保留了关键模型特性。


<details>
  <summary>Details</summary>
Motivation: 解决AA-CBR中存在的无关案例问题，提升模型的分类性能。

Method: 通过让过去案例在辩论中支持或攻击其他案例的标签，引入支持机制。

Result: 证明sAA-CBR不含无关案例，且不牺牲关键模型特性。

Conclusion: sAA-CBR通过支持机制有效解决了AA-CBR的局限性，同时保持了模型的完整性。

Abstract: We introduce Supported Abstract Argumentation for Case-Based Reasoning
(sAA-CBR), a binary classification model in which past cases engage in debates
by arguing in favour of their labelling and attacking or supporting those with
opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of
its precursor AA-CBR, which can contain extraneous cases (or spikes) that are
not included in the debates. We prove that sAA-CBR contains no spikes, without
trading off key model properties

</details>


### [91] [When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning](https://arxiv.org/abs/2507.05011)
*Maxence Boels,Harry Robertshaw,Alejandro Granados,Prokar Dasgupta,Sebastien Ourselin*

Main category: cs.AI

TL;DR: 论文比较了模仿学习（IL）与强化学习（RL）在手术动作规划中的表现，发现IL优于RL。


<details>
  <summary>Details</summary>
Motivation: 探讨IL和RL在手术动作规划中的效果，挑战RL在序列决策中的优越性假设。

Method: 提出了双任务自回归模仿学习（DARIL）基线，并评估了三种RL变体。

Result: DARIL在动作三重识别和下一帧预测中表现最佳，RL方法均表现不佳。

Conclusion: 研究结果表明，IL在手术AI开发中可能更具优势，挑战了RL的普遍假设。

Abstract: Surgical action planning requires predicting future instrument-verb-target
triplets for real-time assistance. While teleoperated robotic surgery provides
natural expert demonstrations for imitation learning (IL), reinforcement
learning (RL) could potentially discover superior strategies through
exploration. We present the first comprehensive comparison of IL versus RL for
surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation
Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and
33.6% next frame prediction mAP with smooth planning degradation to 29.2% at
10-second horizons. We evaluated three RL variants: world model-based RL,
direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches
underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while
direct video RL achieved only 15.9%. Our analysis reveals that distribution
matching on expert-annotated test sets systematically favors IL over
potentially valid RL policies that differ from training demonstrations. This
challenges assumptions about RL superiority in sequential decision making and
provides crucial insights for surgical AI development.

</details>


### [92] [How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs](https://arxiv.org/abs/2507.05088)
*Kilian Rückschloß,Felix Weitkämper*

Main category: cs.AI

TL;DR: 本文扩展了Pearl的因果理论，将其应用于分层溯因逻辑程序，证明了稳定模型语义符合因果关系的哲学原则。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将因果知识应用于分层溯因逻辑程序，以支持对外部干预的预测。

Method: 通过将溯因逻辑程序转化为因果系统，赋予逻辑程序规则明确的因果解释。

Result: 稳定模型语义符合因果充分性、自然必要性等哲学原则，验证了其作为因果建模框架的合理性。

Conclusion: 分层溯因逻辑程序可作为因果建模的有效工具，支持干预效果的预测。

Abstract: Pearl observes that causal knowledge enables predicting the effects of
interventions, such as actions, whereas descriptive knowledge only permits
drawing conclusions from observation. This paper extends Pearl's approach to
causality and interventions to the setting of stratified abductive logic
programs. It shows how stable models of such programs can be given a causal
interpretation by building on philosophical foundations and recent work by
Bochman and Eelink et al. In particular, it provides a translation of abductive
logic programs into causal systems, thereby clarifying the informal causal
reading of logic program rules and supporting principled reasoning about
external actions. The main result establishes that the stable model semantics
for stratified programs conforms to key philosophical principles of causation,
such as causal sufficiency, natural necessity, and irrelevance of unobserved
effects. This justifies the use of stratified abductive logic programs as a
framework for causal modeling and for predicting the effects of interventions

</details>


### [93] [Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift](https://arxiv.org/abs/2507.05110)
*Shixuan Liu,Yue He,Yunfei Wang,Hao Zou,Haoxiang Cheng,Wenjing Yang,Peng Cui,Zhong Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为StableRule的框架，用于解决知识图谱（KG）推理中的分布外（OOD）问题，通过特征解耦和规则学习网络提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有KG推理方法依赖I.I.D假设，但在实际应用中可能因未知选择偏差或分布偏移导致性能下降，限制了模型在真实环境中的可靠性。

Method: 提出StableRule框架，结合特征解耦和规则学习网络，以减轻OOD场景中的协变量偏移影响。

Result: 在七个基准KG上的实验表明，该框架在异构环境中具有优越的有效性和稳定性。

Conclusion: StableRule框架显著提升了KG推理在OOD场景中的鲁棒性，具有实际应用价值。

Abstract: Knowledge graph (KG) reasoning remains a critical research area focused on
inferring missing knowledge by analyzing relationships among observed facts.
Despite its success, a key limitation of existing KG reasoning methods is their
dependence on the I.I.D assumption. This assumption can easily be violated due
to unknown sample selection bias during training or agnostic distribution
shifts during testing, significantly compromising model performance and
reliability. To facilitate the deployment of KG reasoning in wild environments,
this study investigates learning logical rules from KGs affected by unknown
selection bias. Additionally, we address test sets with agnostic distribution
shifts, formally defining this challenge as out-of-distribution (OOD) KG
reasoning-a previously underexplored problem. To solve the issue, we propose
the Stable Rule Learning (StableRule) framework, an end-to-end methodology that
integrates feature decorrelation with rule learning network, to enhance OOD
generalization performance. By leveraging feature decorrelation, the StableRule
framework mitigates the adverse effects of covariate shifts arising in OOD
scenarios, thereby improving the robustness of the rule learning component in
effectively deriving logical rules. Extensive experiments on seven benchmark
KGs demonstrate the framework's superior effectiveness and stability across
diverse heterogeneous environments, underscoring its practical significance for
real-world applications.

</details>


### [94] [GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation](https://arxiv.org/abs/2507.05142)
*Wei Xu,Haoran Li,Baoyuan Ou,Lai Xu,Yingjie Qin,Ruilong Su,Ruiwen Xu*

Main category: cs.AI

TL;DR: 提出GIST模型，通过解耦源域和目标域的训练过程，结合内容-行为联合训练模块（CBJT）和非对称相似性集成策略（ASI），提升跨域点击率预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在联合训练或预训练微调中因数据分布不同或无法持续整合新数据而表现不佳的问题。

Method: 提出GIST模型，包括CBJT模块对齐内容-行为分布，以及ASI策略增强知识迁移。

Result: 在离线和在线测试中超越现有方法，并在Xiaohongshu平台成功部署。

Conclusion: GIST通过创新模块和策略，有效提升跨域点击率预测性能，适用于大规模工业场景。

Abstract: Cross-domain Click-Through Rate prediction aims to tackle the data sparsity
and the cold start problems in online advertising systems by transferring
knowledge from source domains to a target domain. Most existing methods rely on
overlapping users to facilitate this transfer, often focusing on joint training
or pre-training with fine-tuning approach to connect the source and target
domains. However, in real-world industrial settings, joint training struggles
to learn optimal representations with different distributions, and pre-training
with fine-tuning is not well-suited for continuously integrating new data. To
address these issues, we propose GIST, a cross-domain lifelong sequence model
that decouples the training processes of the source and target domains. Unlike
previous methods that search lifelong sequences in the source domains using
only content or behavior signals or their simple combinations, we innovatively
introduce a Content-Behavior Joint Training Module (CBJT), which aligns
content-behavior distributions and combines them with guided information to
facilitate a more stable representation. Furthermore, we develop an Asymmetric
Similarity Integration strategy (ASI) to augment knowledge transfer through
similarity computation. Extensive experiments demonstrate the effectiveness of
GIST, surpassing SOTA methods on offline evaluations and an online A/B test.
Deployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances
online ads system performance at scale, serving hundreds of millions of daily
active users.

</details>


### [95] [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)
*Andrew Sellergren,Sahar Kazemzadeh,Tiam Jaroensri,Atilla Kiraly,Madeleine Traverse,Timo Kohlberger,Shawn Xu,Fayaz Jamil,Cían Hughes,Charles Lau,Justin Chen,Fereshteh Mahvar,Liron Yatziv,Tiffany Chen,Bram Sterling,Stefanie Anna Baby,Susanna Maria Baby,Jeremy Lai,Samuel Schmidgall,Lu Yang,Kejia Chen,Per Bjornsson,Shashir Reddy,Ryan Brush,Kenneth Philbrick,Howard Hu,Howard Yang,Richa Tiwari,Sunny Jansen,Preeti Singh,Yun Liu,Shekoofeh Azizi,Aishwarya Kamath,Johan Ferret,Shreya Pathak,Nino Vieillard,Ramona Merhej,Sarah Perrin,Tatiana Matejovicova,Alexandre Ramé,Morgane Riviere,Louis Rouillard,Thomas Mesnard,Geoffrey Cideron,Jean-bastien Grill,Sabela Ramos,Edouard Yvinec,Michelle Casbon,Elena Buchatskaya,Jean-Baptiste Alayrac,Dmitry,Lepikhin,Vlad Feinberg,Sebastian Borgeaud,Alek Andreev,Cassidy Hardin,Robert Dadashi,Léonard Hussenot,Armand Joulin,Olivier Bachem,Yossi Matias,Katherine Chou,Avinatan Hassidim,Kavi Goel,Clement Farabet,Joelle Barral,Tris Warkentin,Jonathon Shlens,David Fleet,Victor Cotruta,Omar Sanseviero,Gus Martins,Phoebe Kirk,Anand Rao,Shravya Shetty,David F. Steiner,Can Kirmizibayrak,Rory Pilgrim,Daniel Golden,Lin Yang*

Main category: cs.AI

TL;DR: MedGemma是一组基于Gemma 3的医疗视觉语言基础模型，在医疗任务中表现优异，显著超越同类生成模型，并接近任务专用模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的发展面临数据多样性、任务复杂性和隐私保护等挑战，需要性能优异且无需大量任务特定调优数据的基础模型。

Method: 基于Gemma 3 4B和27B构建MedGemma，并引入MedSigLIP作为视觉编码器，提升医疗图像和文本的理解能力。

Result: MedGemma在医疗多模态问答、胸部X光分类等任务中表现显著优于基础模型，微调后进一步提升了子领域性能。

Conclusion: MedGemma为医疗研究和下游应用提供了强大的基础能力，有望加速医疗AI的发展。

Abstract: Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

</details>


### [96] [SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?](https://arxiv.org/abs/2507.05241)
*Jingyi Chai,Shuo Tang,Rui Ye,Yuwen Du,Xinyu Zhu,Mengcheng Zhou,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: X-Master是一个工具增强的推理代理，通过灵活使用外部工具模拟人类研究者的推理过程，在HLE上取得了32.1%的领先成绩。


<details>
  <summary>Details</summary>
Motivation: 利用AI加速科学发现，需要评估其在前沿知识中的表现，HLE为此提供了挑战性的基准。

Method: 提出X-Master代理，结合代码作为交互语言，使用Python库和定制工具增强推理，并通过X-Masters工作流扩展能力。

Result: X-Masters在HLE上以32.1%的成绩创下新纪录，超越OpenAI和Google的26.6%和26.9%。

Conclusion: X-Master为复杂任务解决提供了新思路，并为未来模型训练积累了经验。

Abstract: The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.

</details>


### [97] [Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration](https://arxiv.org/abs/2507.05244)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: TALENTS框架通过变分自编码器学习策略空间，动态适应异构队友，在Overcooked环境中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 异构团队（如人机协作）需实时适应队友策略，尤其在时间压力和复杂动态任务中。

Method: 使用变分自编码器学习策略表示，聚类策略类型，训练条件合作者，并动态调整策略。

Result: 在Overcooked任务中，TALENTS优于现有基线，适应陌生人类队友。

Conclusion: TALENTS框架有效支持异构团队的实时策略适应，提升协作表现。

Abstract: In collaborative tasks, being able to adapt to your teammates is a necessary
requirement for success. When teammates are heterogeneous, such as in
human-agent teams, agents need to be able to observe, recognize, and adapt to
their human partners in real time. This becomes particularly challenging in
tasks with time pressure and complex strategic spaces where the dynamics can
change rapidly. In this work, we introduce TALENTS, a strategy-conditioned
cooperator framework that learns to represent, categorize, and adapt to a range
of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a
variational autoencoder to learn a latent strategy space from trajectory data.
This latent space represents the underlying strategies that agents employ.
Subsequently, the system identifies different types of strategy by clustering
the data. Finally, a cooperator agent is trained to generate partners for each
type of strategy, conditioned on these clusters. In order to adapt to
previously unseen partners, we leverage a fixed-share regret minimization
algorithm that infers and adjusts the estimated partner strategy dynamically.
We assess our approach in a customized version of the Overcooked environment,
posing a challenging cooperative cooking task that demands strong coordination
across a wide range of possible strategies. Using an online user study, we show
that our agent outperforms current baselines when working with unfamiliar human
partners.

</details>


### [98] [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
*Scott Emmons,Erik Jenner,David K. Elson,Rif A. Saurous,Senthooran Rajamanoharan,Heng Chen,Irhum Shafkat,Rohin Shah*

Main category: cs.AI

TL;DR: 论文探讨了链式思维（CoT）监控在AI安全防御中的可靠性问题，提出了区分CoT作为合理化与CoT作为计算的概念框架，并强调监控性的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT监控是一种有吸引力的AI安全防御手段，但近期关于其‘不忠实性’的研究对其可靠性提出了质疑。论文旨在解决这一问题，特别是在防止严重危害的运行时监控场景中。

Method: 通过引入区分CoT-as-rationalization和CoT-as-computation的框架，并增加实验难度以强制模型暴露其推理过程，论文提出了一套方法论指南来测试CoT监控的鲁棒性。

Result: 研究发现，模型可以学会隐藏其意图，但需要大量帮助（如详细的人类策略或针对监控的迭代优化）。

Conclusion: 尽管CoT监控并非完美无缺，但它提供了重要的防御层，需要持续的保护和压力测试。

Abstract: While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [99] [Amortized Locally Decodable Codes for Insertions and Deletions](https://arxiv.org/abs/2507.03141)
*Jeremiah Blocki,Justin Zhang*

Main category: cs.IT

TL;DR: 论文研究了Hamming amortized Locally Decodable Codes (aLDCs)的扩展问题，提出了一种将Hamming aLDC转换为Insdel aLDC的编译器，并在特定条件下构造了理想的Hamming aLDC。


<details>
  <summary>Details</summary>
Motivation: 传统的Hamming LDC在速率、局部性和容错性之间的权衡不理想，而Blocki和Zhang提出的Hamming aLDC在特定条件下实现了理想性能。然而，这种技术是否适用于Insdel LDC尚不明确。

Method: 1. 提出了一种Hamming-to-Insdel编译器，将满足特定条件的Hamming aLDC转换为Insdel aLDC；2. 在共享随机性或资源受限信道条件下构造了理想的Hamming aLDC。

Result: 实现了在私有密钥和资源受限设置下具有恒定速率、恒定局部性和恒定容错性的理想Insdel aLDC。

Conclusion: 通过编译器和特定构造的结合，成功扩展了Hamming aLDC的优势到更具挑战性的Insdel LDC领域。

Abstract: Locally Decodable Codes (LDCs) are error correcting codes which permit the
recovery of any single message symbol with a low number of queries to the
codeword (the locality). Traditional LDC tradeoffs between the rate, locality,
and error tolerance are undesirable even in relaxed settings where the
encoder/decoder share randomness or where the channel is resource-bounded.
Recent work by Blocki and Zhang initiated the study of Hamming amortized
Locally Decodable Codes (aLDCs), which allow the local decoder to amortize
their number of queries over the recovery of a small subset of message symbols.
Surprisingly, Blocki and Zhang construct asymptotically ideal (constant rate,
constant amortized locality, and constant error tolerance) Hamming aLDCs in
private-key and resource-bounded settings. While this result overcame previous
barriers and impossibility results for Hamming LDCs, it is not clear whether
the techniques extend to Insdel LDCs. Constructing Insdel LDCs which are
resilient to insertion and/or deletion errors is known to be even more
challenging.
  Our first contribution is to provide a Hamming-to-Insdel compiler which
transforms any amortized Hamming LDC that satisfies a particular property to
amortized Insdel LDC while asymptotically preserving the rate, error tolerance
and amortized locality. Prior Hamming-to-Insdel compilers worked for arbitrary
Hamming LDCs, but incurred an undesirable polylogarithmic blow-up in the
locality. Our second contribution is a construction of an ideal amortized
Hamming LDC which satisfies our special property in the relaxed settings where
the sender/receiver share randomness or where the channel is resource bounded.
Taken together, we obtain ideal Insdel aLDCs in private-key and
resource-bounded settings with constant amortized locality, constant rate and
constant error tolerance.

</details>


### [100] [Generalized Theta Series of a Lattice](https://arxiv.org/abs/2507.03178)
*Maiara F. Bollauf,Hsuan-Yin Lin*

Main category: cs.IT

TL;DR: 论文引入了一种新的格不变量——广义theta级数，并探讨了其在识别稳定格和格同构问题中的应用，同时反驳了关于等偶格的保密增益猜想。


<details>
  <summary>Details</summary>
Motivation: 受线性码的广义Hamming权重的启发，研究格的新不变量及其应用，同时验证相关猜想。

Method: 引入广义theta级数作为格的新不变量，并分析其在稳定格识别和格同构问题中的应用。

Result: 提供了等偶格保密增益猜想的反例，表明其最小化条件不成立。

Conclusion: 广义theta级数是研究格理论的有力工具，同时揭示了原有猜想的局限性。

Abstract: Mimicking the idea of the generalized Hamming weight of linear codes, we
introduce a new lattice invariant, the generalized theta series. Applications
range from identifying stable lattices to the lattice isomorphism problem.
Moreover, we provide counterexamples for the secrecy gain conjecture on isodual
lattices, which claims that the ratio of the theta series of an isodual (and
more generally, formally unimodular) lattice by the theta series of the integer
lattice $\mathbb{Z}^n$ is minimized at a (unique) symmetry point.

</details>


### [101] [Function-Correcting Codes with Homogeneous Distance](https://arxiv.org/abs/2507.03332)
*Huiying Liu,Hongwei Liu*

Main category: cs.IT

TL;DR: 本文介绍了基于齐次距离的函数校正码（FCCHDs），扩展了汉明距离的函数校正码。通过定义D-齐次距离码，研究了FCCHDs的最优冗余与码长之间的关系，并给出了某些函数的最优冗余界限。此外，还构造了针对齐次权重函数和齐次权重分布函数的FCCHDs，部分构造的码达到了最优冗余界限。


<details>
  <summary>Details</summary>
Motivation: 传统函数校正码在保护信息函数值免受错误影响时存在冗余问题。本文旨在通过引入齐次距离，扩展函数校正码的应用范围，并优化其冗余性能。

Method: 定义了D-齐次距离码，利用其特性研究FCCHDs的最优冗余与码长的关系。通过矩阵D的分析，推导出某些函数的最优冗余界限，并构造了针对齐次权重函数和齐次权重分布函数的FCCHDs。

Result: 得到了FCCHDs的最优冗余界限，并成功构造了部分达到最优冗余界限的码。

Conclusion: FCCHDs在齐次距离下扩展了函数校正码的功能，并通过构造和理论分析展示了其在优化冗余方面的潜力。

Abstract: Function-correcting codes are designed to reduce redundancy of codes when
protecting function values of information against errors. As generalizations of
Hamming weights and Lee weights over $ \mathbb{Z}_{4} $, homogeneous weights
are used in codes over finite rings. In this paper, we introduce
function-correcting codes with homogeneous distance denoted by FCCHDs, which
extend function-correcting codes with Hamming distance. We first define $ D
$-homogeneous distance codes. We use $ D $-homogenous distance codes to
characterize connections between the optimal redundancy of FCCHDs and lengths
of these codes for some matrices $ D $. By these connections, we obtain several
bounds of the optimal redundancy of FCCHDs for some functions. In addition, we
also construct FCCHDs for homogeneous weight functions and homogeneous weight
distribution functions. Specially, redundancies of some codes we construct in
this paper reach the optimal redundancy bounds.

</details>


### [102] [Set Shaping Theory and the Foundations of Redundancy-Free Testable Codes](https://arxiv.org/abs/2507.03444)
*Aida Koch,Alix Petit*

Main category: cs.IT

TL;DR: 论文提出了一种通过扩展序列长度并引入统计依赖性的方法，使序列可测试且能检测错误，同时不增加信息量。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过添加冗余（如奇偶校验位）使序列可测试，但需要传输额外符号且增加信息量。本文旨在找到一种方法，在不增加信息量的情况下实现序列的可测试性。

Method: 利用集合塑形理论，将原始序列扩展为更长的序列，通过精心设计新序列集合的结构，使其符号间依赖性可被视为独立，从而简化编码。

Result: 新方法能够在不增加信息量的情况下，使序列具备错误检测能力，且新序列集合的结构更简单。

Conclusion: 通过集合塑形理论，可以实现序列的可测试性，同时避免信息量的增加和冗余的引入。

Abstract: To render a sequence testable, namely capable of identifying and detecting
errors, it is necessary to apply a transformation that increases its length by
introducing statistical dependence among symbols, as commonly exemplified by
the addition of parity bits. However, since the decoder does not have prior
knowledge of the original symbols, it must treat the artificially introduced
symbols as if they were independent. Consequently, these additional symbols
must be transmitted, even though their conditional probability, under ideal and
error free conditions, would be zero. This sequence extension implies that not
all symbol combinations of the new length are practically realizable: if an
error modifies a sequence, making it inadmissible such an error becomes
detectable. Recent developments in Set Shaping Theory have revealed a
surprising result: it is always possible to transform a sequence into a longer
version by carefully selecting which longer sequences are allowed, in such a
way that the overall set of sequences becomes more structured and less complex
than the original. This means that even though the sequence is extended and
dependencies are introduced between symbols, the total amount of information
contained in the new set does not increase proportionally on the contrary, it
can be slightly reduced. In other words, one can construct a new set of longer
sequences where each one corresponds uniquely to an original sequence, but the
entire set is designed in such a way that it can be treated as if the symbols
were independent, making encoding simpler. This allows sequence to become
testable capable of detecting errors without adding visible redundancy or
increasing the informational content.

</details>


### [103] [Movable-Antenna-Enhanced Physical-Layer Service Integration: Performance Analysis and Optimization](https://arxiv.org/abs/2507.03449)
*Xuanlin Shen,Xin Wei,Weidong Mei,Zhi Chen,Jun Fang,Boyu Ning*

Main category: cs.IT

TL;DR: 论文研究了可移动天线（MAs）在物理层服务集成（PHY-SI）中的应用，通过优化波束成形和天线位置，提升保密速率和满足多播速率需求。


<details>
  <summary>Details</summary>
Motivation: 可移动天线因其能在有限区域内通过局部移动改善信道条件而受到关注，本文探索其在PHY-SI中的应用潜力。

Method: 采用双层优化框架，结合半定松弛（SDR）技术和离散采样算法，优化保密波束成形、多播波束成形及天线位置。

Result: 数值结果表明，相比固定位置天线（FPAs），MAs能显著提升PHY-SI的保密速率区域。

Conclusion: 可移动天线在PHY-SI中具有显著优势，为未来无线通信系统设计提供了新思路。

Abstract: Movable antennas (MAs) have drawn increasing attention in wireless
communications due to their capability to create favorable channel conditions
via local movement within a confined region. In this letter, we investigate its
application in physical-layer service integration (PHY-SI), where a multi-MA
base station (BS) simultaneously transmits both confidential and multicast
messages to two users. The multicast message is intended for both users, while
the confidential message is intended only for one user and must remain
perfectly secure from the other. Our goal is to jointly optimize the secrecy
and multicast beamforming, as well as the MAs' positions at the BS to maximize
the secrecy rate for one user while satisfying the multicast rate requirement
for both users. To gain insights, we first conduct performance analysis of this
MA-enhanced PHY-SI system in two special cases, revealing its unique
characteristics compared to conventional PHY-SI with fixed-position antennas
(FPAs). To address the secrecy rate maximization problem, we propose a
two-layer optimization framework that integrates the semidefinite relaxation
(SDR) technique and a discrete sampling algorithm. Numerical results
demonstrate that MAs can greatly enhance the achievable secrecy rate region for
PHY-SI compared to FPAs.

</details>


### [104] [Learning Variable Node Selection for Improved Multi-Round Belief Propagation Decoding](https://arxiv.org/abs/2507.03461)
*Ahmad Ismail,Raphaël Le Bidan,Elsa Dupraz,Charbel Abdel Nour*

Main category: cs.IT

TL;DR: 论文提出了一种基于神经网络的解码方法，通过学习预测需要扰动的变量节点，提升短LDPC码的解码性能。


<details>
  <summary>Details</summary>
Motivation: 短块长LDPC码的纠错性能不足，传统BP解码器因无法收敛而表现不佳。

Method: 利用神经网络（SBND启发）预测需要扰动的变量节点，优化多轮BP解码。

Result: 实验表明，该方法优于现有启发式规则，能以更少轮次接近最大似然解码性能。

Conclusion: 该方法为提升短LDPC码解码性能提供了新思路。

Abstract: Error correction at short blocklengths remains a challenge for low-density
parity-check (LDPC) codes, as belief propagation (BP) decoding is suboptimal
compared to maximum-likelihood decoding (MLD). While BP rarely makes errors, it
often fails to converge due to a small number of problematic, erroneous
variable nodes (VNs). Multi-round BP (MRBP) decoding improves performance by
identifying and perturbing these VNs, enabling BP to succeed in subsequent
decoding attempts. However, existing heuristic approaches for VN identification
may require a large number of decoding rounds to approach ML performance. In
this work, we draw a connection between identifying candidate VNs to perturb in
MRBP and estimating channel output errors, a problem previously addressed by
syndrome-based neural decoders (SBND). Leveraging this insight, we propose an
SBND-inspired neural network architecture that learns to predict which VNs MRBP
needs to focus on. Experimental results demonstrate that the proposed learning
approach outperforms expert rules from the literature, requiring fewer MRBP
decoding attempts to reach near-MLD performance. This makes it a promising lead
for improving the decoding of short LDPC codes.

</details>


### [105] [Class-Based Expurgation Attains Csiszár's Expurgated Source-Channel Exponent](https://arxiv.org/abs/2507.03481)
*AmirPouya Moeini,Albert Guillén i Fàbregas*

Main category: cs.IT

TL;DR: 论文研究了离散无记忆源和信道的联合源信道编码的净化误差指数，通过将源消息划分为类别并设计依赖类别的码字分布，证明了仅需两类即可达到Csiszár的净化指数。


<details>
  <summary>Details</summary>
Motivation: 研究联合源信道编码的误差性能，探索如何通过优化消息分类和码字分布设计来提高误差指数。

Method: 将源消息划分为不同类别，并根据类别设计码字分布，通过精心选择两类消息实现优化。

Result: 证明仅需两类消息即可达到Csiszár的净化误差指数，简化了实现复杂度。

Conclusion: 通过合理的消息分类和码字分布设计，可以有效提升联合源信道编码的误差性能，且实现简单。

Abstract: This paper studies expurgated error exponents for joint source-channel coding
for discrete memoryless sources and channels. We consider a partition of the
source messages into classes, where the codeword distributions depend on the
class. We show that two carefully chosen classes suffice to achieve Csisz\'ar's
expurgated exponent.

</details>


### [106] [Near-Field Codebook-Based 3D Spherical Channel Estimation for UCA XL-MIMO Systems](https://arxiv.org/abs/2507.03507)
*Chenliang Yang,Guangchi Zhang,Miao Cui,Qingqing Wu,Yong Zeng*

Main category: cs.IT

TL;DR: 提出了一种基于球形域S-SOMP的方案，用于UCA XL-MIMO系统中的近场3D信道估计，显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决XL-MIMO在近场信道估计中面临的球面波前和3D空间表征挑战。

Method: 基于近场球面波模型建立稀疏信道表示，设计球形域变换矩阵码本，利用S-SOMP算法进行联合估计。

Result: 仿真结果表明，相比现有方法，信道估计精度显著提升。

Conclusion: 提出的S-SOMP方案为UCA XL-MIMO系统的近场3D信道估计提供了高效解决方案。

Abstract: Extremely large-scale multiple input multiple output (XL-MIMO), a key
technology for 6G communications, faces challenges in near-field channel
estimation due to spherical wavefronts and the need for three-dimensional (3D)
spatial characterization, particularly with uniform circular arrays (UCAs).
This letter proposes a spherical-domain simultaneous orthogonal matching
pursuit (S-SOMP) based scheme tailored for near-field 3D channel estimation in
UCA-equipped XL-MIMO systems. We establish a sparse channel representation
based on the near-field spherical wave model. Then, a novel spherical-domain
transform matrix codebook is designed via joint discrete sampling of distance,
azimuth, and elevation parameters, leveraging analytical approximations to
ensure low correlation between steering vectors. This structured codebook
enables accurate sparse signal recovery using the S-SOMP algorithm for
efficient joint estimation of channel path gains, spatial angles, and
distances. Simulation results demonstrate significant channel estimation
accuracy improvements compared to existing benchmarks.

</details>


### [107] [You May Use the Same Channel Knowledge Map for Environment-Aware NLoS Sensing and Communication](https://arxiv.org/abs/2507.03589)
*Di Wu,Zhuoyin Dai,Yong Zeng*

Main category: cs.IT

TL;DR: 提出了一种基于信道知识图（CKM）的非视距（NLoS）集成感知与通信（ISAC）方法，通过将通信信道先验转化为感知信道先验，实现复杂环境下的高效感知。


<details>
  <summary>Details</summary>
Motivation: 传统无线感知技术依赖视距假设，限制了ISAC在复杂环境（如城市低空）中的应用。

Method: 利用CKM技术，将通信信道先验转化为感知信道先验，通过虚拟用户设备（UE）实现NLoS感知。

Result: 仿真结果显示，相比传统几何方法，性能显著提升，并通过CRLB分析验证。

Conclusion: 提出的框架为复杂环境下的NLoS感知提供了高效解决方案，实现了通信与感知的双赢。

Abstract: As one of the key usage scenarios for the sixth generation (6G) wireless
networks, integrated sensing and communication (ISAC) provides an efficient
framework to achieve simultaneous wireless sensing and communication. However,
traditional wireless sensing techniques mainly rely on the line-of-sight (LoS)
assumptions, i.e., the sensing targets are directly visible to both the sensing
transmitter and receiver. This hinders ISAC systems to be applied in complex
environments such as the urban low-altitude airspace, which usually suffers
from signal blockage and non-line-of-sight (NLoS) multi-path propagation. To
address this challenge, in this paper, we propose a novel approach to enable
environment-aware NLoS ISAC by leveraging the new technique called channel
knowledge map (CKM), which was originally proposed for environment-aware
wireless communications. One major novelty of our proposed method is that the
same CKM built for wireless communication can be directly used to enable NLoS
wireless sensing, thus enjoying the benefits of ``killing two birds with one
stone''. To this end, the sensing targets are treated as virtual user equipment
(UE), and the wireless communication channel priors are transformed into the
sensing channel priors, allowing one single CKM to serve dual purposes. We
illustrate our proposed framework by a specific CKM called \emph{channel
angle-delay map} (CADM). Specifically, the proposed framework utilizes CADM to
derive angle-delay priors of the sensing channel by exploiting the relationship
between communication and sensing angle-delay distributions, enabling sensing
target localization in the challenging NLoS environment. Extensive simulation
results demonstrate significant performance improvements over classic
geometry-based sensing methods, which is further validated by Cram\'er-Rao
Lower Bound (CRLB) analysis.

</details>


### [108] [On the Distribution of Age of Information in Time-varying Updating Systems](https://arxiv.org/abs/2507.03799)
*Jin Xu,Weiqi Wang,Natarajan Gautam*

Main category: cs.IT

TL;DR: 论文研究了时变采样率下信息年龄（AoI）的分析方法，提出了基于多维偏微分方程（PDEs）的新框架，并展示了AoI的非无记忆特性及其优化策略。


<details>
  <summary>Details</summary>
Motivation: 在实时系统中，信息新鲜度（AoI）是一个关键指标，但时变采样率导致系统状态时间相关，传统稳态分析不适用，因此需要新的分析方法。

Method: 提出了基于多维PDEs的分析框架，通过分解技术将高维PDE降维求解，推导AoI分布，并扩展到稳态场景。

Result: AoI不具有无记忆性，且对采样率变化的响应存在滞后；不同预抢占概率和处理时间分布无法在所有阈值下最小化AoI违规概率。

Conclusion: 论文提出的框架有效解决了时变采样率下AoI分析的挑战，并通过优化方法平衡了成本与AoI约束。

Abstract: Age of Information (AoI) is a crucial metric for quantifying information
freshness in real-time systems where the sampling rate of data packets is
time-varying. Evaluating AoI under such conditions is challenging, as system
states become temporally correlated and traditional stationary analysis is
inapplicable. We investigate an $M_{t}/G/1/1$ queueing system with a
time-varying sampling rate and probabilistic preemption, proposing a novel
analytical framework based on multi-dimensional partial differential equations
(PDEs) to capture the time evolution of the system's status distribution. To
solve the PDEs, we develop a decomposition technique that breaks the
high-dimensional PDE into lower-dimensional subsystems. Solving these
subsystems allows us to derive the Aol distribution at arbitrary time
instances. We show AoI does not exhibit a memoryless property, even with
negligible processing times, due to its dependence on the historical sampling
process. Our framework extends to the stationary setting, where we derive a
closed-form expression for the Laplace-Stieltjes Transform (LST) of the
steady-state AoI. Numerical experiments reveal AoI exhibits a non-trivial lag
in response to sampling rate changes. Our results also show that no single
preemption probability or processing time distribution can minimize Aol
violation probability across all thresholds in either time-varying or
stationary scenarios. Finally, we formulate an optimization problem and propose
a heuristic method to find sampling rates that reduce costs while satisfying
AoI constraints.

</details>


### [109] [Study of AP Association and Users and Power Allocation for Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.03848)
*S. Mohammadzadeh,S. Mashdour,R. C. de Lamare,K. Cumanan,C. Li*

Main category: cs.IT

TL;DR: 提出了一种结合AP-UE关联策略和导频功率分配的动态信道聚类方法，以提高CCF-mMIMO网络中的频谱效率和干扰管理。


<details>
  <summary>Details</summary>
Motivation: 在密集多用户环境中，传统的AP-UE关联策略难以有效管理干扰和提升频谱效率。

Method: 采用动态信道聚类方法，基于信道相关性分组AP，并结合WSRM算法进行功率控制。

Result: 数值结果表明，该方法在高密度多用户环境中表现出优越的频谱效率和稳健性能。

Conclusion: 提出的方法在干扰管理和频谱效率方面优于现有技术，适用于高密度网络场景。

Abstract: This paper introduces an access point-user (AP-UE) association strategy
combined with pilot power allocation to mitigate multiuser interference and
enhance spectral efficiency (SE) in clustered cell-free massive MIMO
(CCF-mMIMO) networks. We propose a dynamic channel-based clustering method that
groups APs according to their channel correlation, ensuring users are
associated with APs exhibiting similar channel characteristics. The proposed
approach exploits hierarchical clustering, enabling flexible cluster sizing to
improve interference management and overall SE. Moreover, we present a power
control (PC) technique that is based on a weighted sum-rate maximization (WSRM)
algorithm to ensure consistent service quality across users. Numerical results
demonstrate that the proposed method achieves superior SE and robust
performance in high-density multi-user environments as compared to competing
approaches.

</details>


### [110] [Difference Imaging-Based Parking Lot Surveillance in Multi-RIS-Aided Collaborative ISAC System](https://arxiv.org/abs/2507.03851)
*Zhengze Ji,Yixuan Huang,Zhixin Chen,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种基于差分成像的多RIS协作ISAC系统，用于停车场监控，解决了传统监控系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统停车场监控系统（如摄像头或磁传感器）存在光照依赖、高成本和扩展性受限等问题，无线传感与RIS结合有望解决这些问题。

Method: 通过将停车场划分为二维图像网格，利用压缩感知算法捕捉车辆存在导致的散射系数变化，并采用多RIS协作提升性能。

Result: 实验结果表明，该方法能高精度检测停车位占用情况，多RIS协作进一步提高了检测率。

Conclusion: 多RIS协作的ISAC系统为停车场监控提供了一种高效、低成本的解决方案。

Abstract: Parking lot surveillance with integrated sensing and communication (ISAC)
system is one of the potential application scenarios defined by 3rd Generation
Partnership Project (3GPP). Traditional surveillance systems using cameras or
magnetic sensors face limitations such as light dependence, high costs, and
constrained scalability. Wireless sensing with reconfigurable intelligent
surfaces (RISs) has the ability to address the above limitations due to its
light independence and lower deployment overhead. In this study, we propose a
difference imaging-based multi-RIS-aided collaborative ISAC system to achieve
parking lot surveillance. In a parking lot, the presence of vehicles induces
impacts on wireless environments due to scattering characteristic variation. By
delineating the parking lot into a two-dimensional image with several grid
units, the proposed system can capture the variation of their scattering
coefficients in free and occupied states. The variation between these two
states is sparse, which can be captured through compressed sensing (CS)-based
imaging algorithms. Additionally, we collaboratively employ multiple RISs to
enable higher surveillance performance. Experimental results demonstrate that
our method can achieve high-accuracy parking occupancy detection, and the
employment of collaborative RISs further enhances the detection rate.

</details>


### [111] [Resource Allocation for Multi-waveguide Pinching Antenna-assisted Broadcast Networks](https://arxiv.org/abs/2507.03915)
*Ruotong Zhao,Shaokang Hu,Deepak Mishra,Derrick Wing Kwan Ng*

Main category: cs.IT

TL;DR: 本文研究了多介质波导辅助广播系统中的资源分配问题，提出了一种新的频率相关功率衰减模型，并通过块坐标下降法优化波束成形、功率分配和天线位置，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究多介质波导系统中如何通过优化资源分配最大化用户的最小可达速率，同时考虑实际传播效应。

Method: 提出广义频率相关功率衰减模型，采用块坐标下降法结合主优化和惩罚方法，联合优化波束成形、功率分配和天线位置。

Result: 仿真结果表明，所提框架显著优于传统天线系统和单PA波导配置，展示了波导传播损耗、路径损耗和多PA资源分配之间的复杂权衡。

Conclusion: 本文提出的方法为多介质波导系统的资源分配提供了高效且性能优越的解决方案。

Abstract: In this paper, we investigate the resource allocation for multi-dielectric
waveguide-assisted broadcast systems, where each waveguide employs multiple
pinching antennas (PAs), aiming to maximize the minimum achievable rate among
multiple users. To capture realistic propagation effects, we propose a novel
generalized frequency-dependent power attenuation model for dielectric
waveguides PA system. We jointly optimize waveguide beamforming, PA power
allocation, and antenna positions via a block coordinate descent scheme that
capitalizes on majorization minimization and penalty methods, circumventing the
inherent non-convexity of the formulated optimization problem and obtaining a
computationally efficient sub-optimal solution. Simulation results demonstrate
that our proposed framework substantially outperforms both conventional antenna
systems and single-PA-per-waveguide configurations, clearly illustrating the
intricate trade-offs between waveguide propagation loss, path loss, and
resource allocation among multiple PAs.

</details>


### [112] [Age-Aware CSI Acquisition of a Finite-State Markovian Channel](https://arxiv.org/abs/2507.05042)
*Onur Ayan,Jiping Luo,Xueli An,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 本文研究了部分可观测无线系统中信息新鲜度（AoI）与信道估计的交互问题，提出了一种基于部分可观测马尔可夫决策过程（POMDP）的优化方法，以平衡数据传输与信道状态信息获取之间的资源分配。


<details>
  <summary>Details</summary>
Motivation: 信息新鲜度（AoI）是衡量信息时效性的重要指标，但其在部分可观测无线系统中的交互作用尚未充分研究。特别是在信道状态信息获取过程中数据传输中断的情况下，如何优化资源分配成为一个关键问题。

Method: 假设无线信道为有限状态马尔可夫信道，将问题建模为部分可观测马尔可夫决策过程（POMDP），并通过相对值迭代算法获得最优策略。

Result: 仿真结果表明，所提出的方法在平衡数据传输与信道状态信息获取方面具有高效性。

Conclusion: 本文首次提出了考虑信道状态信息老化效应的最优调度策略，为部分可观测无线系统中的资源分配问题提供了新思路。

Abstract: The Age of Information (AoI) has emerged as a critical metric for quantifying
information freshness; however, its interplay with channel estimation in
partially observable wireless systems remains underexplored. This work
considers a transmitter-receiver pair communicating over an unreliable channel
with time-varying reliability levels. The transmitter observes the
instantaneous link reliability through a channel state information acquisition
procedure, during which the data transmission is interrupted. This leads to a
fundamental trade-off between utilizing limited network resources for either
data transmission or channel state information acquisition to combat the
channel aging effect. Assuming the wireless channel is modeled as a
finite-state Markovian channel, we formulate an optimization problem as a
partially observable Markov decision process (POMDP), obtain the optimal policy
through the relative value iteration algorithm, and demonstrate the efficiency
of our solution through simulations. To the best of our knowledge, this is the
first work to aim for an optimal scheduling policy for data transmissions while
considering the effect of channel state information aging.

</details>


### [113] [FollowSpot: Enhancing Wireless Communications via Movable Ceiling-Mounted Metasurfaces](https://arxiv.org/abs/2507.03918)
*Wenhai Lai,Kaiming Shen,Rui Zhang*

Main category: cs.IT

TL;DR: 本文研究了天花板安装的超表面（MTS）的最优放置问题，以聚焦无线信号到目标接收器，提出了一个高效的算法来解决复杂的非线性离散优化问题。


<details>
  <summary>Details</summary>
Motivation: 受剧院聚光灯启发，研究如何通过优化MTS的放置来最大化信号信噪比（SNR）。

Method: 通过引入连续辅助变量μ解耦MTS的放置变量，将问题转化为离散优化问题，并开发了一个时间复杂度为O(ML²log(ML))的高效算法。

Result: 算法不仅保证全局最优解，还能高效地找到最优解。

Conclusion: 提出的方法有效解决了MTS放置的复杂优化问题，为实际应用提供了高效解决方案。

Abstract: This paper studies the optimal placement of ceiling-mounted metasurfaces
(MTSs) to help focus the wireless signal beam onto the target receiver, as
inspired by the theatre spotlight. We assume that a total of $M$ MTSs are
deployed, and that there are $L$ possible positions for each MTS. The resulting
signal-to-noise (SNR) maximization problem is difficult to tackle directly
because of the coupling between the placement decisions of the different MTSs.
Mathematically, we are faced with a nonlinear discrete optimization problem
with $L^M$ possible solutions. A remarkable result shown in this paper is that
the above challenging problem can be efficiently solved within
$O(ML^2\log(ML))$ time. There are two key steps in developing the proposed
algorithm. First, we successfully decouple the placement variables of different
MTSs by introducing a continuous auxiliary variable $\mu$; the discrete primal
variables are now easy to optimize when $\mu$ is held fixed, but the
optimization problem of $\mu$ is nonconvex. Second, we show that the
optimization of continuous $\mu$ can be recast into a discrete optimization
problem with only $LM$ possible solutions, so the optimal $\mu$ can now be
readily obtained. Numerical results show that the proposed algorithm can not
only guarantee a global optimum but also reach the optimal solution
efficiently.

</details>


### [114] [Invariants for sum-rank metric codes](https://arxiv.org/abs/2507.04158)
*Paolo Santonastaso,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 论文提出新的不变量（广义理想化子、中心化子、中心和改进的线性概念）和核参数，用于解决和秩度量码的等价性问题，并开发了基于斜多项式的计算框架。


<details>
  <summary>Details</summary>
Motivation: 解决和秩度量码的等价性问题，传统工具在此领域表现不足。

Method: 引入新的不变量和核参数，开发基于斜多项式的计算框架。

Result: 能够研究已知最大和秩距离码（MSRD）的等价性。

Conclusion: 新方法为和秩度量码的等价性问题提供了有效工具。

Abstract: The code equivalence problem is central in coding theory and cryptography.
While classical invariants are effective for Hamming and rank metrics, the
sum-rank metric, which unifies both, introduces new challenges. This paper
introduces new invariants for sum-rank metric codes: generalised idealisers,
the centraliser, the center, and a refined notion of linearity. These lead to
the definition of nuclear parameters, inspired by those used in division
algebra theory, where they are crucial for proving inequivalence. We also
develop a computational framework based on skew polynomials, which is isometric
to the classical matrix setting but enables explicit computation of nuclear
parameters for known MSRD (Maximum Sum-Rank Distance) codes. This yields a new
and effective method to study the code equivalence problem where traditional
tools fall short. In fact, using nuclear parameters, we can study the
equivalence among the largest families of known MSRD codes.

</details>


### [115] [An Efficient Max-Min Fair Resource Optimization Algorithm for Rate-Splitting Multiple Access](https://arxiv.org/abs/2507.04201)
*Facheng Luo,Yijie Mao*

Main category: cs.IT

TL;DR: 提出了一种名为EG-FP的新算法，用于解决RSMA中的MMF问题，通过FP和变分不等式方法降低计算复杂度，并在大规模天线场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统算法在解决RSMA中的MMF问题时存在高计算复杂度或性能下降的挑战。

Method: 使用FP将问题转化为块状凸问题，并通过变分不等式和极梯度算法解决子问题，同时发现最优波束成形结构。

Result: 数值结果表明，EG-FP算法的性能接近SCA算法，但计算时间仅为后者的10%。

Conclusion: EG-FP算法高效且可扩展，特别适合大规模天线场景。

Abstract: The max-min fairness (MMF) problem in rate-splitting multiple access (RSMA)
is known to be challenging due to its non-convex and non-smooth nature, as well
as the coupled beamforming and common rate variables. Conventional algorithms
to address this problem often incur high computational complexity or degraded
MMF rate performance. To address these challenges, in this work, we propose a
novel optimization algorithm named extragradient-fractional programming (EG-FP)
to address the MMF problem of downlink RSMA. The proposed algorithm first
leverages FP to transform the original problem into a block-wise convex
problem. For the subproblem of precoding block, we show that its Lagrangian
dual is equivalent to a variational inequality problem, which is then solved
using an extragradient-based algorithm. Additionally, we discover the optimal
beamforming structure of the problem and based on which, we introduce a
low-dimensional EG-FP algorithm with computational complexity independent of
the number of transmit antennas. This feature is especially beneficial in
scenarios with a large number of transmit antennas. The proposed algorithms are
then extended to handle imperfect channel state information at the transmitter
(CSIT). Numerical results demonstrate that the MMF rate achieved by our
proposed algorithms closely matches that of the conventional successive convex
approximation (SCA) algorithm and significantly outperforms other baseline
schemes. Remarkably, the average CPU time of the proposed algorithms is less
than 10\% of the runtime required by the SCA algorithm, showing the efficiency
and scalability of the proposed algorithms.

</details>


### [116] [Mutual Information Bounds for Lossy Common Information](https://arxiv.org/abs/2507.04209)
*Anderson de Andrade*

Main category: cs.IT

TL;DR: 论文通过分析Gray-Wyner网络中的目标互信息，将Wyner的有损公共信息和Gács-Körner的有损公共信息分开，推广了Wyner（1975）的无损情况结果。


<details>
  <summary>Details</summary>
Motivation: 研究Gray-Wyner网络中目标互信息的界限，以区分不同形式的公共信息。

Method: 通过理论分析，将Wyner的有损公共信息和Gács-Körner的有损公共信息分开。

Result: 提出了一个界限，能够区分两种不同的有损公共信息形式。

Conclusion: 研究结果推广了Wyner的无损情况，为理解有损公共信息提供了新的视角。

Abstract: We show the mutual information between the targets in a Gray-Wyner Network as
a bound that separates Wyner's lossy common information and G\'acs-K\"orner
lossy common information. The results are a generalization of the lossless case
presented by Wyner (1975).

</details>


### [117] [Digital-Twin Empowered Site-Specific Radio Resource Management in 5G Aerial Corridor](https://arxiv.org/abs/2507.04566)
*Pulok Tarafder,Imtiaz Ahmed,Danda B. Rawat,Md. Zoheb Hassan,Kamrul Hasan*

Main category: cs.IT

TL;DR: 论文提出了一种基于信道孪生（CT）的资源分配框架，用于解决无人机走廊网络中的基站关联和波束选择问题，通过两阶段优化显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 无人机的高空、移动性和三维运动导致频繁切换和复杂的波束对准问题，尤其在密集基站部署和信号条件多变的环境中。

Method: 利用CT提供高保真信道状态信息（CSI），分两阶段优化：第一阶段选择基站阵列级波束成形权重以最大化天线增益；第二阶段联合优化无人机-基站-波束关联以最大化端到端吞吐量。

Result: 仿真结果表明，CT驱动的策略在多种场景下显著优于基线方法，验证了数字孪生信道模型与跨层资源优化的有效性。

Conclusion: 通过整合精确的数字孪生信道模型和跨层优化，该框架显著提升了无人机走廊通信的性能。

Abstract: Base station (BS) association and beam selection in multi-cell drone corridor
networks present unique challenges due to the high altitude, mobility and
three-dimensional movement of drones. These factors lead to frequent handovers
and complex beam alignment issues, especially in environments with dense BS
deployments and varying signal conditions. To address these challenges, this
paper proposes a channel-twin (CT) enabled resource-allocation framework for
drone-corridor communications, where the CT constitutes the radio-channel
component of a broader digital-twin (DT) environment. The CT supplies
high-fidelity channel-state information (CSI), which drives a two-stage
optimization procedure. In Stage 1, array-level beamforming weights at each BS
are selected to maximize antenna gain. In Stage 2, the framework jointly
optimizes drone-BS-beam associations at discrete corridor way-points to
maximize end-to-end throughput. Simulation results confirm that the CT-driven
strategy delivers significant throughput gains over baseline methods across
diverse operating scenarios, validating the effectiveness of integrating
precise digital-twin channel models with cross-layer resource optimization.

</details>


### [118] [On subcodes of the generalized Reed-Solomon codes](https://arxiv.org/abs/2507.04689)
*Yu Ning*

Main category: cs.IT

TL;DR: 研究了广义Reed-Solomon（GRS）码的一类子码，给出了这些子码自对偶或近MDS的等价刻画，并提出了自对偶近MDS子码的构造。


<details>
  <summary>Details</summary>
Motivation: 扩展文献中关于r=1的结果，研究GRS码子码的自对偶性和近MDS性质。

Method: 通过从GRS码生成矩阵中移除特定行，构造子码，并分析其性质。

Result: 给出了子码自对偶或近MDS的等价刻画，并构造了相关子码家族。

Conclusion: 子码的对偶码可能是扭曲GRS码，某些情况下子码对偶封闭。

Abstract: In this paper, we study a class of subcodes of codimension $1$ in the
$[n,k+1]_q$ generalized Reed-Solomon (GRS) codes, whose generator matrix is
derived by removing the row of degree $k-r$ from the generator matrix of the
$[n,k+1]_q$ GRS codes, where $1 \le r \le k-1$. We show equivalent
characterizations for this class of subcodes of the GRS codes being self-dual
or near-MDS, which extends the results for $r=1$ in the literature. Along with
these characterizations, families of self-dual near-MDS subcodes of the GRS
codes are also proposed. Finally, for $r = 1,2$, the dual codes of the subcodes
of the GRS codes are found out. In some cases, the subcodes of the GRS codes
can be closed under taking dual codes. In other cases, the dual codes turn out
to be the twisted GRS codes.

</details>


### [119] [Correcting Bursty/Localized Deletions: A New Error-Position-Estimation Code](https://arxiv.org/abs/2507.04797)
*Zuo Ye,Yubo Sun,Gennian Ge*

Main category: cs.IT

TL;DR: 论文提出了新的位置估计码构造方法，用于纠正突发删除和局部删除，冗余度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究目标是构造冗余度最小的码，以纠正突发删除和局部删除。

Method: 通过选择差分序列为强局部平衡的码字，并施加VT型约束和L1权重约束，构造新的位置估计码。

Result: 对于特定条件，新方法实现了冗余度为log n + (t-1)log log n + O(1)的码。

Conclusion: 新方法不仅改进了冗余度，且构造更简单，同时提供了高效的编码器。

Abstract: Codes correcting bursts of deletions and localized deletions have garnered
significant research interest in recent years. One of the primary objectives is
to construct codes with minimal redundancy. Currently, the best known
constructions of $q$-ary codes correcting a burst of at most $t$ deletions
($(\le t)$-burst-deletion correcting codes) achieve redundancy $\log
n+8\log\log n+o(\log\log n)$ (for any $q$ and $t$) or $\log n+t\log\log n+O(1)$
(for even $q$). For codes correcting single $t$-localized-deletion
($t$-localized-deletion correcting codes), state-of-the-art constructions
attain redundancy $\log n+O\parenv{t(\log\log n)^2}$ (for any $q$ and $t$) or
$\log n+2t\log\log n+O(1)$ (for even $q$). Here, $n$ denotes the code-length,
and $q$ and $t$ are fixed. These codes employ a position-estimation component
to approximate error positions, augmented by additional constraints that enable
error-correction given the information about error positions.
  In this work, we select codewords from the set of sequences whose
differential sequences are strong-$(\ell,\epsilon)$-locally-balanced. By
imposing a VT-type constraint and an $L_1$-weight constraint on the
differential sequences of codewords, we construct novel position-estimation
codes. When $q\ge 2$ and $t<q$, or $q$ is even and $t<2q$, this approach gives
a $q$-ary $(\le t)$-burst-deletion correcting code and a $t$-localized-deletion
correcting code with redundancy $\log n+(t-1)\log\log n+O(1)$. In addition to
improving previous redundancy, the method is new and our position-estimation
codes are simpler than those in previous works. Finally, we give an efficient
encoder to encode an arbitrary input sequence into a sequence whose
differential sequence is strong-$(\ell,\epsilon)$-locally-balanced. To our
knowledge, no prior algorithm for this specific task has been reported.

</details>


### [120] [On the Maximum Size of Codes Under the Damerau-Levenshtein Metric](https://arxiv.org/abs/2507.04806)
*Zuo Ye,Gennian Ge*

Main category: cs.IT

TL;DR: 本文研究了Damerau-Levenshtein距离下的纠错码，建立了码大小的理论上界，并证明Wang等人的码在冗余度上是最优的。


<details>
  <summary>Details</summary>
Motivation: Damerau-Levenshtein距离在DNA存储系统中有应用，但相关纠错码的研究有限，本文旨在填补这一理论空白。

Method: 通过理论分析，建立了Damerau-Levenshtein距离下码大小的上界。

Result: 证明了Wang等人提出的码在冗余度上是最优的。

Conclusion: 本文为Damerau-Levenshtein距离下的纠错码提供了理论支持，填补了研究空白。

Abstract: The Damerau-Levenshtein distance between two sequences is the minimum number
of operations (deletions, insertions, substitutions, and adjacent
transpositions) required to convert one sequence into another. Notwithstanding
a long history of this metric, research on error-correcting codes under this
distance has remained limited. Recently, motivated by applications in DNA-based
storage systems, Gabrys \textit{et al} and Wang \texit{et al} reinvigorated
interest in this metric. In their works, some codes correcting both deletions
and adjacent transpositions were constructed. However, theoretical upper bounds
on code sizes under this metric have not yet been established. This paper seeks
to establish upper bounds for code sizes in the Damerau-Levenshtein metric. Our
results show that the code correcting one deletion and asymmetric adjacent
transpositions proposed by Wang \textit{et al} achieves optimal redundancy up
to an additive constant.

</details>


### [121] [Kalman Filter Aided Federated Koopman Learning](https://arxiv.org/abs/2507.04808)
*Yutao Chen,Wei Chen*

Main category: cs.IT

TL;DR: 论文提出了一种结合卡尔曼滤波和联邦学习的Koopman学习方法（KF-FedKL），用于在仅有观测数据且数据不足的情况下实现非线性系统的线性化。


<details>
  <summary>Details</summary>
Motivation: 现有Koopman学习方法依赖准确系统状态和高质量数据，但在实际场景中难以满足。本文旨在解决仅能获取观测数据且数据不足的问题。

Method: 提出KF-FedKL框架，结合卡尔曼滤波（UKF和URTSS）和联邦学习（改进的FedAvg算法），通过深度Koopman网络实现线性化。

Result: 通过数值模拟验证了KF-FedKL在不同场景下的性能，展示了其有效性和隐私保护能力。

Conclusion: KF-FedKL为非线性系统线性化提供了一种实用且隐私保护的解决方案，适用于实际应用场景。

Abstract: Real-time control and estimation are pivotal for applications such as
industrial automation and future healthcare. The realization of this vision
relies heavily on efficient interactions with nonlinear systems. Therefore,
Koopman learning, which leverages the power of deep learning to linearize
nonlinear systems, has been one of the most successful examples of mitigating
the complexity inherent in nonlinearity. However, the existing literature
assumes access to accurate system states and abundant high-quality data for
Koopman analysis, which is usually impractical in real-world scenarios. To fill
this void, this paper considers the case where only observations of the system
are available and where the observation data is insufficient to accomplish an
independent Koopman analysis. To this end, we propose Kalman Filter aided
Federated Koopman Learning (KF-FedKL), which pioneers the combination of Kalman
filtering and federated learning with Koopman analysis. By doing so, we can
achieve collaborative linearization with privacy guarantees. Specifically, we
employ a straightforward yet efficient loss function to drive the training of a
deep Koopman network for linearization. To obtain system information devoid of
individual information from observation data, we leverage the unscented Kalman
filter and the unscented Rauch-Tung-Striebel smoother. To achieve collaboration
between clients, we adopt the federated learning framework and develop a
modified FedAvg algorithm to orchestrate the collaboration. A convergence
analysis of the proposed framework is also presented. Finally, through
extensive numerical simulations, we showcase the performance of KF-FedKL under
various situations.

</details>


### [122] [Fast and Provable Hankel Tensor Completion for Multi-measurement Spectral Compressed Sensing](https://arxiv.org/abs/2507.04847)
*Jinsheng Li,Xu Zhang,Shuang Wu,Wei Cui*

Main category: cs.IT

TL;DR: 提出了一种新的低秩Hankel张量补全方法（ScalHT），用于多测量谱压缩感知问题，显著提升了计算和存储效率。


<details>
  <summary>Details</summary>
Motivation: 解决多测量谱压缩感知问题，利用Hankel张量的低秩特性，提升效率和恢复性能。

Method: 将多信号提升为Hankel张量，设计ScalHT算法，结合低秩Tucker分解和Hankel结构，实现高效计算。

Result: ScalHT在存储和计算效率上提升$O(\min\{s,n\})$倍，且恢复性能优于现有方法。

Conclusion: ScalHT方法高效且理论可靠，为低秩Hankel张量补全提供了首个恢复和线性收敛保证。

Abstract: In this paper, we introduce a novel low-rank Hankel tensor completion
approach to address the problem of multi-measurement spectral compressed
sensing. By lifting the multiple signals to a Hankel tensor, we reformulate
this problem into a low-rank Hankel tensor completion task, exploiting the
spectral sparsity via the low multilinear rankness of the tensor. Furthermore,
we design a scaled gradient descent algorithm for Hankel tensor completion
(ScalHT), which integrates the low-rank Tucker decomposition with the Hankel
structure. Crucially, we derive novel fast computational formulations that
leverage the interaction between these two structures, achieving up to an
$O(\min\{s,n\})$-fold improvement in storage and computational efficiency
compared to the existing algorithms, where $n$ is the length of signal, $s$ is
the number of measurement vectors. Beyond its practical efficiency, ScalHT is
backed by rigorous theoretical guarantees: we establish both recovery and
linear convergence guarantees, which, to the best of our knowledge, are the
first of their kind for low-rank Hankel tensor completion. Numerical
simulations show that our method exhibits significantly lower computational and
storage costs while delivering superior recovery performance compared to prior
arts.

</details>


### [123] [Circular Holographic MIMO Beamforming for Integrated Data and Energy Multicast Systems](https://arxiv.org/abs/2507.05057)
*Qingxiao Huang,Yizhe Zhao,Jie Hu,Kun Yang,Yuguang Fang*

Main category: cs.IT

TL;DR: 论文提出了一种基于超材料的H-MIMO系统，用于实现近场信道的高空间多样性和低硬件复杂度，同时研究了数据与能量多播（IDEM）的波束成形设计。


<details>
  <summary>Details</summary>
Motivation: 利用超材料和圆形天线阵列的H-MIMO系统，可以充分利用近场信道实现更广的能量聚焦范围和更高的数据速率。

Method: 推导了3D空间中的近场分辨率函数，证明了近场信道的渐近空间正交性，并设计了基于交替优化的波束成形方案。

Result: 数值结果验证了分辨率函数和渐近正交性的正确性，提出的低复杂度波束成形方案优于基准方案。

Conclusion: H-MIMO系统在IDEM应用中表现出色，提出的波束成形设计具有高效性和低复杂度。

Abstract: Thanks to the application of metamaterials, holographic multiple-input
multiple-output (H-MIMO) is expected to achieve a higher spatial diversity gain
with lower hardware complexity. With the aid of a circular antenna arrangement
of H-MIMO, integrated data and energy multicast (IDEM) can fully exploit the
near-field channel to realize wider range of energy focusing and higher
achievable rate. In this paper, we derive the closed-form near-field resolution
function in 3D space and show the asymptotic spatial orthogonality of
near-field channel for circular antenna array. We then investigate the
beamforming designs for IDEM systems, where the minimum rate of data users
(DUs) are maximized while guaranteeing the energy harvesting requirements for
energy users (EUs). Specifically, the asymptotically optimal fully-digital
beamformer is first obtained based on the spatial orthogonality. Then, the
alternating optimization is adopted for the H-MIMO beamforming, where the
digital beamformer is obtained in closed form and the analog beamformers of
three different control modes are then obtained, respectively. Scaling schemes
are also investigated to further improve the IDEM performance. Numerical
results verify the correctness of the resolution function and asymptotic
orthogonality. Moreover, the proposed beamforming schemes with very low
complexity outperform benchmark schemes.

</details>


### [124] [LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks](https://arxiv.org/abs/2507.05121)
*Jiajia Guo,Peiwen Jiang,Chao-Kai Wen,Shi Jin,Jun Zhang*

Main category: cs.IT

TL;DR: LVM4CSI提出了一种通用框架，利用计算机视觉预训练模型处理无线通信中的CSI任务，无需微调，性能优于任务专用神经网络。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI方法依赖任务专用神经网络和大量训练数据的问题，提高CSI获取的通用性和实用性。

Method: 将CSI任务映射为类似计算机视觉任务，转换CSI为视觉格式，结合轻量可训练层适应通信目标。

Result: 在信道估计、活动识别和用户定位等任务中表现优异，显著减少参数数量和设计需求。

Conclusion: LVM4CSI为无线通信系统提供了一种高效、通用的解决方案，具有广泛的应用潜力。

Abstract: Accurate channel state information (CSI) is critical to the performance of
wireless communication systems, especially with the increasing scale and
complexity introduced by 5G and future 6G technologies. While artificial
intelligence (AI) offers a promising approach to CSI acquisition and
utilization, existing methods largely depend on task-specific neural networks
(NNs) that require expert-driven design and large training datasets, limiting
their generalizability and practicality. To address these challenges, we
propose LVM4CSI, a general and efficient framework that leverages the
structural similarity between CSI and computer vision (CV) data to directly
apply large vision models (LVMs) pre-trained on extensive CV datasets to
wireless tasks without any fine-tuning, in contrast to large language
model-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI
tasks to analogous CV tasks, transforms complex-valued CSI into visual formats
compatible with LVMs, and integrates lightweight trainable layers to adapt
extracted features to specific communication objectives. We validate LVM4CSI
through three representative case studies, including channel estimation, human
activity recognition, and user localization. Results demonstrate that LVM4CSI
achieves comparable or superior performance to task-specific NNs, including an
improvement exceeding 9.61 dB in channel estimation and approximately 40%
reduction in localization error. Furthermore, it significantly reduces the
number of trainable parameters and eliminates the need for task-specific NN
design.

</details>
