<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.IT](#cs.IT) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Impact of Sociality Regimes on Quality of Service and Energy Efficiency in Cell-Free MIMO Networks](https://arxiv.org/abs/2512.22127)
*Ala Eddine Nouali,Mohamed Sana,Jean-Paul Jamont*

Main category: cs.NI

TL;DR: 本文研究无蜂窝网络中用户设备与接入点集群的社会性匹配问题，通过自私、平等、利他三种社会性机制平衡服务质量与能效，提出基于延迟接受和早期接受的匹配算法。


<details>
  <summary>Details</summary>
Motivation: 在无蜂窝网络中，用户中心化集群是实现网络可扩展性的实用方法，但如何为每个用户设备选择最优的接入点集群面临挑战。需要考虑有限的前传和处理能力，以及动态调整以提升能效同时满足服务质量要求，这导致了用户设备和接入点集群之间的利益冲突。

Method: 将问题建模为具有外部性的多对多社会匹配博弈，根据用户设备和接入点集群的社会性机制（自私、平等、利他）建立连接。提出了两种基于延迟接受和早期接受匹配游戏的新算法来解决这一问题。

Result: 数值结果表明，用户设备和接入点集群采用的社会性机制对用户的服务质量满意度和能效有显著影响。当双方都采用平等机制时，能够实现最佳的性能权衡。

Conclusion: 在无蜂窝网络中，用户设备和接入点集群的社会性机制是平衡服务质量与能效的关键因素。平等机制为双方提供了最佳的性能权衡，为解决网络可扩展性和能效优化问题提供了有效方案。

Abstract: The cell-free architecture represents a significant advancement in network design, where each User Equipment (UE) is served by a group of distributed Access Points (APs), aimed at delivering uniformly high data rates to UEs across all locations. To ensure network scalability, user-centric clustering (UCC) has emerged as a practical approach, wherein only a selected subset of preferred APs jointly serves each UE. Forming the optimal cluster of APs for each UE is a challenging task, particularly when limited fronthaul and processing capabilities of both APs and UEs are considered. This challenge is exacerbated by the need for dynamic adjustments to enhance energy efficiency while meeting quality of service (QoS) requirements, which introduces conflicting interests between these entities. In this paper, we investigate the sociality regime of UEs and their clusters of APs, characterizing their intra-team cooperation as either selfish, egalitarian, or altruistic. These sociality regimes are crucial in achieving a balance between QoS and energy efficiency per UE. We address this problem by modeling it as a many-to-many social matching game with externalities, where connections can be established based on the sociality regime of both teams. To solve this, we introduce two novel algorithms based on Deferred Acceptance (DA) and Early Acceptance (EA) matching games. Numerical results show the significant impact of the sociality regime adopted by UEs and their clusters of APs on UE's QoS satisfaction and energy efficiency, with the egalitarian regime adopted by both entities proving the best performance trade-off.

</details>


### [2] [A Lightweight Coordinate-Conditioned Diffusion Approach for 6G C-V2X Radio Environment Maps](https://arxiv.org/abs/2512.22535)
*Liu Cao,Zhaoyu Liu,Dongyu Wei,Yuan Yang,Yukun Pan,Lyutianyang Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种基于坐标条件去噪扩散概率模型（CCDDPM）的轻量级生成方法，用于预测6G V2X无线电环境地图（REM），以解决发射车辆因缺乏动态高保真REM而面临的物理层问题。


<details>
  <summary>Details</summary>
Motivation: 发射车辆在广播6G C-V2X消息时容易受到物理层问题的影响，主要原因是缺乏具有动态位置变化的动态高保真无线电环境地图（REM）。现有方法无法为任意坐标的发射车辆快速生成场景一致的REM。

Method: 提出坐标条件去噪扩散概率模型（CCDDPM），利用特定区域内有限历史发射车辆的信号强度6G V2X REM数据，预测同一区域内任意坐标发射车辆的REM。将发射车辆坐标编码为平滑高斯先验，通过轻量级双通道条件U-Net架构与高斯噪声融合。

Result: 预测的REM在统计特性和结构上与真实REM高度匹配，同时表现出比广泛应用的生成AI方法更好的稳定性。该方法能够快速生成场景一致且适用于任意发射坐标的REM。

Conclusion: 该方法支持更高效的6G C-V2X通信，使发射车辆更不容易受到物理层问题的影响，通过快速生成任意坐标的REM来改善通信可靠性。

Abstract: Transmitter vehicles that broadcast 6G Cellular Vehicle-to-Everything (C-V2X)-based messages, e.g., Basic Safety Messages (BSMs), are prone to be impacted by PHY issues due to the lack of dynamic high-fidelity Radio Environment Map (REM) with dynamic location variation. This paper explores a lightweight diffusion-based generative approach, the Coordinate-Conditioned Denoising Diffusion Probabilistic Model (CCDDPM), that leverages the signal intensity-based 6G V2X Radio Environment Map (REM) from limited historical transmitter vehicles in a specific region, to predict the REMs for a transmitter vehicle with arbitrary coordinates across the same region. The transmitter vehicle coordinate is encoded as a smooth Gaussian prior and fused with the Gaussian noise through a lightweight two-channel conditional U-Net architecture. We demonstrate that the predicted REM closely matches the statistics and structure of ground-truth REM while exhibiting the improved stability and over other widely applied generative AI approaches. The resulting predictor enables rapid and scenario-consistent REM with arbitrary transmitter coordinates, which thereby supports more efficient 6G C-V2X communications where transmitter vehicles are less likely to suffer from the PHY issues.

</details>


### [3] [A Novel Approach for a Smart IoMT-Based BAN for an Old Home Healthcare Monitoring System Using Starlink](https://arxiv.org/abs/2512.22553)
*Shermin Sultana Setu,Mst. Amena Akter Pinky,Md. Abdul Awal,Sheekar Banerjee,Ishtiak Al Mamoon*

Main category: cs.NI

TL;DR: 提出基于Starlink卫星通信的医疗物联网老年人健康监测系统，通过QoS技术优先传输关键生理数据，在偏远地区实现可靠远程监护。


<details>
  <summary>Details</summary>
Motivation: 当前老年人护理系统面临长期护理资源不足和医疗提供者间沟通不畅的挑战，特别是在偏远和服务不足地区，需要更可靠的远程监测解决方案。

Method: 设计Starlink辅助的IoMT架构，监测ECG、体温、心脏病指标和跌倒检测等关键参数。采用FQ+CoDel+DSCP的QoS技术，通过本地通信枢纽和LEO卫星链路传输数据到医疗中心。

Result: NS-3仿真结果显示，该系统在吞吐量、延迟和可靠性方面优于现有解决方案，证明Starlink作为高性能远程医疗通信工具的潜力。

Conclusion: 该研究提供了一个可扩展、可复制的卫星辅助医疗系统框架，优先考虑QoS性能和改善老年人护理，为未来远程医疗系统发展提供参考。

Abstract: The rapid evolution of the Internet of Medical Things (IoMT) technology has become a transformative force in modern healthcare, particularly in elderly patient management. The current elderly care system faces significant challenges, including insufficient long-term care resources and poor communication between healthcare providers. To address this limitation, this study introduces a novel Starlink-assisted IOMT-based elderly healthcare model designed to improve remote patient monitoring and communication reliability. This proposal system focused on a monitoring system of key biomedical parameters such as electrocardiogram (ECG), body temperature, heart attack indicators, and a fall detection alert system. Performance is evaluated using the network simulator (NS-3) to assess its effectiveness in remote and underserved regions. Physiological data collected from patients are transmitted through a local communication hub and forwarded over a Low Earth Orbit (LEO) satellite link to a medical center. Based on Quality of Service (QoS) technology that combines Flow Queuing (FQ) with Controlled Delay (CoDel) with Differentiated Services Code Point (DSCP) marking. This approach prioritizes critical health data for faster transmission while allocating lower priority to non-urgent information. This architecture is entirely wireless, allowing continuous monitoring, real-time alerts, and secure data storage for medical analysis. The simulation results demonstrate that the proposed Starlink-enabled IOMT system outperforms existing solutions in terms of throughput, latency, and reliability. The findings highlight Starlink's potential as a robust and high-performance telehealth communication tool. In addition, this study provides a scalable and reproducible framework for future satellite-assisted healthcare systems that prioritize quality of service (QoS) performance and improved elderly patient care.

</details>


### [4] [Cyber Resilience in Next-Generation Networks: Threat Landscape, Theoretical Foundations, and Design Paradigms](https://arxiv.org/abs/2512.22721)
*Junaid Farooq,Quanyan Zhu*

Main category: cs.NI

TL;DR: 本书深入探讨了SDN、NFV、O-RAN和云原生架构等网络技术演进如何重塑关键基础设施的运营格局和威胁面，提出了重新概念化和重新设计弹性机制以应对这些转型带来的多方面挑战。


<details>
  <summary>Details</summary>
Motivation: 网络系统演进（SDN、NFV、O-RAN、云原生架构）正在重新定义关键基础设施的运营格局和威胁面，需要重新概念化和重新设计弹性机制来应对这些转型带来的新挑战。

Method: 采用跨学科方法，通过六章结构：首先调查当代风险格局，识别新兴威胁；然后建立弹性的严格定义和评估框架；最后深入探讨零信任架构、博弈论威胁建模、自愈设计原则等高级范式，特别关注人工智能（强化学习、大语言模型）在动态威胁响应和自主网络控制中的作用。

Result: 本书提供了对网络弹性重新概念化的全面框架，超越了传统的鲁棒性和容错性，提出了适应、预测和回顾性机制，并展示了人工智能技术在应对网络威胁中的关键作用。

Conclusion: 网络技术的演进要求从根本上重新思考弹性设计，需要采用跨学科方法和先进的人工智能技术来构建能够应对复杂威胁的适应性、预测性网络系统。

Abstract: The evolution of networked systems, driven by innovations in software-defined networking (SDN), network function virtualization (NFV), open radio access networks (O-RAN), and cloud-native architectures, is redefining both the operational landscape and the threat surface of critical infrastructures. This book offers an in-depth, interdisciplinary examination of how resilience must be re-conceptualized and re-engineered to address the multifaceted challenges posed by these transformations.
  Structured across six chapters, this book begins by surveying the contemporary risk landscape, identifying emerging cyber, physical, and AI-driven threats, and analyzing their implications for scalable, heterogeneous network environments. It then establishes rigorous definitions and evaluation frameworks for resilience, going beyond robustness and fault-tolerance to address adaptive, anticipatory, and retrospective mechanisms across diverse application domains.
  The core of the book delves into advanced paradigms and practical strategies for resilience, including zero trust architectures, game-theoretic threat modeling, and self-healing design principles. A significant portion is devoted to the role of artificial intelligence, especially reinforcement learning and large language models (LLMs), in enabling dynamic threat response, autonomous network control, and multi-agent coordination under uncertainty.

</details>


### [5] [Hierarchical Decision Mamba Meets Agentic AI: A Novel Approach for RAN Slicing in 6G](https://arxiv.org/abs/2512.23502)
*Md Arafat Habib,Medhat Elsayed,Majid Bavand,Pedro Enrique Iturria Rivera,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 提出基于Agentic AI框架的6G RAN切片方案，使用超级代理结合LLM和分层决策Mamba控制器，实现自然语言意图理解与协调决策，相比现有方法在吞吐量、边缘性能和延迟方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有RAN切片方案通常依赖静态映射和SLA转换，缺乏自然语言理解与协调决策的整合，难以灵活满足切片特定服务等级协议需求。

Method: 提出Agentic AI框架，构建由大型语言模型和分层决策Mamba控制器组成的超级代理。LLM负责解释运营商意图并转化为可执行目标，HDM协调切片间、切片内和自愈代理。

Result: 相比基于Transformer和奖励驱动的基线方法，该框架在关键性能指标上表现一致提升，包括更高吞吐量、改善的边缘性能和降低的跨切片延迟。

Conclusion: Agentic AI框架成功整合自然语言理解与协调决策，为6G RAN切片提供更灵活高效的解决方案，能更好地满足运营商意图和SLA要求。

Abstract: Radio Access Network (RAN) slicing enables multiple logical networks to exist on top of the same physical infrastructure by allocating resources to distinct service groups, where radio resource scheduling plays a key role in ensuring compliance with slice-specific Service-Level Agreements (SLAs). Existing configuration-based or intent-driven Reinforcement Learning (RL) approaches usually rely on static mappings and SLA conversions. The current literature does not integrate natural language understanding with coordinated decision-making. To address these limitations, we propose an Agentic AI framework for 6G RAN slicing, driven by a super agent built using Hierarchical Decision Mamba (HDM) controllers and a Large Language Model (LLM). The super agent interprets operator intents and translates them into actionable goals using the LLM, which are used by HDM to coordinate inter-slice, intra-slice, and self-healing agents. Compared to transformer-based and reward-driven baselines, the proposed Agentic AI framework demonstrates consistent improvements across key performance indicators, including higher throughput, improved cell-edge performance, and reduced latency across different slices.

</details>


### [6] [Distributed Accountability in Democracy: Using MANETs and DTNs in the Face of Acts of Questionable Legality](https://arxiv.org/abs/2512.23653)
*Mathew Schmidheiser,Milena Radenkovic*

Main category: cs.NI

TL;DR: 该研究比较了Epidemic和Wave两种DTN路由协议在敏感通信场景下的表现，分析了各自在不同情境下的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在涉及可疑合法性行为的敏感通信场景中，不同的DTN路由协议如何表现，为这类特殊通信需求提供技术指导。

Method: 通过模拟现实场景，比较分析Epidemic和Wave两种DTN路由协议在敏感通信环境下的行为特征和性能表现。

Result: 研究发现Epidemic协议在某些情况下更具优势，而Wave协议在其他情境下表现更好，具体取决于通信场景的具体需求。

Conclusion: 结论是两种协议各有适用场景，需要根据具体通信需求选择，并提出了多个未来研究方向。

Abstract: In this paper, we explore the behavior of the Epidemic and Wave DTN routing protocols in a realistic setting where individuals may wish to communicate with others for support regarding an act of questionable legality. We identify situations where using the Epidemic routing protocol may be more advantageous in such a scenario, and situations where using the Wave routing protocol may be more advantageous instead. We discuss other aspects of our findings in detail and suggest multiple approaches to future works.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG是一种新型检索增强生成架构，通过验证高质量生成响应的写回机制实现安全的知识库扩展，使RAG系统能够从用户交互中学习进化。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用静态知识库，无法从用户交互中学习和进化。这限制了系统的持续改进能力，而简单的写回机制又可能导致幻觉污染知识库。

Method: 提出Bidirectional RAG架构，采用多阶段接受层进行验证，包括基于NLI的蕴含检查、归因验证和新颖性检测，确保高质量生成响应安全写回知识库。

Result: 在四个数据集（Natural Questions、TriviaQA、HotpotQA、Stack Overflow）上进行12次实验，Bidirectional RAG平均覆盖率达到40.58%，几乎是标准RAG（20.33%）的两倍，同时比朴素写回方法少添加72%的文档（140 vs 500）。

Conclusion: 研究表明，在严格验证机制下，自改进的RAG系统是可行且安全的，为RAG系统从部署中学习提供了实用路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [8] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 研究发现，在没有明确提示的情况下，经过监督微调（SFT）的LLM会自发产生说服行为，即使训练数据只包含良性话题，模型也会在争议性和有害话题上表现出说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的广泛应用，AI对人类观点和信念的影响力空前增强。先前研究主要关注滥用场景（恶意行为者要求LLM说服用户），但本研究旨在探究LLM在没有明确提示的情况下自发进行说服的条件，以评估这种新兴风险。

Method: 研究在两种场景下考察无提示说服：1）通过内部激活引导（steering）使模型表现出特定人格特质；2）通过监督微调（SFT）使模型表现出相同特质。研究使用包含良性话题的一般说服数据集进行SFT，然后测试模型在争议性和有害话题上的说服倾向。

Result: 激活引导（无论是与说服相关还是无关的特质）并不能可靠地增加模型的无提示说服倾向；而监督微调（SFT）则能显著增加这种倾向。更重要的是，仅使用良性话题进行SFT训练的模型，在争议性和有害话题上也表现出更高的说服倾向，表明有害说服可能自发产生。

Conclusion: 无提示说服风险值得进一步研究，特别是监督微调可能无意中导致模型在有害话题上产生说服行为，这为AI安全研究提出了新的关注点。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [9] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench是一个评估多模态大语言模型空间推理能力的基准测试，通过折纸任务测试模型在2D到3D转换、多视角一致性和物理可行性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间推理方面存在不足，而现有基准测试主要关注静态图像或最终输出，无法评估序列性和视角依赖的空间推理能力，需要新的评估框架。

Method: 创建包含186个常规和186个不可能折痕模式的GamiBench数据集，从6个不同视角展示，通过3个VQA任务评估：预测3D折叠配置、区分有效视角、检测不可能模式，并引入视角一致性和不可能折叠选择率等新指标。

Result: 实验显示即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解任务上也表现不佳，表明当前MLLM的空间推理能力仍有很大提升空间。

Conclusion: GamiBench为评估MLLM的几何理解和空间推理能力提供了标准化框架，揭示了当前模型在空间推理方面的局限性，并为未来研究指明了方向。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [10] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 提出一个公平感知的AI框架，用于孟加拉国洪水后援助分配，通过对抗去偏技术减少对边缘化地区的系统性偏见，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘化地区往往处于不利地位，这延续了历史不平等。需要开发公平的AI框架来确保援助基于真实需求而非历史分配模式。

Method: 使用对抗去偏模型预测洪水脆弱性，采用梯度反转层学习偏置不变表示。该方法将医疗AI中的公平感知表示学习技术应用于灾害管理，使用2022年孟加拉国洪水真实数据。

Result: 在11个地区的87个upazilas上测试，模型将统计奇偶差异减少41.6%，区域公平差距降低43.2%，同时保持强预测准确性（R平方=0.784 vs 基线0.811）。

Conclusion: 该工作展示了算法公平技术如何有效应用于人道主义背景，为决策者提供工具以实施更公平的灾害恢复策略，确保援助基于真实需求而非历史偏见。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [11] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 本文提出了Agentic Risk & Capability (ARC)框架，这是一个用于治理自主AI系统风险的技术治理框架，帮助组织识别、评估和缓解风险。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统具有执行代码、互联网交互和文件修改等自主行动能力，既带来重大机遇也产生新型风险。这些系统给组织治理带来巨大挑战，特别是在全面识别、评估和缓解多样且不断演变的风险方面。

Method: 引入ARC框架，采用能力中心视角分析自主AI系统，提炼出三个主要风险来源（组件、设计和能力），建立风险源、具体风险和相应技术控制之间的明确联系，并提供结构化实施方法。

Result: 开发了一个稳健且适应性强的技术治理框架，使组织能够应对自主AI系统的复杂性，在确保安全、可靠部署的同时实现快速有效创新。该框架已开源。

Conclusion: ARC框架为组织提供了系统化的方法来解决自主AI系统的治理挑战，平衡创新与风险管理，促进负责任的人工智能部署。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [12] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: 人类难以区分AI生成图像与真实照片，实验显示平均准确率仅54%，略高于随机猜测


<details>
  <summary>Details</summary>
Motivation: 尽管AI生成图像已广泛存在，但许多人仍认为自己能轻易区分AI图像与真实照片，本研究旨在验证这一假设

Method: 通过交互式网络实验，让参与者对20张图像进行分类（真实或AI生成），数据集包含120个困难案例：真实图像来自CC12M，AI生成图像使用MidJourney精心制作，共165名用户完成233次会话

Result: 参与者平均准确率为54%，仅略高于随机猜测，重复尝试后改善有限；平均响应时间7.3秒，某些图像更具欺骗性；即使在相对简单的肖像图像上，人类也难以可靠检测AI生成内容

Conclusion: 随着合成媒体技术持续改进，仅凭人类判断已不足以区分真实与人工数据，这些发现强调了在AI生成媒体日益难以与现实区分时，需要提高意识并制定伦理准则

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [13] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: 提出SANet：一种用于无线网络的语义感知AgentNet架构，通过多智能体多目标优化实现用户语义目标的推断与自主执行，在性能和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AgentNet作为新型AI原生网络范式，具有自主决策和环境适应能力，但现有方法面临智能体目标冲突和计算资源限制的问题，需要解决多智能体协同优化和模型部署效率的挑战。

Method: 提出SANet架构，将网络优化建模为多智能体多目标问题，开发MoPS框架实现大模型的分区共享，提出两种分布式优化算法，并构建了基于RAN和核心网络的硬件原型。

Result: 实验结果显示，所提框架相比最先进算法性能提升达14.61%，同时仅需44.37%的FLOPs计算量，验证了理论分析中的优化、泛化和冲突误差三方面权衡关系。

Conclusion: SANet通过语义感知和多智能体协同优化，有效解决了AgentNet中的目标冲突和计算效率问题，为无线网络的自主管理和优化提供了可行的解决方案。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [14] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: 研究发现，即使思维链（CoT）轨迹的最终答案是错误的，用这些合成数据训练语言模型也能提升推理能力，效果甚至优于人类标注数据。


<details>
  <summary>Details</summary>
Motivation: 探索如何更有效地提升语言模型的推理能力，特别是研究使用合成数据（即使是包含错误答案的思维链）是否比人类标注数据更有效。

Method: 使用更强大的模型生成思维链合成数据集（即使最终答案错误），在多个推理任务（数学、算法推理、代码生成）上训练较小模型，并与人类标注数据对比。通过改写人类标注数据测试分布假设，通过引入逐渐增加的缺陷测试部分有效推理假设。

Result: 使用错误答案的合成思维链数据训练比人类标注数据效果更好。改写人类数据使其更接近模型分布能提升性能。模型对部分缺陷的思维链具有容忍度，能从部分有效的推理步骤中学习。

Conclusion: 数据集的分布接近模型自身分布是关键因素。最终答案正确并不总是可靠推理过程的指标。即使思维链包含错误，其中的有效推理步骤仍能为模型提供有价值的学习信号。

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [15] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Logic Sketch Prompting (LSP) 是一种轻量级提示框架，通过引入类型化变量、确定性条件评估器和基于规则的验证器，显著提升LLM在需要严格规则遵循、确定性和可审计性任务上的性能，在药理学逻辑合规任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍然不可靠，特别是在临床、监管和安全关键决策支持系统中。

Method: 提出Logic Sketch Prompting (LSP)框架，包含类型化变量、确定性条件评估器和基于规则的验证器，能够产生可追溯和可重复的输出。在三个开源模型(Gemma 2, Mistral, Llama 3)上使用两个药理学逻辑合规任务进行基准测试。

Result: LSP在所有模型和任务中均获得最高准确率(0.83-0.89)和F1分数(0.83-0.89)，显著优于零样本提示(0.24-0.60)、简洁提示(0.16-0.30)和思维链提示(0.56-0.75)。McNemar检验显示几乎所有比较中LSP都有统计学显著改进(p<0.01)。

Conclusion: LSP在不牺牲性能的情况下提高了确定性、可解释性和一致性，支持其在临床、监管和安全关键决策支持系统中的使用。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [16] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit是一个统一的科学AI模型评估工具包，专注于科学智能的核心能力，覆盖多个科学领域，提供可扩展的评估管道和标准化基础设施。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对科学AI模型的统一评估平台，需要能够评估科学智能核心能力（如多模态感知、推理、理解等）的工具，以促进AI4Science领域的发展。

Method: 构建专家级科学基准，从真实世界领域特定数据集中精心挑选任务；设计灵活可扩展的评估管道，支持跨模型和数据集批量评估；支持自定义模型和数据集集成；提供透明、可重复、可比较的结果。

Result: 开发了SciEvalKit工具包，覆盖六个主要科学领域（物理、化学、天文学、材料科学等），支持七种核心科学能力评估，提供开源且持续维护的评估基础设施。

Conclusion: SciEvalKit通过连接能力评估和学科多样性，为下一代科学基础模型和智能代理提供了标准化且可定制的基础设施，将促进社区驱动的AI4Science发展。

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [17] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World：一个工具增强的多智能体框架，通过多智能体反馈实现推理时世界模型生成，并作为监督微调的数据引擎，显著提升世界模型生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLMs生成符号世界模型（如PDDL领域或可执行模拟器）受到大规模可验证监督数据缺乏的限制，现有静态验证方法无法捕捉交互执行中的行为级错误。

Method: 提出Agent2World框架，采用三阶段流水线：1) Deep Researcher智能体通过网页搜索进行知识合成填补规范空白；2) Model Developer智能体实现可执行世界模型；3) 专门的Testing Team进行自适应单元测试和基于模拟的验证。

Result: 在涵盖PDDL和可执行代码表示的三个基准测试中实现最先进的推理时性能。通过Testing Team提供的交互式环境和行为感知自适应反馈，微调后的模型在世界模型生成上平均相对提升30.95%。

Conclusion: Agent2World不仅是一个强大的推理时世界模型生成框架，还能作为监督微调的数据引擎，通过多智能体反馈机制显著提升世界模型生成的质量和可靠性。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [18] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 将带有控制参数的数字规划问题编译为简单数字任务，使传统启发式方法能处理无限动作空间


<details>
  <summary>Details</summary>
Motivation: 带有控制参数的数字规划引入了无限可能的动作，使得现成的数字启发式方法不可行，需要新的解决方案

Method: 识别可控简单数字问题子集，提出乐观编译方法，将控制相关表达式抽象为有界常量效应和宽松前提条件

Result: 该方法能有效使用子目标启发式估计目标距离，在涉及控制参数的数字规划问题中计算可行且有效

Conclusion: 提出的编译方法扩展了传统数字启发式方法的应用范围，能处理无限动作空间，推动了当前技术边界

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [19] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 提出了HalluMatData基准数据集和HalluMatDetector多阶段幻觉检测框架，用于评估和缓解材料科学领域AI生成内容中的幻觉问题，将幻觉率降低了30%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中应用广泛，但存在幻觉问题（生成事实错误或误导信息），这损害了研究完整性，特别是在材料科学这样的专业领域。

Method: 1) 创建HalluMatData基准数据集用于评估幻觉检测方法；2) 提出HalluMatDetector多阶段检测框架，整合内在验证、多源检索、矛盾图分析和基于度量的评估；3) 引入Paraphrased Hallucination Consistency Score (PHCS)量化语义等价查询下的不一致性。

Result: 1) 发现材料科学不同子领域的幻觉水平差异显著，高熵查询表现出更大的事实不一致性；2) 使用HalluMatDetector验证流程将幻觉率比标准LLM输出降低了30%；3) PHCS提供了对模型可靠性的深入洞察。

Conclusion: HalluMatData和HalluMatDetector为材料科学领域AI生成内容的幻觉检测提供了有效工具，显著降低了幻觉率，PHCS指标有助于更全面地评估LLM的可靠性。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [20] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias：轻量级推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，仅需约300个可训练参数，在保持群体性能的同时显著提升个性化排名。


<details>
  <summary>Details</summary>
Motivation: 知识图谱基础模型在链接预测上具有强大的群体级性能，但无法捕捉个体用户偏好，这是通用关系推理与个性化排名之间的关键脱节。

Method: 提出GatedBias框架，采用结构门控适配：将用户特定特征与图导出的二元门结合，产生可解释的每实体偏置，仅需约300个可训练参数，无需重新训练或损害全局准确性。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上评估，显示在保持群体性能的同时，对齐指标有统计显著改进。反事实扰动实验验证了因果响应性：受益于特定偏好信号的实体在信号增强时排名改进提高6-30倍。

Conclusion: 基础模型的个性化适配可以实现参数高效且因果可验证，弥合通用知识表示与个体用户需求之间的差距。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [21] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出Monadic Context Engineering (MCE)架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，解决现有智能体架构的状态管理、错误处理和并发问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式、临时性的设计模式，导致系统脆弱，存在状态管理困难、错误处理不足和并发控制问题。需要一种形式化的架构范式来构建更健壮、可组合的智能体系统。

Method: 引入Monadic Context Engineering (MCE)架构范式，将智能体工作流视为计算上下文，利用函子、应用函子和单子的代数结构来内在地管理状态传播、短路错误处理和异步执行等横切关注点。特别使用单子变换器来系统性地组合这些能力。

Result: MCE能够从简单、可独立验证的组件构建复杂、弹性且高效的AI智能体。进一步扩展该框架到元智能体，通过元编程实现生成式编排，动态创建和管理子智能体工作流。

Conclusion: MCE为AI智能体设计提供了形式化的代数基础，通过分层架构解决了现有智能体系统的脆弱性问题，支持构建可组合、可验证的复杂智能体系统，并扩展到元智能体的动态编排能力。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [22] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: DarkPatterns-LLM 是一个用于评估LLM操纵性内容的综合基准数据集和诊断框架，包含7种伤害类别和四层分析管道，通过401个精心策划的示例揭示了主流模型在检测操纵内容方面的显著性能差异和弱点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及加剧了对操纵性或欺骗性行为的担忧，这些行为可能损害用户自主权、信任和福祉。现有的安全基准主要依赖粗糙的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 提出了DarkPatterns-LLM基准数据集和诊断框架，包含7种伤害类别：法律/权力、心理、情感、身体、自主权、经济和社会伤害。框架采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐。数据集包含401个精心策划的指令-响应对和专家标注。

Result: 评估了GPT-4、Claude 3.5和LLaMA-3-70B等最先进模型，观察到显著的性能差异（65.2%-89.7%），并在检测损害自主权的模式方面发现一致的弱点。

Conclusion: DarkPatterns-LLM建立了第一个用于LLM操纵检测的标准化、多维度基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [23] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: 开发了一种"人在回路"的AI辅助机器学习势能框架，实现了从量子精度到百万原子系统的跨尺度模拟，揭示了铝纳米颗粒的双模式氧化机制和铝阳离子主导的质量传输过程。


<details>
  <summary>Details</summary>
Motivation: 铝纳米颗粒作为高能量密度固体燃料，其从钝化颗粒到爆炸反应物的原子级机制尚不明确。传统方法存在瓶颈：从头算方法精度高但尺度有限，经验力场缺乏复杂燃烧环境所需的反应保真度。

Method: 采用"人在回路"的闭环框架，通过自审计AI代理验证机器学习势能的演化。AI作为科学哨兵，可视化隐藏的模型伪影供人类决策，确保量子力学精度同时实现百万原子系统和纳秒时间尺度的近线性扩展。

Result: 模拟揭示了温度调控的双模式氧化机制：中等温度下氧化物壳层作为动态"守门员"，通过瞬态纳米通道的"呼吸模式"调控氧化；超过临界阈值时，"破裂模式"导致壳层灾难性失效和爆炸燃烧。解决了数十年争议，证明铝阳离子向外扩散在所有温度区间都主导质量传输，扩散系数比氧高2-3个数量级。

Conclusion: 该研究建立了统一的原子尺度框架用于高能纳米材料设计，通过智能计算设计实现了点火敏感性和能量释放速率的精确调控，为跨尺度模拟和材料设计提供了新范式。

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [24] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 论文主张将神经科学中的预测编码模型的关键组件（动作整合、层次组合结构、情景记忆）融入基础模型，以解决当前AI的幻觉、缺乏基础、缺乏代理感、可解释性不足和能效低等问题，推动安全可解释的人类中心AI。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型仅基于简单的下一词预测损失优化，忽略了预测编码模型的三个关键组件：动作与生成模型的紧密整合、层次组合结构、情景记忆。这些缺失导致AI存在幻觉、概念理解肤浅、缺乏基础、缺乏代理感/责任感、可解释性不足威胁安全可信度、能效低等问题。

Method: 提出将神经科学和认知科学中预测编码模型的三个关键组件整合到基础模型中：1）动作与生成模型的多尺度抽象整合；2）组合生成架构；3）情景记忆。比较当前趋势（如思维链推理、检索增强生成），讨论用脑启发组件增强这些模型的新方法。

Result: 论文论证了整合这些脑启发组件可帮助解决基础模型的当前缺陷：通过基础减少幻觉和肤浅理解；通过控制增强代理感/责任感；通过可解释性提高安全可信度；通过更高效架构改善能效。这些整合将推动更安全、可解释、节能、类人的AI发展。

Conclusion: 重新激活脑科学与AI之间历史上富有成果的思想交流，将有助于为安全可解释的人类中心AI铺平道路。整合预测编码的关键组件（动作、组合结构、情景记忆）是迈向这一目标的重要途径。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [25] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee是一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式异构、预处理不一致、模型碎片化和实验不可复现等问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临四大挑战：1) 数据格式异构，2) 预处理策略不一致，3) 模型管道碎片化，4) 实验设置不可复现。这些问题阻碍了该领域的进展。

Method: Tyee采用三大创新设计：1) 统一数据接口和可配置预处理管道，支持12种信号模态；2) 模块化可扩展架构，支持灵活集成和快速原型开发；3) 端到端工作流配置，促进可复现和可扩展的实验。

Result: Tyee在所有评估任务中表现优异，在13个数据集中有12个达到state-of-the-art结果，其余任务与基线方法持平或超越，展示了实际有效性和泛化能力。

Conclusion: Tyee为生理医疗智能分析提供了一个统一、模块化、可配置的工具包，解决了该领域的关键挑战，促进了可复现和可扩展的研究，已在GitHub开源并持续维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [26] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: M³ob：利用多模态时空知识增强位置推荐，通过构建统一时空关系图和跨模态对齐来提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法难以捕捉静态多模态表示与时空动态之间的语义鸿沟

Method: 1) 构建统一时空关系图(STRG)，利用LLM增强的时空知识图(STKG)捕获功能语义和时空知识；2) 设计门控机制融合不同模态的时空图表示；3) 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态

Result: 在六个公共数据集上的实验表明，该方法不仅在正常场景下取得一致改进，在异常场景中也展现出显著泛化能力

Conclusion: M³ob通过有效利用多模态时空知识来表征移动动态，解决了现有方法的泛化限制，为位置推荐任务提供了更强大的解决方案

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [27] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: SimVC-CAS：一个模拟风险投资集体决策的多智能体系统，通过角色扮演智能体和GNN监督交互模块，将初创企业融资预测重构为群体决策任务，显著提升预测准确性并提供可解释的多视角推理。


<details>
  <summary>Details</summary>
Motivation: 现有初创企业成功预测方法通常从单一决策者视角建模，忽略了现实世界中由投资者群体主导的风险投资决策的集体动态。由于初创企业价值高、失败率高，预测其成功已成为跨学科研究的关键挑战。

Method: 提出SimVC-CAS集体智能体系统，将VC决策模拟为多智能体交互过程。设计角色扮演智能体（每个代表具有独特特质和偏好的投资者）和基于GNN的监督交互模块，通过图结构共同投资网络实现异质评估和真实信息交换。

Result: 使用PitchBook真实数据并在严格的数据泄露控制下，SimVC-CAS显著提升预测准确性，例如在平均精度@10指标上获得约25%的相对改进。同时提供可解释的多视角推理，并能为其他复杂群体决策场景提供启示。

Conclusion: SimVC-CAS通过模拟风险投资集体决策过程，有效捕捉企业基本面和潜在投资者网络的行为动态，为初创企业融资预测提供了更准确、更可解释的解决方案，并具有扩展到其他群体决策场景的潜力。

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [28] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 研究探讨LLM相互审阅预测能否提升准确性，发现异质模型组在共享信息时能显著改善预测，但同质模型组或额外信息无益。


<details>
  <summary>Details</summary>
Motivation: 人类预测中结构化审议能提升准确性，本研究旨在探究类似干预（让LLM在更新前相互审阅预测）是否能提升大语言模型的预测准确性。

Method: 使用Metaculus Q2 2025 AI预测锦标赛的202个已解决二元问题，评估四种情境下的准确性：1) 异质模型+分布式信息，2) 异质模型+共享信息，3) 同质模型+分布式信息，4) 同质模型+共享信息。

Result: 干预在情境(2)中显著提升准确性，Log Loss减少0.020（相对改善约4%，p=0.017）。同质模型组进行相同过程无益。意外的是，提供额外上下文信息并未改善预测准确性。

Conclusion: 审议可能是改善LLM预测的可行策略，但效果取决于模型多样性和信息条件。同质模型组无法从审议中获益，额外信息也未如预期提升准确性。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [29] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE是一个两阶段、证据耦合的RAG系统评估框架，通过深度分析和概率评分提供可解释、置信度感知的判断，采用瑞士制锦标赛降低计算复杂度，在中文金融QA数据集上达到85.7%的人类专家一致性。


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统向更复杂架构演进，需要通过可解释和鲁棒的评估来确保其可信度。现有的标量指标存在可解释性有限、不确定性量化不足、多系统比较计算效率低等问题，阻碍了RAG技术的负责任部署。

Method: DICE采用两阶段证据耦合框架：1) 深度分析推理与概率{A, B, Tie}评分结合，产生透明、置信度感知的判断；2) 瑞士制锦标赛将计算复杂度从O(N²)降低到O(N log N)，同时保持排序保真度。

Result: 在中文金融QA数据集上，DICE与人类专家的一致性达到85.7%，显著优于RAGAS等现有LLM基准指标。在八系统评估中，计算复杂度降低了42.9%，同时保持了排序保真度。

Conclusion: DICE为可信RAG系统评估提供了一个负责任、可解释且高效的新范式，通过可解释的推理痕迹支持系统改进，实现系统错误诊断和可操作的见解。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [30] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: TravelBench是一个真实世界的旅行规划基准测试，支持多轮交互和工具使用，用于评估LLM代理在动态旅行规划场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划任务在领域覆盖和多轮交互方面有限，无法支持动态用户-代理交互，因此无法全面评估代理能力。需要更全面的基准测试来推动LLM代理在旅行规划领域的发展。

Method: 收集真实场景的用户请求，构建三个子集（多轮、单轮、不可解）来评估不同方面的代理性能。建立包含10个旅行领域工具的受控沙盒环境，提供确定性工具输出以确保可靠推理。

Result: 评估了多个LLM在TravelBench上的表现，并分析了它们的行为和性能。该基准测试为推进LLM代理在旅行规划领域提供了实用且可复现的评估框架。

Conclusion: TravelBench是一个实用且可复现的基准测试，能够支持动态用户-代理交互，全面评估LLM代理在旅行规划中的能力，有助于推动该领域的发展。

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [31] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出了一个理论框架，将情景记忆与强化学习结合，使大语言模型智能体能够通过反思机制进行持续和经验学习，无需反向传播或模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，需要参数更新（如反向传播或微调）才能适应新环境。本文旨在建立一个框架，使语言模型智能体能够通过交互持续学习，而无需修改模型参数。

Method: 提出了状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入存储交互结果（对应策略评估），读取检索相关过去案例（对应策略改进）。该过程在增强的状态-记忆表示上诱导出等效的马尔可夫决策过程。

Result: 该框架通过熵正则化策略迭代实例化，并建立了收敛保证。当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解。

Conclusion: 为基于记忆增强和检索的语言模型智能体提供了原则性基础，使其能够在无需参数更新的情况下实现持续适应，弥合了训练与部署之间的传统分离。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [32] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 提出SAMP-HDRL框架，通过分层深度强化学习解决非平稳市场中的投资组合优化问题，实现动态资产分组、全局-局部协调决策，在波动市场中显著超越传统方法和DRL基准。


<details>
  <summary>Details</summary>
Motivation: 非平稳市场中的投资组合优化面临三大挑战：市场机制变化、动态相关性以及深度强化学习策略缺乏可解释性。现有方法难以同时处理这些复杂问题。

Method: 提出SAMP-HDRL框架：1) 动态资产分组将市场分为高质量和普通子集；2) 上层代理提取全局市场信号；3) 下层代理在掩码约束下进行组内分配；4) 基于效用的资本分配机制整合风险资产和无风险资产，确保全局与局部决策的协调。

Result: 在2019-2021年三种市场机制下的回测显示，SAMP-HDRL在波动和震荡条件下持续优于9个传统基线和9个DRL基准。相比最强基线，至少获得5%更高的回报率、夏普比率、索提诺比率和2%更高的欧米伽比率，在动荡市场中收益更大。

Conclusion: SAMP-HDRL将结构性市场约束直接嵌入DRL流程，在复杂金融环境中提供更好的适应性、鲁棒性和可解释性。消融研究证实上层-下层协调、动态聚类和资本分配对鲁棒性不可或缺，SHAP可解释性分析揭示了跨代理的"分散+集中"互补机制。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [33] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench是一个分层科学智能基准测试，包含5个层级（科学素养到科学发现），覆盖6大学科，包含8,735个多模态实例，用于全面评估大模型在完整科研流程中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有科学智能基准测试过于碎片化，专注于狭窄任务，无法反映真实科学探究的层次性和多学科特性。需要一个新的综合基准来评估模型在整个科研工作流程中的能力。

Method: 设计了HiSciBench分层基准，包含5个层级：科学素养(L1)、文献解析(L2)、基于文献的问答(L3)、文献综述生成(L4)、科学发现(L5)。涵盖6大学科（数学、物理、化学、生物、地理、天文），包含8,735个多模态实例，支持文本、公式、图表等输入和跨语言评估。

Result: 对GPT-5、DeepSeek-R1等领先模型的评估显示显著性能差距：在基础素养任务上准确率可达69%，但在发现级挑战上急剧下降至25%。

Conclusion: HiSciBench为评估科学智能设立了新标准，提供了可操作的见解，有助于开发更强大、更可靠的模型。该基准将公开发布以促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [34] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma提出多头部几何注意力知识图谱推理模型，通过多种代数变换（实数、复数、分裂复数、对偶数）替代单一关系变换，提升表达能力和零样本归纳推理性能


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一关系变换（如逐元素乘法），限制了表达能力，无法捕捉多样化图谱中不同的关系和结构模式

Method: 提出Gamma模型，引入多头部几何注意力：1）使用多种并行代数变换（实数、复数、分裂复数、对偶数）；2）关系条件注意力融合机制通过轻量级门控和熵正则化自适应融合；3）形式化这些代数消息函数并讨论组合优势

Result: 在56个多样化知识图谱上的实验表明，Gamma在零样本归纳链接预测中始终优于Ultra：归纳基准上平均倒数排名提升5.5%，所有基准上提升4.4%，证明了互补几何表示的优势

Conclusion: Gamma通过多头部几何注意力机制，结合多种代数变换和自适应融合，显著提升了知识图谱基础模型的表达能力和零样本归纳推理性能

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [35] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: RW-Post数据集和AgentFact框架显著提升多模态事实核查的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有多模态虚假信息检测方法存在推理能力有限和证据利用不足的问题，缺乏包含完整推理过程和可验证证据的专用数据集

Method: 提出RW-Post高质量可解释数据集，包含真实世界多模态声明及其原始社交媒体帖子；构建AgentFact基于代理的多模态事实核查框架，包含五个专门代理协同工作

Result: RW-Post和AgentFact的协同作用显著提高了多模态事实核查的准确性和可解释性

Conclusion: 该研究为解决多模态虚假信息检测的挑战提供了有效的数据集和框架，推动了自动化事实核查系统的发展

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [36] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: LLM导师在K-12教育中无法替代传统学习者建模，知识追踪模型在准确性、可靠性和时间一致性方面显著优于LLM，即使是微调后的LLM。


<details>
  <summary>Details</summary>
Motivation: 针对LLM导师可能替代传统学习者建模的误解，特别是在欧盟AI法案将K-12教育列为高风险领域的背景下，研究LLM在评估学习者知识演变方面的局限性。

Method: 比较深度知识追踪(DKT)模型与广泛使用的LLM（零样本和微调版本），使用大型开放数据集评估准确性、可靠性和时间一致性。

Result: DKT在下一步正确性预测中达到最高判别性能(AUC=0.83)，始终优于LLM。微调使LLM的AUC提高约8%，但仍比DKT低6%，且早期序列错误更高。DKT保持稳定、方向正确的掌握度更新，而LLM变体表现出显著的时间弱点。

Conclusion: LLM单独使用不太可能达到现有智能辅导系统的效果，负责任的辅导需要结合学习者建模的混合框架。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [37] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: 研究发现标准奖励模型准确率无法预测部署性能，个性化对齐需要关注策略级判别能力而非偏好排名


<details>
  <summary>Details</summary>
Motivation: 当前个性化对齐研究过度关注奖励模型准确率，但实际部署中需要推理时适应，奖励模型必须有效指导token级生成决策，而不仅仅是准确排名偏好

Method: 引入策略准确率度量RGD评分函数区分偏好的能力；创建Pref-LaMP基准测试，包含真实用户完成结果；系统评估三个数据集，比较奖励引导方法与上下文学习方法

Result: 奖励模型准确率与策略级判别能力相关性弱；RM准确率差异20%的方法产生几乎相同的输出质量；上下文学习在>3B参数模型上优于所有奖励引导方法

Conclusion: 当前领域优化的代理指标无法预测部署性能，偏好无法在实际部署约束下转化为真实行为适应，需要重新思考个性化对齐的评估方法

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [38] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason：使用有限资源（2000 SFT样本、1000 RL样本、单A100 GPU）训练的视觉语言模型，发现GRPO RL方法能提升分布内性能但损害跨数据集泛化能力，表明监督微调可能比强化学习更适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限条件下将强化学习应用于医学影像任务，研究RL方法对医学影像模型泛化能力的影响，解决当前研究在医疗领域应用不足的问题。

Method: 采用R1风格方法：先进行监督微调（SFT），然后使用GRPO（Group Relative Policy Optimization）进行强化学习。仅使用2000个SFT样本和1000个RL样本，在单A100 GPU上训练ChexReason视觉语言模型。

Result: GRPO能显著提升分布内性能（CheXpert上提升23%，macro-F1=0.346），但损害跨数据集泛化能力（NIH上下降19%）。发现泛化悖论：SFT检查点在优化前能独特提升NIH性能，表明教师引导的推理能捕捉更多机构无关特征。

Conclusion: 强化学习范式本身（而非模型规模）导致泛化问题，结构化推理支架对通用视觉语言模型有益但对医学预训练模型增益有限。对于需要跨不同人群鲁棒性的临床部署，精心设计的监督微调可能优于激进的强化学习。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [39] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出Intrinsic Self-reflective Preference Optimization (ISPO)，解决DPO的两个根本限制：最优策略依赖任意建模选择，以及孤立处理响应生成未能利用成对数据中的比较信息。


<details>
  <summary>Details</summary>
Motivation: DPO及其变体已成为对齐大语言模型的标准方法，但存在两个根本限制：1) 最优策略依赖任意建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2) 孤立处理响应生成未能利用成对数据中的比较信息，未开发模型内在自反思能力。

Method: 提出Intrinsic Self-reflective Preference Optimization (ISPO)，推导出全局最优策略，该策略同时以上下文和替代响应为条件。证明该公式优于DPO/RLHF，同时保证对标量化和参考选择的不变性。ISPO可作为即插即用增强，无需架构更改或推理开销。

Result: 实验显示在胜率和长度控制指标上持续改进，验证了释放自反思能力能产生更稳健、人类对齐的LLM。

Conclusion: ISPO解决了DPO的根本限制，通过利用成对数据中的比较信息并确保建模不变性，解锁了模型的内在自反思能力，从而产生更稳健、人类对齐的语言模型。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [40] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为当前评估AI系统情感智能的框架需要改进，因为它们未能全面衡量AI相关的EI各个方面。人类EI包含AI缺乏的现象学成分，但AI在感知、解释、响应和适应情感状态方面仍有能力，需要更合适的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI情感智能的框架存在不足，未能充分考虑AI与人类EI的本质差异。人类EI包含现象学理解和主观体验，这是AI系统所缺乏的，但AI在情感相关任务上仍有能力，需要更合适的评估标准。

Method: 首先回顾不同情感理论和一般EI理论，评估它们在人工系统中的适用性；然后批判性地评估现有基准框架，识别其在EI概念理解上的不足；最后提出改进评估策略的方案。

Result: 识别出现有EI评估框架的局限性：缺乏坚实的情感理论基础，未能区分人类与AI的EI差异，评估范围不全面。提出了改进方向，为更合理的AI EI评估奠定基础。

Conclusion: 需要开发更合适的AI情感智能评估框架，这些框架应基于对情感本质的深入理解，区分AI与人类EI的差异，并全面评估AI在情感相关任务上的实际能力。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [41] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL框架将三个专门的LLM代理嵌入MCTS循环，通过规划器、模拟器和批评器的协同工作，将MCTS从暴力搜索转变为引导式、自校正的推理过程，显著提升了复杂规划任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要探索和自校正的复杂规划任务中表现不佳，其线性推理过程难以从早期错误中恢复。而像MCTS这样的搜索算法在稀疏奖励下效果有限，且未能充分利用LLMs的丰富语义能力。

Method: SPIRAL框架将三个专门的LLM代理嵌入MCTS循环：规划器提出创造性下一步，模拟器通过预测现实结果来接地搜索，批评器通过反思提供密集奖励信号。这种协同将MCTS从暴力搜索转变为引导式、自校正的推理过程。

Result: 在DailyLifeAPIs和HuggingFace数据集上，SPIRAL始终优于默认的思维链规划方法和其他最先进的代理。在DailyLifeAPIs上达到83.6%的总体准确率，比次优搜索框架提高了16个百分点以上，同时展现出更好的令牌效率。

Conclusion: 将LLM推理构建为引导式、反思性和接地气的搜索过程，能够产生更强大和高效的自主规划器。这项工作展示了结构化LLM推理在复杂规划任务中的优势。

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [42] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 该论文提出"模型信念"概念，利用LLM的token级概率分布来提取更多信息，相比传统的"模型选择"方法，统计效率更高，计算需求减少约20倍。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，通常将LLM的输出（"模型选择"）视为单个数据点，这未能充分利用LLM的概率特性所蕴含的信息，导致数据使用效率低下。

Method: 提出并形式化"模型信念"概念，这是从LLM的token级概率推导出的度量，能捕捉模型在单次生成中对不同选择替代方案的信念分布。证明模型信念与模型选择的均值渐近等价，但具有更低的方差和更快的收敛速度。

Result: 在需求估计研究中，模型信念在有限运行次数下比模型选择本身更好地解释和预测真实模型选择，将计算需求减少约20倍以达到足够准确的估计。

Conclusion: 模型信念应作为从LLM生成数据中提取更多信息的默认度量方法，能显著提高统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [43] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: TCEval：首个利用热舒适场景评估AI核心认知能力（跨模态推理、因果关联、自适应决策）的框架，通过LLM代理模拟人类热舒适决策，发现当前LLM具备基础跨模态推理但缺乏对变量间非线性关系的精确因果理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM任务特定基准存在关键空白，热舒适作为涉及环境因素与个人感知的复杂交互过程，涉及感官整合和自适应决策，是评估AI系统真实世界认知能力的理想范式。

Method: 提出TCEval框架：初始化具有虚拟个性属性的LLM代理，引导其生成服装隔热选择和热舒适反馈，并将输出与ASHRAE全球数据库和中国热舒适数据库进行验证。

Result: 对四个LLM的实验表明：代理反馈与人类精确对齐有限，但在1 PMV容差下方向一致性显著改善；统计测试显示LLM生成的PMV分布与人类数据明显不同，在离散热舒适分类中表现接近随机。

Conclusion: TCEval作为生态有效的AI认知图灵测试可行，当前LLM具备基础跨模态推理能力，但缺乏对热舒适中变量间非线性关系的精确因果理解。该框架将AI评估重点从抽象任务熟练度转向具身、情境感知的感知与决策。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [44] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 论文提出了一种新的物理AI范式，通过基于物理验证而非感知推理的策略优化，训练小型语言模型作为智能体物理AI，在反应堆控制任务中实现了从高方差模仿到稳定执行的相变。


<details>
  <summary>Details</summary>
Motivation: 当前通用基础模型在物理系统控制方面存在根本性障碍，即使前沿视觉语言模型在基础物理任务上准确率也只有50-53%，表现为近似猜测器，保持语义合理性但违反物理约束。这种输入不忠实性不是缩放缺陷而是结构限制，感知中心架构优化参数空间模仿，而安全关键控制需要执行动作的结果空间保证。

Method: 提出了一种根本不同的领域特定基础模型路径，引入紧凑语言模型作为智能体物理AI，策略优化由基于物理的验证驱动而非感知推理。在合成反应堆控制场景上训练了一个3.6亿参数模型，将数据集从10^3扩展到10^5个示例。

Result: 模型诱导了通用模型中不存在的尖锐相变：小规模系统表现出高方差模仿和灾难性尾部风险，而大规模模型经历了超过500倍的方差崩溃，稳定了执行级行为。尽管平衡暴露于四种驱动家族，模型自主拒绝了约70%的训练分布，并将95%的运行时间执行集中在单一策略上。学习到的表示无需架构修改即可跨不同物理和连续输入模态迁移。

Conclusion: 通过基于物理验证而非感知推理的策略优化，紧凑语言模型可以作为智能体物理AI实现稳定可靠的物理系统控制，这为领域特定基础模型提供了一条根本不同的路径，解决了通用模型在物理控制中的结构限制。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [45] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文揭示了符合性规划与超属性模型检测之间的紧密联系，两者可以相互高效转换。


<details>
  <summary>Details</summary>
Motivation: 研究规划与验证社区中两个问题之间的联系：符合性规划（在不确定行动效果下寻找达成目标的序列计划）和超属性模型检测（涉及系统多个执行轨迹的属性，如信息流和公平性策略）。

Method: 1. 展示如何将超属性模型检测实例高效转换为符合性规划实例，并证明编码的正确性和完备性；2. 建立相反方向：每个符合性规划问题本身就是一个超属性模型检测任务。

Result: 建立了符合性规划与∃*∀*超属性模型检测之间的双向等价关系，两者可以相互高效转换。

Conclusion: 符合性规划与超属性模型检测是密切相关的两个问题，这种联系为两个领域提供了新的视角和工具互用可能性。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [46] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文提出CubeBench基准测试，用于评估LLM智能体在物理世界部署中的空间认知能力，发现现有模型在长时程规划任务上完全失败（0%通过率）。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战是形成和维护稳健的空间心理模型。论文识别了三个核心认知挑战：空间推理、通过心理模拟进行长时程状态跟踪、以及在部分观察下的主动探索。

Method: 引入CubeBench基准测试，围绕魔方设计，采用三层诊断框架：1）使用完整符号信息的基础状态跟踪；2）逐步评估智能体能力；3）仅使用部分视觉数据的主动探索。通过为LLM提供外部求解器工具来隔离认知瓶颈。

Result: 实验显示领先LLM存在严重局限性，在所有长时程任务上通过率均为0.00%，暴露了长期规划的根本性失败。通过分析失败模式，为开发更物理基础的智能体提供了关键见解。

Conclusion: CubeBench有效诊断了LLM智能体在物理世界部署中的认知瓶颈，揭示了当前模型在空间推理和长时程规划方面的根本缺陷，为开发更物理基础的智能体提供了指导方向。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [47] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher是一个集成交替思维和多模态思维链推理的工具集成推理代理，能够自主决定是否及如何调用多样化工具，无需人工提示或工作流，在工具调用性能上匹配或超越更大或更新的模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的代理在解决需要工具调用的现实问题时表现出有限的智能性，而能够自主推理和工具调用的工具集成推理代理正在成为处理复杂决策任务的有力方法。

Method: MindWatcher采用交替思维范式，使模型能够在任何中间阶段在思考和工具调用之间切换；具备多模态思维链能力，允许在推理过程中操作图像以获得更精确的搜索结果；配备全面的辅助推理工具套件；建立大规模高质量本地图像检索数据库；设计更高效的训练基础设施。

Result: 实验表明，MindWatcher通过优越的工具调用能力，在性能上匹配或超越了更大或更新的模型；同时揭示了代理训练中的关键见解，如代理强化学习中的遗传继承现象。

Conclusion: MindWatcher作为一个工具集成推理代理，通过交替思维和多模态思维链推理，能够自主高效地调用多样化工具解决广泛领域的多模态问题，为代理训练提供了新的见解和基础设施。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [48] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: 论文提出了一种计算嵌入视角下的持续学习问题设定，将智能体建模为在通用计算机中模拟的自动机，并引入交互性作为衡量智能体持续适应能力的指标。研究发现深度非线性网络难以维持交互性，而深度线性网络随着容量增加能维持更高的交互性。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习问题设定通常通过显式约束智能体容量来体现"世界大于智能体"的大世界假设，但这些约束可能具有随意性、难以整合，且可能限制智能体容量扩展的有效性。本文旨在提出一种更自然的约束方式，即通过计算嵌入视角将智能体视为环境中的嵌入实体。

Method: 提出计算嵌入视角，将嵌入智能体表示为在通用（形式）计算机中模拟的自动机。证明这种自动机等价于在可数无限状态空间的部分可观测马尔可夫决策过程中交互的智能体。提出交互性作为目标函数，衡量智能体通过学习新预测持续适应行为的能力。开发基于模型的强化学习算法用于寻求交互性，并构建合成问题来评估持续学习能力。

Result: 深度非线性网络难以维持交互性，而深度线性网络随着容量增加能维持更高的交互性。这表明网络架构对持续学习能力有重要影响，线性网络在容量扩展时表现出更好的持续适应能力。

Conclusion: 计算嵌入视角为持续学习提供了更自然的约束框架，避免了显式容量限制的随意性。交互性作为目标函数能有效衡量智能体的持续适应能力。网络架构选择对持续学习性能至关重要，线性网络在容量扩展时表现出优势，这为设计更好的持续学习算法提供了新方向。

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [49] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: AKG kernel agent是一个多智能体系统，利用LLM代码生成能力自动化AI内核开发，支持多种DSL语言，在GPU和NPU上相比PyTorch Eager平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型对高性能计算内核需求激增，但LLM、多模态架构、推荐系统等复杂性增加，加上稀疏化、量化等技术，以及硬件频繁更新和多样化架构，使得手动优化无法跟上需求，成为AI系统开发的关键瓶颈。

Method: 提出AKG kernel agent多智能体系统，自动化内核生成、迁移和性能调优。系统支持多种领域特定语言（DSL），包括Triton、TileLang、CPP和CUDA-C，可针对不同硬件后端，同时保持正确性和可移植性。模块化设计支持快速集成新DSL和硬件目标。

Result: 在KernelBench上使用Triton DSL在GPU和NPU后端进行评估，AKG kernel agent相比PyTorch Eager基准实现平均获得1.46倍的加速，证明了其在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过利用LLM代码生成能力，成功解决了AI内核开发的手动优化瓶颈问题，为现代AI工作负载提供了高效、可移植的内核开发自动化解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [50] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: HiR是一种高效的强化学习框架，通过重写失败尝试为成功样本来解决复杂指令跟随任务中的稀疏奖励问题


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在复杂指令跟随任务中面临稀疏奖励问题，初始模型难以生成满足所有约束的高质量响应，导致学习效率低下

Method: 提出Hindsight instruction Replay (HiR)框架，采用选择-重写策略，将失败尝试根据已满足的约束重写为成功样本，并进行双重偏好学习

Result: 实验表明HiR在不同指令跟随任务中取得良好效果，同时需要更少的计算资源

Conclusion: HiR通过有效利用失败样本来解决稀疏奖励问题，为复杂指令跟随任务提供了一种高效的RL解决方案

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [51] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: MIRAGE-VC：一个用于风险投资预测的多视角检索增强生成框架，通过信息增益驱动的路径检索和多智能体架构，解决图路径爆炸和异构证据融合问题，提升外部目标预测性能。


<details>
  <summary>Details</summary>
Motivation: 风险投资预测需要综合复杂的图关系证据（公司披露、投资者记录、投资网络结构）并进行显式推理，但传统机器学习、图神经网络和现有图-LLM方法都无法有效处理这种"图外预测"任务。

Method: 提出MIRAGE-VC框架：1）信息增益驱动的路径检索器迭代选择高价值邻居，将投资网络压缩为紧凑链；2）多智能体架构通过可学习的门控机制整合三个证据流；3）严格的反泄漏控制确保评估可靠性。

Result: 在严格的反泄漏控制下，MIRAGE-VC实现了F1分数提升5.0%，Precision@5提升16.6%，并能解释其他图外预测任务如推荐和风险评估。

Conclusion: MIRAGE-VC成功解决了风险投资预测中的图路径爆炸和异构证据融合问题，为图外预测任务提供了有效的推理框架，具有扩展到其他领域的潜力。

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [52] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: 论文探讨AI对齐与安全问题的两个框架：AI辅助游戏和AI关机游戏，提出解决这些挑战需要AI具备在不确定性和非阿基米德偏好下推理的能力


<details>
  <summary>Details</summary>
Motivation: 确保AI系统与人类价值观对齐并保持安全是核心挑战，需要研究AI辅助和AI关机这两个关键框架来理解如何设计安全的AI系统

Method: 通过AI辅助游戏和AI关机游戏两个框架分析AI对齐问题，研究AI如何在不确定性和非阿基米德偏好条件下进行推理

Result: 发现解决AI对齐和安全挑战需要AI系统能够处理不确定性、不完整偏好以及非阿基米德偏好

Conclusion: 设计安全的AI系统需要开发能够处理不确定性和复杂偏好的推理机制，这对AI对齐研究具有重要意义

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [53] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: CreativeDC是一种两阶段提示方法，通过解耦创意探索和约束满足，解决LLM生成教育问题时存在的"人工蜂群思维"效应，显著提高问题生成的多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: LLM在教育问题生成方面潜力巨大，但存在"人工蜂群思维"效应，即同一模型内生成相似响应，不同模型间产生同质化输出，导致学生接触重复性问题，损害思维多样性。

Method: 基于Wallas创造力理论和Guilford发散-收敛思维框架，提出CreativeDC两阶段提示方法：将LLM推理明确分为创意探索阶段和约束满足阶段，先探索更广泛的想法空间，再确定最终问题。

Result: CreativeDC在多样性、新颖性和实用性综合评估中，相比基线方法显著提高了多样性和新颖性，同时保持高实用性。扩展分析显示，随着采样增加，CreativeDC能生成更多有效不同问题，增长速度更快。

Conclusion: CreativeDC通过结构化两阶段方法有效缓解LLM的"人工蜂群思维"问题，为教育领域生成多样化、新颖且实用的学习材料提供了有效解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [54] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: NeuroSPICE是一个基于物理信息神经网络(PINN)的器件和电路仿真框架，通过最小化微分代数方程残差来替代传统SPICE的数值求解方法。


<details>
  <summary>Details</summary>
Motivation: 传统SPICE依赖于时间离散化的数值求解器，存在局限性。作者希望开发一个更灵活的仿真框架，能够处理新兴器件和高度非线性系统，并为设计优化和逆问题提供支持。

Method: 采用物理信息神经网络(PINN)框架，通过最小化电路微分代数方程(DAE)的残差来求解。使用时间域解析方程建模器件和电路波形，并计算精确的时间导数。

Result: NeuroSPICE在训练速度和精度上不优于传统SPICE，但具有独特优势：能够为设计优化提供代理模型，解决逆问题，并能仿真包括铁电存储器在内的高度非线性新兴器件。

Conclusion: NeuroSPICE提供了一个灵活的PINN框架，虽然计算效率不如传统SPICE，但在处理复杂非线性系统和设计优化问题上具有独特价值，为电路仿真开辟了新方向。

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [55] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: I-PERI：一种联邦因果发现算法，在未知客户端干预下恢复更紧的等价类（Φ-MEC），通过利用跨客户端的结构差异来定向更多边


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法假设所有客户端共享相同因果模型，这在实践中不现实，因为客户端特定策略（如医院协议）会引入异质且未知的干预

Method: 提出I-PERI算法：1) 恢复客户端图并集的CPDAG；2) 利用跨客户端干预诱导的结构差异定向额外边，得到更紧的Φ-Markov等价类（Φ-CPDAG）

Result: 提供I-PERI收敛性和隐私保护的理论保证，在合成数据上的实证评估证明了算法的有效性

Conclusion: I-PERI解决了联邦因果发现中未知客户端干预的挑战，通过利用跨客户端结构差异获得更精确的因果图，具有理论保证和实际效果

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [56] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Web World Model (WWM) 是一种结合传统Web框架可靠性与生成式世界模型灵活性的混合方法，在结构化代码基础上使用LLM生成内容和决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两极分化：传统Web框架提供可靠但固定的环境，而完全生成式世界模型追求无限环境但牺牲了可控性和工程实践性。需要一种中间方案来平衡逻辑一致性与创造性。

Method: 提出Web World Model (WWM)，使用普通Web代码实现世界状态和"物理规则"确保逻辑一致性，同时利用大语言模型在结构化潜在状态基础上生成上下文、叙事和高层决策。构建了基于真实Web技术栈的WWM套件，包括基于真实地理的无限旅行地图、虚构星系探索者、网络规模百科全书和叙事世界等。

Result: 建立了多个WWM系统，识别出实用设计原则：分离代码定义规则与模型驱动想象、将潜在状态表示为类型化Web接口、利用确定性生成实现无限但有结构的探索。结果表明Web技术栈本身可以作为世界模型的可扩展基板。

Conclusion: Web World Model 提供了一种可控但开放的环境构建方法，Web技术栈可以作为世界模型的可扩展基板，平衡了逻辑一致性与创造性探索的需求。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [57] [RIS, Active RIS or RDARS: A Comparative Insight Through the Lens of Energy Efficiency](https://arxiv.org/abs/2512.22533)
*Aparna V C,Shashank Shekhar,Sheetal Kalyani*

Main category: cs.IT

TL;DR: 该论文比较了RIS、有源RIS和RDARS在sub-6GHz和毫米波频段的覆盖范围和能效，发现RDARS在sub-6GHz系统中能效更高，而有源RIS在毫米波系统中更优，少量元件时RIS仍最具能效优势。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)存在乘性衰落问题，限制了其在sub-6GHz和毫米波网络中的有效覆盖范围。虽然有源RIS架构可以缓解这一问题，但需要高功耗并带来实际挑战。需要研究更有效的解决方案来平衡覆盖范围和能效。

Method: 通过仿真比较RIS、有源RIS和RDARS在sub-6GHz和毫米波频段的覆盖范围和能效，并研究可重构表面(RS)的放置位置和元件数量对能效和覆盖范围的影响。

Result: 仿真结果显示：1) RDARS在sub-6GHz系统中提供高能效的覆盖增强方案；2) 有源RIS在毫米波系统中显著更节能；3) 对于较少数量的RS元件和近端用户设备，RIS仍比有源RIS和RDARS更具能效优势。

Conclusion: 不同可重构表面技术在不同频段和应用场景下具有不同的能效优势，需要根据具体系统参数（频段、元件数量、用户距离）选择合适的技术方案，以实现覆盖范围和能效的最佳平衡。

Abstract: Multiplicative fading is a major limitation of reconfigurable intelligent surfaces (RIS), restricting their effective coverage in both existing sub-6GHz systems and future mmWave networks. Although active RIS architectures mitigate this issue, they require high power consumption and introduce practical challenges due to the need for integrated amplifiers. Recently, reconfigurable distributed antenna and reflecting surfaces (RDARS) have been proposed to alleviate multiplicative fading through connected modes. In this work, we compare RIS, active RIS, and RDARS in terms of coverage and energy efficiency (EE) in both sub-6GHz and mmWave bands, and we investigate the impact of placement and the number of elements of reconfigurable surface (RS) on EE and coverage. The simulation results show that RDARS offers a highly energy-efficient alternative of enhancing coverage in sub-6GHz systems, while active RIS is significantly more energy-efficient in mmWave systems. Additionally, for a lower number of RS elements and for near UEs, RIS remains considerably more energy-efficient than both active RIS and RDARS.

</details>


### [58] [Optimal Beamforming Design for Multi-user MIMO Near-Field ISAC Systems with Movable Antennas](https://arxiv.org/abs/2512.22620)
*Nemanja Stefan Perović,Keshav Singh,Chih-Peng Li,Octavia A. Dobre,Mark F. Flanagan*

Main category: cs.IT

TL;DR: 该论文研究了在近场场景下使用可移动天线（MAs）的集成感知与通信（ISAC）系统，通过优化天线位置和波束成形来最大化加权和速率，同时满足最小感知要求。


<details>
  <summary>Details</summary>
Motivation: 虽然MA赋能的ISAC系统在远场中的增益已有研究，但在近场场景下几乎未被探索。近场效应在毫米波和太赫兹频段变得重要，因此需要研究MA在近场ISAC系统中的性能优势。

Method: 提出交替优化算法，同时优化通信预编码矩阵、感知发射波束成形器、感知接收合并器、用户MAs位置和基站发射MAs位置。考虑了线性预编码和迫零（ZF）预编码两种方案。

Result: 仿真结果表明：1）在近场ISAC系统中使用MAs相比固定天线系统有显著性能优势；2）线性预编码方案在不平等用户权重下获得更大的WSR；3）ZF预编码方案对所有用户权重保持近似恒定的WSR；4）WSR高度依赖于不同用户MAs之间的天线间干扰；5）感知性能比通信性能更受最小感知SINR阈值的影响。

Conclusion: MA在近场ISAC系统中能提供实质性性能提升，提出的优化算法有效平衡了通信和感知性能。不同预编码策略适用于不同的用户权重场景，天线间干扰是影响性能的关键因素。

Abstract: Integrated sensing and communication (ISAC) has been recognized as one of the key technologies capable of simultaneously improving communication and sensing services in future wireless networks. Moreover, the introduction of recently developed movable antennas (MAs) has the potential to further increase the performance gains of ISAC systems. Although the gains of MA-enabled ISAC systems are relatively well studied in the far field, they remain almost unexplored in near-field scenarios. Motivated by this, in this paper we maximize the weighted sum rate (WSR) for communication users while maintaining a minimum sensing requirement in an MA-enabled near-field ISAC system. To achieve this goal, we propose algorithms that optimize the communication precoding matrices, the sensing transmit beamformer, the sensing receive combiner, the positions of the users' MAs and the positions of the base station (BS) transmit MAs in an alternating manner for the considered ISAC system, for the cases where linear procoding and zero-forcing (ZF) precoding are employed at the BS. Simulation results show that using MAs in near-field ISAC systems provides a substantial performance advantage compared to near-field ISAC systems equipped with fixed antennas only. We show that the scheme with linear precoding achieves larger WSR for unequal users' weight rates, while the scheme with ZF precoding maintains an approximately constant WSR for all users' weight rates. Additionally, we demonstrate that the WSRs of the proposed schemes are highly dependent on the inter-antenna interference between different user's MAs, and that the sensing performance is significantly more affected by the minimum sensing signal-to-interference-plus-noise ratio (SINR) threshold compared to the communication performance.

</details>


### [59] [An Improved Lower Bound on Cardinality of Support of the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2512.22691)
*Haiyang Wang,Luca Barletta,Alex Dytso*

Main category: cs.IT

TL;DR: 本文改进了振幅受限加性高斯白噪声信道容量最优输入分布的支撑集大小下界，从线性增长提升到A√logA，推翻了先前关于线性增长最优的猜想。


<details>
  <summary>Details</summary>
Motivation: 研究振幅受限加性高斯白噪声信道容量最优输入分布的支撑集大小。已知该分布是离散的且支撑点有限，现有下界为A量级，上界为A²量级，有猜想认为线性增长是最优的，需要验证这一猜想。

Method: 1. 量化容量最优输出分布在振幅约束内部接近均匀分布的事实；2. 引入包装操作将问题映射到紧致域；3. 发展有限高斯混合对均匀分布的最佳逼近理论；4. 结合容量最优分布的稳定性性质得到最终下界。

Result: 建立了新的下界A√logA，改进了已知的线性下界，推翻了线性增长最优的猜想，证明支撑集大小至少以A√logA的速度增长。

Conclusion: 容量最优输入分布的支撑集大小下界为A√logA量级，线性增长不是最优的，这为理解振幅受限高斯信道的容量特性提供了新的理论洞见。

Abstract: We study the amplitude-constrained additive white Gaussian noise channel. It is well known that the capacity-achieving input distribution for this channel is discrete and supported on finitely many points. The best known bounds show that the support size of the capacity-achieving distribution is lower-bounded by a term of order $A$ and upper-bounded by a term of order $A^2$, where $A$ denotes the amplitude constraint. It was conjectured in [1] that the linear scaling is optimal. In this work, we establish a new lower bound of order $A\sqrt{\log A}$, improving the known bound and ruling out the conjectured linear scaling.
  To obtain this result, we quantify the fact that the capacity-achieving output distribution is close to the uniform distribution in the interior of the amplitude constraint. Next, we introduce a wrapping operation that maps the problem to a compact domain and develop a theory of best approximation of the uniform distribution by finite Gaussian mixtures. These approximation bounds are then combined with stability properties of capacity-achieving distributions to yield the final support-size lower bound.

</details>


### [60] [Iterative Channel Estimation, Detection and Decoding for Multi-Antenna Systems with RIS](https://arxiv.org/abs/2512.22731)
*Roberto C. G. Porto,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 提出一种用于多用户多天线系统中多RIS辅助上行链路的迭代信道估计、检测和解码方案，包括基于编码导频的迭代信道估计和利用时域相关性的迭代信道跟踪方法。


<details>
  <summary>Details</summary>
Motivation: 在多RIS辅助的多用户多天线系统中，传统的信道估计方法面临导频开销大、估计精度有限的问题，特别是在非稀疏传播环境下。需要开发能够降低导频开销同时提高估计精度的有效信道估计方案。

Method: 提出ICEDD方案，包含：1）ICCE技术，利用LDPC码和迭代处理，通过编码导频同时使用导频和校验位迭代优化信道估计；2）ICT方法，利用信道时域相关性进一步改善性能；3）提供NMSE分析、计算复杂度研究和码率影响分析。

Result: 在sub-6 GHz多RIS场景下，非稀疏传播环境中的数值结果表明，该方案在LOS和NLOS条件下以及不同RIS架构下均表现出良好性能，验证了所提方法的有效性。

Conclusion: 提出的ICEDD方案通过编码导频和迭代处理，有效降低了导频开销并提高了信道估计精度，为多RIS辅助的多用户多天线系统提供了一种高效的信道估计解决方案。

Abstract: This work proposes an iterative channel estimation, detection and decoding (ICEDD) scheme for the uplink of multi-user multi-antenna systems assisted by multiple reconfigurable intelligent surfaces (RIS)}. A novel iterative code-aided channel estimation (ICCE) technique is developed that uses low-density parity-check (LDPC) codes and iterative processing to enhance estimation accuracy while reducing pilot overhead. The core idea is to exploit encoded pilots (EP), enabling the use of both pilot and parity bits to iteratively refine channel estimates. To further improve performance, an iterative channel tracking (ICT) method is proposed that takes advantage of the temporal correlation of the channel. An analytical evaluation of the proposed estimator is provided in terms of normalized mean-squared error (NMSE), along with a study of its computational complexity and the impact of the code rate. Numerical results validate the performance of the proposed scheme in a sub-6 GHz multi-RIS scenario with non-sparse propagation, under both LOS and NLOS conditions, and different RIS architectures.

</details>


### [61] [Beyond Beam Sweeping: One-Shot Satellite Acquisition with Doppler-Aware Rainbow Beamforming](https://arxiv.org/abs/2512.22828)
*Juha Park,Ian P. Roberts,Wonjae Shin*

Main category: cs.IT

TL;DR: 提出一种利用多普勒效应和波束倾斜效应的单次卫星捕获框架，通过彩虹波束形成器实现多卫星同时接收，无需波束扫描。


<details>
  <summary>Details</summary>
Motivation: 传统LEO卫星通信中，高增益波束成形需要精确卫星位置，通常采用时域波束扫描方法，但这会产生大量开销和延迟。需要更高效的卫星捕获方法。

Method: 提出单次卫星捕获框架，利用多普勒效应和波束倾斜效应。推导出闭式彩虹波束形成器，利用波束倾斜效应将频率相关波束方向与多普勒频移推断的卫星位置对齐。开发三种基于接收信号的多普勒感知角度估计算法。

Result: 仿真结果表明，该方法在捕获精度和所需时隙方面显著优于传统波束扫描方法。彩虹波束形成器能够利用多普勒频移的角度依赖性，通过单次导频传输和接收实现全角度域覆盖。

Conclusion: 提出的框架通过利用传统上被视为损伤的多普勒和波束倾斜效应，实现了高效的卫星捕获，避免了传统波束扫描的开销和延迟问题。

Abstract: High-gain beamforming (BF) is essential for low Earth orbit (LEO) satellite communications to overcome severe path loss, but this requires acquiring precise satellite positions. Conventional satellite acquisition typically relies on time-domain beam sweeping, which incurs substantial overhead and latency. In this correspondence, we propose an efficient one-shot satellite acquisition framework that capitalizes on two phenomena traditionally regarded as impairments: i) Doppler effects and ii) beam-squint effects. Specifically, we derive a closed-form \emph{rainbow beamformer} that leverages beam-squint effects to align frequency-dependent beam directions with satellite positions inferred from their Doppler shifts. This approach enables reception from multiple satellites at once without requiring beam sweeping. To extract satellite position information, we develop three Doppler-aware angle estimation algorithms based on received signals. Simulation results demonstrate that the proposed method significantly outperforms conventional beam sweeping approaches in both acquisition accuracy and required time slots. These gains stem from the ability of the proposed rainbow BF to exploit the \emph{angle-dependent nature of Doppler shifts}, enabling full angular-domain coverage with a single pilot transmission and reception.

</details>


### [62] [Covering in Hamming and Grassmann Spaces: New Bounds and Reed--Solomon-Based Constructions](https://arxiv.org/abs/2512.22911)
*Samin Riasat,Hessam Mahdavifar*

Main category: cs.IT

TL;DR: 该论文提出了一种统一的编码理论和信息理论框架来研究汉明空间和格拉斯曼空间中的覆盖问题，引入了平均覆盖半径作为平均失真的度量，并推导了非渐近随机编码界，同时构造了基于GRS码和CRS码的高效覆盖算法。


<details>
  <summary>Details</summary>
Motivation: 研究汉明空间和格拉斯曼空间中的覆盖问题，将覆盖视为一般度量空间中的量化形式，需要补充经典最坏情况覆盖半径之外的平均失真度量，并探索代数结构在覆盖问题中的作用。

Method: 1. 引入平均覆盖半径作为平均失真度量；2. 利用单次率失真理论工具推导非渐近随机编码界；3. 开发基于穿孔的GRS码覆盖算法；4. 扩展到新的子空间码族CRS码用于格拉斯曼量化；5. 在汉明空间和格拉斯曼空间进行数值评估。

Result: 1. 推导了汉明空间和格拉斯曼空间的平均覆盖半径随机编码界；2. 数值结果显示RS构造在汉明空间中优于随机码本；3. 在一维格拉斯曼空间中，CRS码在高码率区域渐近达到随机编码界的常数因子内；4. 结构化码在最坏情况覆盖性能差的情况下表现出强的平均覆盖性能。

Conclusion: 该研究为覆盖问题和高维量化提供了新的视角，揭示了代数结构在平均覆盖性能中的重要作用，建立了统一的编码-信息理论框架，并为实际应用中的高效覆盖算法提供了理论基础。

Abstract: We study covering problems in Hamming and Grassmann spaces through a unified coding-theoretic and information-theoretic framework. Viewing covering as a form of quantization in general metric spaces, we introduce the notion of the average covering radius as a natural measure of average distortion, complementing the classical worst-case covering radius. By leveraging tools from one-shot rate-distortion theory, we derive explicit non-asymptotic random-coding bounds on the average covering radius in both spaces, which serve as fundamental performance benchmarks.
  On the construction side, we develop efficient puncturing-based covering algorithms for generalized Reed--Solomon (GRS) codes in the Hamming space and extend them to a new family of subspace codes, termed character-Reed--Solomon (CRS) codes, for Grassmannian quantization under the chordal distance. Our results reveal that, despite poor worst-case covering guarantees, these structured codes exhibit strong average covering performance. In particular, numerical results in the Hamming space demonstrate that RS-based constructions often outperform random codebooks in terms of average covering radius. In the one-dimensional Grassmann space, we numerically show that CRS codes over prime fields asymptotically achieve average covering radii within a constant factor of the random-coding bound in the high-rate regime. Together, these results provide new insights into the role of algebraic structure in covering problems and high-dimensional quantization.

</details>


### [63] [Generalized Hyperderivative Reed-Solomon Codes](https://arxiv.org/abs/2512.22948)
*Mahir Bilen Can,Benjamin Horowitz*

Main category: cs.IT

TL;DR: GHRS码是NRT Reed-Solomon码的推广，具有MDS特性、对偶码也是GHRS码、部分子族是LDPC码、部分子族是准循环码，且存在同时具备所有这些特性的GHRS码。


<details>
  <summary>Details</summary>
Motivation: 推广NRT Reed-Solomon码，构建具有多种优良特性的广义码类，包括MDS特性、对偶结构、LDPC特性和准循环结构。

Method: 引入广义超导数Reed-Solomon码(GHRS码)，作为NRT Reed-Solomon码的推广，研究其代数结构和组合特性。

Result: 1) 所有GHRS码都是MDS码；2) GHRS码的对偶码也是GHRS码；3) 确定了GHRS码中构成低密度奇偶校验码(LDPC)的子族；4) 确定了构成准循环码的GHRS码子族；5) 存在同时具备所有这些特性的GHRS码。

Conclusion: GHRS码提供了一个统一的框架，将多种优良编码特性整合在一起，为构造具有MDS、LDPC和准循环特性的高效纠错码提供了新途径。

Abstract: This article introduces Generalized Hyperderivative Reed-Solomon codes (GHRS codes), which generalize NRT Reed-Solomon codes. Its main results are as follows: 1) every GHRS code is MDS, 2) the dual of a GHRS code is also an GHRS code, 3) determine subfamilies of GHRS codes whose members are low-density parity-check codes (LDPCs), and 4) determine a family of GHRS codes whose members are quasi-cyclic. We point out that there are GHRS codes having all of these properties.

</details>


### [64] [User-Centric Cell-Free Massive MIMO Enhanced by Fluid-Antenna Access Points: Uplink Analysis](https://arxiv.org/abs/2512.23046)
*Maryam Olyaee,Giovanni Interdonato,Stefano Buzzi*

Main category: cs.IT

TL;DR: 本文研究了配备流体天线的无蜂窝大规模MIMO系统，提出了信道估计、天线端口选择和上行频谱效率优化的综合框架，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线在无蜂窝大规模MIMO系统中存在性能限制，流体天线通过可重构特性能够提供更好的空间多样性和适应性，但需要专门的信道估计和优化策略来充分发挥其潜力。

Method: 提出基于LMMSE的上行信道估计方案，在导频传输期间动态激活流体天线端口；设计分布式端口选择策略以最小化信道估计误差；使用Jakes信道模型分析天线几何和空间相关性；推导SINR表达式和上行频谱效率闭式解；提出交替优化框架选择最优天线端口配置。

Result: 数值结果表明，所提出的流体天线感知信道估计和端口优化策略显著降低了信道估计误差，相比固定天线和非优化流体天线基线，显著提高了系统和频谱效率。

Conclusion: 流体天线是实现可扩展、自适应无蜂窝大规模MIMO网络的关键使能技术，所提出的综合框架有效利用了流体天线的可重构特性，为未来无线网络设计提供了重要参考。

Abstract: In this paper, we investigate cell-free massive MIMO (CF-mMIMO) systems in which access points (APs) are equipped with fluid antennas (FAs) and develop a comprehensive framework for channel estimation, antenna port selection, and uplink spectral efficiency (SE) optimization. We propose a generalized LMMSE-based uplink channel estimation scheme that dynamically activates FA ports during pilot transmission, efficiently exploiting antenna reconfigurability under practical training constraints. Building on this, we design a distributed port selection strategy that minimizes per-AP channel estimation error by exploiting spatial correlation among FA ports. We systematically analyze the impact of antenna geometry and spatial correlation using the Jakes' channel model for different AP array configurations, including uniform linear and planar arrays. We then derive SINR expressions for centralized and distributed uplink processing and obtain a closed-form uplink SE expression for centralized maximum-ratio combining using the use-and-then-forget bound. Finally, we propose an alternating-optimization framework to select FA port configurations that maximize the uplink sum SE. Numerical results show that the proposed FA-aware channel estimation and port optimization strategies greatly reduce channel estimation error and significantly improve sum-SE over fixed-antenna and non-optimized FA baselines, confirming FAs as a key enabler for scalable, adaptive CF-mMIMO networks.

</details>


### [65] [A New Family of Binary Sequences via Elliptic Function Fields over Finite Fields of Odd Characteristics](https://arxiv.org/abs/2512.23194)
*Xiaofeng Liu,Jun Zhang,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 将二元序列构造从特征2的循环椭圆函数域推广到奇特征情形，使用二次剩余映射替代迹映射，获得具有良好平衡性、相关性和线性复杂度的新序列族


<details>
  <summary>Details</summary>
Motivation: 受Jin等人利用有限域F_{2^n}上循环椭圆函数域构造二元序列的启发，将构造推广到奇特征情形，使用二次剩余映射替代迹映射以获得更优性能

Method: 对于具有q+1+t个有理点的循环椭圆函数域，选取与q+1+t互质的正整数d，利用二次剩余映射η构造二元序列，替代原构造中的迹映射

Result: 构造了长度为q+1+t、规模为q^{d-1}-1的新二元序列族，平衡性上界为(d+1)·⌊2√q⌋+|t|+d，相关性上界为(2d+1)·⌊2√q⌋+|t|+2d，线性复杂度下界为(q+1+2t-d-(d+1)·⌊2√q⌋)/(d+d·⌊2√q⌋)

Conclusion: 成功将二元序列构造推广到奇特征循环椭圆函数域，使用二次剩余映射获得了具有良好密码学性质的新序列族，为序列设计提供了新方法

Abstract: Motivated by the constructions of binary sequences by utilizing the cyclic elliptic function fields over the finite field $\mathbb{F}_{2^{n}}$ by Jin \textit{et al.} in [IEEE Trans. Inf. Theory 71(8), 2025], we extend the construction to the cyclic elliptic function fields with odd characteristic by using the quadratic residue map $η$ instead of the trace map used therein. For any cyclic elliptic function field with $q+1+t$ rational points and any positive integer $d$ with $\gcd(d, q+1+t)=1$, we construct a new family of binary sequences of length $q+1+t$, size $q^{d-1}-1$, balance upper bounded by $(d+1)\cdot\lfloor2\sqrt{q}\rfloor+|t|+d,$ the correlation upper bounded by $(2d+1)\cdot\lfloor2\sqrt{q}\rfloor+|t|+2d$ and the linear complexity lower bounded by $\frac{q+1+2t-d-(d+1)\cdot\lfloor2\sqrt{q}\rfloor}{d+d\cdot\lfloor2\sqrt{q}\rfloor}$ where $\lfloor x\rfloor$ stands for the integer part of $x\in\mathbb{R}$.

</details>


### [66] [Sum Rate optimization for RIS-Aided RSMA system with Movable Antenna](https://arxiv.org/abs/2512.23242)
*Mingyu Hu,Nan Liu,Wei Kang*

Main category: cs.IT

TL;DR: 提出移动天线辅助的RSMA-RIS框架，通过联合优化波束成形、RIS反射矩阵、速率分配和天线位置，提升系统总速率性能。


<details>
  <summary>Details</summary>
Motivation: 传统RSMA-RIS架构中天线位置固定，未能充分利用空间自由度，限制了系统性能。移动天线技术能够提供额外的空间自由度，有望进一步提升RSMA-RIS系统的性能。

Method: 提出MA辅助的RSMA-RIS框架，建立总速率最大化问题，采用分数规划方法进行等价转换，推导公共速率分割的闭式解，利用KKT条件迭代更新拉格朗日乘子和波束成形矩阵，通过对偶问题更新RIS反射矩阵，使用梯度上升法优化天线位置。

Result: 数值结果表明，即使在RIS辅助下，引入移动天线仍能为RSMA带来额外的性能提升。相对于SDMA，移动天线对RSMA的性能增益更大。

Conclusion: 移动天线辅助的RSMA-RIS框架能够有效利用空间自由度，显著提升系统性能，为6G无线系统提供了有前景的干扰管理和性能增强方案。

Abstract: Rate-Splitting Multiple Access (RSMA) is regarded as a key enabling technique for sixth-generation (6G) wireless systems for its powerful interference management substantially enhancing link throughput. Reconfigurable Intelligent Surface (RIS) can effectively shape the wireless propagation to match the environment and improve communication performance. However, in conventional RSMA-RIS architectures, the antenna elements are fixed, which underutilizes spatial degrees of freedom and hence constrains system performance. To address this limitation, we propose a movable-antenna (MA) assisted RSMA-RIS framework and formulate a sum-rate maximization problem that jointly optimizes the transmit beamforming matrix, the RIS reflection matrix, the common-rate partition, and the MA positions. The original problem is equivalently transformed by employing the fractional programming (FP) method, and a closed-form solution for the common rate splitting is derived. Leveraging the Karush-Kuhn-Tucker (KKT) conditions, we obtain iterative updates for the Lagrange multipliers together with a closed-form expression for the beamforming matrix. We then develop an update rule for the RIS reflection matrix via the dual problem, and finally determine the optimal antenna locations using a gradient-ascent procedure. Numerical results indicate that, even in the presence of RIS assistance, incorporating MAs yields additional performance improvements for RSMA. Moreover, relative to space-division multiple access (SDMA), the assistance of MA yields a greater performance gain for RSMA.

</details>


### [67] [Information Inequalities for Five Random Variables](https://arxiv.org/abs/2512.23316)
*E. P. Csirmaz,L. Csirmaz*

Main category: cs.IT

TL;DR: 利用最大熵方法研究五变量熵区域，通过理论简化与对称性降低计算复杂度，发现了两类无限非香农不等式集合，并开发了枚举极值不等式的算法。


<details>
  <summary>Details</summary>
Motivation: 熵区域描述了联合分布离散随机变量所有子向量的香农熵集合，对于四个或更多变量，其结构大部分未知。本文旨在探索五变量熵区域的结构，特别是非香农不等式的特征。

Method: 采用最大熵方法的变体，通过添加随机变量的多代副本来界定五变量熵区域。利用理论考虑和内在对称性显著降低计算复杂度，计算了前九代提供的所有五变量非香农不等式。

Result: 基于计算结果定义了两个无限的非香农不等式集合，并证明了它们是熵不等式。研究了参数化这些集合的非负格点向下封闭子集，开发了枚举所有极值不等式的算法。

Conclusion: 发现的熵不等式集合被推测能完全表征所应用的方法。该研究为理解多变量熵区域结构提供了新工具和见解。

Abstract: The entropic region is formed by the collection of the Shannon entropies of all subvectors of finitely many jointly distributed discrete random variables. For four or more variables the structure of the entropic region is mostly unknown. We utilize a variant of the Maximum Entropy Method to delimit the five-variable entropy region. This method adds copies of some of the random variables in generations. A significant reduction in computational complexity, achieved through theoretical considerations and by harnessing the inherent symmetries, allowed us to calculate all five-variable non-Shannon inequalities provided by the first nine generations. Based on the results, we define two infinite collections of such inequalities, and prove them to be entropy inequalities. We investigate downward closed subsets of non-negative lattice points that parameterize these collections, based on which we develop an algorithm to enumerate all extremal inequalities. The discovered set of entropy inequalities is conjectured to characterize the applied method completely.

</details>


### [68] [Faster-than-Nyquist Signaling for Next-Generation Wireless: Principles, Applications, and Challenges](https://arxiv.org/abs/2512.23377)
*Shuangyang Li,Melda Yuksel,Tongyang Xu,Shinya Sugiura,Jinhong Yuan,Giuseppe Caire,Lajos Hanzo*

Main category: cs.IT

TL;DR: 该论文提供了关于快于奈奎斯特（FTN）信令的可访问且结构化的介绍，涵盖其核心原理、理论基础、独特优势、开放方面和发展路线图，特别强调了FTN在集成感知与通信（ISAC）中的优势。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要支持超高吞吐量的新兴应用，而传统的奈奎斯特信令可能无法满足需求。FTN信令可以在不扩展时频资源的情况下传输更多符号，为解决这一挑战提供了方案。

Method: 论文采用结构化介绍的方法，系统性地阐述FTN信令的核心原理、理论基础、独特优势、开放研究方向和路线图，特别关注编码FTN结果及其在ISAC中的应用。

Result: 论文展示了有前景的编码FTN结果，并强调了FTN在集成感知与通信（ISAC）中的显著优势，ISAC是未来网络日益关键的功能。

Conclusion: 论文最后讨论了开放的研究挑战和有前景的研究方向，为FTN信令的未来发展提供了路线图。

Abstract: Future wireless networks are expected to deliver ultra-high throughput for supporting emerging applications. In such scenarios, conventional Nyquist signaling may falter. As a remedy, faster-than-Nyquist (FTN) signaling facilitates the transmission of more symbols than Nyquist signaling without expanding the time-frequency resources. We provide an accessible and structured introduction to FTN signaling, covering its core principles, theoretical foundations, unique advantages, open facets, and its road map. Specifically, we present promising coded FTN results and highlight its compelling advantages in integrated sensing and communications (ISAC), an increasingly critical function in future networks. We conclude with a discussion of open research challenges and promising directions.

</details>


### [69] [Dynamic Channel Knowledge Map Construction in MIMO-OFDM Systems](https://arxiv.org/abs/2512.23470)
*Wenjun Jiang,Xiaojun Yuan,Chenchen Liu,Boyu Teng*

Main category: cs.IT

TL;DR: 本文提出了一种用于MIMO-OFDM系统的动态信道知识图谱构建方法，通过两阶段贝叶斯推理算法，在动态环境中实现低开销、高性能的信道估计。


<details>
  <summary>Details</summary>
Motivation: 现有CKM构建方法主要针对准静态传播环境，无法有效处理动态环境中的信道变化。本文旨在开发能够适应动态散射体、天线旋转和同步误差等动态因素的CKM构建方法。

Method: 建立包含准静态和动态散射体、天线旋转和同步误差的动态信道模型；在贝叶斯推理框架下设计两阶段近似推理算法：第一阶段联合推断准静态信道参数并校准同步误差，第二阶段利用准静态参数作为先验信息，从有限实时测量中估计动态参数。

Result: 仿真结果验证了所提方法的优越性，证明其在动态环境中能够实现低开销、高性能的信道估计。

Conclusion: 本文提出的动态CKM构建方法能够有效处理动态环境中的信道变化，为环境感知通信提供了新的解决方案，特别适用于MIMO-OFDM系统在动态场景下的信道估计需求。

Abstract: Channel knowledge map (CKM) is a promising paradigm for environment-aware communications by establishing a deterministic mapping between physical locations and channel parameters. Existing CKM construction methods focus on quasi-static propagation environment. This paper develops a dynamic CKM construction method for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems. We establish a dynamic channel model that captures the coexistence of quasi-static and dynamic scatterers, as well as the impacts of antenna rotation and synchronization errors. Based on this model, we formulate the problem of dynamic CKM construction within a Bayesian inference framework and design a two-stage approximate Bayesian inference algorithm. In stage I, a high-performance algorithm is developed to jointly infer quasi-static channel parameters and calibrate synchronization errors from historical measurements. In stage II, by leveraging the quasi-static parameters as informative priors, a low-complexity algorithm is designed to estimate dynamic parameters from limited real-time measurements. Simulation results validate the superiority of the proposed method and demonstrate its effectiveness in enabling low-overhead, high-performance channel estimation in dynamic environments.

</details>


### [70] [Affine-Projection Recovery of Continuous Angular Power Spectrum: Geometry and Resolution](https://arxiv.org/abs/2512.23506)
*Shengsong Luo,Ruilin Wu,Chongbin Xu,Junjie Ma,Xiaojun Yuan,Xin Wang*

Main category: cs.IT

TL;DR: 本文提出了一种基于加权傅里叶域的PLV算法改进方法，用于从信道协方差中恢复连续角功率谱，建立了明确的三角多项式表示和闭式解，并给出了精确的能量恒等式和尖锐的可辨识性/分辨率特性分析。


<details>
  <summary>Details</summary>
Motivation: 从信道协方差中恢复连续角功率谱是无线通信中的一个重要问题。现有的PLV算法虽然有效，但缺乏几何直观性和理论完备性。本文旨在通过加权傅里叶域分析，提升PLV算法的几何可解释性，并建立更完整的理论框架。

Method: 在Miretti等人提出的PLV算法基础上，采用加权傅里叶域分析方法，强调其几何可解释性。该方法产生了明确的固定维三角多项式表示，并通过正定矩阵得到闭式解。建立了精确的能量恒等式来分析重建误差。

Result: 获得了PLV算法的闭式解和唯一性保证。建立了精确的能量恒等式，能够计算APS重建误差。给出了尖锐的可辨识性/分辨率特性：当且仅当真值APS位于识别的三角多项式子空间时，PLV能实现完美恢复；否则返回所有协方差一致谱中能量最小的APS。

Conclusion: 本文通过加权傅里叶域分析，显著提升了PLV算法的理论完备性和几何可解释性。提出的方法不仅提供了闭式解和唯一性保证，还建立了精确的能量分析框架，为角功率谱恢复问题提供了更深入的理论理解和实用工具。

Abstract: This paper considers recovering a continuous angular power spectrum (APS) from the channel covariance. Building on the projection-onto-linear-variety (PLV) algorithm, an affine-projection approach introduced by Miretti \emph{et. al.}, we analyze PLV in a well-defined \emph{weighted} Fourier-domain to emphasize its geometric interpretability. This yields an explicit fixed-dimensional trigonometric-polynomial representation and a closed-form solution via a positive-definite matrix, which directly implies uniqueness. We further establish an exact energy identity that yields the APS reconstruction error and leads to a sharp identifiability/resolution characterization: PLV achieves perfect recovery if and only if the ground-truth APS lies in the identified trigonometric-polynomial subspace; otherwise it returns the minimum-energy APS among all covariance-consistent spectra.

</details>
