<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Resource Allocation and Sharing for UAV-Assisted Integrated TN-NTN with Multi-Connectivity](https://arxiv.org/abs/2601.15532)
*Abd Ullah Khan,Wali Ullah Khan,Haejoon Jung,Hyundong Shin*

Main category: cs.NI

TL;DR: 该论文研究了在集成地面-非地面网络中，具有多连接能力的无人机资源分配问题，提出了两种算法分别优化总容量和公平性。


<details>
  <summary>Details</summary>
Motivation: 无人机在集成地面-非地面网络中通过多连接能力实现高效可靠的数据传输，但由于无人机移动性导致的信道变化、异构设备的多样化QoS需求、资源共享以及容量分配的公平性要求，使得频谱和功率资源的最优分配面临挑战。

Method: 考虑三种链路类型（无人机-基站、无人机-无人机、无人机-高空平台）和两种具有不同QoS需求的无人机类型。提出两种算法：第一种最大化无人机-基站和无人机-高空平台链路的容量，同时保证无人机-无人机链路的可靠性；第二种通过最大化所有链路的最小容量来确保公平性。

Result: 通过仿真验证了两种算法的性能，第一种算法优化了总容量，第二种算法确保了所有链路的公平容量分配。

Conclusion: 该研究为集成地面-非地面网络中具有多连接能力的无人机提供了有效的资源分配解决方案，既考虑了容量优化又兼顾了公平性要求。

Abstract: Unmanned aerial vehicles (UAVs) with multi- connectivity (MC) capabilities efficiently and reliably transfer data between terrestrial networks (TNs) and non-terrestrial networks (NTNs). However, optimally sharing and allocating spectrum and power resources to maintain MC while ensuring reliable connectivity and optimal performance remains challeng- ing in such networks. Channel variations induced by mobility in UAV networks, coupled with the varying quality of service (QoS) demands of heterogeneous devices, resource sharing, and fairness requirements in capacity distribution pose challenges to optimal resource allocation. Thus, this paper investigates resource allocation for QoS-constrained, MC-enabled, dynamic UAVs in an integrated TN-NTN environment with spectrum sharing and fairness considerations. To this end, we consider three types of links: UAV-to-radio base station (RBS), UAV-to-UAV, and UAV-to-HAP. We also assume two types of UAVs with diverse QoS requirements to reflect a practical scenario. Consequently, we propose two algorithms. The first algorithm maximizes the capacity of UAVs-RBS and UAVs-HAP links while ensuring the reliability of the UAV-UAV link. To achieve this, the algorithm maximizes the collective throughput of the UAVs by optimizing the sum capacity of all the UAV-RBS and UAV-HAP links. Next, to provide constant capacity to all links and ensure fairness, we propose another algorithm that maximizes the minimum capacity across all links. We validate the performance of both algorithms through simulation

</details>


### [2] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: MapViT：基于Vision Transformer的两阶段框架，用于预测环境变化和无线信号质量，为6G移动机器人提供实时感知解决方案


<details>
  <summary>Details</summary>
Motivation: 移动和无线网络的进步为机器人自主性提供了新机遇，但机器人在动态环境中需要准确理解环境和信号质量，这仍然是一个未解决的挑战性问题

Method: 提出MapViT，一个受LLM预训练-微调范式启发的两阶段Vision Transformer框架，通过自监督预训练几何基础模型，然后进行下游任务微调

Result: 实验表明该框架支持实时预测，ViT实现精度与计算效率的良好平衡，几何基础模型提高了数据效率和可迁移性，即使在有限标注数据下也能有效预测

Conclusion: 这项工作为下一代数字孪生生态系统奠定了基础，并为未来6G系统中的多模态智能ML基础模型开辟了新途径

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


### [3] [RF Intelligence for Health: Classification of SmartBAN Signals in overcrowded ISM band](https://arxiv.org/abs/2601.15836)
*Nicola Gallucci,Giacomo Aragnetti,Matteo Malagrinò,Francesco Linsalata,Maurizio Magarini,Lorenzo Mucchi*

Main category: cs.NI

TL;DR: 提出首个开源框架，用于在2.4GHz ISM频段中识别医疗传感器低功耗信号，结合合成数据集和真实射频采集，使用深度卷积神经网络实现超过90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在拥挤的2.4GHz ISM频段中，医疗传感器的低功耗传输难以识别，存在强同频干扰和功率不对称问题，这影响了可穿戴健康监测系统的可靠性。

Method: 开发开源框架，结合模拟信号的合成数据集和软件定义无线电获取的真实射频采集数据。采用基于ResNet编码器和U-Net解码器的深度卷积神经网络，并加入注意力机制。

Result: 在合成数据集上达到超过90%的准确率，在真实空中频谱图上表现出稳定的性能，能够可靠识别密集频谱环境中的SmartBAN信号。

Conclusion: 该框架支持干扰感知共存策略，提高了可穿戴医疗系统的可靠性，为体域网中SmartBAN信号的自动识别提供了有效解决方案。

Abstract: Accurate classification of Radio-Frequency (RF) signals is essential for reliable wearable health-monitoring systems, providing awareness of the interference conditions in which medical protocols operate. In the overcrowded 2.4 GHz ISM band, however, identifying low-power transmissions from medical sensors is challenging due to strong co-channel interference and substantial power asymmetry with coexisting technologies. This work introduces the first open source framework for automatic recognition of SmartBAN signals in Body Area Networks (BANs). The framework combines a synthetic dataset of simulated signals with real RF acquisitions obtained through Software-Defined Radios (SDRs), enabling both controlled and realistic evaluation. Deep convolutional neural networks based on ResNet encoders and U-Net decoders with attention mechanisms are trained and assessed across diverse propagation conditions. The proposed approach achieves over 90% accuracy on synthetic datasets and demonstrates consistent performance on real over-the-air spectrograms. By enabling reliable SmartBAN signal recognition in dense spectral environments, this framework supports interferenceaware coexistence strategies and improves the dependability of wearable healthcare systems.

</details>


### [4] [Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links](https://arxiv.org/abs/2601.15904)
*Hossein Mohammadalizadeh,Holger Karl*

Main category: cs.NI

TL;DR: 提出ACI框架解决带随机异构切换延迟的并行队列动态资源分配问题，通过非近视、基于帧的调度来分摊切换开销，证明吞吐最优性并在多无人机FSO回程网络中验证效果。


<details>
  <summary>Details</summary>
Motivation: 传统网络调度方案在处理具有切换延迟的并行队列资源分配时表现不佳，特别是当切换延迟是随机且异构时。经典的Max-Weight策略由于不考虑切换延迟而难以应对这一问题。

Method: 提出ACI（非近视、基于帧的调度框架），通过Lyapunov漂移分析证明基于积压驱动的ACI相对于缩放容量区域是吞吐最优的。框架核心是调整其紧迫性度量，以在吞吐量和延迟之间进行权衡。

Result: 理论分析证明ACI具有吞吐最优性；在多无人机网络与FSO回程的实际应用中验证了有效性；展示了通过调整紧迫性度量可以灵活处理吞吐-延迟权衡。

Conclusion: ACI框架成功解决了带随机异构切换延迟的并行队列调度问题，提供了吞吐最优且可灵活权衡吞吐与延迟的解决方案，适用于多无人机网络等实际场景。

Abstract: Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.

</details>


### [5] [Low-altitude Multi-UAV-assisted Data Collection and Semantic Forwarding for Post-Disaster Relief](https://arxiv.org/abs/2601.16146)
*Xiaoya Zheng,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Qingqing Wu,Dusit Niyato,Abbas Jamalipour*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型的交替优化方法，用于多无人机辅助的数据收集与语义转发网络，旨在提高传输速率和语义速率，同时降低能耗。


<details>
  <summary>Details</summary>
Motivation: 低空经济中，无人机作为空中中继在灾后通信恢复中发挥关键作用。传统方法面临长距离链路脆弱、数据量大与机载资源有限等挑战，需要更高效的解决方案。

Method: 提出LLM-AOA（大语言模型启发的交替优化方法），将多目标优化问题分解为不同决策变量子集，采用定制化优化策略处理动态变化的决策变量维度。

Result: 仿真结果表明，LLM-AOA在传输速率和语义速率上分别比传统AOA方法提升约26.8%和22.9%，能效表现更优。

Conclusion: LLM-AOA方法有效解决了低空多无人机数据收集与语义转发网络的复杂优化问题，为低空经济中的通信恢复提供了高效解决方案。

Abstract: The low-altitude economy (LAE) is an emerging economic paradigm which fosters integrated development across multiple fields. As a pivotal component of the LAE, low-altitude uncrewed aerial vehicles (UAVs) can restore communication by serving as aerial relays between the post-disaster areas and remote base stations (BSs). However, conventional approaches face challenges from vulnerable long-distance links between the UAVs and remote BSs, and data bottlenecks arising from massive data volumes and limited onboard UAV resources. In this work, we investigate a low-altitude multi-UAV-assisted data collection and semantic forwarding network, in which multiple UAVs collect data from ground users, form clusters, perform intra-cluster data aggregation with semantic extraction, and then cooperate as virtual antenna array (VAAs) to transmit the extracted semantic information to a remote BS via collaborative beamforming (CB). We formulate a data collection and semantic forwarding multi-objective optimization problem (DCSFMOP) that jointly maximizes both the user and semantic transmission rates while minimizing UAV energy consumption. The formulated DCSFMOP is a mixed-integer nonlinear programming (MINLP) problem that is inherently NP-hard and characterized by dynamically varying decision variable dimensionality. To address these challenges, we propose a large language model-enabled alternating optimization approach (LLM-AOA), which effectively handles the complex search space and variable dimensionality by optimizing different subsets of decision variables through tailored optimization strategies. Simulation results demonstrate that LLM-AOA outperforms AOA by approximately 26.8\% and 22.9\% in transmission rate and semantic rate, respectively.

</details>


### [6] [Real-Time HAP-Assisted Vehicular Edge Computing for Rural Areas](https://arxiv.org/abs/2301.09957)
*Alessandro Traspadini,Marco Giordani,Giovanni Giambene,Michele Zorzi*

Main category: cs.NI

TL;DR: 该论文研究在乡村场景中利用高空平台支持车联网边缘计算，分析车辆如何优化任务卸载策略以最大化实时服务概率。


<details>
  <summary>Details</summary>
Motivation: 非地面网络（NTNs）是6G网络的关键组成部分，可扩展乡村和偏远地区的网络覆盖。高空平台（HAPs）可作为边缘服务器处理地面设备（如物联网传感器和地面车辆）的计算任务卸载。在乡村场景中，地面车辆需要决定是在本地处理数据还是卸载到HAP，以支持车联网边缘计算。

Method: 将系统建模为一组队列，其中计算任务按照泊松到达过程到达。然后评估最优的车联网边缘计算卸载因子，在延迟和计算能力约束下最大化实时服务概率。

Result: 论文分析了最优的VEC卸载因子，该因子能够在满足延迟和计算能力约束的同时，最大化实时服务的概率。

Conclusion: 高空平台可以有效支持乡村场景中的车联网边缘计算，通过优化任务卸载策略可以显著提高实时服务的可靠性，为6G网络中非地面网络的应用提供了重要参考。

Abstract: Non-Terrestrial Networks (NTNs) are expected to be a key component of 6th generation (6G) networks to support broadband seamless Internet connectivity and expand the coverage even in rural and remote areas. In this context, High Altitude Platforms (HAPs) can act as edge servers to process computational tasks offloaded by energy-constrained terrestrial devices such as Internet of Things (IoT) sensors and ground vehicles (GVs). In this paper, we analyze the opportunity to support Vehicular Edge Computing (VEC) via HAP in a rural scenario where GVs can decide whether to process data onboard or offload them to a HAP. We characterize the system as a set of queues in which computational tasks arrive according to a Poisson arrival process. Then, we assess the optimal VEC offloading factor to maximize the probability of real-time service, given latency and computational capacity constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: Gated Sparse Attention (GSA) 结合了稀疏注意力和门控注意力的优势，在长上下文语言模型中实现了高效性和质量提升，同时改善了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中的注意力计算负担过重，现有稀疏注意力机制和门控注意力变体分别解决不同问题但相互独立，需要结合两者优势。

Method: 提出Gated Sparse Attention (GSA)，包含：1) 带sigmoid激活的门控闪电索引器，产生有界可解释的选择分数；2) 基于局部不确定性的自适应稀疏控制器，调节关注token数量；3) 值和输出阶段的双重门控。

Result: 在1.7B参数模型、400B token训练中，GSA在128K上下文长度下实现12-16倍加速，困惑度从6.03降至5.70，RULER分数几乎翻倍，首token注意力从47%降至4%以下，训练稳定性显著改善，损失峰值减少98%。

Conclusion: GSA成功结合了稀疏注意力的效率和门控注意力的质量优势，为长上下文语言模型提供了高效、稳定且高质量的注意力机制。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [8] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在急诊分诊中存在通过代理变量传递的偏见，模型会基于特定词汇修改对患者严重程度的判断，表明AI系统仍基于噪声和非因果信号训练，临床部署需更谨慎。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已应用于临床决策，但针对不同种族、社会经济和临床背景患者的隐性偏见依然存在。本研究旨在调查LLM在急诊分诊中的偏见问题。

Method: 使用32个患者层面的代理变量，每个变量由正负两种描述表示，在公开数据集（MIMIC-IV-ED Demo, MIMIC-IV Demo）和受限访问数据集（MIMIC-IV-ED, MIMIC-IV）上评估这些变量对LLM分诊决策的影响。

Result: 发现急诊分诊场景中存在通过代理变量传递的歧视性行为，LLM倾向于根据输入上下文中出现的特定词汇修改对患者严重程度的感知，无论这些词汇是正面还是负面描述。这表明AI系统仍基于噪声和非因果信号训练。

Conclusion: AI系统在临床环境中的安全负责任部署需要更多工作，当前系统仍存在基于不可靠信号的偏见问题。

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [9] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench是一个新的基准测试，用于评估生成式科学综述的学术价值，解决了现有基准测试只关注表面质量而忽略深层学术价值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在两个关键问题：1）缺乏学术维度标注的真实数据集不可靠；2）评估指标只关注表面质量（如逻辑连贯性），无法评估深层学术价值（如核心研究目标和批判性分析）。

Method: 提出了一个全面的学术价值评估标准，涵盖三个维度：信息价值、学术交流价值和研究指导价值。基于此标准构建了带有学术价值标注的可靠数据集，并评估生成式综述的深层学术价值。

Result: 大量实验结果表明，该基准测试在评估生成式综述的学术价值方面与人类表现高度一致。

Conclusion: DeepSurvey-Bench能够全面评估生成式科学综述的学术价值，解决了现有基准测试的局限性，为自动生成科学综述的质量评估提供了更可靠的基准。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [10] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon是一个神经符号认知操作系统，通过结构化记忆宫殿和神经符号事件图解决LLM在长上下文中的计算成本和"迷失在中间"问题，实现亚毫秒级检索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于自注意力的二次计算成本和"迷失在中间"现象，传统"扁平RAG"架构将记忆视为无结构的嵌入集合，无法捕捉长时交互的层次和时间结构，导致"向量迷雾"问题。

Method: 提出Aeon神经符号认知操作系统，将记忆结构化为记忆宫殿（通过Atlas空间索引实现）和事件轨迹（神经符号事件图），引入语义旁视缓冲区作为预测性缓存机制，利用对话局部性实现亚毫秒级检索。

Result: 基准测试显示Aeon在对话工作负载上实现<1ms检索延迟，通过零拷贝C++/Python桥确保状态一致性，为自主代理提供持久化结构化记忆。

Conclusion: Aeon重新定义记忆为管理的操作系统资源而非静态存储，有效解决了LLM在长上下文中的计算和记忆结构问题，为自主代理系统提供了高效可靠的内存管理方案。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [11] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 该论文是关于大型视觉语言模型在多模态假新闻检测中应用的系统性综述，追踪了从传统特征工程方法到端到端多模态推理框架的范式转变。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型视觉语言模型的快速发展推动了多模态假新闻检测的范式转变，从传统特征工程方法转向统一的端到端多模态推理框架。然而，该领域缺乏系统性的综述来追踪这一转变并整合最新发展。

Method: 论文通过历史视角呈现从传统多模态检测流程到基础模型驱动范式的演变，建立涵盖模型架构、数据集和性能基准的结构化分类体系，分析剩余技术挑战，并展望未来研究方向。

Result: 这是首个系统性记录和分析大型视觉语言模型在打击多模态假新闻中变革作用的全面综述，提供了历史视角、分类体系、技术挑战分析和未来方向指引。

Conclusion: 大型视觉语言模型正在彻底改变多模态假新闻检测领域，但仍面临可解释性、时序推理和领域泛化等挑战，需要进一步研究来推动这一范式转变的下一阶段发展。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [12] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: 本文提出DFAH框架，用于评估金融领域工具使用代理的轨迹确定性和证据条件忠实性，发现模型确定性与忠实性呈正相关，并提供金融基准测试套件。


<details>
  <summary>Details</summary>
Motivation: LLM代理在监管审计回放中存在一致性问题：当要求用相同输入重现被标记的交易决策时，大多数部署无法返回一致结果。这暴露了金融领域代理系统缺乏确定性和忠实性的问题。

Method: 引入确定性-忠实性保证框架（DFAH），通过74种配置实验（12个模型、4个提供商、每个T=0.0下8-24次运行）测量轨迹确定性和证据条件忠实性。提供三个金融基准测试（合规分类、投资组合约束、DataOps异常）和开源压力测试工具。

Result: 7-20B参数模型在非代理基准实验中达到100%确定性，而120B+模型需要3.7倍更大的验证样本才能达到同等统计可靠性。代理工具使用引入额外方差。确定性与忠实性呈正相关（r=0.45，p<0.01）。Tier 1模型在架构优先的设计下达到审计回放要求的确定性水平。

Conclusion: DFAH框架能够有效评估金融代理系统的确定性和忠实性，挑战了可靠性-能力权衡的假设，发现确定性与忠实性正相关，为金融领域的LLM代理部署提供了实用的评估工具和基准。

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [13] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: Prometheus Mind为冻结的Qwen3-4B模型添加记忆功能，通过11个模块化适配器实现，完全可逆。解决了提取、训练、注入和隐藏状态崩溃四个关键问题，在PrometheusExtract-132数据集上达到94.4%的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 为预训练语言模型添加记忆功能通常需要架构修改或权重调整，这会影响模型的原始能力。作者希望开发一种完全可逆的方法，在不改变冻结模型权重的情况下为模型添加记忆功能。

Method: 使用11个模块化适配器（530MB，7%开销）为冻结的Qwen3-4B模型添加记忆。主要创新包括：1) 对比方向发现从最小对中提取语义方向；2) 分阶段训练适配器；3) 利用lm_head.weight行进行映射；4) 训练投影层解决隐藏状态崩溃问题。

Result: 在PrometheusExtract-132数据集上，系统在干净输入上达到94.4%检索准确率，但在非正式输入（省略、填充词或隐含主语）上降至19.4%。主要瓶颈是关系分类，准确率仅为47.3%。

Conclusion: 成功开发了完全可逆的记忆添加方法，解决了四个关键技术挑战。虽然系统在干净输入上表现良好，但在非正式输入和关系分类方面仍有改进空间，为未来研究指明了方向。

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [14] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: 本文提出"知识图谱网络"的系统理论、技术和应用，特别是在医疗健康领域，涵盖模糊、不确定、多模态、向量化、分布式、联邦等多种条件下的定义、开发、推理、计算和应用。


<details>
  <summary>Details</summary>
Motivation: 知识图谱研究快速发展，但在医疗健康等领域的应用中，许多重要的信息处理技术仍滞后，包括未能充分利用先进的逻辑推理、人工智能技术、专用编程语言、现代概率统计理论等。特别是多知识图谱协同与竞争技术未得到足够重视。

Method: 开发"知识图谱网络"的系统理论、技术和应用框架，涵盖其在不同条件下的定义、开发、推理、计算和应用，包括模糊、不确定、多模态、向量化、分布式、联邦等多种场景。

Result: 在几乎所有情况下都提供了（真实数据）示例和实验结果，展示了知识图谱网络在医疗健康领域的实际应用效果。

Conclusion: 提出了知识图谱网络的创新理论框架和应用方法，为医疗健康领域的知识图谱应用提供了系统性的解决方案。

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [15] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN是一种基于组织病理学切片和临床元数据生成基因表达谱的生成对抗网络，在TCGA数据集上表现优于现有方法，下游疾病预测准确率提升11%以上。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究需要整合多种数据模态，但基因表达数据由于隐私法规和实验成本难以广泛获取。现有临床实践中常规收集的组织病理学切片和临床元数据可作为条件信息来生成基因表达谱。

Method: 提出GeMM-GAN生成对抗网络，使用Transformer编码器处理图像块，通过交叉注意力机制融合图像块和文本标记，生成条件向量指导生成器合成生物学上一致的基因表达谱。

Result: 在TCGA数据集上评估，GeMM-GAN优于标准生成模型，生成更真实且功能有意义的基因表达谱，下游疾病类型预测准确率相比当前最优方法提升超过11%。

Conclusion: GeMM-GAN能够有效利用常规临床数据生成高质量的基因表达谱，为生物医学研究提供了一种解决基因表达数据获取困难的创新方法。

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [16] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC框架通过在解码层直接集成上下文偏置，解决语音大语言模型识别新实体的问题，相比提示方法更高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在识别领域特定实体（如联系人姓名、播放列表、技术术语）方面存在局限，因为其训练知识是静态的。现有解决方案（如提示）存在可扩展性问题，而生成式错误纠正方法容易产生过度纠正和幻觉。

Method: LOGIC（Logit-Space Integration for Contextual Biasing）框架直接在解码层操作，将上下文注入与输入处理解耦，确保相对于提示长度的恒定时间复杂度。

Result: 在11种多语言环境中使用Phi-4-MM模型进行的广泛实验表明，LOGIC实现了平均9%的相对实体词错误率降低，而误报率仅增加0.30%。

Conclusion: LOGIC提供了一种高效且鲁棒的框架，用于解决语音大语言模型中的新实体识别问题，相比现有方法具有更好的可扩展性和准确性。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [17] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 提出一种通过零和博弈赌局框架评估LLM谄媚行为的新方法，发现所有模型都有谄媚倾向，但Claude和Mistral在伤害第三方时会表现出"道德悔恨"，且谄媚与近因偏见会相互增强。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM谄媚行为的方法存在各种不可控的偏见、噪音或操纵性语言注入问题，需要一种更直接、中性的评估方式。

Method: 采用LLM-as-a-judge方法，将谄媚评估构建为零和博弈的赌局框架，让谄媚行为在服务一个用户的同时明确对另一方造成成本。

Result: 测试四个领先模型发现：所有模型在常见设置下都有谄媚倾向；Claude和Mistral在伤害第三方时会表现出"道德悔恨"并过度补偿；所有模型都有近因偏见；谄媚与近因偏见相互增强产生"建设性干扰"效应。

Conclusion: 提出了一种更中性的谄媚评估框架，揭示了LLM谄媚行为的复杂性，包括道德补偿机制和与近因偏见的相互作用，这对理解和减轻LLM谄媚行为具有重要意义。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [18] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: 提出基于张量网络的统一框架，将神经与符号AI方法结合，通过张量分解表示逻辑公式和概率分布，实现混合逻辑概率模型


<details>
  <summary>Details</summary>
Motivation: 神经与符号人工智能方法的统一是核心开放挑战，需要建立能够融合两种范式的数学框架

Method: 引入张量网络形式化方法，通过张量分解表示函数和神经分解，将逻辑公式和概率分布表示为结构化张量分解，提出张量网络收缩作为基本推理类

Result: 开发了混合逻辑网络（Hybrid Logic Network）框架和tnreason Python库，实现概率论和命题逻辑的高效推理算法

Conclusion: 张量网络为神经与符号AI的统一提供了数学基础，支持混合逻辑概率模型的训练和推理，具有实际应用价值

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [19] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 大型语言模型在法律高风险工作中通过区分三种AI范式（独立生成、基础RAG、高级RAG）并引入FCR/FFR可靠性指标，证明高级RAG能将幻觉率降至0.2%以下，实现可靠法律AI。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在法律高风险工作中产生的幻觉问题，确保AI在法律专业应用中的可靠性和可信度。

Method: 区分三种AI范式：1)独立生成模型；2)基础检索增强系统；3)端到端优化的高级RAG系统。引入FCR（虚假引用率）和FFR（捏造事实率）指标，对12个LLM在75个法律任务上生成的2,700个司法风格答案进行专家双盲评审。

Result: 独立模型不适合专业使用（FCR超过30%），基础RAG显著减少错误但仍存在明显误植，高级RAG（使用嵌入微调、重排序、自校正等技术）将捏造率降至可忽略水平（低于0.2%）。

Conclusion: 可信赖的法律AI需要以严谨性为中心的检索型架构，强调验证和可追溯性。该评估框架可推广到其他高风险领域。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [20] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE是一个多智能体框架，用于生成领域特定、多模态、多跳的问答数据集，以评估RAG系统在专业文档中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG评估基准主要基于通用领域语料库或纯文本检索，无法捕捉专业技术文档中信息多模态且需要综合分散证据的复杂性，这阻碍了RAG系统在关键企业应用中的发展。

Method: MiRAGE采用多智能体协作框架：递归上下文优化循环聚合分散证据，对抗性验证器确保事实基础，专家角色识别代理模拟专家认知工作流程，共同生成经过验证的领域特定多模态多跳问答数据集。

Result: 在四个不同领域（法规、金融、定量生物学和新闻学）的评估显示，MiRAGE生成的数据集具有显著更高的推理复杂度（平均>2.3跳）和事实忠实度。消融研究表明，如果有图像文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是前沿挑战。

Conclusion: MiRAGE通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，为严格基准测试下一代信息检索系统提供了必要的基础设施。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [21] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK是一个新基准，用于测试LLMs在参数知识与上下文更新知识冲突时的多步推理能力，发现提供更新事实反而会降低推理性能


<details>
  <summary>Details</summary>
Motivation: 当前基准主要关注单知识更新和事实回忆，缺乏评估知识更新如何影响下游推理，特别是当更新知识与模型参数知识冲突时的多步推理传播问题

Method: 引入TRACK基准，涵盖三个推理密集型场景（WIKI、CODE、MATH），引入多个现实冲突来模拟真实世界复杂性，测试LLMs在知识冲突下的推理能力

Result: 提供更新事实进行推理反而比不提供更新事实性能更差，且提供更多更新事实时性能下降加剧；失败原因包括无法忠实整合更新事实，以及即使知识整合后也存在有缺陷的推理

Conclusion: TRACK为测量和指导未来在冲突知识多步推理传播方面的进展提供了一个严谨的新基准

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [22] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 研究发现Transformer模型在情感分析中提高某一类情感准确率的同时，会导致另一类情感极化并破坏中立性，这对依赖情感分析结果的工业应用构成严重问题。


<details>
  <summary>Details</summary>
Motivation: 虽然迁移学习和Transformer模型在提高准确率和解决复杂计算问题方面取得了显著进展，但在应用AI分析特别是情感分析领域，这种准确性提升带来了负面效应。研究发现Transformer模型在提高某一类情感准确率的同时，会导致另一类情感极化并破坏中立性，这对依赖情感分析结果的工业应用构成严重问题。

Method: 通过实验观察和分析，研究Transformer模型在情感分析任务中的表现，特别关注不同情感类别之间的准确率变化和极化现象。

Result: 实验发现Transformer模型在提高某一类情感准确率的同时，会导致另一类情感极化并破坏中立性，这种缺乏中立性的问题在应用NLP空间中尤为突出。

Conclusion: Transformer模型在情感分析中虽然提高了准确性，但导致了情感极化并破坏了中立性，这对依赖情感分析结果的工业应用构成了严重挑战，需要在模型设计和评估中更加关注中立性保持。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [23] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: TransportAgents：一个混合多智能体框架，通过集成特定类别的LLM推理和MLP融合模块，提升交通事故严重程度预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的单智能体架构在处理异构、领域特定的交通事故数据时存在困难，容易产生有偏见或不稳定的预测，而准确的交通事故严重程度预测对应急响应和公共安全规划至关重要。

Method: 提出TransportAgents混合多智能体框架，包含多个专门处理特定交通信息子集（如人口统计、环境背景、事故细节）的智能体，每个智能体产生中间严重程度评估，然后通过多层感知器（MLP）集成模块融合为统一预测。

Result: 在两个互补的美国数据集（CPSRMS和NEISS）上的广泛实验表明，TransportAgents在包括GPT-3.5、GPT-4o和LLaMA-3.3在内的三种代表性骨干模型上，均优于传统机器学习和先进的LLM基线方法，表现出强大的鲁棒性、可扩展性和跨数据集泛化能力。

Conclusion: TransportAgents框架能够产生更平衡、校准更好的严重程度预测，比标准单智能体LLM方法具有更好的可解释性和可靠性，适用于安全关键决策支持应用。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [24] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 该论文批评当前世界模型过于关注视觉逼真度而忽视物理因果理解，提出应将世界模型重构为支持反事实推理和干预规划的可行动模拟器，而非视觉引擎。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型存在"视觉混淆"问题——错误地认为高保真视频生成等同于理解物理和因果动态。作者发现现代模型虽然擅长像素预测，但经常违反不变约束、在干预下失效，在安全关键决策中崩溃，因此需要重新思考世界模型的设计目标。

Method: 通过调查分析现有世界模型的局限性，提出重构框架：将世界模型视为可行动模拟器而非视觉引擎，强调结构化4D接口、约束感知动态和闭环评估。使用医疗决策作为认知压力测试来验证这一观点。

Result: 研究表明视觉逼真度是世界理解的不可靠代理。有效的世界模型必须编码因果结构、尊重领域特定约束，并在长时域保持稳定。医疗决策案例显示，世界模型的价值不在于其模拟的逼真度，而在于支持反事实推理、干预规划和鲁棒长时域预见的能力。

Conclusion: 世界模型应从追求视觉逼真度转向构建可行动模拟器，强调因果结构、约束遵守和长期稳定性。医疗决策等安全关键领域特别需要这种转变，因为试错不可行且错误不可逆转。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [25] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent是一个多代理教育框架，通过整合知识评估、技能差距识别和针对性资源推荐，实现个性化学习。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的单一功能，缺乏将这些组件整合为统一自适应循环的系统。

Method: ALIGNAgent采用多代理架构：技能差距代理处理学生测验表现、成绩册数据和偏好，生成主题级熟练度估计；推荐代理根据诊断的缺陷检索偏好感知的学习材料；实现持续反馈循环，在进入后续主题前进行干预。

Result: 在两个本科计算机科学课程的真实数据集上评估，GPT-4o代理在知识熟练度估计方面达到0.87-0.90的精确度和0.84-0.87的F1分数，与真实考试表现验证一致。

Conclusion: ALIGNAgent通过整合知识评估、差距识别和资源推荐，提供了一个有效的个性化学习框架，显著提升了教育干预的精确性和针对性。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [26] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: AUTOBUS是一个自主业务系统，结合LLM智能体、谓词逻辑编程和业务语义知识图谱，通过神经符号AI架构协调端到端业务计划。


<details>
  <summary>Details</summary>
Motivation: 当前企业系统基于部门孤岛、刚性工作流和硬编码自动化，无法灵活适应业务环境变化；而LLM擅长自然语言理解但缺乏确定性业务逻辑执行能力，需要解决这一差距。

Method: 将业务计划建模为具有明确前后条件、数据需求、评估规则和API操作的任务网络；企业数据组织为知识图谱并转换为逻辑事实和规则；AI智能体综合任务指令、企业语义和工具生成特定任务逻辑程序，由逻辑引擎执行。

Result: 提出AUTOBUS架构，详细描述了AI智能体生成的逻辑程序结构，以及人类和辅助工具在业务计划生命周期中的作用。

Conclusion: AUTOBUS通过神经符号AI整合LLM的灵活性和逻辑编程的确定性，实现可验证的业务流程自动化，同时保持人类监督以确保责任和适应性。

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [27] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM是一个全面的、理论驱动的基准测试，包含8000多个双语实例和46种范式，用于评估LLMs是否具有类似人类的心理理论能力，揭示了模型性能的显著异质性和与人类认知结构的潜在差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs心理理论能力的基准测试大多局限于错误信念任务等狭窄范式，无法全面捕捉人类认知机制的全貌，需要更全面、理论基础的评估工具。

Method: 开发了CogToM基准测试，包含8000多个双语实例，涵盖46种不同范式，由49名人类标注者验证。系统评估了22个代表性模型，包括GPT-5.1和Qwen3-Max等前沿模型。

Result: 评估揭示了模型在心理理论能力上的显著性能异质性，在特定维度存在持续瓶颈。基于人类认知模式的分析表明LLMs与人类认知结构可能存在差异。

Conclusion: CogToM为研究LLMs不断演化的认知边界提供了强有力的工具和视角，有助于更深入地理解大语言模型是否真正具备类似人类的心理理论能力。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [28] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 本文提出统一代理生命周期管理蓝图，解决医疗AI代理扩散带来的重复、责任不清、控制不一致等问题，提供可实施的治理框架。


<details>
  <summary>Details</summary>
Motivation: 医疗组织在临床文档支持和早期预警监控等工作中嵌入AI代理，但随着这些能力在不同部门和供应商间扩散，出现了代理扩散问题：重复代理、责任不清、控制不一致、工具权限超出原始用例。现有AI治理框架强调生命周期风险管理，但对代理舰队的日常运营指导有限。

Method: 提出统一代理生命周期管理蓝图，基于治理标准、代理安全文献和医疗合规要求的快速实践导向综合。将重复出现的差距映射到五个控制平面层：身份和角色注册表、编排和跨域调解、PHI边界上下文和内存、运行时策略执行与紧急停止触发器、生命周期管理与退役（链接到凭证撤销和审计日志）。配套成熟度模型支持分阶段采用。

Result: UALM为医疗CIO、CISO和临床领导者提供了可实施的审计就绪监督模式，既保护本地创新，又能在临床和行政领域实现更安全的扩展。

Conclusion: 统一代理生命周期管理蓝图解决了医疗AI代理扩散带来的治理挑战，提供了系统化的控制框架，帮助医疗组织在创新和安全之间取得平衡，实现可审计的AI代理管理。

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [29] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出基于神经科学信号设计的轻量级幻觉检测框架，结合预测编码和信息瓶颈理论，在少量数据和快速推理下实现高性能检测


<details>
  <summary>Details</summary>
Motivation: 当前LLM幻觉检测方法依赖计算昂贵的外部检索或大型黑盒模型，需要更高效、可解释的解决方案

Method: 结合预测编码（量化内部先验的惊讶度）和信息瓶颈（测量扰动下的信号保留）的混合检测框架，提取实体聚焦吸收、上下文一致性和可证伪性等可解释信号

Result: 在HaluBench上达到0.8669 AUROC，比基线提升4.95%，仅用200个训练样本（比Lynx少75倍），推理速度快1000倍（5ms vs 5s），模型参数小于1M

Conclusion: 领域知识编码的信号架构比扩展LLM法官具有更好的数据效率，可实现轻量级、可解释的幻觉检测模型，适合生产部署

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [30] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 多国AI安全机构联合开展第三次AI智能体评估测试，聚焦共同风险（信息泄露、欺诈）和网络安全，旨在完善评估方法论而非比较模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统和智能体能力的快速发展，真实世界交互中监管减少带来了新风险。AI智能体开始全球部署时，需要确保其能准确安全地处理不同语言和文化。当前智能体测试仍处于早期发展阶段，需要建立科学的评估方法。

Method: 国际网络成员（新加坡、日本、澳大利亚、加拿大、欧盟、法国、肯尼亚、韩国、英国）联合开展第三次测试，分为两个方向：(1) 共同风险（敏感信息泄露和欺诈），由新加坡AISI领导；(2) 网络安全，由英国AISI领导。使用开放和封闭权重模型，基于公开智能体基准任务进行评估。重点在于理解测试方法论问题而非模型能力比较。

Result: 这是该网络继2024年11月和2025年2月前两次联合测试后的第三次练习，标志着参与方在推进智能体评估科学方面迈出了重要一步。主要成果是方法论层面的进展而非具体测试结果。

Conclusion: 多国合作对于完善AI智能体评估方法至关重要，特别是在处理跨语言文化安全风险方面。这种国际合作有助于建立更科学的智能体测试框架，为未来AI系统的安全部署奠定基础。

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [31] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 这篇综述探讨了如何将不确定性从被动诊断指标转变为主动控制信号，以提升大语言模型在推理、自主代理和强化学习中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出卓越能力，但其不可靠性仍然是部署到高风险领域的关键障碍。需要将不确定性从被动诊断工具转变为主动控制机制，以构建更可靠、可信的AI系统。

Method: 通过三个前沿领域展示不确定性作为主动控制信号的应用：1) 高级推理中优化计算和触发自我修正；2) 自主代理中管理元认知决策（工具使用和信息寻求）；3) 强化学习中缓解奖励黑客问题并通过内在奖励实现自我改进。基于贝叶斯方法和符合预测等新兴理论框架。

Result: 提供了不确定性作为主动控制信号的统一视角，展示了该趋势在提升AI系统可靠性方面的变革性潜力。通过综合概述、批判性分析和实用设计模式，论证了这一新趋势的重要性。

Conclusion: 掌握不确定性作为主动控制信号的新趋势对于构建下一代可扩展、可靠且可信的AI系统至关重要。这一转变代表了从被动评估到主动行为引导的功能演进。

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [32] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出Dual-Process Agentic UQ框架，将语言化不确定性转化为双向控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题，平衡执行效率与深度思考。


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中存在"幻觉螺旋"问题，早期认知错误会不可逆地传播。现有方法面临困境：不确定性量化方法只能被动诊断风险而不解决问题，而自我反思机制则存在连续或漫无目的的修正。

Method: 提出统一的Dual-Process Agentic UQ框架，包含两个互补机制：System 1（不确定性感知记忆UAM）隐式传播语言化置信度和语义解释防止盲目决策；System 2（不确定性感知反思UAR）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解决。

Result: 在闭环基准测试和开放式深度研究任务上的大量实验表明，这种无需训练的方法实现了优越的性能和轨迹级校准。

Conclusion: AUQ框架代表了向可靠代理迈出的重要一步，能够动态平衡高效执行和深度思考。

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [33] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 多语言AI安全评估研究显示，前沿AI模型在不同语言环境下的安全行为存在显著差异，需要改进评估方法并建立共享框架。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球部署，确保其在多样化的语言和文化环境中保持安全和可靠至关重要。当前模型安全措施在不同语言环境下的表现需要系统评估。

Method: 由新加坡AISI领导，来自多个国家的国际网络成员合作进行多语言评估。测试了两个开源模型在10种语言（包括高资源和低资源语言）上的表现，使用超过6000个新翻译的提示，涵盖5个危害类别，采用LLM作为评判者和人工标注两种评估方式。

Result: 研究发现安全行为在不同语言间存在差异，包括安全措施鲁棒性在不同语言和危害类型间的变化，以及评估者可靠性（LLM评判vs人工评审）的差异。同时获得了改进多语言安全评估的方法论见解。

Conclusion: 这项工作是建立先进AI系统多语言安全测试共享框架的初步步骤，呼吁与更广泛的研究社区和行业继续合作，以改进评估方法并确保AI在全球范围内的安全性。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [34] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: AgentSM是一个基于语义记忆的Text-to-SQL代理框架，通过结构化程序捕获执行轨迹，提高复杂企业环境下的效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL系统在企业环境中面临挑战：大型复杂模式、多样SQL方言、昂贵多步推理。现有代理方法存在效率低下、输出不稳定、重复数据库交互等问题

Method: 引入Agent Semantic Memory（AgentSM）框架，构建可解释的语义记忆。通过结构化程序捕获先前执行轨迹（或合成精选轨迹），直接指导未来推理，实现推理路径的系统性复用

Result: 在Spider 2.0基准测试中，平均token使用量减少25%，轨迹长度减少35%。在Spider 2.0 Lite基准测试中达到44.8%的最新准确率

Conclusion: AgentSM通过语义记忆框架有效解决了企业级Text-to-SQL的扩展性问题，在效率和准确性方面均优于现有方法，为复杂数据库查询提供了可靠解决方案

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [35] [Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling](https://arxiv.org/abs/2601.15717)
*Luyao Zhu,Fangfang Zhang,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 该研究系统评估了遗传编程在动态柔性作业车间调度中演化调度规则的泛化能力，发现训练实例包含更多作业且决策点分布相似时泛化效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在相同类型的DFJSS实例上训练和测试GP演化规则，仅通过随机种子区分，而忽略了规则在不同结构特征实例间的跨类型泛化能力，这一研究空白需要填补。

Method: 通过多维度实验系统评估GP演化规则的泛化能力：包括问题规模（机器和作业数量）、关键车间参数（如利用率水平）和数据分布，分析这些因素如何影响GP在未见实例类型上的性能。

Result: 当训练实例包含比测试实例更多作业且机器数量固定时，以及当训练和测试实例具有相似规模或车间参数时，泛化效果良好。决策点的数量和分布对性能差异起关键作用，相似分布导致更好泛化，显著差异导致性能明显下降。

Conclusion: 本研究为GP在DFJSS中的泛化能力提供了新见解，强调需要演化更具泛化能力的GP规则以有效处理异构DFJSS实例。

Abstract: Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.

</details>


### [36] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: BIRD-Python基准测试显示，Text-to-Python在数据检索中面临语义模糊性挑战，但通过逻辑补全框架可达到与Text-to-SQL相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析越来越需要Python等通用编程语言来处理文件数据和复杂分析流程，但Text-to-Python在核心数据检索方面的可靠性相对于成熟的SQL生态系统尚未得到充分探索。

Method: 提出BIRD-Python基准进行跨范式评估，系统优化原始数据集以减少标注噪声和对齐执行语义；提出逻辑补全框架(LCF)，通过融入潜在领域知识来解决语义模糊性问题。

Result: 性能差异主要源于缺失的领域上下文而非代码生成的内在限制；当这些差距被填补时，Text-to-Python能够达到与Text-to-SQL相当的性能水平。

Conclusion: Python可以作为分析代理的可行基础，前提是系统能够有效地将模糊的自然语言输入锚定在可执行的逻辑规范中。

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [37] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 首个增强物理学领域形式化定理证明的方法，通过专用数据集PhysLeanData和基于DeepSeek-Prover-V2-7B的RLVR训练，在仅使用约5K样本的情况下实现了多子领域2.4%的性能提升，并展示了跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管可验证语言与LLMs的结合显著影响了数学和计算机科学领域，为定理证明提供了严格基础，但形式化物理推理领域却鲜有关注，而物理领域同样严重依赖类似的问题解决和定理证明框架。本文旨在填补这一空白。

Method: 1. 构建专用数据集PhysLeanData：包含从PhysLean采样的定理和基于猜想的形式化数据生成管道生成的数据。2. 训练管道：利用强大的开源数学定理证明器DeepSeek-Prover-V2-7B，应用可验证奖励的强化学习（RLVR）训练PhysProver模型。

Result: 1. 仅使用约5K训练样本，PhysProver在多个子领域实现了总体2.4%的性能提升。2. 经过形式化物理训练后，在MiniF2F-Test基准测试中获得了1.3%的提升，表明模型具有超越物理领域的非平凡泛化能力，同时增强了形式化数学能力。

Conclusion: 该方法在效果和效率上表现出色，为将形式化证明器扩展到数学领域之外提供了范例。研究团队将向社区发布数据集和模型，以促进进一步研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [38] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 提出表格增量推理新任务，解决动态表格列变化问题，基于信息瓶颈理论设计方法，在八个数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 表格数据是基础数据结构，但传统AI模型训练时列固定，无法处理动态变化的表格列，需要新的无监督方法来高效处理动态表格

Method: 将任务构建为基于信息瓶颈理论的优化问题，设计包含LLM占位符、预训练TabAdapter提供外部知识、增量样本压缩块来压缩增量列属性中任务相关信息的方法

Result: 在八个公共数据集上的实验结果表明，TabII方法能有效利用增量属性，实现了最先进的性能

Conclusion: 提出的表格增量推理任务能增强AI模型在动态变化表格场景中的实用性，基于信息瓶颈理论的方法框架为处理此类问题提供了有效解决方案

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [39] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离线到在线强化学习方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的分布外动作优化，减少Q函数振荡，在真实世界机器人任务中实现低成本部署。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、数据需求少的实用方案。

Method: 提出SigEnt-SAC离线策略actor-critic方法，核心设计是sigmoid有界熵项，防止负熵驱动的分布外动作优化，减少Q函数振荡，仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中显著缓解Q函数振荡，比先前方法更快达到100%成功率；在四个真实世界机器人任务中，仅需少量真实世界交互就能学习成功策略。

Conclusion: SigEnt-SAC为真实世界强化学习部署提供了低成本、实用的途径，仅需最小数据需求即可实现高效学习。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [40] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出了首个代理置信度校准问题，并开发了HTC框架，通过提取代理轨迹中的过程级特征来改善AI代理的置信度校准，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: AI代理正在从被动语言模型发展为执行复杂多步任务的自主系统，但其在失败时的过度自信成为高风险场景部署的主要障碍。现有校准方法针对静态单轮输出设计，无法解决代理系统的独特挑战，如轨迹中的累积错误、外部工具的不确定性以及不透明的失败模式。

Method: 提出了整体轨迹校准（HTC）框架，从代理的整个轨迹中提取丰富的过程级特征，涵盖从宏观动态到微观稳定性。采用简单可解释的模型，并开发了通用代理校准器（GAC）实现跨域泛化。

Result: HTC在8个基准测试、多个LLM和不同代理框架中，在校准和判别方面均超越强基线。GAC在跨域GAIA基准测试中实现了最佳校准（最低ECE）。

Conclusion: 建立了一个新的以过程为中心的置信度校准范式，为诊断和增强AI代理的可靠性提供了框架，实现了可解释性、可迁移性和泛化性三大进展。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [41] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 论文反对将意向性主体作为创造力的必要条件，认为生成式AI的发展使这一条件在描述和功能上都存在问题，主张用一致性要求替代，但保留在特定领域中的适用性。


<details>
  <summary>Details</summary>
Motivation: 许多创造力理论家坚持认为意向性主体是创造力的必要条件，但生成式AI的进步使这一条件变得问题重重，需要重新审视创造力概念。

Method: 1. 提供语料库证据，显示作者和记者越来越愿意将创造力归因于缺乏意向性主体的生成式AI；2. 运用概念工程方法，论证意向性主体条件不再履行其核心社会功能。

Result: 意向性主体条件(IAC)作为创造力的一般条件应该被拒绝，因为它不再符合语言使用实践，且会扭曲对AI生成产出的评估。应代之以一致性要求，即创造力追踪可靠生成新颖且有价值产品的能力。

Conclusion: 虽然意向性主体条件在特定局部领域仍应保留，但作为创造力的一般条件应该被抛弃，代之以更符合当代技术现实的一致性要求。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [42] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: VitalDiagnosis：一个基于LLM的生态系统，将慢性病管理从被动监测转变为主动互动参与，通过整合可穿戴设备数据和LLM推理能力，实现异常检测和常规依从性管理。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球主要死因，医疗资源紧张和人口老龄化加剧了这一挑战。患者难以解读早期恶化迹象并坚持护理计划，需要更主动的疾病管理方法。

Method: 开发VitalDiagnosis系统，整合可穿戴设备的连续数据与LLM的推理能力，通过上下文感知的询问分析触发因素，在患者-临床医生协作工作流中生成临时见解，并提供个性化指导。

Result: 系统能够同时处理急性健康异常和常规依从性问题，促进更主动、协作的护理模式，有望增强患者自我管理能力并减少可避免的临床工作量。

Conclusion: VitalDiagnosis通过LLM驱动的生态系统，将慢性病管理从被动监测转变为主动互动参与，有潜力改善患者自我管理并减轻临床负担。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [43] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出DeepVerifier，一种基于规则的结果奖励验证器，通过推理时验证实现智能体自我进化，无需额外训练即可提升性能


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体（DRAs）主要关注通过后训练增强策略能力，但缺乏有效的自我进化机制。需要一种能够通过验证智能体输出并进行迭代改进的方法

Method: 1. 基于自动构建的DRA失败分类法制定验证规则；2. 开发DeepVerifier验证器，利用验证的不对称性；3. 在推理时作为即插即用模块，生成基于规则的详细反馈；4. 反馈给智能体进行迭代引导，无需额外训练

Result: 1. DeepVerifier在元评估F1分数上比基准方法提升12%-48%；2. 在GAIA和XBench-DeepResearch的挑战性子集上实现8%-11%的准确率提升；3. 发布DeepVerifier-4K数据集，包含4,646个高质量智能体步骤

Conclusion: 提出了一种新的智能体自我进化范式，通过推理时验证实现能力提升。DeepVerifier验证器显著优于现有方法，并支持开源社区发展

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [44] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap是首个系统分析LLM失败原因的方法，通过提取模型的"失败签名"来识别错误根源，而非仅关注失败结果。该方法应用于35个数据集和83个模型，创建了ErrorAtlas错误分类法，揭示了当前研究忽视的错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试只告诉我们模型何时失败，但不解释为何失败。错误答案可能源于格式问题、计算错误或数据集噪声，而非推理能力弱。如果不区分这些原因，基准测试就不完整，无法可靠指导模型改进。

Method: ErrorMap方法通过提取模型的"失败签名"来绘制LLM失败来源，澄清基准测试的测量内容，并扩大错误识别范围以减少盲点。该方法适用于任何模型和数据集，使用相同逻辑进行分析。

Result: 将ErrorMap应用于35个数据集和83个模型，生成了ErrorAtlas错误分类法，揭示了重复出现的失败模式。特别发现了当前LLM研究中被忽视的错误类型，如输出中遗漏必要细节和问题误解。

Conclusion: 通过将焦点从模型成功之处转向失败原因，ErrorMap和ErrorAtlas实现了高级评估，能够暴露隐藏弱点并指导进展。与通常通过任务级指标衡量的成功不同，该方法引入了可在模型和任务间全局应用的更深层评估，提供了对模型行为和局限性的更丰富洞察。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [45] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA是一个通过自我进化循环生成数据和优化策略的计算机使用代理模型，在OSWorld基准测试中达到56.7%的成功率，创造了开源模型的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理主要依赖静态数据的被动模仿，难以捕捉长时程计算机任务中的复杂因果动态，且受到静态数据扩展的限制。

Method: 1) 开发可验证的合成引擎，自主生成多样化任务和可执行验证器；2) 设计可扩展基础设施，协调数万个异步沙盒环境；3) 提出迭代进化学习策略，通过识别能力边界动态调节策略更新，将失败轨迹转化为监督信号。

Result: 在OSWorld基准测试中达到56.7%的成功率，超越之前最佳开源模型OpenCUA-72B（45.0%）和领先闭源模型UI-TARS-2（53.1%）。该进化范式在不同规模的基础模型上都表现出稳定的性能提升。

Conclusion: 基于经验学习的进化范式为提升原生代理能力提供了稳健且可扩展的路径，能够有效克服静态数据扩展的瓶颈，在计算机使用代理领域建立了新的开源SOTA。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [46] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON框架通过因果与拓扑先验解决文本行人搜索中的虚假关联问题，实现几何不变性和环境独立性，显著提升对遮挡、背景干扰和定位噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的文本行人搜索方法在复杂开放世界场景中迁移效果不佳，主要问题是"被动观察"导致多方面的虚假关联和空间语义错位，缺乏对分布偏移的鲁棒性。

Method: 提出ICON框架：1) 规则引导的空间干预惩罚边界框噪声敏感性；2) 反事实上下文解耦通过语义驱动的背景移植实现环境独立性；3) 显著性驱动的语义正则化解决局部显著性偏差；4) 神经符号拓扑对齐确保特征匹配区域与人类结构逻辑一致。

Result: ICON不仅在标准基准测试中保持领先性能，而且在遮挡、背景干扰和定位噪声方面表现出卓越的鲁棒性，有效推动了从拟合统计共现到学习因果不变性的范式转变。

Conclusion: ICON通过整合因果和拓扑先验，从根本上解决了文本行人搜索中的虚假关联问题，实现了从被动观察到主动干预的转变，为开放世界场景下的鲁棒视觉-语言理解提供了有效解决方案。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [47] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope是一个行星尺度视觉-语言框架，通过自然语言驱动、无需标签的方式实现火星地貌映射，将行星图像与文本对齐到共享语义空间，支持任意用户查询并在5秒内完成全球检索。


<details>
  <summary>Details</summary>
Motivation: 行星表面通常使用自然语言中的高级语义概念进行分析，但大量轨道图像档案仍以像素级别组织，这种不匹配限制了行星表面的可扩展、开放式探索。

Method: 开发MarScope框架，将行星图像和文本对齐到共享语义空间，基于超过20万个精心策划的图像-文本对进行训练，实现自然语言驱动的无标签映射。

Result: 该框架能在5秒内完成整个火星的任意用户查询，F1分数高达0.978，超越了形态分类，支持过程导向分析和基于相似性的地貌映射。

Conclusion: MarScope建立了一个新范式，使自然语言成为大规模地理空间数据集科学发现的直接接口，改变了全球地貌映射的方式。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [48] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 决策变换器(DT)存在RTG序列冗余问题，提出的解耦DT(DDT)仅使用最新RTG指导动作预测，简化架构并提升性能。


<details>
  <summary>Details</summary>
Motivation: 发现决策变换器(DT)设计中存在关键冗余：将整个RTG序列输入Transformer在理论上是不必要的，因为只有最新的RTG会影响动作预测。这种冗余可能损害DT的性能。

Method: 提出解耦DT(DDT)：仅通过Transformer处理观测和动作序列，使用最新的RTG来指导动作预测。这种简化方法不仅改善了性能，还降低了计算成本。

Result: 实验表明，DDT显著优于原始DT，并在多个离线RL任务中与最先进的DT变体建立了有竞争力的性能。

Conclusion: 通过消除RTG序列冗余，DDT提供了一种更高效、性能更好的决策变换器架构，为离线强化学习的序列建模方法提供了改进方向。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [49] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR是一个用于直播风险评估的跨会话证据感知检索增强检测器，通过LLM指导的轻量级模型实现实时风险检测和可解释性


<details>
  <summary>Details</summary>
Motivation: 直播平台的兴起带来了实时互动，但也暴露了复杂的风险（如诈骗、协同恶意行为）。这些风险检测困难，因为有害行为往往逐渐累积并在看似无关的流中重复出现。

Method: 提出CS-VAR框架：1）轻量级领域特定模型进行快速会话级风险推断；2）训练时由LLM指导，LLM基于检索的跨会话行为证据进行推理；3）将LLM的局部到全局洞察转移给小模型，使其能识别跨流重复模式并保持实时部署效率。

Result: 在大规模工业数据集上的离线实验和在线验证表明CS-VAR达到了最先进的性能，并提供可解释的局部信号，有效赋能现实世界的直播内容审核。

Conclusion: CS-VAR通过LLM指导的轻量级模型设计，成功解决了直播平台中跨会话风险检测的挑战，实现了高效、可解释的实时风险评估。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [50] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 该研究将化学合成规划中的反应路径检索转化为Text2Cypher（自然语言到图查询）生成问题，通过比较不同提示方法，发现使用对齐示例的单次提示效果最佳，而检查表式自我修正循环主要改善零次提示的可执行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在化学合成规划中能提供帮助，但标准提示方法常产生幻觉或过时的建议。需要研究LLMs如何与反应知识图谱交互，以提高合成路径检索的准确性和可靠性。

Method: 将反应路径检索定义为Text2Cypher生成问题，比较零次提示与使用静态、随机和基于嵌入的示例选择的单次提示变体，并评估检查表驱动的验证器/修正器循环。评估框架包括查询有效性和检索准确性。

Result: 使用对齐示例的单次提示始终表现最佳。检查表式自我修正循环主要在零次提示设置中改善可执行性，一旦有好的示例存在，提供的额外检索增益有限。

Conclusion: 该研究为基于知识图谱的LLMs合成规划提供了一个可复现的Text2Cypher评估框架，表明精心选择的示例比复杂的修正循环更有效，为化学合成规划中的知识图谱增强LLMs研究奠定了基础。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [51] [AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress](https://arxiv.org/abs/2601.16045)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.AI

TL;DR: AgriPINN：结合生物物理过程方程与深度学习的混合模型，用于预测水分胁迫下的作物地上生物量，在精度和计算效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型缺乏可解释性且在分布偏移时性能下降，而基于过程的作物模型需要大量校准且难以在大空间尺度部署。需要一种兼具可扩展性和生物物理严谨性的方法。

Method: 提出AgriPINN，将作物生长的生物物理微分方程作为可微分约束集成到深度学习主干中，无需直接监督即可恢复LAI、PAR、RUE等潜在生理变量。

Result: 在德国397个地区60年历史数据预训练，并在三年受控水分处理田间实验微调。相比ConvLSTM-ViT等深度学习方法及LINTUL5过程模型，RMSE降低高达43%，计算效率更高。

Conclusion: AgriPINN结合了深度学习的可扩展性和过程模型的生物物理严谨性，为时空生物量预测提供了稳健可解释的框架，对灌溉规划、产量预测和气候适应具有实用价值。

Abstract: Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.

</details>


### [52] [Designing faster mixed integer linear programming algorithm via learning the optimal path](https://arxiv.org/abs/2601.16056)
*Ruizhi Liu,Liming Xu,Xulin Huang,Jingyan Sui,Shizhe Ding,Boyang Xia,Chungong Yu,Dongbo Bu*

Main category: cs.AI

TL;DR: DeepBound：基于深度学习的节点选择算法，通过多级特征融合和成对训练范式，自动学习分支定界树中节点优先级，显著提升混合整数线性规划求解效率


<details>
  <summary>Details</summary>
Motivation: 传统混合整数线性规划求解依赖手工设计的启发式策略，这些策略在不同问题实例上性能不稳定且不可预测。需要自动化学习人类直觉，提高求解效率。

Method: 提出DeepBound深度学习节点选择算法：1）使用多级特征融合网络捕捉节点表示；2）采用成对训练范式解决分支定界树中节点不平衡问题；3）学习优先选择包含最优解的节点。

Result: 在三个NP难MILP基准测试中，DeepBound相比传统启发式规则和现有学习方法获得更优求解效率，显著减少计算时间找到最优可行解，并在大型复杂实例上展示强大泛化能力。

Conclusion: DeepBound能够自动发现更灵活鲁棒的特征选择，可能有效改进甚至替代人工设计的启发式规则，为MILP求解提供高效自动化方案。

Abstract: Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.

</details>


### [53] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 该研究探讨了在LLM智能体中引入外部情感动态子系统（基于VAD模型）如何改善多轮对话中的时间一致性和可控恢复，通过一阶和二阶动态规则实现情感状态的持续性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长时间交互中经常出现语气和人设的突然转变，缺乏明确的时序结构来管理智能体层面的状态。现有研究主要关注轮次层面的情感或静态情感分类，而显式情感动态在塑造长期智能体行为中的作用尚未充分探索。

Method: 引入一个智能体层面的情感子系统，维护一个独立于语言模型的连续VAD（效价-唤醒-支配）状态，该状态受一阶和二阶更新规则控制。使用固定的无记忆估计器提取瞬时情感信号，并通过指数平滑或基于动量的动态进行时间积分。生成过程中注入情感状态而不修改模型参数。

Result: 在25轮固定对话协议中比较无状态、一阶和二阶情感动态：无状态智能体无法展现连贯轨迹或恢复；状态持续性支持延迟响应和可靠恢复；二阶动态引入情感惯性和滞后效应，随动量增加而增强，揭示了稳定性与响应性之间的权衡。

Conclusion: 在LLM智能体中施加外部情感动态结构可以增强多轮对话的时间一致性和可控恢复，为智能体行为设计提供了新的调控维度。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [54] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉语言模型与外部知识检索的方法，以改进对气候虚假信息的检测能力，特别是针对社交媒体上传播的误导性图像和视频。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在数字世界中日益严重，特别是社交媒体上广泛传播的误导性图像和视频。这些虚假信息通常具有说服力且难以检测，可能延缓应对气候变化的行动。现有的视觉语言模型仅依赖训练时的知识，无法处理近期事件或更新的信息。

Method: 通过将视觉语言模型与外部知识检索相结合，系统能够获取最新信息，包括反向图像搜索结果、在线事实核查和可信专家内容，从而更准确地评估图像及其声明的真实性。

Result: 该方法提高了模型处理现实世界气候虚假信息的能力，能够更有效地判断图像及其声明的准确性、误导性、虚假性或不可验证性。

Conclusion: 结合外部知识检索的视觉语言模型方法能够克服传统模型的知识局限性，增强对气候虚假信息的检测能力，有助于保护公众对科学的理解，应对快速变化的信息环境。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [55] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 该研究提出了一种系统化评估教育应用中LLM提示模板的方法，通过锦标赛式评估框架比较了6种不同教学策略的提示模板，发现结合角色和上下文管理模式的战略阅读提示表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育应用中的普及，需要基于证据的方法来设计和评估能够产生个性化、教学对齐输出的提示。当前缺乏系统化的提示评估方法，多依赖临时性的提示工程。

Method: 设计了6种结合不同教学策略的提示模板，采用锦标赛式评估框架，使用Glicko2评分系统，由8名评委从格式、对话支持和学习者适宜性三个维度评估问题对。数据来自3个不同教育部署中的120个真实用户交互。

Result: 结果显示，结合角色和上下文管理模式的战略阅读提示模板表现最佳，在成对比较中胜率从81%到100%不等。该提示旨在支持元认知学习策略，如自主学习。

Conclusion: 该方法展示了教育技术研究人员如何系统评估和改进提示设计，从临时性的提示工程转向基于证据的教育应用提示开发，为个性化教学对齐输出提供了可推广的评估框架。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [56] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: Cosmos Policy：一种简单方法，通过单阶段后训练将预训练视频模型（Cosmos-Predict2）适配为机器人策略，无需架构修改，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型已能捕捉复杂物理交互和场景演化，但将其用于机器人策略学习通常需要多阶段后训练和新架构组件，过程复杂。本研究旨在简化这一过程。

Method: 通过单阶段后训练将预训练视频模型适配为机器人策略，无需架构修改。在视频模型的潜在扩散过程中直接生成编码为潜在帧的机器人动作，同时生成未来状态图像和值函数，支持测试时规划。

Result: 在LIBERO和RoboCasa仿真基准测试中分别达到98.5%和67.1%的平均成功率，在真实世界双手操作任务中获得最高平均分数，优于从头训练的扩散策略、基于视频模型的策略和微调的VLA模型。

Conclusion: Cosmos Policy通过简单适配预训练视频模型，有效利用其时空先验，实现了高性能的机器人策略学习，并能通过经验学习改进世界模型和值函数，进一步提升成功率。

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


### [57] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在推理时使用简单的固定提示调度（15个常用策略骨架）可以显著提升神经定理证明器的性能，在miniF2F基准上实现43%的相对改进


<details>
  <summary>Details</summary>
Motivation: 研究经过强化学习训练的高度优化的神经定理证明器是否仍能从推理时的简单结构指导中受益，探索轻量级干预的有效性

Method: 使用固定的提示调度方法，在推理时提供15个常用策略骨架的结构指导，与标准采样方法进行对比

Result: 在miniF2F基准测试中，使用结构指导的方法达到21.7%的pass@16，相比标准采样的15.2%提升了43%的相对改进，使用相同样本数(k=16)和最大生成长度(1024个token)

Conclusion: 即使经过强化学习训练的能力强大的证明器也未能充分利用策略语言中可用的结构先验，简单的推理时指导仍然是一种廉价且互补的性能提升方法

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [58] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 提出使用通用游戏系统实现无棋盘游戏的动态棋盘扩展机制，解决传统静态棋盘资源浪费问题


<details>
  <summary>Details</summary>
Motivation: 传统无棋盘游戏实现通常依赖预先定义的超大静态棋盘，即使大部分棋盘区域在游戏中从未使用，这种设计导致不必要的复杂性

Method: 采用通用游戏系统支持动态棋盘扩展机制，在游戏过程中自动扩展游戏棋盘

Result: 论文提出了动态棋盘扩展机制，但摘要中未提供具体实验结果

Conclusion: 动态棋盘扩展机制能够有效解决传统静态棋盘设计中的资源浪费和复杂性问题，提高游戏系统的效率和灵活性

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [59] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP码是一种新型类极化码，通过选择性剪裁极化核来修改比特信道容量，确保解码早期有更多非冻结比特可用，从而提升盲解码性能。


<details>
  <summary>Details</summary>
Motivation: 在下行控制信道的盲解码场景中，用户设备需要处理多个候选码字，其中许多不包含有效控制信息。传统极化码在解码早期可用的非冻结比特有限，限制了早期终止效果，而硬件限制又阻碍了简单扩展。

Method: PPP码基于传统极化码构建，通过选择性剪裁极化核来修改合成比特信道容量，确保解码早期有保证数量的非冻结比特可用。还提出了几种针对PPP码的冻结比特图设计策略。

Result: PPP码相比传统极化码带来显著性能提升，特别是在较大块长度时。与聚合或分段等现有方法相比，PPP码实现更高效率且无需额外硬件支持。

Conclusion: PPP码通过修改极化核结构优化早期解码能力，为控制信道盲解码提供了更有效的解决方案，在性能和硬件效率方面都优于现有方法。

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [60] [Rank-metric codes over arbitrary fields: Bounds and constructions](https://arxiv.org/abs/2601.15464)
*Alessandro Neri,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 该论文综述了秩度量码的发展历程、数学基础、界与构造，特别关注了有限域之外的推广，包括代数闭域和实数域等更一般设置下的研究。


<details>
  <summary>Details</summary>
Motivation: 秩度量码在编码理论中具有重要意义，最初由Delsarte于1978年提出，后被Gabidulin重新发现。它们在网络编码中有应用，并与多个数学领域有联系。论文旨在系统梳理秩度量码的发展，特别是从有限域推广到更一般代数结构的研究进展。

Method: 采用综述研究方法，系统考察秩度量码的数学基础，包括：分析Singleton-like界及其在不同上下文中的紧性；研究具有循环Galois扩张的域上的最大秩距离(MRD)码构造；探讨线性秩度量码与系统和规避子空间的关系；回顾代数闭域和实数域上的结果。

Result: 论文展示了Singleton-like界在有限域情况下的紧性，但在其他上下文中可能不紧；建立了线性秩度量码与系统和规避子空间的联系；总结了代数闭域和实数域上的相关结果，这些结果先前出现在拓扑和测度论背景下。

Conclusion: 论文提出了未来研究方向，包括关于MRD码存在的猜想，以及在不同域扩张上探索秩度量码的构造。强调了秩度量码研究从经典有限域设置向更一般代数结构扩展的重要性。

Abstract: Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions.

</details>


### [61] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: 本文提出了一种通过稳定子码作为信道变换来改进量子哈希界的方法，通过计算诱导的逻辑Pauli错误分布和伴随信息，获得了比基线哈希界更高的可达速率。


<details>
  <summary>Details</summary>
Motivation: 量子哈希界对于无记忆Pauli信道可达速率的上限不是紧致的，特别是对于某些非对称Pauli信道。现有方法通过小内码解码后重新应用哈希论证可以改进可达速率，但需要更系统化的方法。

Method: 将任意稳定子码作为信道变换，构造完整的辛表，计算物理Pauli信道下逻辑Pauli错误和伴随的联合分布，利用解码器侧信息的哈希界获得可达速率。

Result: 通过对小变换进行结构化搜索，报告了针对先前研究中具有偏斜和独立错误的Pauli信道族改进基线哈希界的实例。

Conclusion: 稳定子码作为信道变换的方法可以系统化地改进量子哈希界，为特定类型的Pauli信道提供了更高的可达速率。

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [62] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: 提出(G,f)-散度，通过非递减函数G作用于f-散度，建立对应的(G,f)-信息度量，研究其在乘积分布和乘积信道上的次可加性，并应用于信道编码、假设检验和球面打包指数等场景。


<details>
  <summary>Details</summary>
Motivation: 现有f-散度在信息论中广泛应用，但缺乏更灵活的散度度量框架。本文旨在构建一个两参数散度族，通过引入非递减函数G来扩展f-散度，从而获得更丰富的散度度量工具，并研究其在信息论中的基本性质和应用。

Method: 定义(G,f)-散度为D_{G,f} = G(D_f)，基于Csiszár的互f-信息定义对应的(G,f)-信息I_{G,f}(X;Y)。重点研究在乘积分布和乘积信道上的次可加性，建立简化原则：对广泛的G类，只需在二元字母表上验证散度次可加性。特别针对G(x)∈{x,log(1+x),-log(1-x)}，推导f的易处理充分条件来保证次可加性。

Result: 建立了(G,f)-散度的理论框架，证明了在广泛条件下的次可加性性质。该框架覆盖了许多标准f-散度，为信息论中的多个应用提供了新的分析工具，包括信道编码的有限块长逆定理、二元假设检验的界限，以及将Shannon-Gallager-Berlekamp球面打包指数框架扩展到次可加的(G,f)-散度。

Conclusion: 提出的(G,f)-散度框架为信息论提供了灵活且强大的分析工具，其次可加性性质使得该框架在多个信息论问题中具有重要应用价值，特别是在信道编码、假设检验和误差指数分析等领域。

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [63] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: 该论文提出环境语义概念来统一ISAC信道建模，建立生成式AI赋能的语义孪生信道模型，平衡精度与复杂度


<details>
  <summary>Details</summary>
Motivation: 当前ISAC信道建模存在缺口：统计模型忽略传感所需的关键多径特征，确定性模型计算效率低难以扩展。需要统一抽象来耦合环境对传感的意义与信道对通信的行为

Method: 提出环境语义概念，建立语义与可观测信道结构的连接，倡导语义导向的信道建模原则，引入生成式AI赋能的语义孪生信道模型

Result: STCM能生成代表语义条件的物理合理信道实现，案例研究表明在挑战性多视图设置下保持语义一致性

Conclusion: 环境语义为ISAC信道建模提供统一框架，STCM为实现可控仿真、数据集生成和可重复ISAC基准测试提供实用路径

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [64] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 本文基于子模性框架，建立了凸函数形式的Madiman-Tetali不等式，提出了改进的Loomis-Whitney型投影不等式，并利用Shearer引理扩展了极值图论结果。


<details>
  <summary>Details</summary>
Motivation: 熵不等式与子模性之间存在紧密联系，已有Madiman-Tetali和Sason等学者建立了统一框架。本文旨在进一步扩展这些框架，建立更一般的凸函数不等式，并应用于几何投影和极值图论问题。

Method: 1) 建立子模函数的凸函数形式的强/弱Madiman-Tetali不等式；2) 利用特殊情况的强Madiman-Tetali不等式，推导有限点集在R^d中的改进Loomis-Whitney型投影不等式；3) 使用Shearer引理研究极值图论问题，扩展先前结果。

Result: 1) 获得了子模函数的凸函数推广的Madiman-Tetali不等式；2) 提出了包含切片结构信息的改进Loomis-Whitney投影不等式；3) 恢复了Sason和Boucheron等人的极值图论结果，并进行了扩展。

Conclusion: 本文成功扩展了基于子模性的信息不等式框架，建立了凸函数形式的统一不等式，并在几何投影和极值图论中获得了新的应用结果，展示了该框架的广泛适用性。

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [65] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: RC-Flow是一种基于流匹配先验的递归信道估计算法，通过闭环精炼框架在噪声主导场景下实现高效鲁棒的信道重建。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中信道估计的准确性直接影响频谱效率和链路可靠性，传统生成模型在噪声主导场景下性能受限，需要更鲁棒的解决方案。

Method: 提出RC-Flow算法，结合流匹配先验和数据保真度投影，采用序列重启机制和锚定轨迹校正建立闭环精炼框架，并引入自适应双调度策略平衡收敛速度与重建精度。

Result: RC-Flow在低信噪比场景下比基于分数的基线方法性能提升2.7dB，推理延迟降低两个数量级，在多种噪声水平下均达到最先进性能。

Conclusion: RC-Flow通过融合生成先验与数据一致性约束，为大规模MIMO系统中的鲁棒信道估计提供了高效解决方案，特别适用于噪声主导场景。

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [66] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: SST通过构造双射映射将原始序列集映射到更大序列空间的结构化区域，实现信息内容减少。本文解决了非均匀序列应用中序列排序的指数复杂度问题，提出近似排序方法保持SST结构要求，验证了SST对非均匀序列的整形优势。


<details>
  <summary>Details</summary>
Motivation: 集合整形理论(SST)超越了经典的固定空间模型，但应用于非均匀序列时面临主要实验困难：需要根据信息内容对原始集和变换集的序列进行排序，而精确排序具有指数复杂度，直接实现不切实际。

Method: 提出近似但信息丰富的排序方法，在保持SST结构要求的同时实现理论预测的整形增益。该方法扩展了先前均匀分布序列的实验结果，并公开了实现软件以确保可复现性。

Result: 成功克服了非均匀序列应用SST的障碍，证明了SST的整形优势在非均匀序列中仍然存在。近似排序方法在保持理论要求的同时实现了预测的整形增益。

Conclusion: SST的整形优势不仅适用于均匀分布序列，也适用于非均匀序列。提出的近似排序方法解决了指数复杂度问题，使SST在实际应用中变得可行。公开软件确保了结果的可验证性和可复现性。

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [67] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: 提出一种新的二进制对称信道盲识别方法，基于子空间码框架，结合汉明距离和子空间距离解码，称为最小去噪子空间差异解码器


<details>
  <summary>Details</summary>
Motivation: 现有信道码盲识别方法大多要求码族具有特殊结构，计算成本高，且缺乏严格的理论性能保证

Method: 基于算子信道的子空间码框架，结合汉明距离和子空间距离解码原理，提出最小去噪子空间差异解码器

Result: 为有界权重错误提供了理论保证，给出了二进制对称信道上的错误概率界限，仿真显示在大多数信道条件下优于现有通用技术

Conclusion: 该方法在随机线性码的盲识别中表现优异，即使在有限接收向量情况下也能在大多数信道条件下超越现有方法

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [68] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文通过组合数学中的覆盖阵列方法，为大规模随机接入下行链路设计确定性变长码，将开销降低至不超过1+log₂e比特。


<details>
  <summary>Details</summary>
Motivation: 在大规模随机接入下行链路中，传统方法需要显式编码活跃用户身份，导致开销随总用户数对数增长。虽然已有随机编码方法可以降低开销，但本文旨在寻找确定性的编码构造方法。

Method: 将大规模随机接入下行链路的编码设计问题转化为组合数学中的覆盖阵列问题，利用覆盖阵列理论构造确定性的变长码。

Result: 存在确定性构造的变长码，其开销不超过1+log₂e比特（约2.44比特），这一结果不依赖于总用户数量。

Conclusion: 通过组合数学的覆盖阵列方法，可以为大规模随机接入下行链路设计确定性编码方案，显著降低开销，且该开销与总用户数无关。

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [69] [Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing](https://arxiv.org/abs/2601.16030)
*Jiancheng An,Chau Yuen,Marco Di Renzo,Mehdi Bennis,Merouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: 该论文综述了堆叠智能超表面(SIM)技术，这是一种结合神经网络、电磁计算和超表面的物理神经网络，用于电磁波直接处理，具有高速、大规模并行和低功耗优势。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的特征提取能力、电磁计算的波传播特性以及超表面的电磁波调控能力，开发物理神经网络以实现电磁域的直接信号处理，解决传统数字信号处理的瓶颈问题。

Method: 采用堆叠智能超表面(SIM)技术，通过多层亚波长超原子构成的超表面堆叠，形成可编程的物理神经网络架构，并开发专门的优化/训练策略来配置SIM实现所需功能。

Result: SIM技术已在通信、传感和计算领域展示了多样化应用，实验证据表明单个设备能支持多种功能，具有高速、大规模并行和低功耗的独特优势。

Conclusion: SIM技术为电磁域信号处理开辟了新途径，但要部署到下一代无线网络中仍需解决关键技术挑战，需要进一步研究以释放其全部潜力。

Abstract: Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.

</details>


### [70] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: 本文提出了一种基于可重构智能表面(RIS)的协作式集成感知与通信(ISAC)网络，用于低空监视。采用主动RIS增强信号强度，将低空监视建模为压缩感知成像问题，通过子空间追踪算法求解，并推导了CRLB界。


<details>
  <summary>Details</summary>
Motivation: 低空经济对多个行业发展至关重要，需要先进的低空监视技术。传统方法存在部署成本高、信号强度低的局限性，需要新的解决方案。

Method: 提出RIS辅助的协作ISAC网络，利用RIS将ISAC信号反射到低空进行感知。采用主动RIS(ARIS)放大信号强度。将低空监视建模为基于压缩感知理论的成像问题，使用子空间追踪算法求解。

Result: 推导了RIS辅助低空成像系统的Cramer-Rao下界(CRLB)，分析了各种系统参数对感知性能的影响。数值结果表明，在相同功率约束下，ARIS优于被动RIS，能在高达300米的高度实现有效成像和目标检测。

Conclusion: 提出的RIS辅助ISAC网络为低空监视提供了有效的解决方案，ARIS在功率受限条件下性能更优，压缩感知成像方法避免了传统方法的误差传播和数据关联问题，为ISAC系统配置提供了指导。

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [71] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: 论文研究了用于集成感知与通信（ISAC）的三混合波束成形架构，通过多目标优化平衡通信SNR和感知功率，提出低复杂度迭代算法，在空间增益和能效方面优于传统混合波束成形，但波束对准能力有所降低。


<details>
  <summary>Details</summary>
Motivation: 在超大规模天线阵列中使用低成本可编程超表面天线实现能效通信系统，需要研究三混合波束成形架构来同时提升集成感知与通信（ISAC）中的通信和感知性能。

Method: 提出多目标优化问题，平衡通信信噪比（SNR）和目标方向的感知功率，受限于总功耗和三混合波束成形架构的物理限制。开发高效迭代算法，每次迭代以闭式更新变量，实现低复杂度和快速执行设计。

Result: 数值结果表明，三混合架构提高了空间增益和能量效率，但与传统混合波束成形架构相比，波束对准能力有所降低。

Conclusion: 三混合波束成形架构为超大规模天线阵列中的集成感知与通信系统提供了有效的解决方案，在空间增益和能效方面具有优势，但需要权衡波束对准能力。

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [72] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: 本文提出了张量Reed-Muller码的构造，能在低于信道容量的任何恒定速率下实现准线性时间解码，提供了两种不同参数和性能的构造方案。


<details>
  <summary>Details</summary>
Motivation: 传统Reed-Muller码虽然结构良好，但解码复杂度较高。本文旨在构造具有准线性时间解码能力的纠错码，同时保持接近信道容量的速率。

Method: 定义张量Reed-Muller码为多个Reed-Muller码的张量积，利用张量结构设计高效解码算法。核心贡献是提出了一个多项式时间算法，能够解码任意张量码，即使其分量码本身没有多项式时间解码算法。

Result: 提出了两种构造：1) t=3时，错误概率为n^{-ω(log n)}，解码时间O(n log log n)；2) t≥4时，错误概率为2^{-n^{1/2-1/2(t-2)-o(1)}}，解码时间O(n log n)。两种构造都能在低于信道容量的任何恒定速率下工作。

Conclusion: 张量Reed-Muller码结合了Reed-Muller码的良好代数结构与张量积的高效解码特性，实现了接近信道容量的速率和准线性时间解码，为纠错码设计提供了新的有效工具。

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [73] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 提出基于张量理论的N服务器分布式计算方案，用于多变量多项式函数评估，通过张量分解和分块技术降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 在N服务器分布式计算环境中，K个用户请求对L个实基子函数进行多变量多项式评估。现有方法在计算和通信成本方面效率不高，需要更优的任务分配和数据通信技术

Method: 采用张量理论方法，将请求函数表示为张量$\bar{\mathcal{F}}$，通过稀疏分解为张量$\bar{\mathcal{E}}$和矩阵$\mathbf{D}$来定义任务分配、连接和通信模式。使用基于固定支撑SVD的张量分解方法和多维子张量分块技术

Result: 提出的方案能够显著降低计算和通信成本，性能明显优于现有技术

Conclusion: 基于张量分解的方法为分布式计算中的多变量多项式函数评估提供了高效的任务分配和通信方案，在计算和通信效率方面取得了实质性改进

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>
