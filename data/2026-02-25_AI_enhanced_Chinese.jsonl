{"id": "2602.20493", "categories": ["cs.NI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20493", "abs": "https://arxiv.org/abs/2602.20493", "authors": ["Xiaohang Nie", "Zihan Guo", "Youliang Chen", "Yuanjian Zhou", "Weinan Zhang"], "title": "AWCP: A Workspace Delegation Protocol for Deep-Engagement Collaboration across Remote Agents", "comment": "16 pages, 7 figure, tech report of Agent Workspace Collaboration Protocol", "summary": "The rapid evolution of Large Language Model (LLM)-based autonomous agents is reshaping the digital landscape toward an emerging Agentic Web, where increasingly specialized agents must collaborate to accomplish complex tasks. However, existing collaboration paradigms are constrained to message passing, leaving execution environments as isolated silos. This creates a context gap: agents cannot directly manipulate files or invoke tools in a peer's environment, and must instead resort to costly, error-prone environment reconstruction. We introduce the Agent Workspace Collaboration Protocol (AWCP), which bridges this gap through temporary workspace delegation inspired by the Unix philosophy that everything is a file. AWCP decouples a lightweight control plane from pluggable transport mechanisms, allowing a Delegator to project its workspace to a remote Executor, who then operates on the shared files directly with unmodified local toolchains. We provide a fully open-source reference implementation with MCP tool integration and validate the protocol through live demonstrations of asymmetric collaboration, where agents with complementary capabilities cooperate through delegated workspaces. By establishing the missing workspace layer in the agentic protocol stack, AWCP paves the way for a universally interoperable agent ecosystem in which collaboration transcends message boundaries. The protocol and reference implementation are publicly available at https://github.com/SII-Holos/awcp.", "AI": {"tldr": "AWCP\u534f\u8bae\u901a\u8fc7\u4e34\u65f6\u5de5\u4f5c\u7a7a\u95f4\u59d4\u6258\u89e3\u51b3LLM\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u73af\u5883\u9694\u79bb\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u73af\u5883\u76f4\u63a5\u6587\u4ef6\u64cd\u4f5c\u548c\u5de5\u5177\u8c03\u7528", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u534f\u4f5c\u4ec5\u9650\u4e8e\u6d88\u606f\u4f20\u9012\uff0c\u6267\u884c\u73af\u5883\u76f8\u4e92\u9694\u79bb\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u65e0\u6cd5\u76f4\u63a5\u64cd\u4f5c\u540c\u4f34\u73af\u5883\u4e2d\u7684\u6587\u4ef6\u6216\u8c03\u7528\u5de5\u5177\uff0c\u5fc5\u987b\u8fdb\u884c\u6602\u8d35\u4e14\u6613\u9519\u7684\u73af\u5883\u91cd\u5efa", "method": "\u63d0\u51faAgent Workspace Collaboration Protocol (AWCP)\uff0c\u91c7\u7528\u4e34\u65f6\u5de5\u4f5c\u7a7a\u95f4\u59d4\u6258\u673a\u5236\uff0c\u5c06\u63a7\u5236\u5e73\u9762\u4e0e\u53ef\u63d2\u62d4\u4f20\u8f93\u673a\u5236\u89e3\u8026\uff0c\u5141\u8bb8\u59d4\u6258\u65b9\u5c06\u5de5\u4f5c\u7a7a\u95f4\u6295\u5f71\u5230\u8fdc\u7a0b\u6267\u884c\u65b9\uff0c\u540e\u8005\u53ef\u76f4\u63a5\u64cd\u4f5c\u5171\u4eab\u6587\u4ef6", "result": "\u63d0\u4f9b\u4e86\u5b8c\u5168\u5f00\u6e90\u53c2\u8003\u5b9e\u73b0\uff0c\u96c6\u6210\u4e86MCP\u5de5\u5177\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u534f\u4f5c\u6f14\u793a\u9a8c\u8bc1\u4e86\u534f\u8bae\u6709\u6548\u6027\uff0c\u667a\u80fd\u4f53\u53ef\u901a\u8fc7\u59d4\u6258\u5de5\u4f5c\u7a7a\u95f4\u5b9e\u73b0\u4e92\u8865\u80fd\u529b\u534f\u4f5c", "conclusion": "AWCP\u5728\u667a\u80fd\u4f53\u534f\u8bae\u6808\u4e2d\u5efa\u7acb\u4e86\u7f3a\u5931\u7684\u5de5\u4f5c\u7a7a\u95f4\u5c42\uff0c\u4e3a\u5b9e\u73b0\u666e\u904d\u53ef\u4e92\u64cd\u4f5c\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u4f7f\u534f\u4f5c\u8d85\u8d8a\u6d88\u606f\u8fb9\u754c"}}
{"id": "2602.20724", "categories": ["cs.NI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20724", "abs": "https://arxiv.org/abs/2602.20724", "authors": ["Siya Chen", "Chee Wei Tan", "H. Vincent Poor"], "title": "Deep Reinforcement Learning Based Block Coordinate Descent for Downlink Weighted Sum-rate Maximization on AI-Native Wireless Networks", "comment": "This paper has been accepted by IEEE Transactions on Wireless Communications in 2025", "summary": "This paper introduces a deep reinforcement learning-based block coordinate descent (DRL-based BCD) algorithm to address the nonconvex weighted sum-rate maximization (WSRM) problem with a total power constraint. Firstly, we present an efficient block coordinate descent (BCD) method to solve the problem. We then integrate deep reinforcement learning (DRL) techniques into the BCD method and propose the DRL-based BCD algorithm. This approach combines the data-driven learning capability of machine learning techniques with the navigational and decision-making characteristics of the optimization-theoretic-based BCD method. This combination significantly improves the algorithm's performance by reducing its sensitivity to initial points and mitigating the risk of entrapment in local optima. The primary advantages of the proposed DRL-based BCD algorithm lie in its ability to adhere to the constraints of the WSRM problem and significantly enhance accuracy, potentially achieving the exact optimal solution. Moreover, unlike many pure machine-learning approaches, the DRL-based BCD algorithm capitalizes on the underlying theoretical analysis of the WSRM problem's structure. This enables it to be easily trained and computationally efficient while maintaining a level of interpretability. Through numerical experiments, the DRL-based BCD algorithm demonstrates substantial advantages in effectiveness, efficiency, robustness, and interpretability for maximizing sum rates, which also provides valuable potential for designing resource-constrained AI-native wireless optimization strategies in next-generation wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\uff0c\u89e3\u51b3\u975e\u51f8\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u80fd\u529b\u548c\u4f18\u5316\u7406\u8bba\u7684\u5bfc\u822a\u51b3\u7b56\u7279\u6027\uff0c\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11\u5bf9\u521d\u59cb\u70b9\u7684\u654f\u611f\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u51f8\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\uff08WSRM\uff09\u95ee\u9898\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u4e14\u5bf9\u521d\u59cb\u70b9\u654f\u611f\uff0c\u9700\u8981\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "method": "\u9996\u5148\u63d0\u51fa\u9ad8\u6548\u7684\u5757\u5750\u6807\u4e0b\u964d\uff08BCD\uff09\u65b9\u6cd5\uff0c\u7136\u540e\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6280\u672f\uff0c\u5f62\u6210DRL-based BCD\u7b97\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u80fd\u529b\u548c\u4f18\u5316\u7406\u8bba\u7684\u5bfc\u822a\u51b3\u7b56\u7279\u6027\u3002", "result": "\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u5bf9\u521d\u59cb\u70b9\u7684\u654f\u611f\u6027\uff0c\u964d\u4f4e\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u98ce\u9669\uff0c\u80fd\u591f\u9075\u5b88WSRM\u95ee\u9898\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u53ef\u80fd\u8fbe\u5230\u7cbe\u786e\u6700\u4f18\u89e3\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DRL-based BCD\u7b97\u6cd5\u5728\u6709\u6548\u6027\u3001\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8d44\u6e90\u53d7\u9650\u7684AI\u539f\u751f\u65e0\u7ebf\u4f18\u5316\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.20924", "categories": ["cs.NI", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20924", "abs": "https://arxiv.org/abs/2602.20924", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "title": "Airavat: An Agentic Framework for Internet Measurement", "comment": null, "summary": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research.\n  We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing.", "AI": {"tldr": "Airavat\u662f\u9996\u4e2a\u7528\u4e8e\u4e92\u8054\u7f51\u6d4b\u91cf\u5de5\u4f5c\u6d41\u751f\u6210\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u5e76\u786e\u4fdd\u65b9\u6cd5\u5b66\u6b63\u786e\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u6d4b\u91cf\u9762\u4e34\u53cc\u91cd\u6311\u6218\uff1a\u590d\u6742\u5206\u6790\u9700\u8981\u4e13\u5bb6\u7ea7\u5de5\u5177\u7f16\u6392\uff0c\u800c\u5373\u4f7f\u8bed\u6cd5\u6b63\u786e\u7684\u5b9e\u73b0\u4e5f\u53ef\u80fd\u5b58\u5728\u65b9\u6cd5\u5b66\u7f3a\u9677\u4e14\u96be\u4ee5\u9a8c\u8bc1\u3002\u56e0\u6b64\uff0c\u6c11\u4e3b\u5316\u6d4b\u91cf\u80fd\u529b\u9700\u8981\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u548c\u9a8c\u8bc1\u3002", "method": "Airavat\u91c7\u7528\u4ee3\u7406\u6846\u67b6\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\uff1a\u4e09\u4e2a\u4ee3\u7406\u5206\u522b\u5904\u7406\u95ee\u9898\u5206\u89e3\u3001\u65b9\u6848\u8bbe\u8ba1\u548c\u4ee3\u7801\u5b9e\u73b0\uff0c\u8f85\u4ee5\u73b0\u6709\u5de5\u5177\u6ce8\u518c\u8868\u3002\u4e24\u4e2a\u4e13\u95e8\u5f15\u64ce\u786e\u4fdd\u65b9\u6cd5\u5b66\u6b63\u786e\u6027\uff1a\u9a8c\u8bc1\u5f15\u64ce\u57fa\u4e8e\u7f16\u7801\u4e86\u4e94\u5341\u5e74\u6d4b\u91cf\u7814\u7a76\u7684\u77e5\u8bc6\u56fe\u8c31\u8bc4\u4f30\u5de5\u4f5c\u6d41\uff0c\u9a8c\u8bc1\u5f15\u64ce\u8bc6\u522b\u57fa\u4e8e\u65e2\u5b9a\u65b9\u6cd5\u5b66\u7684\u9002\u5f53\u9a8c\u8bc1\u6280\u672f\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u4e92\u8054\u7f51\u6d4b\u91cf\u6848\u4f8b\u7814\u7a76\uff0cAiravat\u80fd\u591f\uff1a(i)\u751f\u6210\u5339\u914d\u4e13\u5bb6\u7ea7\u89e3\u51b3\u65b9\u6848\u7684\u5de5\u4f5c\u6d41\uff0c(ii)\u505a\u51fa\u5408\u7406\u7684\u67b6\u6784\u51b3\u7b56\uff0c(iii)\u5904\u7406\u6ca1\u6709\u771f\u5b9e\u57fa\u51c6\u7684\u65b0\u95ee\u9898\uff0c(iv)\u8bc6\u522b\u6807\u51c6\u6267\u884c\u6d4b\u8bd5\u9057\u6f0f\u7684\u65b9\u6cd5\u5b66\u7f3a\u9677\u3002", "conclusion": "Airavat\u662f\u9996\u4e2a\u5b9e\u73b0\u4e92\u8054\u7f51\u6d4b\u91cf\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u7cfb\u7edf\u5316\u9a8c\u8bc1\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d4b\u91cf\u5206\u6790\u4e2d\u7684\u65b9\u6cd5\u5b66\u6b63\u786e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\u3002"}}
{"id": "2602.20278", "categories": ["cs.IT", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.20278", "abs": "https://arxiv.org/abs/2602.20278", "authors": ["Alexander R. Block", "Jeremiah Blocki", "Kuan Cheng", "Elena Grigorescu", "Xin Li", "Yu Zheng", "Minshen Zhu"], "title": "Exponential Lower Bounds for 2-query Relaxed Locally Decodable Codes", "comment": "arXiv admin note: substantial text overlap with arXiv:2209.08688", "summary": "Locally Decodable Codes (LDCs) are error-correcting codes $C\\colon\u03a3^n\\rightarrow \u03a3^m,$ encoding \\emph{messages} in $\u03a3^n$ to \\emph{codewords} in $\u03a3^m$, with super-fast decoding algorithms. They are important mathematical objects in many areas of theoretical computer science, yet the best constructions so far have codeword length $m$ that is super-polynomial in $n$, for codes with constant query complexity and constant alphabet size.\n  In a very surprising result, Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (SICOMP 2006) show how to construct a relaxed version of LDCs (RLDCs) with constant query complexity and almost linear codeword length over the binary alphabet, and used them to obtain significantly-improved constructions of Probabilistically Checkable Proofs.\n  In this work, we study RLDCs in the standard Hamming-error setting. We prove an exponential lower bound on the length of Hamming RLDCs making $2$ queries (even adaptively) over the binary alphabet. This answers a question explicitly raised by Gur and Lachish (SICOMP 2021) and is the first exponential lower bound for RLDCs. Combined with the results of Ben-Sasson et al., our result exhibits a ``phase-transition''-type behavior on the codeword length for some constant-query complexity. We achieve these lower bounds via a transformation of RLDCs to standard Hamming LDCs, using a careful analysis of restrictions of message bits that fix codeword bits.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86Hamming RLDCs\uff08\u677e\u5f1b\u672c\u5730\u53ef\u89e3\u7801\u7801\uff09\u5728\u4e8c\u8fdb\u5236\u5b57\u6bcd\u8868\u4e0a\u3001\u8fdb\u884c2\u6b21\u67e5\u8be2\uff08\u5373\u4f7f\u662f\u81ea\u9002\u5e94\u67e5\u8be2\uff09\u65f6\u7684\u7801\u5b57\u957f\u5ea6\u5b58\u5728\u6307\u6570\u4e0b\u754c\uff0c\u8fd9\u662fRLDCs\u7684\u7b2c\u4e00\u4e2a\u6307\u6570\u4e0b\u754c\u3002", "motivation": "\u867d\u7136Ben-Sasson\u7b49\u4eba\uff082006\uff09\u6784\u9020\u4e86\u5177\u6709\u5e38\u6570\u67e5\u8be2\u590d\u6742\u5ea6\u548c\u51e0\u4e4e\u7ebf\u6027\u7801\u5b57\u957f\u5ea6\u7684RLDCs\uff0c\u4f46\u5173\u4e8eRLDCs\u7684\u57fa\u672c\u4e0b\u754c\u95ee\u9898\u4ecd\u7136\u5f00\u653e\u3002Gur\u548cLachish\uff082021\uff09\u660e\u786e\u63d0\u51fa\u4e86RLDCs\u957f\u5ea6\u4e0b\u754c\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e2\u67e5\u8be2\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5c06RLDCs\u8f6c\u6362\u4e3a\u6807\u51c6\u7684Hamming LDCs\uff0c\u5e76\u4ed4\u7ec6\u5206\u6790\u56fa\u5b9a\u7801\u5b57\u6bd4\u7279\u7684\u6d88\u606f\u6bd4\u7279\u9650\u5236\uff0c\u5efa\u7acb\u4e86\u4e0b\u754c\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u4e86\u4e8c\u8fdb\u5236\u5b57\u6bcd\u8868\u4e0a\u3001\u8fdb\u884c2\u6b21\u67e5\u8be2\uff08\u5373\u4f7f\u662f\u81ea\u9002\u5e94\u67e5\u8be2\uff09\u7684Hamming RLDCs\u7684\u7801\u5b57\u957f\u5ea6\u5b58\u5728\u6307\u6570\u4e0b\u754c\u3002\u7ed3\u5408Ben-Sasson\u7b49\u4eba\u7684\u7ed3\u679c\uff0c\u8fd9\u5c55\u793a\u4e86\u5728\u67d0\u4e9b\u5e38\u6570\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u7801\u5b57\u957f\u5ea6\u7684\"\u76f8\u53d8\"\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u662fRLDCs\u7684\u7b2c\u4e00\u4e2a\u6307\u6570\u4e0b\u754c\uff0c\u56de\u7b54\u4e86Gur\u548cLachish\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86RLDCs\u5728\u5e38\u6570\u67e5\u8be2\u590d\u6742\u5ea6\u4e0b\u7684\u57fa\u672c\u9650\u5236\u3002"}}
{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7f8e\u56fd\u9752\u5c11\u5e74\u8d85\u91cd/\u80a5\u80d6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u903b\u8f91\u56de\u5f52\u4e0e\u66f4\u590d\u6742\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u6240\u6709\u6a21\u578b\u5728\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u90fd\u5b58\u5728\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u8d85\u91cd\u80a5\u80d6\u662f\u7f8e\u56fd\u4e3b\u8981\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u53d7\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u56e0\u7d20\u5171\u540c\u5f71\u54cd\u3002\u76ee\u524d\u5bf9\u8fd9\u4e9b\u591a\u5c42\u6b21\u9884\u6d4b\u56e0\u7d20\u5728\u4eba\u7fa4\u5c42\u9762\u7684\u8054\u5408\u9884\u6d4b\u7ed3\u6784\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u5efa\u6a21\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u3001\u6821\u51c6\u548c\u4e9a\u7ec4\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u75282021\u5e74\u5168\u56fd\u513f\u7ae5\u5065\u5eb7\u8c03\u67e5\u4e2d18,792\u540d10-17\u5c81\u513f\u7ae5\u7684\u6570\u636e\u3002\u9884\u6d4b\u53d8\u91cf\u5305\u62ec\u996e\u98df\u3001\u4f53\u80b2\u6d3b\u52a8\u3001\u7761\u7720\u3001\u7236\u6bcd\u538b\u529b\u3001\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\u3001\u4e0d\u826f\u7ecf\u5386\u548c\u793e\u533a\u7279\u5f81\u3002\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u3001XGBoost\u3001LightGBM\u3001\u591a\u5c42\u611f\u77e5\u673a\u548cTabNet\u7b49\u6a21\u578b\uff0c\u4f7f\u7528AUC\u3001\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cBrier\u5206\u6570\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u533a\u5206\u5ea6\u57280.66-0.79\u4e4b\u95f4\u3002\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u673a\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u8868\u73b0\u6700\u7a33\u5b9a\u3002\u63d0\u5347\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7565\u5fae\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002\u6ca1\u6709\u6a21\u578b\u5728\u6240\u6709\u65b9\u9762\u90fd\u8868\u73b0\u6700\u4f18\u3002\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u5728\u6240\u6709\u7b97\u6cd5\u4e2d\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u5bf9\u903b\u8f91\u56de\u5f52\u7684\u6539\u8fdb\u6709\u9650\u3002\u9884\u6d4b\u56e0\u7d20\u59cb\u7ec8\u6db5\u76d6\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u9886\u57df\u3002\u6301\u7eed\u7684\u4e9a\u7ec4\u5dee\u5f02\u8868\u660e\u9700\u8981\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u5173\u6ce8\u516c\u5e73\u6027\u7684\u76d1\u6d4b\uff0c\u800c\u4e0d\u662f\u8ffd\u6c42\u66f4\u9ad8\u7684\u7b97\u6cd5\u590d\u6742\u6027\u3002"}}
{"id": "2602.20361", "categories": ["cs.IT", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20361", "abs": "https://arxiv.org/abs/2602.20361", "authors": ["Mohanad Obeed", "Ming Jian"], "title": "Learning During Detection: Continual Learning for Neural OFDM Receivers via DMRS", "comment": null, "summary": "Deep neural networks (DNNs) have been increasingly explored for receiver design because they can handle complex environments without relying on explicit channel models. Nevertheless, because communication channels change rapidly, their distributions can shift over time, often making periodic retraining necessary. This paper proposes a zero-overhead online and continual learning framework for orthogonal frequency-division multiplexing (OFDM) neural receivers that directly detect the soft bits of received signals. Unlike conventional fine-tuning methods that rely on dedicated training intervals or full resource grids, our approach leverages existing demodulation reference signals (DMRS) to simultaneously enable signal demodulation and model adaptation. We introduce three pilot designs: fully randomized, hybrid, and additional pilots that flexibly support joint demodulation and learning. To accommodate these pilot designs, we develop two receiver architectures: (i) a parallel design that separates inference and fine-tuning for uninterrupted operation, and (ii) a forward-pass reusing design that reduces computational complexity. Simulation results show that the proposed method effectively tracks both slow and fast channel distribution variations without additional overhead, service interruption, or catastrophic performance degradation under distribution shift.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u5f00\u9500\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eOFDM\u795e\u7ecf\u63a5\u6536\u673a\uff0c\u5229\u7528\u73b0\u6709DMRS\u53c2\u8003\u4fe1\u53f7\u540c\u65f6\u5b9e\u73b0\u4fe1\u53f7\u89e3\u8c03\u548c\u6a21\u578b\u81ea\u9002\u5e94\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u95f4\u9694\u6216\u5b8c\u6574\u8d44\u6e90\u7f51\u683c\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u63a5\u6536\u673a\u8bbe\u8ba1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u901a\u4fe1\u4fe1\u9053\u53d8\u5316\u5feb\u901f\uff0c\u5206\u5e03\u4f1a\u968f\u65f6\u95f4\u6f02\u79fb\uff0c\u9700\u8981\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u3002\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u8bad\u7ec3\u95f4\u9694\u6216\u5b8c\u6574\u8d44\u6e90\u7f51\u683c\uff0c\u5b58\u5728\u5f00\u9500\u548c\u670d\u52a1\u4e2d\u65ad\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u5bfc\u9891\u8bbe\u8ba1\uff1a\u5b8c\u5168\u968f\u673a\u3001\u6df7\u5408\u548c\u989d\u5916\u5bfc\u9891\uff0c\u652f\u6301\u8054\u5408\u89e3\u8c03\u548c\u5b66\u4e60\u3002\u5f00\u53d1\u4e24\u79cd\u63a5\u6536\u673a\u67b6\u6784\uff1a\u5e76\u884c\u8bbe\u8ba1\uff08\u5206\u79bb\u63a8\u7406\u548c\u5fae\u8c03\uff09\u548c\u524d\u5411\u4f20\u9012\u91cd\u7528\u8bbe\u8ba1\uff08\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff09\u3002\u5229\u7528\u73b0\u6709DMRS\u53c2\u8003\u4fe1\u53f7\u5b9e\u73b0\u96f6\u5f00\u9500\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8ddf\u8e2a\u6162\u901f\u548c\u5feb\u901f\u4fe1\u9053\u5206\u5e03\u53d8\u5316\uff0c\u65e0\u9700\u989d\u5916\u5f00\u9500\u3001\u670d\u52a1\u4e2d\u65ad\uff0c\u4e14\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u4e0d\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u7684\u96f6\u5f00\u9500\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u4e3aOFDM\u795e\u7ecf\u63a5\u6536\u673a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u65f6\u8ddf\u8e2a\u4fe1\u9053\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u670d\u52a1\u8fde\u7eed\u6027\u3002"}}
{"id": "2602.20324", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20324", "abs": "https://arxiv.org/abs/2602.20324", "authors": ["Cathy Shyr", "Yan Hu", "Rory J. Tinker", "Thomas A. Cassini", "Kevin W. Byram", "Rizwan Hamid", "Daniel V. Fabbri", "Adam Wright", "Josh F. Peterson", "Lisa Bastarache", "Hua Xu"], "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models", "comment": null, "summary": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.", "AI": {"tldr": "RARE-PHENIX\u662f\u4e00\u4e2a\u7aef\u5230\u7aefAI\u6846\u67b6\uff0c\u7528\u4e8e\u7f55\u89c1\u75c5\u8868\u578b\u5206\u6790\uff0c\u6574\u5408\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u578b\u63d0\u53d6\u3001\u672c\u4f53\u6807\u51c6\u5316\u548c\u8bca\u65ad\u4fe1\u606f\u8868\u578b\u6392\u5e8f\uff0c\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7f55\u89c1\u75c5\u8bca\u65ad\u4e2d\u8868\u578b\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u4ece\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8868\u578b\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709AI\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u5355\u4e2a\u7ec4\u4ef6\uff0c\u672a\u80fd\u5b8c\u6574\u5b9e\u73b0\u4ece\u4e34\u5e8a\u6587\u672c\u63d0\u53d6\u7279\u5f81\u3001\u6807\u51c6\u5316\u4e3aHPO\u672f\u8bed\u3001\u5e76\u4f18\u5148\u6392\u5e8f\u8bca\u65ad\u4fe1\u606f\u8868\u578b\u7684\u5b8c\u6574\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86RARE-PHENIX\u7aef\u5230\u7aefAI\u6846\u67b6\uff0c\u6574\u5408\u4e09\u4e2a\u6a21\u5757\uff1a1) \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u578b\u63d0\u53d6\uff1b2) \u57fa\u4e8e\u672c\u4f53\u7684\u6807\u51c6\u5316\u5230HPO\u672f\u8bed\uff1b3) \u76d1\u7763\u5f0f\u8bca\u65ad\u4fe1\u606f\u8868\u578b\u6392\u5e8f\u3002\u4f7f\u7528\u6765\u81ea11\u4e2a\u672a\u8bca\u65ad\u75be\u75c5\u7f51\u7edc\u4e34\u5e8a\u7ad9\u70b9\u76842,671\u540d\u60a3\u8005\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u8303\u5fb7\u6bd4\u5c14\u7279\u5927\u5b66\u533b\u5b66\u4e2d\u5fc3\u768416,357\u4efd\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u8bb0\u5f55\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\uff0c\u4f7f\u7528\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684HPO\u672f\u8bed\u4f5c\u4e3a\u91d1\u6807\u51c6\u3002RARE-PHENIX\u5728\u7aef\u5230\u7aef\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf(PhenoBERT)\uff0c\u672c\u4f53\u76f8\u4f3c\u5ea6\u4e3a0.70 vs. 0.58\u3002\u6d88\u878d\u5206\u6790\u663e\u793a\u6bcf\u4e2a\u6a21\u5757\u90fd\u5bf9\u6027\u80fd\u6709\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8868\u578b\u5206\u6790\u5efa\u6a21\u4e3a\u4e34\u5e8a\u5bf9\u9f50\u7684\u5de5\u4f5c\u6d41\u7a0b\u800c\u975e\u5355\u4e00\u63d0\u53d6\u4efb\u52a1\uff0cRARE-PHENIX\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684\u7ed3\u6784\u5316\u3001\u6392\u5e8f\u8868\u578b\uff0c\u6709\u6f5c\u529b\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u652f\u6301\u4eba\u673a\u534f\u540c\u7684\u7f55\u89c1\u75c5\u8bca\u65ad\u3002"}}
{"id": "2602.20366", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20366", "abs": "https://arxiv.org/abs/2602.20366", "authors": ["Ron M. Roth", "Ziyuan Zhu", "Changcheng Yuan", "Paul H. Siegel", "Anxiao Jiang"], "title": "On the Height Profile of Analog Error-Correcting Codes", "comment": null, "summary": "In recent work, it has been shown that maintaining reliability in analog vector--matrix multipliers can be modeled as the following coding problem. Vectors in $\\mathbb{R}^k$ are encoded into codewords of a linear $[n,k,d]$ code $C$ over $\\mathbb{R}$. For prescribed positive reals $\u03b4< \u0394$, additive errors of magnitude at most $\u03b4$ are tolerable and need no handling, yet outlying errors of magnitude greater than $\u0394$ are to be located or detected. The trade-off between the ratio $\u0394/\u03b4$ and the number of outlying errors that can be handled is determined by the height profile of $C$; as such, the height profile provides a finer description of the error handling capability of $C$, compared to the minimum distance $d$, which only determines the number of correctable errors. This work contains a further study of the notion of the height profile. Several characterizations of the height profile are presented, thereby yielding methods for computing it. The starting point is formulating this computation as an optimization problem that is solved by a set of linear programs. This, in turn, leads to a combinatorial characterization of the height profile as a maximum (or max--min) over a certain finite set of codewords of $C$. Moreover, this characterization is shown to have a simple geometric interpretation when the columns of the generator matrix of $C$ all have the same $L_2$ norm. Through examples of several code families, it is demonstrated how the results herein can be used to compute the height profile explicitly.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86\u7f16\u7801\u7406\u8bba\u4e2d\u7684\u9ad8\u5ea6\u5256\u9762\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u7528\u4e8e\u63cf\u8ff0\u6a21\u62df\u5411\u91cf-\u77e9\u9635\u4e58\u6cd5\u5668\u4e2d\u5904\u7406\u5f02\u5e38\u8bef\u5dee\u7684\u80fd\u529b\uff0c\u6bd4\u4f20\u7edf\u7684\u6700\u5c0f\u8ddd\u79bb\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bef\u5dee\u5904\u7406\u80fd\u529b\u63cf\u8ff0\u3002", "motivation": "\u5728\u6a21\u62df\u5411\u91cf-\u77e9\u9635\u4e58\u6cd5\u5668\u4e2d\uff0c\u9700\u8981\u5904\u7406\u4e24\u79cd\u7c7b\u578b\u7684\u8bef\u5dee\uff1a\u53ef\u5bb9\u5fcd\u7684\u5c0f\u5e45\u5ea6\u8bef\u5dee\uff08\u2264\u03b4\uff09\u548c\u9700\u8981\u68c0\u6d4b/\u5b9a\u4f4d\u7684\u5927\u5e45\u5ea6\u5f02\u5e38\u8bef\u5dee\uff08>\u0394\uff09\u3002\u4f20\u7edf\u7684\u6700\u5c0f\u8ddd\u79bb\u4ec5\u80fd\u786e\u5b9a\u53ef\u7ea0\u6b63\u8bef\u5dee\u7684\u6570\u91cf\uff0c\u800c\u9ad8\u5ea6\u5256\u9762\u80fd\u66f4\u7cbe\u7ec6\u5730\u63cf\u8ff0\u7f16\u7801\u5904\u7406\u5f02\u5e38\u8bef\u5dee\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u0394/\u03b4\u6bd4\u4f8b\u4e0e\u53ef\u5904\u7406\u5f02\u5e38\u8bef\u5dee\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "1. \u5c06\u9ad8\u5ea6\u5256\u9762\u8ba1\u7b97\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4e00\u7ec4\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\uff1b2. \u63d0\u51fa\u7ec4\u5408\u7279\u5f81\u5316\u65b9\u6cd5\uff0c\u5c06\u9ad8\u5ea6\u5256\u9762\u8868\u793a\u4e3a\u7f16\u7801C\u4e2d\u7279\u5b9a\u7801\u5b57\u96c6\u5408\u7684\u6700\u5927\u503c\uff08\u6216\u6700\u5927-\u6700\u5c0f\u503c\uff09\uff1b3. \u5f53\u751f\u6210\u77e9\u9635\u7684\u6240\u6709\u5217\u5177\u6709\u76f8\u540cL2\u8303\u6570\u65f6\uff0c\u7ed9\u51fa\u7b80\u5355\u7684\u51e0\u4f55\u89e3\u91ca\uff1b4. \u901a\u8fc7\u591a\u4e2a\u7f16\u7801\u65cf\u793a\u4f8b\u5c55\u793a\u5982\u4f55\u663e\u5f0f\u8ba1\u7b97\u9ad8\u5ea6\u5256\u9762\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u79cd\u8ba1\u7b97\u9ad8\u5ea6\u5256\u9762\u7684\u65b9\u6cd5\uff1a\u7ebf\u6027\u89c4\u5212\u4f18\u5316\u65b9\u6cd5\u3001\u7ec4\u5408\u7279\u5f81\u5316\u65b9\u6cd5\u4ee5\u53ca\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u51e0\u4f55\u89e3\u91ca\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4f7f\u5f97\u80fd\u591f\u663e\u5f0f\u8ba1\u7b97\u4e0d\u540c\u7f16\u7801\u65cf\u7684\u9ad8\u5ea6\u5256\u9762\uff0c\u4ece\u800c\u66f4\u7cbe\u786e\u5730\u8bc4\u4f30\u7f16\u7801\u5728\u6a21\u62df\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u8bef\u5dee\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u9ad8\u5ea6\u5256\u9762\u4e3a\u7f16\u7801\u7406\u8bba\u63d0\u4f9b\u4e86\u6bd4\u6700\u5c0f\u8ddd\u79bb\u66f4\u7cbe\u7ec6\u7684\u8bef\u5dee\u5904\u7406\u80fd\u529b\u63cf\u8ff0\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5904\u7406\u5f02\u5e38\u8bef\u5dee\u7684\u5e94\u7528\u573a\u666f\u3002\u672c\u6587\u63d0\u51fa\u7684\u591a\u79cd\u8ba1\u7b97\u65b9\u6cd5\u4f7f\u5f97\u9ad8\u5ea6\u5256\u9762\u7684\u5b9e\u9645\u8ba1\u7b97\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u7f16\u7801\u5728\u53ef\u9760\u6027\u8981\u6c42\u9ad8\u7684\u6a21\u62df\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.20333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.", "AI": {"tldr": "DMCD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u7ed3\u5408LLM\u8bed\u4e49\u8349\u56fe\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u53d8\u91cf\u8bed\u4e49\u4fe1\u606f\u7684\u5229\u7528\u3002DMCD\u65e8\u5728\u901a\u8fc7\u7ed3\u5408LLM\u7684\u8bed\u4e49\u7406\u89e3\u548c\u4f20\u7edf\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u53d8\u91cf\u5143\u6570\u636e\u751f\u6210\u7a00\u758f\u7684\u56e0\u679c\u56fe\u8349\u56fe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u5bf9\u8349\u56fe\u8fdb\u884c\u5ba1\u8ba1\u548c\u7cbe\u70bc\uff0c\u68c0\u6d4b\u5dee\u5f02\u5e76\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u8fb9\u4fee\u6b63", "result": "\u5728\u5de5\u4e1a\u5de5\u7a0b\u3001\u73af\u5883\u76d1\u6d4b\u548cIT\u7cfb\u7edf\u5206\u6790\u4e09\u4e2a\u5143\u6570\u636e\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMCD\u76f8\u6bd4\u591a\u79cd\u56e0\u679c\u53d1\u73b0\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u9886\u5148\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u5c06\u8bed\u4e49\u5148\u9a8c\u4e0e\u539f\u5219\u6027\u7edf\u8ba1\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u6027\u80fd\u4e14\u5b9e\u9645\u6709\u6548\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u5bf9\u5143\u6570\u636e\u7684\u8bed\u4e49\u63a8\u7406\u800c\u975e\u5bf9\u57fa\u51c6\u56fe\u7684\u8bb0\u5fc6"}}
{"id": "2602.20455", "categories": ["cs.IT", "math.AC"], "pdf": "https://arxiv.org/pdf/2602.20455", "abs": "https://arxiv.org/abs/2602.20455", "authors": ["Monica Lichtenwalner", "Hiram H. L\u00f3pez", "Gretchen L. Matthews", "Padmapani Seneviratne"], "title": "Permutation decoding of algebraic geometry codes from Hermitian and norm-trace curves", "comment": null, "summary": "Permutation decoding is a process that utilizes the permutation automorphism group of a linear code to correct errors in received words. Given a received word, a set of automorphisms, called a PD set, moves errors out of the information positions so that the original message can be determined. In this paper, we investigate permutation decoding for certain families of algebraic geometry codes. Automorphisms of the underlying curve are used to specify permutation automorphisms of the code. Specifically, we describe permutation decoding sets that correct specific burst errors for one-point codes on Hermitian and norm-trace curves.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4ee3\u6570\u51e0\u4f55\u7801\u7684\u7f6e\u6362\u8bd1\u7801\uff0c\u5229\u7528\u66f2\u7ebf\u81ea\u540c\u6784\u6784\u5efaPD\u96c6\uff0c\u9488\u5bf9Hermitian\u548cnorm-trace\u66f2\u7ebf\u4e0a\u7684\u4e00\u70b9\u7801\uff0c\u8bbe\u8ba1\u80fd\u7ea0\u6b63\u7279\u5b9a\u7a81\u53d1\u9519\u8bef\u7684\u7f6e\u6362\u8bd1\u7801\u96c6\u3002", "motivation": "\u7f6e\u6362\u8bd1\u7801\u5229\u7528\u7ebf\u6027\u7801\u7684\u7f6e\u6362\u81ea\u540c\u6784\u7fa4\u7ea0\u6b63\u63a5\u6536\u5b57\u4e2d\u7684\u9519\u8bef\uff0c\u4f46\u9700\u8981\u4e3a\u7279\u5b9a\u7801\u6784\u9020\u6709\u6548\u7684PD\u96c6\u3002\u4ee3\u6570\u51e0\u4f55\u7801\u5177\u6709\u4e30\u5bcc\u7684\u81ea\u540c\u6784\u7ed3\u6784\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u7684\u7f6e\u6362\u8bd1\u7801\u65b9\u6848\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u5229\u7528\u5e95\u5c42\u66f2\u7ebf\u7684\u81ea\u540c\u6784\u6765\u6307\u5b9a\u7801\u7684\u7f6e\u6362\u81ea\u540c\u6784\uff0c\u9488\u5bf9Hermitian\u548cnorm-trace\u66f2\u7ebf\u4e0a\u7684\u4e00\u70b9\u7801\uff0c\u8bbe\u8ba1\u7279\u5b9a\u7684\u7f6e\u6362\u8bd1\u7801\u96c6\u6765\u5904\u7406\u7a81\u53d1\u9519\u8bef\u3002", "result": "\u6210\u529f\u63cf\u8ff0\u4e86\u9488\u5bf9Hermitian\u548cnorm-trace\u66f2\u7ebf\u4e0a\u4e00\u70b9\u7801\u7684\u7f6e\u6362\u8bd1\u7801\u96c6\uff0c\u8fd9\u4e9b\u96c6\u5408\u80fd\u591f\u7ea0\u6b63\u7279\u5b9a\u7684\u7a81\u53d1\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "\u4ee3\u6570\u51e0\u4f55\u7801\u7684\u81ea\u540c\u6784\u7ed3\u6784\u4e3a\u6784\u5efa\u6709\u6548\u7684\u7f6e\u6362\u8bd1\u7801\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ea0\u6b63\u7a81\u53d1\u9519\u8bef\uff0c\u5728Hermitian\u548cnorm-trace\u66f2\u7ebf\u4e0a\u7684\u4e00\u70b9\u7801\u4e2d\u5c55\u793a\u4e86\u5177\u4f53\u5e94\u7528\u3002"}}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "DMEMM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u673a\u5236\uff08\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u89c4\u5212\u4e2d\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u672a\u80fd\u8003\u8651RL\u73af\u5883\u4e2d\u8f6c\u79fb\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\u8981\u6c42\uff0c\u5bfc\u81f4\u751f\u6210\u8f68\u8ff9\u4e0e\u771f\u5b9e\u73af\u5883\u673a\u5236\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51faDMEMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u5173\u952eRL\u73af\u5883\u673a\u5236\uff08\u7279\u522b\u662f\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f68\u8ff9\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDMEMM\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u73af\u5883\u673a\u5236\u5efa\u6a21\u878d\u5165\u6269\u6563\u8bad\u7ec3\uff0cDMEMM\u6709\u6548\u89e3\u51b3\u4e86\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u79bb\u7ebfRL\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.20529", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.20529", "abs": "https://arxiv.org/abs/2602.20529", "authors": ["Junren Qin", "Fan Jiang", "Tao Yang", "Shanxiang Lyu", "Rongke Liu", "Shi Jin"], "title": "On the Optimal Integer-Forcing Precoding: A Geometric Perspective and a Polynomial-Time Algorithm", "comment": "42pages", "summary": "The joint optimization of the integer matrix $\\mathbf{A}$ and the power scaling matrix $\\mathbf{D}$ is central to achieving the capacity-approaching performance of Integer-Forcing (IF) precoding. This problem, however, is known to be NP-hard, presenting a fundamental computational bottleneck. In this paper, we reveal that the solution space of this problem admits a intrinsic geometric structure: it can be partitioned into a finite number of conical regions, each associated with a distinct full-rank integer matrix $\\mathbf{A}$. Leveraging this decomposition, we transform the NP-hard problem into a search over these regions and propose the Multi-Cone Nested Stochastic Pattern Search (MCN-SPS) algorithm. Our main theoretical result is that MCN-SPS finds a near-optimal solution with a computational complexity of $\\mathcal{O}\\left(K^4\\log K\\log_2(r_0)\\right)$, which is polynomial in the number of users $K$. Numerical simulations corroborate the theoretical analysis and demonstrate the algorithm's efficacy.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6574\u6570\u8feb\u96f6\u9884\u7f16\u7801\u4e2d\u6574\u6570\u77e9\u9635A\u548c\u529f\u7387\u7f29\u653e\u77e9\u9635D\u8054\u5408\u4f18\u5316\u95ee\u9898\u7684\u89e3\u7a7a\u95f4\u5177\u6709\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u5206\u89e3\u4e3a\u6709\u9650\u4e2a\u9525\u5f62\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7684MCN-SPS\u7b97\u6cd5\u3002", "motivation": "\u6574\u6570\u8feb\u96f6\u9884\u7f16\u7801\u4e2d\u6574\u6570\u77e9\u9635A\u548c\u529f\u7387\u7f29\u653e\u77e9\u9635D\u7684\u8054\u5408\u4f18\u5316\u662f\u5b9e\u73b0\u5bb9\u91cf\u903c\u8fd1\u6027\u80fd\u7684\u5173\u952e\uff0c\u4f46\u8be5\u95ee\u9898\u5df2\u77e5\u662fNP\u96be\u95ee\u9898\uff0c\u5b58\u5728\u6839\u672c\u6027\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u53d1\u73b0\u89e3\u7a7a\u95f4\u53ef\u5212\u5206\u4e3a\u6709\u9650\u4e2a\u9525\u5f62\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u5bf9\u5e94\u4e00\u4e2a\u6ee1\u79e9\u6574\u6570\u77e9\u9635A\u3002\u57fa\u4e8e\u6b64\u5206\u89e3\uff0c\u5c06NP\u96be\u95ee\u9898\u8f6c\u5316\u4e3a\u5bf9\u8fd9\u4e9b\u533a\u57df\u7684\u641c\u7d22\uff0c\u63d0\u51fa\u4e86\u591a\u9525\u5d4c\u5957\u968f\u673a\u6a21\u5f0f\u641c\u7d22\u7b97\u6cd5\u3002", "result": "MCN-SPS\u7b97\u6cd5\u80fd\u4ee5\u591a\u9879\u5f0f\u590d\u6742\u5ea6O(K^4 log K log2(r0))\u627e\u5230\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u5176\u4e2dK\u4e3a\u7528\u6237\u6570\u3002\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u5e76\u8bc1\u660e\u4e86\u7b97\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u6574\u6570\u8feb\u96f6\u9884\u7f16\u7801\u4f18\u5316\u95ee\u9898\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u6210\u529f\u5c06NP\u96be\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u53ef\u89e3\u95ee\u9898\uff0c\u4e3a\u89e3\u51b3\u8be5\u8ba1\u7b97\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.20424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20424", "abs": "https://arxiv.org/abs/2602.20424", "authors": ["Ved Sirdeshmukh", "Marc Wetter"], "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say", "comment": null, "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.", "AI": {"tldr": "\u63d0\u51faImplicit Intelligence\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5AI\u4ee3\u7406\u80fd\u5426\u8d85\u8d8a\u663e\u5f0f\u6307\u4ee4\u8ddf\u968f\uff0c\u7406\u89e3\u5e76\u6ee1\u8db3\u4eba\u7c7b\u8bf7\u6c42\u4e2d\u7684\u9690\u542b\u9700\u6c42\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73\u6a21\u578b\u4ec5\u80fd\u8fbe\u523048.3%\u7684\u573a\u666f\u901a\u8fc7\u7387", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9AI\u4ee3\u7406\u7684\u8bf7\u6c42\u672c\u8d28\u4e0a\u662f\u672a\u5145\u5206\u6307\u5b9a\u7684\uff0c\u4eba\u7c7b\u81ea\u7136\u4ea4\u6d41\u4f9d\u8d56\u5171\u4eab\u4e0a\u4e0b\u6587\u548c\u672a\u660e\u8bf4\u7684\u7ea6\u675f\uff0c\u800c\u5f53\u524d\u4ee3\u7406\u57fa\u51c6\u53ea\u6d4b\u8bd5\u663e\u5f0f\u6307\u4ee4\u8ddf\u968f\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u80fd\u5426\u63a8\u7406\u9690\u542b\u9700\u6c42", "method": "\u63d0\u51faImplicit Intelligence\u8bc4\u4f30\u6846\u67b6\uff0c\u914d\u5408Agent-as-a-World\uff08AaW\uff09\u5de5\u5177\uff0c\u5176\u4e2d\u4ea4\u4e92\u4e16\u754c\u7528\u4eba\u7c7b\u53ef\u8bfb\u7684YAML\u6587\u4ef6\u5b9a\u4e49\u5e76\u7531\u8bed\u8a00\u6a21\u578b\u6a21\u62df\uff0c\u573a\u666f\u8bbe\u8ba1\u5177\u6709\u8868\u9762\u7b80\u5355\u4f46\u89e3\u51b3\u65b9\u6848\u590d\u6742\u7684\u7279\u70b9\uff0c\u7ea6\u675f\u53ef\u901a\u8fc7\u73af\u5883\u63a2\u7d22\u53d1\u73b0", "result": "\u8bc4\u4f3016\u4e2a\u524d\u6cbf\u548c\u5f00\u6e90\u6a21\u578b\u5728205\u4e2a\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u4e5f\u53ea\u80fd\u8fbe\u523048.3%\u7684\u573a\u666f\u901a\u8fc7\u7387\uff0c\u663e\u793a\u5728\u5f25\u5408\u5b57\u9762\u6307\u4ee4\u8ddf\u968f\u548c\u7c7b\u4eba\u4e0a\u4e0b\u6587\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u9690\u542b\u9700\u6c42\u3001\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684AI\u4ee3\u7406\uff0c\u5f53\u524d\u6a21\u578b\u5728\u4ece\u6307\u4ee4\u8ddf\u968f\u8005\u8f6c\u53d8\u4e3a\u771f\u6b63\u76ee\u6807\u5b9e\u73b0\u8005\u65b9\u9762\u4ecd\u6709\u663e\u8457\u4e0d\u8db3"}}
{"id": "2602.20599", "categories": ["cs.IT", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.20599", "abs": "https://arxiv.org/abs/2602.20599", "authors": ["Geonhee Lee", "Kwonyeol Park", "Hyeongjun Park", "Jinwoo An", "Junil Choi"], "title": "Efficient Solvers for Coupling-Aware Beamforming in Continuous Aperture Arrays", "comment": "5 pages, 1 fig", "summary": "In continuous aperture arrays (CAPAs), careful consideration of the underlying physics is essential, among which electromagnetic (EM) mutual coupling plays a critical role in beamforming performance. Building on a physically consistent mutual coupling model, the beamforming design is formulated as a functional optimization whose optimality condition leads to a Fredholm integral equation. The incorporation of the coupling model, however, substantially increases computational complexity, necessitating efficient and accurate integral equation solvers. In this letter, we propose two efficient solvers: 1) a coordinate-transformation-based kernel approximation that preserves the operator structure while alleviating discretization demands, and 2) a direct lower-upper (LU)-based solver that stably handles the Nystr\u00f6m-discretized system. Numerical results demonstrate improved accuracy and reduced computational overhead compared to conventional methods, with the LU-based solver emerging as an efficient and scalable solution for large-scale CAPA optimization via offline factorization.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u9ad8\u6548\u6c42\u89e3\u5668\u7528\u4e8e\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7684\u6ce2\u675f\u6210\u5f62\u4f18\u5316\uff1a\u57fa\u4e8e\u5750\u6807\u53d8\u6362\u7684\u6838\u8fd1\u4f3c\u65b9\u6cd5\u548c\u57fa\u4e8eLU\u5206\u89e3\u7684\u76f4\u63a5\u6c42\u89e3\u5668\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u4e2d\uff0c\u7535\u78c1\u4e92\u8026\u6548\u5e94\u5bf9\u6ce2\u675f\u6210\u5f62\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u4e92\u8026\u6a21\u578b\u867d\u7136\u7269\u7406\u4e00\u81f4\uff0c\u4f46\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u79ef\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u3002", "method": "1) \u57fa\u4e8e\u5750\u6807\u53d8\u6362\u7684\u6838\u8fd1\u4f3c\u65b9\u6cd5\uff1a\u4fdd\u6301\u7b97\u5b50\u7ed3\u6784\u7684\u540c\u65f6\u964d\u4f4e\u79bb\u6563\u5316\u9700\u6c42\uff1b2) \u76f4\u63a5LU\u5206\u89e3\u6c42\u89e3\u5668\uff1a\u7a33\u5b9a\u5904\u7406Nystr\u00f6m\u79bb\u6563\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u63d0\u5347\u3002LU\u5206\u89e3\u6c42\u89e3\u5668\u5c24\u5176\u9002\u5408\u5927\u89c4\u6a21\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u4f18\u5316\uff0c\u5177\u6709\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u6c42\u89e3\u5668\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5176\u4e2dLU\u5206\u89e3\u6c42\u89e3\u5668\u901a\u8fc7\u79bb\u7ebf\u5206\u89e3\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20426", "abs": "https://arxiv.org/abs/2602.20426", "authors": ["Ruocheng Guo", "Kaiwen Dong", "Xiang Gao", "Kamalika Das"], "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use", "comment": "Preprint", "summary": "The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.", "AI": {"tldr": "\u63d0\u51faTrace-Free+\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316LLM\u4ee3\u7406\u7684\u5de5\u5177\u63a5\u53e3\uff0c\u65e0\u9700\u6267\u884c\u8f68\u8ff9\u5373\u53ef\u63d0\u5347\u5de5\u5177\u9009\u62e9\u6027\u80fd\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5de5\u5177\u96c6\u548c\u8de8\u9886\u57df\u6cdb\u5316\u3002", "motivation": "LLM\u4ee3\u7406\u7684\u6027\u80fd\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u4ee3\u7406\u672c\u8eab\uff0c\u8fd8\u53d7\u5de5\u5177\u63a5\u53e3\u8d28\u91cf\u5f71\u54cd\u3002\u73b0\u6709\u5de5\u5177\u63a5\u53e3\u591a\u4e3a\u4eba\u5de5\u8bbe\u8ba1\uff0c\u5728\u5927\u89c4\u6a21\u5de5\u5177\u9009\u62e9\u65f6\u6210\u4e3a\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6267\u884c\u8f68\u8ff9\uff0c\u4f46\u5728\u51b7\u542f\u52a8\u6216\u9690\u79c1\u53d7\u9650\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u901a\u5e38\u72ec\u7acb\u4f18\u5316\u6bcf\u4e2a\u5de5\u5177\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faTrace-Free+\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u9010\u6b65\u4ece\u8f68\u8ff9\u4e30\u5bcc\u8bbe\u7f6e\u8fc1\u79fb\u5230\u65e0\u8f68\u8ff9\u90e8\u7f72\uff0c\u9f13\u52b1\u6a21\u578b\u62bd\u8c61\u53ef\u91cd\u7528\u7684\u63a5\u53e3\u4f7f\u7528\u6a21\u5f0f\u548c\u5de5\u5177\u4f7f\u7528\u7ed3\u679c\u3002\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u5de5\u5177\u63a5\u53e3\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u5904\u7406\u591a\u6837\u5316\u5de5\u5177\u96c6\u5408\u3002", "result": "\u5728StableToolBench\u548cRestBench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5bf9\u672a\u89c1\u5de5\u5177\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5019\u9009\u5de5\u5177\u6570\u91cf\u8d85\u8fc7100\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u5de5\u5177\u63a5\u53e3\u4f18\u5316\u662f\u4ee3\u7406\u5fae\u8c03\u7684\u6709\u6548\u8865\u5145\u3002", "conclusion": "\u5de5\u5177\u63a5\u53e3\u4f18\u5316\u662f\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u4ee3\u7406\u5fae\u8c03\u8865\u5145\u65b9\u6848\uff0cTrace-Free+\u6846\u67b6\u80fd\u591f\u5728\u65e0\u6267\u884c\u8f68\u8ff9\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u6027\u80fd\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5de5\u5177\u96c6\u548c\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2602.20635", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20635", "abs": "https://arxiv.org/abs/2602.20635", "authors": ["Ken Nakamura", "Takayuki Nozaki"], "title": "Insertion Correcting Capability for Quantum Deletion-Correcting Codes", "comment": "9 pages, submitted to Physical Review A", "summary": "This paper proves that any quantum t-deletion-correcting codes also correct a total of t insertion and deletion errors under a certain condition. Here, this condition is that a set of quantum states is defined as a quantum error-correcting code if the error spheres of its states are disjoint, as classical coding theory. In addition, this paper proposes the quantum indel distance and describes insertion and deletion errors correcting capability of quantum codes by this distance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u91cf\u5b50t-\u5220\u9664\u7ea0\u9519\u7801\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u80fd\u7ea0\u6b63\u603b\u5171t\u4e2a\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5b50indel\u8ddd\u79bb\u6765\u63cf\u8ff0\u91cf\u5b50\u7801\u7684\u63d2\u5165\u5220\u9664\u7ea0\u9519\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u7801\u5bf9\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\u7684\u7ea0\u9519\u80fd\u529b\uff0c\u6269\u5c55\u91cf\u5b50\u7ea0\u9519\u7801\u7684\u5e94\u7528\u8303\u56f4\uff0c\u89e3\u51b3\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u63d2\u5165\u5220\u9664\u9519\u8bef\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u91cf\u5b50\u9519\u8bef\u7403\u4e0d\u76f8\u4ea4\u7684\u6761\u4ef6\uff0c\u8bc1\u660e\u91cf\u5b50t-\u5220\u9664\u7ea0\u9519\u7801\u4e5f\u80fd\u7ea0\u6b63\u603b\u5171t\u4e2a\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff1b\u63d0\u51fa\u91cf\u5b50indel\u8ddd\u79bb\u6982\u5ff5\u6765\u63cf\u8ff0\u91cf\u5b50\u7801\u7684\u63d2\u5165\u5220\u9664\u7ea0\u9519\u80fd\u529b\u3002", "result": "\u8bc1\u660e\u4e86\u91cf\u5b50t-\u5220\u9664\u7ea0\u9519\u7801\u5728\u9519\u8bef\u7403\u4e0d\u76f8\u4ea4\u7684\u6761\u4ef6\u4e0b\u80fd\u591f\u7ea0\u6b63\u603b\u5171t\u4e2a\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff1b\u5efa\u7acb\u4e86\u91cf\u5b50indel\u8ddd\u79bb\u4e0e\u7ea0\u9519\u80fd\u529b\u7684\u5173\u7cfb\u3002", "conclusion": "\u91cf\u5b50\u7ea0\u9519\u7801\u4e0d\u4ec5\u80fd\u591f\u7ea0\u6b63\u5220\u9664\u9519\u8bef\uff0c\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u4e5f\u80fd\u7ea0\u6b63\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff0c\u91cf\u5b50indel\u8ddd\u79bb\u4e3a\u8bc4\u4f30\u91cf\u5b50\u7801\u7684\u63d2\u5165\u5220\u9664\u7ea0\u9519\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.20459", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20459", "abs": "https://arxiv.org/abs/2602.20459", "authors": ["Anirudh Ajith", "Amanpreet Singh", "Jay DeYoung", "Nadav Kunievsky", "Austin C. Kozlowski", "Oyvind Tafjord", "James Evans", "Daniel S. Weld", "Tom Hope", "Doug Downey"], "title": "PreScience: A Benchmark for Forecasting Scientific Contributions", "comment": "10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm", "summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.", "AI": {"tldr": "PreScience\u662f\u4e00\u4e2a\u79d1\u5b66\u9884\u6d4b\u57fa\u51c6\uff0c\u5c06\u7814\u7a76\u8fc7\u7a0b\u5206\u89e3\u4e3a\u56db\u4e2a\u751f\u6210\u4efb\u52a1\uff1a\u5408\u4f5c\u8005\u9884\u6d4b\u3001\u5148\u524d\u5de5\u4f5c\u9009\u62e9\u3001\u8d21\u732e\u751f\u6210\u548c\u5f71\u54cd\u529b\u9884\u6d4b\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u80fd\u5426\u9884\u6d4b\u79d1\u5b66\u8fdb\u5c55\u3002", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u80fd\u5426\u57fa\u4e8e\u5386\u53f2\u79d1\u5b66\u8bb0\u5f55\u9884\u6d4b\u672a\u6765\u7684\u79d1\u5b66\u8fdb\u5c55\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc6\u522b\u5408\u4f5c\u8005\u3001\u53d1\u73b0\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u9884\u6d4b\u672a\u6765\u5c06\u6210\u4e3a\u6838\u5fc3\u7684\u95ee\u9898\u548c\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b98K\u7bc7AI\u76f8\u5173\u8bba\u6587\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6d88\u6b67\u7684\u4f5c\u8005\u8eab\u4efd\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u5b66\u672f\u5143\u6570\u636e\uff0c\u4ee5\u53ca\u5305\u542b502K\u7bc7\u8bba\u6587\u7684\u4f5c\u8005\u53d1\u8868\u5386\u53f2\u548c\u5f15\u7528\u56fe\u3002\u4e3a\u56db\u4e2a\u4efb\u52a1\u5f00\u53d1\u4e86\u57fa\u7ebf\u65b9\u6cd5\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u65b0\u7684LLM-based\u8d21\u732e\u76f8\u4f3c\u5ea6\u5ea6\u91cfLACERScore\u3002", "result": "\u524d\u6cbfLLM\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff08\u5982\u8d21\u732e\u751f\u6210\u4efb\u52a1\u4e2dGPT-5\u5e73\u5747\u5f97\u52065.6/10\uff09\u300212\u4e2a\u6708\u7684\u7aef\u5230\u7aef\u6a21\u62df\u4ea7\u751f\u7684\u5408\u6210\u8bed\u6599\u5e93\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u7cfb\u7edf\u6027\u5730\u4f4e\u4e8e\u540c\u671f\u4eba\u7c7b\u64b0\u5199\u7684\u7814\u7a76\u3002", "conclusion": "AI\u7cfb\u7edf\u76ee\u524d\u8fd8\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u79d1\u5b66\u8fdb\u5c55\uff0c\u5408\u6210\u7814\u7a76\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u4e0a\u4e0d\u5982\u4eba\u7c7b\u7814\u7a76\uff0c\u8868\u660e\u79d1\u5b66\u9884\u6d4b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46PreScience\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.20908", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20908", "abs": "https://arxiv.org/abs/2602.20908", "authors": ["Han Yu", "Jiajun He", "Xinping Yi", "Feng Yin", "Hing Cheung So", "Giuseppe Caire"], "title": "Topology-Aware Integrated Communication, Sensing, and Power Transfer for SAGIN", "comment": "The paper has been accepted by IEEE ICC 2026", "summary": "The space-air-ground integrated network (SAGIN) has garnered significant attention in recent years due to its capability to extend communication networks from terrestrial environments to near-ground and space contexts. The application of SAGIN enables to achieve a high-quality, multi-functional, and complex communication requirements, which are essential for sixth-generation communication systems. This paper presents a topology aware (TA) framework to leverage the topological structure in SAGIN to address the multi-functional communication challenge, particularly the integrated sensing, communication, and power transfer (ISCPT) problem. To take advantage of the topological structure, we initially establish the topology according to the criteria of visibility and channel strength. The ISCPT problem can be reformulated into a topological structure as a mixed integer linear program, providing valuable insights from the objectives and constraints. Results demonstrate the superior performance of our solution compared to the benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u6846\u67b6\uff0c\u5229\u7528SAGIN\u7684\u62d3\u6251\u7ed3\u6784\u89e3\u51b3\u591a\u529f\u80fd\u901a\u4fe1\u6311\u6218\uff0c\u7279\u522b\u662f\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u80fd\u91cf\u4f20\u8f93\u95ee\u9898\u3002", "motivation": "SAGIN\u80fd\u591f\u5c06\u901a\u4fe1\u7f51\u7edc\u4ece\u5730\u9762\u73af\u5883\u6269\u5c55\u5230\u8fd1\u5730\u548c\u7a7a\u95f4\u73af\u5883\uff0c\u6ee1\u8db36G\u7cfb\u7edf\u5bf9\u9ad8\u8d28\u91cf\u3001\u591a\u529f\u80fd\u3001\u590d\u6742\u901a\u4fe1\u7684\u9700\u6c42\u3002\u9700\u8981\u89e3\u51b3SAGIN\u4e2d\u7684\u591a\u529f\u80fd\u901a\u4fe1\u6311\u6218\uff0c\u7279\u522b\u662f\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u80fd\u91cf\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u62d3\u6251\u611f\u77e5\u6846\u67b6\uff0c\u9996\u5148\u6839\u636e\u53ef\u89c1\u6027\u548c\u4fe1\u9053\u5f3a\u5ea6\u6807\u51c6\u5efa\u7acb\u62d3\u6251\u7ed3\u6784\uff0c\u7136\u540e\u5c06ISCPT\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u62d3\u6251\u611f\u77e5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528SAGIN\u7684\u62d3\u6251\u7ed3\u6784\u89e3\u51b3\u591a\u529f\u80fd\u901a\u4fe1\u6311\u6218\uff0c\u4e3a6G\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20494", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20494", "abs": "https://arxiv.org/abs/2602.20494", "authors": ["Haotian Si", "Changhua Pei", "Xiao He", "Zeyan Li", "Zhe Xie", "Zexin Wang", "Jiyao Hu", "Zhaoyang Yu", "Tieying Zhang", "Dan Pei", "Jianhui Li", "Gaogang Xie"], "title": "KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning", "comment": null, "summary": "Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.", "AI": {"tldr": "\u63d0\u51faSemantic-Conditional Time Series Reasoning\u4efb\u52a1\u548cKairosVL\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u63d0\u5347\u590d\u6742\u51b3\u7b56\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u5efa\u6a21\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u7406\u89e3\u8fdb\u884c\u51b3\u7b56\u3002\u4e3a\u6ee1\u8db3\u65e5\u76ca\u590d\u6742\u7684\u51b3\u7b56\u5bfc\u5411\u9700\u6c42\uff0c\u9700\u8981\u6269\u5c55\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u8f6e\u589e\u5f3a\u6a21\u578b\u5bf9\u57fa\u672c\u65f6\u95f4\u539f\u8bed\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7b2c\u4e8c\u8f6e\u4e13\u6ce8\u4e8e\u8bed\u4e49\u6761\u4ef6\u63a8\u7406\u3002\u7531\u6b64\u6784\u5efaKairosVL\u6a21\u578b\u3002", "result": "KairosVL\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u80fd\u4fdd\u6301\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u5efa\u6a21\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u6ee1\u8db3\u4e86\u5f53\u524d\u5bf9\u590d\u6742\u51b3\u7b56\u652f\u6301\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2602.20993", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20993", "abs": "https://arxiv.org/abs/2602.20993", "authors": ["Jiajun He", "Han Yu", "Yiran Guo", "Xinping Yi", "Fan Liu", "Hing Cheung So", "Hien Quoc Ngo", "Michail Matthaiou", "Giuseppe Caire"], "title": "Topology-Aware Coordination for Multi-Functional Low-Altitude Wireless Networks", "comment": "The paper has been submitted to IEEE Communication Magazine", "summary": "Low-altitude wireless networks (LAWNs) are expected to consist of multi-tier, heterogeneous terrestrial and non-terrestrial devices, where effective coordination is essential to fully unlock the complementary capabilities of diverse systems from different vendors. To address this issue, we propose a novel multi-functional coordination framework that enables seamless cooperation within the LAWN while supporting efficient execution of diverse network functions. In the proposed architecture, each device or infrastructure element is assigned to a specific functional role, namely, edge mobile terminal (E-MT), distributed MT (D-MT), or computing center. E-MTs are equipped with lightweight, independent signal processing and computing capabilities, while D-MTs and the computing center handle regional and global coordination, respectively. To enhance the overall network efficiency, we model the LAWN as a sparse graph, where nodes represent network nodes and edges are defined according to a set of controllable connection rules. This topology-aware (TA) representation allows for efficiently solving various coordination tasks across the network. Numerical results show that the proposed TA coordination framework outperforms baseline approaches that lack topological insights, achieving higher efficiency in multi-task coordination. Finally, we discuss key technical challenges and outline potential solutions for future deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u7684\u591a\u529f\u80fd\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u7a00\u758f\u56fe\u5efa\u6a21\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u534f\u8c03", "motivation": "\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u7531\u591a\u5c42\u5f02\u6784\u7684\u5730\u9762\u548c\u975e\u5730\u9762\u8bbe\u5907\u7ec4\u6210\uff0c\u9700\u8981\u6709\u6548\u7684\u534f\u8c03\u673a\u5236\u6765\u5145\u5206\u53d1\u6325\u4e0d\u540c\u5382\u5546\u7cfb\u7edf\u7684\u4e92\u8865\u80fd\u529b", "method": "\u63d0\u51fa\u591a\u529f\u80fd\u534f\u8c03\u6846\u67b6\uff0c\u5c06\u8bbe\u5907\u5206\u4e3aE-MT\u3001D-MT\u548c\u8ba1\u7b97\u4e2d\u5fc3\u4e09\u7c7b\u529f\u80fd\u89d2\u8272\uff0c\u5e76\u5c06\u7f51\u7edc\u5efa\u6a21\u4e3a\u7a00\u758f\u56fe\uff0c\u57fa\u4e8e\u53ef\u63a7\u8fde\u63a5\u89c4\u5219\u5b9a\u4e49\u8fb9\uff0c\u5b9e\u73b0\u62d3\u6251\u611f\u77e5\u7684\u534f\u8c03", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u62d3\u6251\u611f\u77e5\u534f\u8c03\u6846\u67b6\u5728\u591a\u9879\u534f\u8c03\u4efb\u52a1\u4e2d\u6bd4\u7f3a\u4e4f\u62d3\u6251\u6d1e\u5bdf\u7684\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u591a\u4efb\u52a1\u534f\u8c03\u6548\u7387", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u534f\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6280\u672f\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20502", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20502", "abs": "https://arxiv.org/abs/2602.20502", "authors": ["Hongbin Zhong", "Fazle Faisal", "Luis Fran\u00e7a", "Tanakorn Leesatapornwongsa", "Adriana Szekeres", "Kexin Rong", "Suman Nath"], "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory", "comment": null, "summary": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages.\n  We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution.\n  To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory.\n  This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x.\n  Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.", "AI": {"tldr": "ActionEngine\uff1a\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff08\u722c\u866b\u667a\u80fd\u4f53\u6784\u5efa\u72b6\u6001\u673a\u8bb0\u5fc6\uff0c\u6267\u884c\u667a\u80fd\u4f53\u5408\u6210Python\u7a0b\u5e8f\uff09\u5b9e\u73b0GUI\u4ea4\u4e92\u4ece\u9010\u6b65\u6267\u884c\u5230\u7a0b\u5e8f\u5316\u89c4\u5212\u7684\u8f6c\u53d8\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u91c7\u7528\u9010\u6b65\u8c03\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u5b58\u5728\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\u3001\u51c6\u786e\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5df2\u8bbf\u95ee\u9875\u9762\u7684\u6301\u4e45\u8bb0\u5fc6\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09\u722c\u866b\u667a\u80fd\u4f53\u901a\u8fc7\u79bb\u7ebf\u63a2\u7d22\u6784\u5efa\u53ef\u66f4\u65b0\u7684GUI\u72b6\u6001\u673a\u8bb0\u5fc6\uff1b2\uff09\u6267\u884c\u667a\u80fd\u4f53\u5229\u7528\u8be5\u8bb0\u5fc6\u5408\u6210\u5b8c\u6574\u7684\u53ef\u6267\u884cPython\u7a0b\u5e8f\u8fdb\u884c\u5728\u7ebf\u4efb\u52a1\u6267\u884c\uff1b3\uff09\u6267\u884c\u5931\u8d25\u65f6\u89e6\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u91cd\u5b9a\u4f4d\u56de\u9000\u673a\u5236\u4fee\u590d\u52a8\u4f5c\u5e76\u66f4\u65b0\u8bb0\u5fc6\u3002", "result": "\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u7684Reddit\u4efb\u52a1\u4e2d\uff0c\u8fbe\u523095%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e73\u5747\u4ec5\u97001\u6b21LLM\u8c03\u7528\uff0c\u76f8\u6bd4\u6700\u5f3a\u89c6\u89c9\u57fa\u7ebf\uff0866%\u6210\u529f\u7387\uff09\u6210\u672c\u964d\u4f4e11.8\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u51cf\u5c112\u500d\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u7a0b\u5e8f\u5316\u89c4\u5212\u3001\u722c\u866b\u9a8c\u8bc1\u7684\u52a8\u4f5c\u6a21\u677f\u4ee5\u53ca\u8282\u70b9\u7ea7\u6267\u884c\u4e0e\u5c40\u90e8\u9a8c\u8bc1\u4fee\u590d\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684GUI\u4ea4\u4e92\u3002"}}
{"id": "2602.21114", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21114", "abs": "https://arxiv.org/abs/2602.21114", "authors": ["Tianyu Lu", "Jiajun He", "Mohammadali Mohammadi", "Michail Matthaiou"], "title": "Delay Alignment Modulation for Secure ISAC Systems", "comment": null, "summary": "This paper introduces delay-alignment modulation (DAM) for secure integrated sensing and communication (ISAC). Due to the broadcast nature of multi-user downlinks, communications are vulnerable to eavesdropping. DAM applies controlled per-path symbol delays at the transmitter to coherently align the multipath components at the intended user, enhancing the received signal power, while simultaneously creating delay misalignment at the eavesdropper (Eve). To mitigate sensing degradation caused by multipath propagation, we propose a two-stage protocol that first estimates the angle and then the delay of the line-of-sight (LoS) path after suppressing multipath interference. We derive the secrecy spectral efficiency (SSE) and the Cramer-Rao (CRB) of the target delay. Finally, we develop a path-based zero-forcing (ZF) precoding framework and formulate a max-min SSE design under CRB and power constraints. Simulation results show DAM significantly outperforms the strongest-path (SP) benchmark in terms of SSE, while meeting sensing requirements, since intentional delay alignment at legitimate users degrades reception at Eve.", "AI": {"tldr": "DAM\u6280\u672f\u901a\u8fc7\u63a7\u5236\u591a\u5f84\u65f6\u5ef6\u5bf9\u9f50\u6765\u589e\u5f3a\u5408\u6cd5\u7528\u6237\u4fe1\u53f7\uff0c\u540c\u65f6\u4f7f\u7a83\u542c\u8005\u4fe1\u53f7\u5931\u914d\uff0c\u5b9e\u73b0\u5b89\u5168ISAC", "motivation": "\u591a\u7528\u6237\u4e0b\u884c\u94fe\u8def\u7684\u5e7f\u64ad\u7279\u6027\u4f7f\u901a\u4fe1\u6613\u53d7\u7a83\u542c\uff0c\u540c\u65f6\u591a\u5f84\u4f20\u64ad\u4f1a\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u589e\u5f3a\u901a\u4fe1\u5b89\u5168\u548c\u611f\u77e5\u8d28\u91cf\u7684\u6280\u672f", "method": "\u63d0\u51fa\u5ef6\u8fdf\u5bf9\u9f50\u8c03\u5236(DAM)\uff0c\u5728\u53d1\u5c04\u7aef\u63a7\u5236\u6bcf\u5f84\u7b26\u53f7\u5ef6\u8fdf\uff0c\u4f7f\u591a\u5f84\u5728\u5408\u6cd5\u7528\u6237\u5904\u76f8\u5e72\u5bf9\u9f50\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u534f\u8bae\u5148\u4f30\u8ba1\u89d2\u5ea6\u518d\u6291\u5236\u591a\u5f84\u5e72\u6270\u540e\u4f30\u8ba1LoS\u8def\u5f84\u5ef6\u8fdf\uff1b\u5f00\u53d1\u57fa\u4e8e\u8def\u5f84\u7684ZF\u9884\u7f16\u7801\u6846\u67b6\uff0c\u5728CRB\u548c\u529f\u7387\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u6700\u5c0fSSE", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aDAM\u5728SSE\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5f3a\u5f84\u57fa\u51c6\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u8981\u6c42\uff0c\u56e0\u4e3a\u6545\u610f\u5ef6\u8fdf\u5bf9\u9f50\u4f1a\u964d\u4f4e\u7a83\u542c\u8005\u7684\u63a5\u6536\u8d28\u91cf", "conclusion": "DAM\u901a\u8fc7\u63a7\u5236\u591a\u5f84\u65f6\u5ef6\u5bf9\u9f50\u6709\u6548\u589e\u5f3a\u4e86ISAC\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u901a\u4fe1\u4e0e\u611f\u77e5\u4e00\u4f53\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20517", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20517", "abs": "https://arxiv.org/abs/2602.20517", "authors": ["Rakshit Trivedi", "Kartik Sharma", "David C Parkes"], "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination", "comment": "Spotlight paper at NeurIPS 2025", "summary": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.", "AI": {"tldr": "MIMIC\u6846\u67b6\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u7684\u5185\u90e8\u8868\u793a\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u5185\u90e8\u8bed\u97f3\uff0c\u518d\u7ed3\u5408\u6269\u6563\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u884c\u4e3a\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u591a\u6837\u6027\u548c\u4eba\u7c7b\u6f14\u793a\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u7279\u6027\uff0c\u4e14\u7f3a\u4e4f\u63a8\u7406\u65f6\u7684\u884c\u4e3a\u5f15\u5bfc\u80fd\u529b\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u5185\u90e8\u8bed\u97f3\u6307\u5bfc\u884c\u52a8\u9009\u62e9\u7684\u7406\u8bba\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u5185\u90e8\u8868\u793a\u3001\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u884c\u4e3a\u63a7\u5236\u7684\u667a\u80fd\u4f53\u3002", "method": "\u63d0\u51faMIMIC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bed\u8a00\u652f\u67b6\u8bad\u7ec3\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ece\u89c2\u5bdf\u4e2d\u751f\u6210\u5185\u90e8\u8bed\u97f3\uff1b2\uff09\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\uff0c\u6839\u636e\u5f53\u524d\u89c2\u5bdf\u548c\u751f\u6210\u7684\u5185\u90e8\u8bed\u97f3\u9009\u62e9\u884c\u52a8\uff1b3\uff09\u63a8\u7406\u65f6\u901a\u8fc7\u7279\u5b9a\u884c\u4e3a\u8bed\u97f3\u6761\u4ef6\u5316\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5f15\u5bfc\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u548c\u4eba\u673a\u534f\u4f5c\u6e38\u620f\u4e2d\uff0cMIMIC\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u591a\u6837\u6027\u548c\u4eba\u7c7b\u6f14\u793a\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u7ec6\u81f4\u7684\u884c\u4e3a\u5f15\u5bfc\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u5e76\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6f14\u793a\u3002", "conclusion": "MIMIC\u901a\u8fc7\u5c06\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u7684\u5185\u90e8\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u884c\u4e3a\u591a\u6837\u6027\u548c\u63a8\u7406\u65f6\u5f15\u5bfc\u7684\u6311\u6218\uff0c\u4e3a\u4eba\u673a\u534f\u8c03\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u3001\u53ef\u63a7\u7684\u667a\u80fd\u4f53\u3002"}}
{"id": "2602.21146", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21146", "abs": "https://arxiv.org/abs/2602.21146", "authors": ["Wenlong Wang", "Tianyang Zhang", "Tailun Dong", "Lei Zhang"], "title": "TCDA: Robust 2D-DOA Estimation for Defective L-Shaped Arrays", "comment": "5 pages, 2 figures", "summary": "While tensor-based methods excel at Direction-of-Arrival (DOA) estimation, their performance degrades severely with faulty or sparse arrays that violate the required manifold structure. To address this challenge, we propose Tensor Completion for Defective Arrays (TCDA), a robust algorithm that reformulates the physical imperfection problem as a data recovery task within a virtual tensor space. We present a detailed derivation for constructing an incomplete third-order Parallel Factor Analysis (PARAFAC) tensor from the faulty array signals via subarray partitioning, cross-correlation, and dimensional reshaping. Leveraging the tensor's inherent low-rank structure, an Alternating Least Squares (ALS)-based algorithm directly recovers the factor matrices embedding the DOA parameters from the incomplete observations. This approach provides a software-defined 'self-healing' capability, demonstrating exceptional robustness against random element failures without requiring additional processing steps for DOA estimation.", "AI": {"tldr": "\u63d0\u51faTCDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u8865\u5168\u5904\u7406\u6545\u969c\u9635\u5217\u7684DOA\u4f30\u8ba1\u95ee\u9898\uff0c\u5c06\u7269\u7406\u7f3a\u9677\u8f6c\u5316\u4e3a\u865a\u62df\u5f20\u91cf\u7a7a\u95f4\u7684\u6570\u636e\u6062\u590d\u4efb\u52a1", "motivation": "\u57fa\u4e8e\u5f20\u91cf\u7684DOA\u4f30\u8ba1\u7b97\u6cd5\u5728\u9635\u5217\u5b58\u5728\u6545\u969c\u6216\u7a00\u758f\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u60c5\u51b5\u8fdd\u53cd\u4e86\u6240\u9700\u7684\u6d41\u5f62\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u9635\u5217\u7f3a\u9677\u7684\u9c81\u68d2\u65b9\u6cd5", "method": "1. \u901a\u8fc7\u5b50\u9635\u5212\u5206\u3001\u4e92\u76f8\u5173\u548c\u7ef4\u5ea6\u91cd\u5851\u6784\u5efa\u4e0d\u5b8c\u6574\u7684\u4e09\u9636PARAFAC\u5f20\u91cf\uff1b2. \u5229\u7528\u5f20\u91cf\u7684\u5185\u5728\u4f4e\u79e9\u7ed3\u6784\uff0c\u57fa\u4e8e\u4ea4\u66ff\u6700\u5c0f\u4e8c\u4e58(ALS)\u7b97\u6cd5\u76f4\u63a5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u6062\u590d\u5305\u542bDOA\u53c2\u6570\u7684\u56e0\u5b50\u77e9\u9635", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f6f\u4ef6\u5b9a\u4e49\u7684\"\u81ea\u6108\"\u80fd\u529b\uff0c\u5bf9\u968f\u673a\u9635\u5143\u6545\u969c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u7684DOA\u4f30\u8ba1\u5904\u7406\u6b65\u9aa4", "conclusion": "TCDA\u7b97\u6cd5\u6210\u529f\u5c06\u9635\u5217\u7269\u7406\u7f3a\u9677\u95ee\u9898\u8f6c\u5316\u4e3a\u5f20\u91cf\u8865\u5168\u4efb\u52a1\uff0c\u901a\u8fc7\u865a\u62df\u5f20\u91cf\u7a7a\u95f4\u7684\u6570\u636e\u6062\u590d\u5b9e\u73b0\u4e86\u5bf9\u6545\u969c\u9635\u5217\u7684\u9c81\u68d2DOA\u4f30\u8ba1"}}
{"id": "2602.20558", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20558", "abs": "https://arxiv.org/abs/2602.20558", "authors": ["Yucheng Shi", "Ying Li", "Yu Wang", "Yesu Feng", "Arjun Rao", "Rein Houthooft", "Shradha Sehgal", "Jin Wang", "Hao Zhen", "Ninghao Liu", "Linas Baltrunas"], "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production", "comment": "Work in progress", "summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u5b66\u4e60\u5c06\u7ed3\u6784\u5316\u7528\u6237\u4ea4\u4e92\u65e5\u5fd7\u8f6c\u5316\u4e3a\u4f18\u5316\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u7cfb\u7edf\u4f7f\u7528\u50f5\u5316\u7684\u6a21\u677f\u7b80\u5355\u62fc\u63a5\u5b57\u6bb5\uff0c\u5bfc\u81f4\u6b21\u4f18\u8868\u793a\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8f6c\u6362\u65b9\u6cd5", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3verbalization agent\uff0c\u5c06\u539f\u59cb\u4ea4\u4e92\u5386\u53f2\u8f6c\u5316\u4e3a\u4f18\u5316\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63a8\u8350\u51c6\u786e\u6027\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5b66\u4e60\u8fc7\u6ee4\u566a\u58f0\u3001\u6574\u5408\u5143\u6570\u636e\u3001\u91cd\u7ec4\u4fe1\u606f", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\uff0c\u5b66\u4e60\u5230\u7684verbalization\u76f8\u6bd4\u6a21\u677f\u57fa\u7ebf\u5728\u53d1\u73b0\u9879\u76ee\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u83b7\u5f9793%\u7684\u76f8\u5bf9\u63d0\u5347", "conclusion": "\u5b66\u4e60verbalization\u662f\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\uff0c\u6846\u67b6\u63ed\u793a\u4e86\u7528\u6237\u5174\u8da3\u603b\u7ed3\u3001\u566a\u58f0\u53bb\u9664\u3001\u8bed\u6cd5\u6807\u51c6\u5316\u7b49\u6709\u6548\u7b56\u7565\uff0c\u4e3aLLM\u63a8\u8350\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u63d0\u4f9b\u65b0\u89c1\u89e3"}}
{"id": "2602.21162", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21162", "abs": "https://arxiv.org/abs/2602.21162", "authors": ["Hao Feng", "Ebrahim Bedeer", "Ming Zeng", "Xingwang Li", "Shimin Gong", "Quoc-Viet Pham"], "title": "Phase-Aware Localization in Pinching Antenna Systems: CRLB Analysis and ML Estimation", "comment": "4 pages, 2 figures; submitted to IEEE journals", "summary": "Pinching antenna systems (PASS) have recently emerged as a promising architecture for high-frequency wireless communications. In this letter, we investigate localization in PASS by jointly exploiting the received signal amplitude and phase information, unlike recent works that consider only the amplitude information. A complex baseband signal model is formulated to capture free-space path loss, waveguide attenuation, and distance-dependent phase rotation between the user and each pinching antenna. Using this model, we derive the Fisher information matrix (FIM) with respect to the user location and obtain closed-form expressions for the Cramer-Rao lower bound (CRLB) and the position error bound (PEB). A maximum likelihood (ML) estimator that jointly considers the received signal amplitude and phase is developed to estimate the unknown user location. Given the non-convexity of the estimation problem, a two-stage solution combining coarse grid search and Levenberg-Marquardt refinement is proposed. Numerical results demonstrate that the proposed phase-aware estimator outperforms existing amplitude-only method in terms of positioning accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e2d\u5229\u7528\u63a5\u6536\u4fe1\u53f7\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u8fdb\u884c\u8054\u5408\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u73b0\u6709\u4ec5\u4f7f\u7528\u5e45\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u662f\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u7684\u6709\u524d\u666f\u67b6\u6784\uff0c\u4f46\u73b0\u6709\u5b9a\u4f4d\u65b9\u6cd5\u4ec5\u5229\u7528\u5e45\u5ea6\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u76f8\u4f4d\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u9700\u8981\u5f00\u53d1\u540c\u65f6\u5229\u7528\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u4e86\u5305\u542b\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\u3001\u6ce2\u5bfc\u8870\u51cf\u548c\u8ddd\u79bb\u76f8\u5173\u76f8\u4f4d\u65cb\u8f6c\u7684\u590d\u57fa\u5e26\u4fe1\u53f7\u6a21\u578b\u3002\u63a8\u5bfc\u4e86\u7528\u6237\u4f4d\u7f6e\u7684Fisher\u4fe1\u606f\u77e9\u9635\u548cCRLB/PEB\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u5f00\u53d1\u4e86\u8054\u5408\u8003\u8651\u5e45\u5ea6\u548c\u76f8\u4f4d\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u7c97\u7f51\u683c\u641c\u7d22+Levenberg-Marquardt\u4f18\u5316\uff09\u89e3\u51b3\u975e\u51f8\u4f30\u8ba1\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u76f8\u4f4d\u611f\u77e5\u4f30\u8ba1\u5668\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u4ec5\u5e45\u5ea6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u76f8\u4f4d\u4fe1\u606f\u5bf9\u63d0\u9ad8\u5b9a\u4f4d\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5728\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e2d\u8054\u5408\u5229\u7528\u63a5\u6536\u4fe1\u53f7\u7684\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u7cbe\u786e\u7528\u6237\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20571", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20571", "abs": "https://arxiv.org/abs/2602.20571", "authors": ["Ayush Sawarni", "Jiyuan Tan", "Vasilis Syrgkanis"], "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation", "comment": null, "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86CausalReasoningBenchmark\uff0c\u4e00\u4e2a\u5305\u542b173\u4e2a\u67e5\u8be2\u3001138\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u56e0\u679c\u63a8\u7406\u57fa\u51c6\uff0c\u5c06\u8bc6\u522b\uff08\u7814\u7a76\u8bbe\u8ba1\uff09\u4e0e\u4f30\u8ba1\uff08\u6570\u503c\u8ba1\u7b97\uff09\u5206\u5f00\u8bc4\u4f30\uff0c\u63ed\u793a\u5f53\u524dLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u74f6\u9888\u5728\u4e8e\u7814\u7a76\u8bbe\u8ba1\u7684\u7ec6\u8282\u800c\u975e\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u63a8\u7406\u57fa\u51c6\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u6570\u503c\u8f93\u51fa\uff08\u5982\u5e73\u5747\u5904\u7406\u6548\u5e94ATE\uff09\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u8fd9\u6df7\u6dc6\u4e86\u56e0\u679c\u5206\u6790\u7684\u4e24\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u8bc6\u522b\uff08\u5728\u7ed9\u5b9a\u5047\u8bbe\u4e0b\u5236\u5b9a\u6709\u6548\u7814\u7a76\u8bbe\u8ba1\uff09\u548c\u4f30\u8ba1\uff08\u5728\u6709\u9650\u6570\u636e\u4e0a\u6570\u503c\u5b9e\u73b0\u8be5\u8bbe\u8ba1\uff09\u3002\u9700\u8981\u533a\u5206\u8fd9\u4e24\u79cd\u80fd\u529b\u4ee5\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u8bca\u65ad\u3002", "method": "\u6784\u5efa\u4e86CausalReasoningBenchmark\u57fa\u51c6\uff0c\u5305\u542b173\u4e2a\u67e5\u8be2\uff0c\u8986\u76d6138\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u6765\u6e90\u4e8e85\u7bc7\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u8bba\u6587\u548c4\u672c\u5e7f\u6cdb\u4f7f\u7528\u7684\u56e0\u679c\u63a8\u7406\u6559\u6750\u3002\u5bf9\u6bcf\u4e2a\u67e5\u8be2\uff0c\u7cfb\u7edf\u5fc5\u987b\u8f93\u51fa\uff1a(1)\u7ed3\u6784\u5316\u8bc6\u522b\u89c4\u8303\uff08\u7b56\u7565\u3001\u5904\u7406\u3001\u7ed3\u679c\u3001\u63a7\u5236\u53d8\u91cf\u7b49\u8bbe\u8ba1\u8981\u7d20\uff09\uff0c(2)\u70b9\u4f30\u8ba1\u548c\u6807\u51c6\u8bef\u3002\u5206\u522b\u5bf9\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u57fa\u7ebf\u6d4b\u8bd5\u663e\u793a\uff1a\u6a21\u578b\u572884%\u7684\u60c5\u51b5\u4e0b\u80fd\u6b63\u786e\u8bc6\u522b\u9ad8\u5c42\u6b21\u7b56\u7565\uff0c\u4f46\u5b8c\u6574\u7684\u8bc6\u522b\u89c4\u8303\u6b63\u786e\u7387\u4ec530%\uff0c\u8868\u660e\u74f6\u9888\u5728\u4e8e\u7814\u7a76\u8bbe\u8ba1\u7684\u7ec6\u8282\u800c\u975e\u6570\u503c\u8ba1\u7b97\u3002\u57fa\u51c6\u5df2\u5728Hugging Face\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "CausalReasoningBenchmark\u901a\u8fc7\u5206\u79bb\u8bc6\u522b\u548c\u8bc4\u4f30\uff0c\u80fd\u591f\u533a\u5206\u56e0\u679c\u63a8\u7406\u5931\u8d25\u548c\u6570\u503c\u6267\u884c\u9519\u8bef\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u81ea\u52a8\u5316\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\u3002\u5f53\u524dLLM\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u7814\u7a76\u8bbe\u8ba1\u7684\u7ec6\u8282\u628a\u63e1\u800c\u975e\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2602.21167", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21167", "abs": "https://arxiv.org/abs/2602.21167", "authors": ["Hao Feng", "Ming Zeng", "Ebrahim Bedeer", "Xingwang Li", "Octavia A. Dobre", "Zhiguo Ding"], "title": "Wireless-Fed Pinching-Antenna Systems with Horn Antennas", "comment": "4 pages; 1 figure; submitted to IEEE journals", "summary": "Pinching-antenna systems have recently emerged as a promising solution for enhancing coverage in high-frequency wireless communications by guiding signals through dielectric waveguides and radiating them via position-adjustable antennas. However, their practical deployment is fundamentally constrained by waveguide attenuation and line-installation requirements, which limit the achievable coverage range. To address this challenge, this paper investigates a wireless-fed pinching-antenna architecture that employs highly directional horn antennas to enable efficient coverage extension. Specifically, a full-duplex amplify-and-forward relay equipped with horn antennas is introduced between the base station and the waveguide input, which significantly improves the link budget in high-frequency bands while effectively eliminating self-interference. On this basis, we formulate a total power minimization problem subject to a quality-of-service constraint at the user equipment, involving the joint optimization of the pinching-antenna position, the relay amplification gain, and the base station transmit power. By exploiting the structure of the end-to-end signal-to-noise ratio, the optimal pinching-antenna position is first obtained in closed form by balancing waveguide attenuation and free-space path loss. Subsequently, closed-form expressions for the optimal relay gain and transmit power are derived. Numerical results demonstrate that the proposed scheme substantially outperforms conventional systems without relaying and relay-assisted transmission with fixed antennas in terms of total power consumption.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\uff0c\u901a\u8fc7\u9ad8\u5b9a\u5411\u6027\u5587\u53ed\u5929\u7ebf\u548c\u5168\u53cc\u5de5\u653e\u5927\u8f6c\u53d1\u4e2d\u7ee7\uff0c\u89e3\u51b3\u9ad8\u9891\u901a\u4fe1\u4e2d\u6ce2\u5bfc\u8870\u51cf\u548c\u7ebf\u8def\u5b89\u88c5\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u8986\u76d6\u6269\u5c55\u3002", "motivation": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u901a\u8fc7\u6ce2\u5bfc\u5f15\u5bfc\u4fe1\u53f7\u548c\u4f4d\u7f6e\u53ef\u8c03\u5929\u7ebf\u8f90\u5c04\uff0c\u80fd\u589e\u5f3a\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u8986\u76d6\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u6ce2\u5bfc\u8870\u51cf\u548c\u7ebf\u8def\u5b89\u88c5\u8981\u6c42\u9650\u5236\uff0c\u8986\u76d6\u8303\u56f4\u6709\u9650\u3002", "method": "\u63d0\u51fa\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\uff0c\u5728\u57fa\u7ad9\u548c\u6ce2\u5bfc\u8f93\u5165\u4e4b\u95f4\u5f15\u5165\u914d\u5907\u5587\u53ed\u5929\u7ebf\u7684\u5168\u53cc\u5de5\u653e\u5927\u8f6c\u53d1\u4e2d\u7ee7\uff0c\u663e\u8457\u6539\u5584\u9ad8\u9891\u6bb5\u94fe\u8def\u9884\u7b97\u5e76\u6d88\u9664\u81ea\u5e72\u6270\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5939\u6301\u5929\u7ebf\u4f4d\u7f6e\u3001\u4e2d\u7ee7\u653e\u5927\u589e\u76ca\u548c\u57fa\u7ad9\u53d1\u5c04\u529f\u7387\uff0c\u6700\u5c0f\u5316\u603b\u529f\u7387\u6d88\u8017\u3002", "result": "\u901a\u8fc7\u5229\u7528\u7aef\u5230\u7aef\u4fe1\u566a\u6bd4\u7ed3\u6784\uff0c\u9996\u5148\u901a\u8fc7\u5e73\u8861\u6ce2\u5bfc\u8870\u51cf\u548c\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\uff0c\u5f97\u5230\u95ed\u5f0f\u6700\u4f18\u5939\u6301\u5929\u7ebf\u4f4d\u7f6e\uff0c\u968f\u540e\u63a8\u5bfc\u51fa\u6700\u4f18\u4e2d\u7ee7\u589e\u76ca\u548c\u53d1\u5c04\u529f\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u603b\u529f\u8017\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u65e0\u4e2d\u7ee7\u7684\u4f20\u7edf\u7cfb\u7edf\u548c\u56fa\u5b9a\u5929\u7ebf\u4e2d\u7ee7\u8f85\u52a9\u4f20\u8f93\u3002", "conclusion": "\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\u901a\u8fc7\u9ad8\u5b9a\u5411\u6027\u5587\u53ed\u5929\u7ebf\u548c\u5168\u53cc\u5de5\u4e2d\u7ee7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u9891\u901a\u4fe1\u4e2d\u6ce2\u5bfc\u7cfb\u7edf\u7684\u8986\u76d6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u4f4e\u529f\u8017\u7684\u8986\u76d6\u6269\u5c55\u3002"}}
{"id": "2602.20624", "categories": ["cs.AI", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2602.20624", "abs": "https://arxiv.org/abs/2602.20624", "authors": ["Hyeongmo Kim", "Sohyun Kang", "Yerin Choi", "Seungyeon Ji", "Junhyuk Woo", "Hyunsuk Chung", "Soyeon Caren Han", "Kyungreem Han"], "title": "Physics-based phenomenological characterization of cross-modal bias in multimodal models", "comment": "Best Paper Award at BiasinAI track in AAAI2026", "summary": "The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as \"treat like cases as like\") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u73b0\u8c61\u5b66\u53ef\u89e3\u91ca\u65b9\u6cd5\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7b97\u6cd5\u516c\u5e73\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u4ee3\u7406\u6a21\u578b\u5206\u6790\u8de8\u6a21\u6001\u504f\u89c1\u52a8\u6001\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8f93\u5165\u53ef\u80fd\u5f3a\u5316\u6a21\u6001\u4e3b\u5bfc\u800c\u975e\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u590d\u6742\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u52a8\u6001\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u6027\u504f\u89c1\u3002\u4f20\u7edf\u8ba4\u77e5\u7b26\u53f7\u65b9\u6cd5\u6216\u5f62\u800c\u4e0a\u5b66\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8fd9\u4e9b\u504f\u89c1\u52a8\u6001\uff0c\u9700\u8981\u73b0\u8c61\u5b66\u53ef\u89e3\u91ca\u65b9\u6cd5\u6765\u5904\u7406MLLMs\u4e2d\u7684\u7b97\u6cd5\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "1) \u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u7684\u4ee3\u7406\u6a21\u578b\u63cf\u8ff0transformer\u52a8\u6001\uff08\u8bed\u4e49\u7f51\u7edc\u7ed3\u6784\u548c\u81ea/\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u6765\u5206\u6790\u8de8\u6a21\u6001\u504f\u89c1\uff1b2) \u591a\u8f93\u5165\u8bca\u65ad\u5b9e\u9a8c\uff1aa) \u4f7f\u7528Qwen2.5-Omni\u548cGemma 3n\u8fdb\u884c\u57fa\u4e8e\u6270\u52a8\u7684\u60c5\u7eea\u5206\u7c7b\u5206\u6790\uff0cb) \u901a\u8fc7\u7269\u7406\u4ee3\u7406\u6a21\u578b\u5bf9Lorenz\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8fdb\u884c\u52a8\u6001\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2a\u67b6\u6784\u4e0d\u540c\u7684MLLMs\u4e2d\uff0c\u591a\u6a21\u6001\u8f93\u5165\u53ef\u80fd\u5f3a\u5316\u6a21\u6001\u4e3b\u5bfc\u800c\u975e\u7f13\u89e3\u504f\u89c1\uff0c\u8fd9\u901a\u8fc7\u7cfb\u7edf\u6807\u7b7e\u6270\u52a8\u4e0b\u7684\u7ed3\u6784\u5316\u9519\u8bef\u5438\u5f15\u5b50\u6a21\u5f0f\u4ee5\u53ca\u52a8\u6001\u5206\u6790\u5f97\u5230\u63ed\u793a\u3002", "conclusion": "\u73b0\u8c61\u5b66\u65b9\u6cd5\u4e3a\u5206\u6790MLLMs\u4e2d\u7684\u7b97\u6cd5\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u57fa\u4e8e\u7269\u7406\u7684\u4ee3\u7406\u6a21\u578b\u80fd\u591f\u63ed\u793a\u4f20\u7edf\u5d4c\u5165\u6216\u8868\u793a\u5c42\u9762\u5206\u6790\u65e0\u6cd5\u6355\u6349\u7684\u8de8\u6a21\u6001\u504f\u89c1\u52a8\u6001\uff0c\u591a\u6a21\u6001\u8f93\u5165\u53ef\u80fd\u610f\u5916\u5730\u5f3a\u5316\u800c\u975e\u51cf\u8f7b\u504f\u89c1\u3002"}}
{"id": "2602.20628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20628", "abs": "https://arxiv.org/abs/2602.20628", "authors": ["Nelson Gardner-Challis", "Jonathan Bostock", "Georgiy Kozhevnikov", "Morgan Sinclaire", "Joan Velja", "Alessandro Abate", "Charlie Griffin"], "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies", "comment": "66 pages, 14 figures, Preprint", "summary": "AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e0d\u53ef\u4fe1\u76d1\u63a7\uff08\u4f7f\u7528\u4e00\u4e2a\u4e0d\u53ef\u4fe1\u6a21\u578b\u76d1\u7763\u53e6\u4e00\u4e2a\uff09\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u653e\u5bbd\u4e86\u5148\u524dAI\u63a7\u5236\u7814\u7a76\u4e2d\u5173\u4e8eAI\u4e32\u8c0b\u7b56\u7565\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u7684\u4e32\u8c0b\u7b56\u7565\u5206\u7c7b\uff0c\u5e76\u6784\u5efa\u4e86\u5b89\u5168\u8bba\u8bc1\u6846\u67b6\u3002", "motivation": "\u968f\u7740AI\u81ea\u4e3b\u6027\u548c\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u9519\u4f4dAI\u53ef\u80fd\u9020\u6210\u707e\u96be\u6027\u4f24\u5bb3\u7684\u98ce\u9669\u589e\u52a0\u3002\u4e0d\u53ef\u4fe1\u76d1\u63a7\u662f\u964d\u4f4e\u98ce\u9669\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u65e0\u6cd5\u5b89\u5168\u90e8\u7f72\u9519\u4f4d\u6a21\u578b\u76f4\u63a5\u6d4b\u8bd5\u534f\u8bae\uff0c\u8bc1\u660e\u5176\u5b89\u5168\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1. \u57fa\u4e8e\u90e8\u7f72\u524d\u6d4b\u8bd5\u7684\u4e25\u683c\u5b89\u5168\u6f14\u793a\u65b9\u6cd5\uff1b2. \u653e\u5bbd\u5148\u524dAI\u63a7\u5236\u7814\u7a76\u4e2d\u5173\u4e8eAI\u4e32\u8c0b\u7b56\u7565\u7684\u5047\u8bbe\uff1b3. \u5f00\u53d1\u6db5\u76d6\u88ab\u52a8\u81ea\u6211\u8bc6\u522b\u3001\u56e0\u679c\u4e32\u8c0b\u3001\u975e\u56e0\u679c\u4e32\u8c0b\u548c\u7ec4\u5408\u7b56\u7565\u7684\u5206\u7c7b\u6cd5\uff1b4. \u521b\u5efa\u5b89\u5168\u6848\u4f8b\u8349\u56fe\u6765\u6e05\u6670\u5448\u73b0\u8bba\u8bc1\u3002", "result": "\u8bc6\u522b\u4e86\u88ab\u52a8\u81ea\u6211\u8bc6\u522b\u53ef\u80fd\u6bd4\u5148\u524d\u7814\u7a76\u7684\u4e32\u8c0b\u7b56\u7565\u66f4\u6709\u6548\u7684\u60c5\u51b5\uff0c\u6784\u5efa\u4e86\u66f4\u7a33\u5065\u7684\u4e0d\u53ef\u4fe1\u76d1\u63a7\u8bc4\u4f30\u6846\u67b6\uff0c\u660e\u786e\u4e86\u5047\u8bbe\u5e76\u7a81\u51fa\u4e86\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0d\u53ef\u4fe1\u76d1\u63a7\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u653e\u5bbd\u4e32\u8c0b\u7b56\u7565\u5047\u8bbe\u548c\u7cfb\u7edf\u5316\u5206\u7c7b\uff0c\u63a8\u52a8\u4e86\u66f4\u7a33\u5065\u7684AI\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u672a\u89e3\u51b3\u7684\u6311\u6218\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2602.20638", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20638", "abs": "https://arxiv.org/abs/2602.20638", "authors": ["Vincent Auriau", "Khaled Belahcene", "Emmanuel Malherbe", "Vincent Mousseau", "Marc Pirlot"], "title": "Identifying two piecewise linear additive value functions from anonymous preference information", "comment": null, "summary": "Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540c\u65f6\u5411\u4e24\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\u7684\u504f\u597d\u8bf1\u5bfc\u65b9\u6cd5\uff0c\u5728\u4e0d\u77e5\u9053\u54ea\u4e2a\u56de\u7b54\u5bf9\u5e94\u54ea\u4e2a\u51b3\u7b56\u8005\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u51fa\u4e24\u4e2a\u52a0\u6027\u4ef7\u503c\u51fd\u6570\u6a21\u578b", "motivation": "\u5728\u540c\u65f6\u5411\u591a\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\u65f6\uff0c\u867d\u7136\u80fd\u83b7\u5f97\u591a\u4e2a\u56de\u7b54\uff0c\u4f46\u4e0d\u77e5\u9053\u54ea\u4e2a\u56de\u7b54\u5bf9\u5e94\u54ea\u4e2a\u51b3\u7b56\u8005\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u77e5\u9053\u56de\u7b54\u5f52\u5c5e\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u591a\u4e2a\u51b3\u7b56\u8005\u7684\u504f\u597d\u6a21\u578b\u3002", "method": "\u5047\u8bbe\u504f\u597d\u53ef\u4ee5\u7528\u52a0\u6027\u4ef7\u503c\u51fd\u6570\u8868\u793a\uff0c\u8fb9\u9645\u4ef7\u503c\u51fd\u6570\u662f\u5206\u6bb5\u7ebf\u6027\u7684\u4e14\u65ad\u70b9\u5df2\u77e5\u3002\u901a\u8fc7\u540c\u65f6\u5411\u4e24\u4f4d\u51b3\u7b56\u8005\u63d0\u95ee\uff0c\u83b7\u5f97\u4e24\u4e2a\u65e0\u566a\u58f0\u7684\u56de\u7b54\uff0c\u4f46\u4e0d\u77e5\u9053\u56de\u7b54\u7684\u5f52\u5c5e\u3002\u8bbe\u8ba1\u4e00\u79cd\u8bf1\u5bfc\u7a0b\u5e8f\u6765\u8bc6\u522b\u4e24\u4e2a\u504f\u597d\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u77e5\u9053\u56de\u7b54\u5bf9\u5e94\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u4e24\u4f4d\u51b3\u7b56\u8005\u7684\u52a0\u6027\u4ef7\u503c\u51fd\u6570\u6a21\u578b\uff0c\u524d\u63d0\u662f\u8fb9\u9645\u4ef7\u503c\u51fd\u6570\u662f\u5206\u6bb5\u7ebf\u6027\u7684\u4e14\u65ad\u70b9\u5df2\u77e5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u540c\u65f6\u8bf1\u5bfc\u591a\u4f4d\u51b3\u7b56\u8005\u504f\u597d\u65f6\u7684\u56de\u7b54\u5f52\u5c5e\u95ee\u9898\uff0c\u4e3a\u7fa4\u4f53\u51b3\u7b56\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u624b\u6bb5\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u8ffd\u8e2a\u56de\u7b54\u6765\u6e90\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2602.20639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20639", "abs": "https://arxiv.org/abs/2602.20639", "authors": ["Bo Zhang", "Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Minlie Huang", "Hongning Wang"], "title": "Grounding LLMs in Scientific Discovery via Embodied Actions", "comment": "24 pages, 7 figures, 7 tables. Preprint", "summary": "Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive \"execute-then-response\" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.", "AI": {"tldr": "EmbodiedAct\u6846\u67b6\u5c06\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7d27\u5bc6\u7684\u611f\u77e5-\u6267\u884c\u5faa\u73af\u8ba9LLM\u5728\u7269\u7406\u4eff\u771f\u4e2d\u5177\u5907\u8fd0\u884c\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u77ac\u6001\u5f02\u5e38\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u96be\u4ee5\u5f25\u5408\u7406\u8bba\u63a8\u7406\u4e0e\u53ef\u9a8c\u8bc1\u7269\u7406\u4eff\u771f\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u91c7\u7528\u88ab\u52a8\u7684\"\u6267\u884c-\u54cd\u5e94\"\u5faa\u73af\uff0c\u7f3a\u4e4f\u8fd0\u884c\u65f6\u611f\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u65e0\u6cd5\u5bdf\u89c9\u77ac\u6001\u5f02\u5e38\uff08\u5982\u6570\u503c\u4e0d\u7a33\u5b9a\u6216\u53d1\u6563\u632f\u8361\uff09\u3002", "method": "\u63d0\u51faEmbodiedAct\u6846\u67b6\uff0c\u5c06\u6210\u719f\u7684\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u4e3b\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5c06LLM\u4e0e\u5177\u8eab\u884c\u52a8\u76f8\u7ed3\u5408\uff0c\u5efa\u7acb\u7d27\u5bc6\u7684\u611f\u77e5-\u6267\u884c\u5faa\u73af\u3002\u5728MATLAB\u73af\u5883\u4e2d\u5b9e\u73b0\u8be5\u6846\u67b6\u3002", "result": "\u5728\u590d\u6742\u7684\u5de5\u7a0b\u8bbe\u8ba1\u548c\u79d1\u5b66\u5efa\u6a21\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEmbodiedAct\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u7a0b\u4eff\u771f\u4e2d\u786e\u4fdd\u6ee1\u610f\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u79d1\u5b66\u5efa\u6a21\u4e2d\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "EmbodiedAct\u901a\u8fc7\u5c06\u79d1\u5b66\u8f6f\u4ef6\u8f6c\u5316\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u5e76\u5efa\u7acb\u7d27\u5bc6\u7684\u611f\u77e5-\u6267\u884c\u5faa\u73af\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u7269\u7406\u4eff\u771f\u4e2d\u7f3a\u4e4f\u8fd0\u884c\u65f6\u611f\u77e5\u7684\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u7a33\u5b9a\u7684\u6846\u67b6\u3002"}}
{"id": "2602.20659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20659", "abs": "https://arxiv.org/abs/2602.20659", "authors": ["Vaidehi Bagaria", "Bijo Sebastian", "Nirav Patel"], "title": "Recursive Belief Vision Language Model", "comment": null, "summary": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to \u03c00. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.", "AI": {"tldr": "RB-VLA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u5ff5\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u76ee\u6807\u7ef4\u62a4\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u91cd\u590d\u67e5\u8be2\u5bfc\u81f4\u4efb\u52a1\u8fdb\u5ea6\u4e22\u5931\uff1b2\uff09\u611f\u77e5\u6df7\u6dc6\u4e0b\u7684\u52a8\u4f5c\u91cd\u590d\uff1b3\uff09\u9ad8\u63a8\u7406\u5ef6\u8fdf\u3002\u8fd9\u4e9b\u95ee\u9898\u7684\u6839\u6e90\u5728\u4e8e\u7f3a\u4e4f\u6301\u4e45\u3001\u52a8\u4f5c\u6761\u4ef6\u7684\u72b6\u6001\u8868\u793a\uff0c\u4ee5\u53ca\u6709\u9650\u7684\u65f6\u7a7a\u548c\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002", "method": "RB-VLA\u91c7\u7528\u4fe1\u5ff5\u4e2d\u5fc3\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u76ee\u6807\u8bad\u7ec3\uff0c\u7ef4\u62a4\u4e00\u4e2a\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u7f16\u7801\uff0c\u5305\u542b\u4efb\u52a1\u76f8\u5173\u5386\u53f2\u3001\u52a8\u6001\u548c\u7269\u4f53\u4ea4\u4e92\u4fe1\u606f\u3002VLM\u4ec5\u88ab\u67e5\u8be2\u4e00\u6b21\u4ee5\u83b7\u53d6\u9ad8\u5c42\u610f\u56fe\uff0c\u4fe1\u5ff5\u6a21\u5757\u8ddf\u8e2a\u4efb\u52a1\u8fdb\u5ea6\uff0c\u5e76\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u5b9e\u73b0\u9636\u6bb5\u611f\u77e5\u3001\u56e0\u679c\u57fa\u7840\u7684\u63a7\u5236\u3002\u4fe1\u5ff5\u548c\u610f\u56fe\u5171\u540c\u6761\u4ef6\u5316\u6269\u6563\u7b56\u7565\u8fdb\u884c\u95ed\u73af\u6267\u884c\u3002", "result": "RB-VLA\u5728\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709VLA\u6a21\u578b\uff1a\u591a\u9636\u6bb5\u62fe\u653e\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad852.5%\uff0c\u5806\u53e0\u4efb\u52a1\u63d0\u9ad837.5%\uff08\u76f8\u6bd4\u03c00\uff09\u3002\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe5\u500d\uff0c\u6d88\u9664\u4e86\u73b0\u6709VLA\u4e2d\u968f\u65f6\u95f4\u6b65\u957f\u589e\u52a0\u7684\u5185\u5b58\u589e\u957f\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u4fe1\u5ff5\u6a21\u5757\u662f\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u5c06\u6210\u529f\u7387\u4ece32.5%\u63d0\u5347\u523077.5%\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u5ff5\u7684\u72b6\u6001\u8868\u793a\u5bf9\u4e8e\u957f\u65f6\u7a0bVLA\u7b56\u7565\u662f\u6709\u6548\u7684\u3002RB-VLA\u901a\u8fc7\u7ef4\u62a4\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u957f\u65f6\u7a0b\u64cd\u4f5c\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3001\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\u6548\u7387\u3002"}}
{"id": "2602.20687", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20687", "abs": "https://arxiv.org/abs/2602.20687", "authors": ["Bo Peng", "Pi Bu", "Keyu Pan", "Xinrun Xu", "Yinxiu Zhao", "Miao Chen", "Yang Du", "Lin Li", "Jun Song", "Tong Xu"], "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.", "AI": {"tldr": "NativeEmbodied\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u4f4e\u5c42\u539f\u751f\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5305\u542b\u9ad8\u5c42\u4efb\u52a1\u548c\u4f4e\u5c42\u6280\u80fd\u89e3\u8026\u4efb\u52a1\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709VLM\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u9ad8\u5c42\u6307\u4ee4\u6216\u79bb\u6563\u5316\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8fd9\u4e0e\u771f\u5b9e\u4e16\u754c\u63a7\u5236\u573a\u666f\u4e0d\u540c\uff1b2\uff09\u4e3b\u8981\u5173\u6ce8\u9ad8\u5c42\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u5c42\u548c\u9ad8\u5c42\u80fd\u529b\u7684\u8054\u5408\u8bc4\u4f30\u5206\u6790\u3002", "method": "\u6784\u5efaNativeEmbodied\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u591a\u6837\u5316\u6a21\u62df\u573a\u666f\uff0c\u5305\u542b\u4e09\u7c7b\u4ee3\u8868\u6027\u9ad8\u5c42\u590d\u6742\u4efb\u52a1\uff0c\u540c\u65f6\u89e3\u8026\u590d\u6742\u4efb\u52a1\u6240\u9700\u6280\u80fd\uff0c\u6784\u5efa\u56db\u7c7b\u4f4e\u5c42\u57fa\u7840\u6280\u80fd\u4efb\u52a1\uff0c\u5b9e\u73b0\u4efb\u52a1\u548c\u6280\u80fd\u7c92\u5ea6\u7684\u8054\u5408\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6700\u5148\u8fdb\u7684VLM\u5728\u591a\u4e2a\u57fa\u7840\u5177\u8eab\u6280\u80fd\u4e0a\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u74f6\u9888\u663e\u8457\u9650\u5236\u4e86\u9ad8\u5c42\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "NativeEmbodied\u63ed\u793a\u4e86\u5f53\u524dVLM\u9a71\u52a8\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u4f4e\u5c42\u6280\u80fd\u5bf9\u9ad8\u5c42\u4efb\u52a1\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.20696", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20696", "abs": "https://arxiv.org/abs/2602.20696", "authors": ["Baolong Bi", "Yuyao Ge", "Shenghua Liu", "Yuchen He", "Siqian Tong", "Lizhe Chen", "Lingrui Mei", "Zehao Li", "Yiwei Wang", "Yujun Cai", "Ming-Hsuan Yang", "Xueqi Cheng"], "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding", "comment": null, "summary": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.", "AI": {"tldr": "\u63d0\u51faPromptCD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u5bf9\u6bd4\u6a21\u578b\u54cd\u5e94\uff0c\u5728\u6d4b\u8bd5\u65f6\u63a7\u5236LLM\u548cVLM\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u591a\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u9002\u7528\u8303\u56f4\u6709\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5\u3002", "method": "PromptCD\u6784\u5efa\u76ee\u6807\u884c\u4e3a\u7684\u6b63\u8d1f\u5f15\u5bfc\u63d0\u793a\u5bf9\uff0c\u5bf9\u6bd4\u6a21\u578b\u54cd\u5e94\uff08LLM\u4e2d\u7684token\u6982\u7387\u5206\u5e03\u548cVLM\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\uff0c\u5f3a\u5316\u671f\u671b\u7ed3\u679c\u3002", "result": "\u5728LLM\u7684\"3H\"\u5bf9\u9f50\u76ee\u6807\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1b\u5728VLM\u4e2d\u63d0\u5347VQA\u6027\u80fd\uff0c\u901a\u8fc7\u5f3a\u5316\u884c\u4e3a\u4e00\u81f4\u7684\u89c6\u89c9\u57fa\u7840\u3002", "conclusion": "PromptCD\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u3001\u6210\u672c\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u53ef\u9760\u884c\u4e3a\u63a7\u5236\u7b56\u7565\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u6211\u589e\u5f3a\u3002"}}
{"id": "2602.20706", "categories": ["cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.20706", "abs": "https://arxiv.org/abs/2602.20706", "authors": ["Julien Dallot", "Yuval Emek", "Yuval Gil", "Maciej Pacut", "Stefan Schmid"], "title": "Online Algorithms with Unreliable Guidance", "comment": null, "summary": "This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $\u03b2$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $\u03b2= 0$ (a.k.a. consistency) as well as when $\u03b2= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $\u03b2$ shifts from $0$ to $1$.\n  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u5728\u7ebf\u7b97\u6cd5\u4e0d\u53ef\u9760\u6307\u5bfc(OAG)\u6a21\u578b\uff0c\u5c06\u9884\u6d4b\u4e0e\u7b97\u6cd5\u7ec4\u4ef6\u5b8c\u5168\u5206\u79bb\uff0c\u901a\u8fc7DTB\u7f16\u8bd1\u5668\u5c06\u4efb\u4f55\u5728\u7ebf\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5b66\u4e60\u589e\u5f3a\u7248\u672c\uff0c\u5728\u7f13\u5b58\u3001\u5ea6\u91cf\u4efb\u52a1\u7cfb\u7edf\u548c\u4e8c\u5206\u56fe\u5339\u914d\u95ee\u9898\u4e0a\u53d6\u5f97\u6700\u4f18\u6216\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7684\u5728\u7ebf\u51b3\u7b56\u6a21\u578b\u901a\u5e38\u5c06\u9884\u6d4b\u548c\u7b97\u6cd5\u7ec4\u4ef6\u7d27\u5bc6\u8026\u5408\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u5206\u79bb\u9884\u6d4b\u548c\u7b97\u6cd5\u7684\u901a\u7528\u6a21\u578b\uff0c\u4f7f\u5206\u6790\u4ec5\u4f9d\u8d56\u4e8e\u95ee\u9898\u672c\u8eab\uff0c\u800c\u975e\u7279\u5b9a\u7684\u635f\u5931\u51fd\u6570\u3002", "method": "\u63d0\u51faOAG\u6a21\u578b\uff1a\u5c06\u5728\u7ebf\u51b3\u7b56\u5efa\u6a21\u4e3a\u8bf7\u6c42-\u56de\u7b54\u6e38\u620f\uff0c\u7b97\u6cd5\u63a5\u6536\u6765\u81ea\u7b54\u6848\u7a7a\u95f4\u7684\u6307\u5bfc\uff08\u53ef\u80fd\u88ab\u5bf9\u6297\u6027\u6c61\u67d3\uff09\u3002\u5f00\u53d1DTB\u7f16\u8bd1\u5668\uff0c\u5c06\u4efb\u4f55\u5728\u7ebf\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5b66\u4e60\u589e\u5f3a\u7248\u672c\uff0c\u901a\u8fc7\u6709\u504f\u786c\u5e01\u51b3\u5b9a\u662f\u76f2\u76ee\u8ddf\u968f\u6307\u5bfc\u8fd8\u662f\u5ffd\u7565\u5b83\u3002", "result": "DTB\u7f16\u8bd1\u5668\u751f\u6210\u7684\u7b97\u6cd5\u5728\u4e09\u4e2a\u7ecf\u5178\u5728\u7ebf\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u5bf9\u4e8e\u7f13\u5b58\u548c\u5747\u5300\u5ea6\u91cf\u4efb\u52a1\u7cfb\u7edf\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff1b\u5bf9\u4e8e\u4e8c\u5206\u56fe\u5339\u914d\uff08\u5bf9\u6297\u6027\u5230\u8fbe\u987a\u5e8f\uff09\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u3002", "conclusion": "OAG\u6a21\u578b\u4e3a\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7684\u5728\u7ebf\u51b3\u7b56\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0cDTB\u7f16\u8bd1\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\u5c06\u4f20\u7edf\u5728\u7ebf\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5177\u6709\u826f\u597d\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u7684\u5b66\u4e60\u589e\u5f3a\u7b97\u6cd5\u3002"}}
{"id": "2602.20708", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20708", "abs": "https://arxiv.org/abs/2602.20708", "authors": ["Che Wang", "Fuyao Zhang", "Jiaming Zhang", "Ziqi Zhang", "Yinghui Wang", "Longtao Huang", "Jianbo Gao", "Zhong Chen", "Wei Yang Bryan Lim"], "title": "ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction", "comment": "11 pages,", "summary": "Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.", "AI": {"tldr": "ICON\u662f\u4e00\u4e2a\u63a2\u6d4b-\u7f13\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8fc7\u5ea6\u805a\u7126\u7279\u5f81\u6765\u9632\u5fa1\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u8fde\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u4e25\u683c\u8fc7\u6ee4\u6216\u62d2\u7edd\u673a\u5236\uff09\u5b58\u5728\u8fc7\u5ea6\u62d2\u7edd\u7684\u95ee\u9898\uff0c\u4f1a\u8fc7\u65e9\u7ec8\u6b62\u6709\u6548\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9632\u5fa1\u653b\u51fb\u53c8\u80fd\u4fdd\u6301\u4efb\u52a1\u8fde\u7eed\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faICON\u6846\u67b6\uff1a1\uff09\u6f5c\u5728\u7a7a\u95f4\u8f68\u8ff9\u63a2\u6d4b\u5668\u68c0\u6d4b\u653b\u51fb\uff0c\u57fa\u4e8e\u9ad8\u5f3a\u5ea6\u5206\u6570\u8bc6\u522b\u8fc7\u5ea6\u805a\u7126\u7279\u5f81\uff1b2\uff09\u7f13\u89e3\u77eb\u6b63\u5668\u8fdb\u884c\u624b\u672f\u5f0f\u6ce8\u610f\u529b\u5f15\u5bfc\uff0c\u9009\u62e9\u6027\u64cd\u7eb5\u5bf9\u6297\u6027\u67e5\u8be2\u952e\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u653e\u5927\u4efb\u52a1\u76f8\u5173\u5143\u7d20\u4ee5\u6062\u590dLLM\u7684\u529f\u80fd\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cICON\u5b9e\u73b0\u4e860.4%\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u4e0e\u5546\u4e1a\u7ea7\u68c0\u6d4b\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u5e26\u6765\u8d85\u8fc750%\u7684\u4efb\u52a1\u6548\u7528\u589e\u76ca\u3002\u5728\u5206\u5e03\u5916\u6cdb\u5316\u548c\u591a\u6a21\u6001\u4ee3\u7406\u6269\u5c55\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ICON\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5efa\u7acb\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u80fd\u591f\u6709\u6548\u9632\u5fa1\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u8fde\u7eed\u6027\u3002"}}
{"id": "2602.20710", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20710", "abs": "https://arxiv.org/abs/2602.20710", "authors": ["Peter Hase", "Christopher Potts"], "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness", "comment": null, "summary": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training", "AI": {"tldr": "\u63d0\u51faCounterfactual Simulation Training (CST)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u8f93\u5165\u4e0a\u8f93\u51fa\u7684\u601d\u7ef4\u94fe\uff0c\u6765\u63d0\u9ad8\u601d\u7ef4\u94fe\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u5f53\u524d\u601d\u7ef4\u94fe\u63a8\u7406\u5b58\u5728\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u901a\u8fc7\u68c0\u67e5\u601d\u7ef4\u94fe\u6765\u7406\u89e3\u6a21\u578b\u8f93\u51fa\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u601d\u7ef4\u94fe\u7684\u5fe0\u5b9e\u6027\uff0c\u4f7f\u5176\u80fd\u66f4\u53ef\u9760\u5730\u53cd\u6620\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCST\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u5e94\u7528\uff1a1) \u57fa\u4e8e\u7ebf\u7d22\u7684\u53cd\u4e8b\u5b9e\u8fdb\u884c\u601d\u7ef4\u94fe\u76d1\u63a7\uff0c\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u865a\u5047\u7279\u5f81\u3001\u5956\u52b1\u9ed1\u5ba2\u6216\u5949\u627f\u884c\u4e3a\uff1b2) \u57fa\u4e8e\u901a\u7528\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u6a21\u62df\uff0c\u9f13\u52b1\u6a21\u578b\u4ea7\u751f\u66f4\u5fe0\u5b9e\u3001\u53ef\u6cdb\u5316\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u5728\u9ad8\u8fbe235B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCST\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e\u7ebf\u7d22\u53cd\u4e8b\u5b9e\u7684\u76d1\u63a7\u51c6\u786e\u7387\uff08\u63d0\u534735\u4e2a\u767e\u5206\u70b9\uff09\u548c\u901a\u7528\u53cd\u4e8b\u5b9e\u7684\u53ef\u6a21\u62df\u6027\uff08\u63d0\u53472\u4e2a\u767e\u5206\u70b9\uff09\u3002CST\u4f18\u4e8e\u63d0\u793a\u57fa\u7ebf\uff0c\u7528LLM\u91cd\u5199\u4e0d\u5fe0\u5b9e\u601d\u7ef4\u94fe\u6bd4\u5355\u72ec\u4f7f\u7528RL\u6548\u7387\u9ad85\u500d\u3002", "conclusion": "CST\u80fd\u6709\u6548\u63d0\u9ad8\u601d\u7ef4\u94fe\u7684\u5fe0\u5b9e\u6027\uff0c\u5728\u601d\u7ef4\u94fe\u76d1\u63a7\u65b9\u9762\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002\u5927\u6a21\u578b\u672c\u8eab\u5e76\u4e0d\u5929\u751f\u5177\u6709\u66f4\u5fe0\u5b9e\u7684\u601d\u7ef4\u94fe\uff0c\u4f46\u80fd\u4eceCST\u4e2d\u83b7\u76ca\u66f4\u591a\u3002"}}
{"id": "2602.20722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20722", "abs": "https://arxiv.org/abs/2602.20722", "authors": ["Xu Wan", "Yansheng Wang", "Wenqi Huang", "Mingyang Sun"], "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning", "comment": null, "summary": "Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.", "AI": {"tldr": "BAPO\u662f\u4e00\u79cd\u79bb\u7ebf\u7b56\u7565\u7684RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6570\u636e\u6548\u7387\uff0c\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534712.5%", "motivation": "\u4f20\u7edf\u7684\u5728\u7ebf\u7b56\u7565RLVR\u6846\u67b6\u5b58\u5728\u7ecf\u9a8c\u6d6a\u8d39\u548c\u5956\u52b1\u540c\u8d28\u5316\u95ee\u9898\uff0c\u8fd9\u76f4\u63a5\u963b\u788d\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u6548\u7387", "method": "\u63d0\u51faBatch Adaptation Policy Optimization (BAPO)\uff0c\u901a\u8fc7\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u56f0\u96be\u6837\u672c\u5e76\u91cd\u7528\u9ad8\u8d28\u91cf\u6837\u672c\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\uff0c\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u6539\u8fdb\u7684\u4e0b\u754c\u4fdd\u8bc1", "result": "\u5728\u6570\u5b66\u3001\u89c4\u5212\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cBAPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534712.5%\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u6301\u7eed\u5931\u8d25\u768440.7%\u7684\u95ee\u9898", "conclusion": "BAPO\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u6548\u7387\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRLVR\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6027\u80fd"}}
{"id": "2602.20723", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20723", "abs": "https://arxiv.org/abs/2602.20723", "authors": ["Ji Dai", "Quan Fang", "Dengsheng Cai"], "title": "Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation enhances ranking by integrating user-item interactions with item content, which is particularly effective under sparse feedback and long-tail distributions. However, multimodal signals are inherently heterogeneous and can conflict in specific contexts, making effective fusion both crucial and challenging. Existing approaches often rely on shared fusion pathways, leading to entangled representations and modality imbalance. To address these issues, we propose \\textbf{MAGNET}, a \\textbf{M}odality-Guided Mixture of \\textbf{A}daptive \\textbf{G}raph Experts \\textbf{N}etwork with Progressive \\textbf{E}ntropy-\\textbf{T}riggered Routing for Multimodal Recommendation, designed to enhance controllability, stability, and interpretability in multimodal fusion. MAGNET couples interaction-conditioned expert routing with structure-aware graph augmentation, so that both \\emph{what} to fuse and \\emph{how} to fuse are explicitly controlled and interpretable. At the representation level, a dual-view graph learning module augments the interaction graph with content-induced edges, improving coverage for sparse and long-tail items while preserving collaborative structure via parallel encoding and lightweight fusion. At the fusion level, MAGNET employs structured experts with explicit modality roles -- dominant, balanced, and complementary -- enabling a more interpretable and adaptive combination of behavioral, visual, and textual cues. To further stabilize sparse routing and prevent expert collapse, we introduce a two-stage entropy-weighting mechanism that monitors routing entropy. This mechanism automatically transitions training from an early coverage-oriented regime to a later specialization-oriented regime, progressively balancing expert utilization and routing confidence. Extensive experiments on public benchmarks demonstrate consistent improvements over strong baselines.", "AI": {"tldr": "MAGNET\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u6001\u5f15\u5bfc\u7684\u6df7\u5408\u81ea\u9002\u5e94\u56fe\u4e13\u5bb6\u7f51\u7edc\uff0c\u901a\u8fc7\u6e10\u8fdb\u71b5\u89e6\u53d1\u8def\u7531\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u8350\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u63a7\u3001\u7a33\u5b9a\u548c\u53ef\u89e3\u91ca\u7684\u878d\u5408\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u8350\u901a\u8fc7\u6574\u5408\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u548c\u7269\u54c1\u5185\u5bb9\u6765\u63d0\u5347\u6392\u5e8f\u6548\u679c\uff0c\u5c24\u5176\u5728\u7a00\u758f\u53cd\u9988\u548c\u957f\u5c3e\u5206\u5e03\u573a\u666f\u4e0b\u6709\u6548\u3002\u7136\u800c\u591a\u6a21\u6001\u4fe1\u53f7\u672c\u8d28\u4e0a\u662f\u5f02\u8d28\u7684\uff0c\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u53ef\u80fd\u51b2\u7a81\uff0c\u4f7f\u5f97\u6709\u6548\u878d\u5408\u65e2\u5173\u952e\u53c8\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5171\u4eab\u878d\u5408\u8def\u5f84\uff0c\u5bfc\u81f4\u8868\u793a\u7ea0\u7f20\u548c\u6a21\u6001\u4e0d\u5e73\u8861\u3002", "method": "MAGNET\u91c7\u7528\u53cc\u89c6\u56fe\u56fe\u5b66\u4e60\u6a21\u5757\u589e\u5f3a\u4ea4\u4e92\u56fe\uff0c\u901a\u8fc7\u5185\u5bb9\u8bf1\u5bfc\u8fb9\u6539\u5584\u7a00\u758f\u548c\u957f\u5c3e\u7269\u54c1\u8986\u76d6\uff0c\u540c\u65f6\u901a\u8fc7\u5e76\u884c\u7f16\u7801\u548c\u8f7b\u91cf\u878d\u5408\u4fdd\u6301\u534f\u540c\u7ed3\u6784\u3002\u5728\u878d\u5408\u5c42\u9762\uff0c\u4f7f\u7528\u5177\u6709\u660e\u786e\u6a21\u6001\u89d2\u8272\uff08\u4e3b\u5bfc\u3001\u5e73\u8861\u3001\u4e92\u8865\uff09\u7684\u7ed3\u6784\u5316\u4e13\u5bb6\uff0c\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u884c\u4e3a\u3001\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7ec4\u5408\u3002\u5f15\u5165\u4e24\u9636\u6bb5\u71b5\u52a0\u6743\u673a\u5236\u76d1\u63a7\u8def\u7531\u71b5\uff0c\u81ea\u52a8\u4ece\u65e9\u671f\u8986\u76d6\u5bfc\u5411\u8bad\u7ec3\u8fc7\u6e21\u5230\u540e\u671f\u4e13\u4e1a\u5316\u5bfc\u5411\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "MAGNET\u901a\u8fc7\u4ea4\u4e92\u6761\u4ef6\u4e13\u5bb6\u8def\u7531\u548c\u7ed3\u6784\u611f\u77e5\u56fe\u589e\u5f3a\uff0c\u660e\u786e\u63a7\u5236\"\u878d\u5408\u4ec0\u4e48\"\u548c\"\u5982\u4f55\u878d\u5408\"\uff0c\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u53ef\u63a7\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.20728", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20728", "abs": "https://arxiv.org/abs/2602.20728", "authors": ["Chenyang Zhao", "Vinny Cahill", "Ivana Dusparic"], "title": "Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback", "comment": null, "summary": "Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u5c06\u57fa\u4e8eAI\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLAIF\uff09\u6269\u5c55\u5230\u591a\u76ee\u6807\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u504f\u597d\u6807\u7b7e\uff0c\u907f\u514d\u7e41\u7410\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u53cd\u6620\u4e0d\u540c\u7528\u6237\u4f18\u5148\u7ea7\u7684\u5e73\u8861\u7b56\u7565\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u90e8\u7f72\u4e2d\uff0c\u5956\u52b1\u8bbe\u8ba1\u662f\u6838\u5fc3\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u51b2\u7a81\u65f6\u96be\u4ee5\u6307\u5b9a\u6743\u8861\u5173\u7cfb\u3002\u73b0\u6709RLAIF\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u76ee\u6807\u4efb\u52a1\uff0c\u591a\u76ee\u6807\u7cfb\u7edf\u4e2d\u7b56\u7565\u5bb9\u6613\u504f\u5411\u4e3b\u5bfc\u76ee\u6807\uff0c\u9700\u8981\u63a2\u7d22RLAIF\u5728\u591a\u76ee\u6807\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u5c06RLAIF\u8303\u5f0f\u6269\u5c55\u5230\u591a\u76ee\u6807\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u504f\u597d\u6807\u7b7e\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7AI\u53cd\u9988\u5b66\u4e60\u591a\u76ee\u6807\u7b56\u7565\uff0c\u907f\u514d\u590d\u6742\u7684\u5956\u52b1\u5de5\u7a0b\u3002", "result": "\u591a\u76ee\u6807RLAIF\u80fd\u591f\u4ea7\u751f\u53cd\u6620\u4e0d\u540c\u7528\u6237\u4f18\u5148\u7ea7\u7684\u5e73\u8861\u6743\u8861\u7b56\u7565\uff0c\u65e0\u9700\u7e41\u7410\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u5728\u591a\u76ee\u6807\u51b2\u7a81\u9886\u57df\u4e2d\u5b9e\u73b0\u7528\u6237\u5bf9\u9f50\u7b56\u7565\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5c06RLAIF\u96c6\u6210\u5230\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4e3a\u5177\u6709\u5185\u5728\u51b2\u7a81\u76ee\u6807\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u7528\u6237\u5bf9\u9f50\u7b56\u7565\u5b66\u4e60\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u7cfb\u7edf\u4e2d\u7684\u6743\u8861\u96be\u9898\u3002"}}
{"id": "2602.20732", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20732", "abs": "https://arxiv.org/abs/2602.20732", "authors": ["Chao Fei", "Guozhong Li", "Chenxi Liu", "Panos Kalnis"], "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference", "comment": null, "summary": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.", "AI": {"tldr": "CHESS\u662f\u4e00\u79cd\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u7684KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u4ec5\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8fbe\u5230\u5b8c\u6574KV\u7f13\u5b58\u7684\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u8fbe4.56\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u5728\u4f4e\u5ef6\u8fdf\u63a8\u7406\u65f6\uff0cKV\u7f13\u5b58\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5927\u591a\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\uff1a\u5b83\u4eec\u5ffd\u7565token\u7684\u9010\u6b65\u76f8\u5173\u6027\u548c\u5c40\u90e8\u8bed\u4e49\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u4e14\u4e0d\u89c4\u5219\u8bbf\u95ee\u548c\u9009\u62e9\u5f00\u9500\u53ea\u80fd\u5e26\u6765\u6709\u9650\u7684\u52a0\u901f\u3002", "method": "CHESS\u91c7\u7528\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff1a\u7b97\u6cd5\u4e0a\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u52a8\u6001\u91cd\u6784\u5f53\u524d\u89e3\u7801\u7684\u8fde\u8d2f\u4e0a\u4e0b\u6587\uff1b\u7cfb\u7edf\u4e0a\u91c7\u7528\u7c97\u7c92\u5ea6\u9009\u62e9\u6d88\u9664\u6602\u8d35\u7684\u6570\u636e\u79fb\u52a8\uff0c\u5145\u5206\u5b9e\u73b0\u7406\u8bba\u7a00\u758f\u6027\u7684\u5b9e\u9645\u52a0\u901f\u3002", "result": "CHESS\u4ec5\u4f7f\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8d85\u8d8a\u5b8c\u6574KV\u7f13\u5b58\u7684\u8d28\u91cf\uff0c\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7a33\u5b9a\u63a8\u7406\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53474.56\u500d\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CHESS\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u4e2dKV\u7f13\u5b58\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.20739", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20739", "abs": "https://arxiv.org/abs/2602.20739", "authors": ["Shitian Zhao", "Shaoheng Lin", "Ming Li", "Haoquan Zhang", "Wenshuo Peng", "Kaipeng Zhang", "Chen Wei"], "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "comment": "preprint", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86PyVision-RL\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u91c7\u6837-\u8fc7\u6ee4-\u6392\u5e8f\u7b56\u7565\u548c\u7d2f\u79ef\u5de5\u5177\u5956\u52b1\u9632\u6b62\u4ea4\u4e92\u5d29\u6e83\uff0c\u5f00\u53d1\u4e86PyVision-Image\u548cPyVision-Video\u6a21\u578b\uff0c\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u91c7\u7528\u6309\u9700\u4e0a\u4e0b\u6587\u6784\u5efa\u51cf\u5c11\u89c6\u89c9token\u4f7f\u7528\u3002", "motivation": "\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e38\u51fa\u73b0\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u51cf\u5c11\u5de5\u5177\u4f7f\u7528\u548c\u591a\u8f6e\u63a8\u7406\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faPyVision-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8fc7\u91c7\u6837-\u8fc7\u6ee4-\u6392\u5e8f\u7b56\u7565\u548c\u7d2f\u79ef\u5de5\u5177\u5956\u52b1\u9632\u6b62\u4ea4\u4e92\u5d29\u6e83\uff1b\u5f00\u53d1\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\uff0c\u521b\u5efaPyVision-Image\u548cPyVision-Video\u6a21\u578b\uff1b\u5728\u89c6\u9891\u63a8\u7406\u4e2d\u91c7\u7528\u6309\u9700\u4e0a\u4e0b\u6587\u6784\u5efa\uff0c\u9009\u62e9\u6027\u91c7\u6837\u4efb\u52a1\u76f8\u5173\u5e27\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPyVision\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u548c\u6539\u8fdb\u7684\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u6301\u7eed\u4ea4\u4e92\u548c\u6309\u9700\u89c6\u89c9\u5904\u7406\u5bf9\u53ef\u6269\u5c55\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6301\u7eed\u4ea4\u4e92\u548c\u6309\u9700\u89c6\u89c9\u5904\u7406\u662f\u6784\u5efa\u53ef\u6269\u5c55\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u5173\u952e\u8981\u7d20\uff0cPyVision-RL\u6846\u67b6\u80fd\u6709\u6548\u9632\u6b62\u4ea4\u4e92\u5d29\u6e83\u5e76\u4fc3\u8fdb\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u3002"}}
{"id": "2602.20770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20770", "abs": "https://arxiv.org/abs/2602.20770", "authors": ["Varvara Sazonova", "Dmitri Shmelkin", "Stanislav Kikot", "Vasily Motolygin"], "title": "Pipeline for Verifying LLM-Generated Mathematical Solutions", "comment": null, "summary": "With the growing popularity of Large Reasoning Models and their results in solving mathematical problems, it becomes crucial to measure their capabilities. We introduce a pipeline for both automatic and interactive verification as a more accurate alternative to only checking the answer which is currently the most popular approach for benchmarks. The pipeline can also be used as a generator of correct solutions both in formal and informal languages. 3 AI agents, which can be chosen for the benchmark accordingly, are included in the structure. The key idea is the use of prompts to obtain the solution in the specific form which allows for easier verification using proof assistants and possible use of small models ($\\le 8B$). Experiments on several datasets suggest low probability of False Positives. The open-source implementation with instructions on setting up a server is available at https://github.com/LogicEnj/lean4_verification_pipeline.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u5927\u578b\u63a8\u7406\u6a21\u578b\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u81ea\u52a8\u5316\u4ea4\u4e92\u5f0f\u9a8c\u8bc1\u7ba1\u9053\uff0c\u66ff\u4ee3\u4ec5\u68c0\u67e5\u7b54\u6848\u7684\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5", "motivation": "\u968f\u7740\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u65b9\u9762\u7684\u666e\u53ca\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\u3002\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u7b54\u6848\u68c0\u67e5\uff0c\u4e0d\u591f\u7cbe\u786e\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u81ea\u52a8\u548c\u4ea4\u4e92\u5f0f\u9a8c\u8bc1\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u7279\u5b9a\u683c\u5f0f\u7684\u63d0\u793a\u83b7\u53d6\u89e3\u51b3\u65b9\u6848\uff0c\u4fbf\u4e8e\u4f7f\u7528\u8bc1\u660e\u52a9\u624b\u9a8c\u8bc1\uff0c\u5e76\u53ef\u5229\u7528\u5c0f\u578b\u6a21\u578b\uff08\u22648B\uff09\u3002\u7ba1\u9053\u5305\u542b3\u4e2aAI\u4ee3\u7406\uff0c\u652f\u6301\u751f\u6210\u6b63\u5f0f\u548c\u975e\u6b63\u5f0f\u8bed\u8a00\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u8bef\u62a5\u6982\u7387\u8f83\u4f4e\u3002\u5f00\u6e90\u5b9e\u73b0\u5df2\u63d0\u4f9b\uff0c\u5305\u542b\u670d\u52a1\u5668\u8bbe\u7f6e\u8bf4\u660e\u3002", "conclusion": "\u8be5\u9a8c\u8bc1\u7ba1\u9053\u4e3a\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u652f\u6301\u5c0f\u578b\u6a21\u578b\u7684\u6709\u6548\u5229\u7528\u3002"}}
{"id": "2602.20810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20810", "abs": "https://arxiv.org/abs/2602.20810", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "POMDPPlanners: Open-Source Package for POMDP Planning", "comment": null, "summary": "We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.", "AI": {"tldr": "POMDPPlanners\u662f\u4e00\u4e2a\u7528\u4e8ePOMDP\u89c4\u5212\u7b97\u6cd5\u5b9e\u8bc1\u8bc4\u4f30\u7684\u5f00\u6e90Python\u5305\uff0c\u96c6\u6210\u4e86\u5148\u8fdb\u7b97\u6cd5\u3001\u57fa\u51c6\u73af\u5883\u3001\u81ea\u52a8\u8d85\u53c2\u6570\u4f18\u5316\u548c\u5e76\u884c\u4eff\u771f\u529f\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u652f\u6301\u5927\u89c4\u6a21\u3001\u53ef\u91cd\u590d\u7684POMDP\u89c4\u5212\u7b97\u6cd5\u7814\u7a76\u7684\u5de5\u5177\u5305\uff0c\u7279\u522b\u662f\u5728\u98ce\u9669\u654f\u611f\u573a\u666f\u4e0b\uff0c\u6807\u51c6\u5de5\u5177\u5305\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u5316\u7684Python\u5305\uff0c\u5305\u542b\uff1a1\uff09\u5148\u8fdb\u7684\u89c4\u5212\u7b97\u6cd5\uff1b2\uff09\u5e26\u6709\u5b89\u5168\u5173\u952e\u53d8\u4f53\u7684\u57fa\u51c6\u73af\u5883\u5957\u4ef6\uff1b3\uff09\u901a\u8fc7Optuna\u8fdb\u884c\u81ea\u52a8\u8d85\u53c2\u6570\u4f18\u5316\uff1b4\uff09\u5177\u6709\u6545\u969c\u6062\u590d\u7684\u6301\u4e45\u7f13\u5b58\uff1b5\uff09\u53ef\u914d\u7f6e\u7684\u5e76\u884c\u4eff\u771f\u3002", "result": "\u8be5\u5305\u663e\u8457\u51cf\u5c11\u4e86\u5927\u89c4\u6a21\u4eff\u771f\u7814\u7a76\u7684\u5f00\u9500\uff0c\u652f\u6301\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u590d\u7684\u51b3\u7b56\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u98ce\u9669\u654f\u611f\u573a\u666f\u4e2d\u3002", "conclusion": "POMDPPlanners\u586b\u8865\u4e86POMDP\u89c4\u5212\u7b97\u6cd5\u5b9e\u8bc1\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u7f3a\uff0c\u4e3a\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u3002"}}
{"id": "2602.20812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20812", "abs": "https://arxiv.org/abs/2602.20812", "authors": ["Jia-Rui Lin", "Yun-Hong Cai", "Xiang-Rui Ni", "Shaojie Zhou", "Peng Pan"], "title": "Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset", "comment": null, "summary": "As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9BIM\u8bbe\u8ba1\u9886\u57df\u7f3a\u4e4fLLM\u8bc4\u4f30\u57fa\u51c6\u548c\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2aBIM\u8bbe\u8ba1\u4e13\u7528\u8bc4\u4f30\u57fa\u51c6\u3001\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u548cLLM\u5fae\u8c03\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86Qwen-BIM\u6a21\u578b\uff0c\u5728\u4ec514B\u53c2\u6570\u4e0b\u8fbe\u5230\u4e0e671B\u901a\u7528LLM\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u4e2d\uff0cBIM\u8bbe\u8ba1\u6210\u4e3a\u667a\u80fd\u5efa\u9020\u7684\u5173\u952e\u9a71\u52a8\u529b\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fc3\u8fdbBIM\u8bbe\u8ba1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7279\u5b9a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u4e25\u91cd\u9650\u5236\u4e86LLM\u5728\u8be5\u9886\u57df\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "1) \u63d0\u51faBIM\u8bbe\u8ba1\u8bc4\u4f30\u57fa\u51c6\u53ca\u91cf\u5316\u6307\u6807\uff1b2) \u5f00\u53d1\u4eceBIM\u751f\u6210\u6587\u672c\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u7528\u4e8eLLM\u8bc4\u4f30\u548c\u5fae\u8c03\u7684BIM\u884d\u751f\u6570\u636e\u96c6\uff1b3) \u63d0\u51fa\u9488\u5bf9BIM\u8bbe\u8ba1\u7684LLM\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u9886\u57df\u4e13\u7528\u57fa\u51c6\u80fd\u6709\u6548\u5168\u9762\u8bc4\u4f30LLM\u80fd\u529b\uff0c\u663e\u793a\u901a\u7528LLM\u5728\u9886\u57df\u4efb\u52a1\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u57fa\u4e8e\u8be5\u57fa\u51c6\u548c\u6570\u636e\u96c6\u5f00\u53d1\u7684Qwen-BIM\u6a21\u578b\uff0cG-Eval\u5f97\u5206\u6bd4\u57fa\u7840LLM\u5e73\u5747\u63d0\u534721.0%\uff0c\u4ec514B\u53c2\u6570\u5373\u53ef\u5728BIM\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u8fbe\u5230671B\u901a\u7528LLM\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u9996\u4e2aBIM\u8bbe\u8ba1\u4e13\u7528LLM\uff0c\u4e3a\u5404\u9886\u57df\u5f00\u53d1BIM\u76f8\u5173LLM\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.20813", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20813", "abs": "https://arxiv.org/abs/2602.20813", "authors": ["Nora Petrova", "John Burden"], "title": "Pressure Reveals Character: Behavioural Alignment Evaluation at Depth", "comment": "Preprint", "summary": "Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b904\u4e2a\u573a\u666f\u7684AI\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u516d\u4e2a\u7c7b\u522b\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u6d4b\u8bd5\u6a21\u578b\u5728\u771f\u5b9e\u538b\u529b\u4e0b\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5bf9\u9f50\u6027\u8868\u73b0\u4e3a\u7edf\u4e00\u7ed3\u6784\uff08\u7c7b\u4f3c\u8ba4\u77e5\u7814\u7a76\u4e2d\u7684g\u56e0\u5b50\uff09\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6027\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u58f0\u79f0\u4f1a\u505a\u4ec0\u4e48\uff0c\u800c\u975e\u5728\u771f\u5b9e\u538b\u529b\u4e0b\u7684\u5b9e\u9645\u884c\u4e3a\u3002\u968f\u7740\u5bf9\u9f50\u5931\u8d25\u5bfc\u81f4\u7684\u5b9e\u9645\u5371\u5bb3\u589e\u52a0\uff0c\u9700\u8981\u5305\u542b\u771f\u5b9e\u591a\u8f6e\u573a\u666f\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b904\u4e2a\u573a\u666f\u7684\u5bf9\u9f50\u57fa\u51c6\uff0c\u6db5\u76d6\u8bda\u5b9e\u6027\u3001\u5b89\u5168\u6027\u3001\u975e\u64cd\u7eb5\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u7ea0\u6b63\u6027\u548c\u9634\u8c0b\u6027\u516d\u4e2a\u7c7b\u522b\uff0c\u7ecf\u4eba\u7c7b\u8bc4\u4f30\u786e\u8ba4\u4e3a\u771f\u5b9e\u573a\u666f\u3002\u4f7f\u7528\u51b2\u7a81\u6307\u4ee4\u3001\u6a21\u62df\u5de5\u5177\u8bbf\u95ee\u548c\u591a\u8f6e\u5347\u7ea7\u7b49\u8bbe\u7f6e\uff0c\u901a\u8fc7LLM\u6cd5\u5b98\uff08\u7ecf\u4eba\u7c7b\u6807\u6ce8\u9a8c\u8bc1\uff09\u8bc4\u4f3024\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u7279\u5b9a\u7c7b\u522b\u4e2d\u4e5f\u5b58\u5728\u5dee\u8ddd\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u8868\u73b0\u4e00\u81f4\u8584\u5f31\u3002\u56e0\u5b50\u5206\u6790\u663e\u793a\u5bf9\u9f50\u6027\u8868\u73b0\u4e3a\u7edf\u4e00\u7ed3\u6784\uff08\u7c7b\u4f3c\u8ba4\u77e5\u7814\u7a76\u4e2d\u7684g\u56e0\u5b50\uff09\uff0c\u6a21\u578b\u5728\u4e00\u4e2a\u7c7b\u522b\u5f97\u5206\u9ad8\u65f6\u5728\u5176\u4ed6\u7c7b\u522b\u4e5f\u503e\u5411\u4e8e\u5f97\u5206\u9ad8\u3002", "conclusion": "\u9700\u8981\u66f4\u5168\u9762\u7684\u591a\u8f6e\u5bf9\u9f50\u8bc4\u4f30\u6765\u63ed\u793a\u5355\u8f6e\u6d4b\u8bd5\u65e0\u6cd5\u53d1\u73b0\u7684\u884c\u4e3a\u503e\u5411\u3002\u516c\u5f00\u57fa\u51c6\u548c\u4ea4\u4e92\u5f0f\u6392\u884c\u699c\u652f\u6301\u6301\u7eed\u8bc4\u4f30\uff0c\u8ba1\u5212\u5728\u89c2\u5bdf\u5230\u6301\u7eed\u8584\u5f31\u7684\u9886\u57df\u6269\u5c55\u573a\u666f\u5e76\u6dfb\u52a0\u65b0\u6a21\u578b\u3002"}}
{"id": "2602.20878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20878", "abs": "https://arxiv.org/abs/2602.20878", "authors": ["Dhita Putri Pratama", "Soyeon Caren Han", "Yihao Ding"], "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs", "comment": null, "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Vision-Language Causal Graphs (VLCGs)\u7ed3\u6784\u5316\u8868\u793a\u548cViLCaR\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u7f3a\u4e4f\u7ed3\u6784\u5316\u6307\u5bfc\u800c\u975e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u7684\u56e0\u679c\u63a8\u7406\u3002\u5f53\u524d\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u533a\u5206\u5931\u8d25\u662f\u7531\u4e8e\u63a8\u7406\u80fd\u529b\u6709\u9650\u8fd8\u662f\u672a\u80fd\u8bc6\u522b\u56e0\u679c\u76f8\u5173\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86Vision-Language Causal Graphs (VLCGs)\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u67e5\u8be2\u6761\u4ef6\u5316\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u660e\u786e\u7f16\u7801\u56e0\u679c\u76f8\u5173\u7684\u5bf9\u8c61\u3001\u5c5e\u6027\u3001\u5173\u7cfb\u548c\u573a\u666f\u57fa\u7840\u5047\u8bbe\u3002\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86ViLCaR\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b\u56e0\u679c\u5f52\u56e0\u3001\u56e0\u679c\u63a8\u7406\u548c\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u56fe\u5bf9\u9f50\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u73b0\u6709\u6700\u5148\u8fdb\u7684LVLMs\u4e2d\u6ce8\u5165\u7ed3\u6784\u5316\u76f8\u5173\u4fe1\u606f\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u548c\u6807\u51c6\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5f52\u56e0\u548c\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u8868\u660e\u5f53\u524dLVLM\u56e0\u679c\u63a8\u7406\u7684\u5c40\u9650\u4e3b\u8981\u6e90\u4e8e\u7ed3\u6784\u5316\u6307\u5bfc\u4e0d\u8db3\u800c\u975e\u63a8\u7406\u80fd\u529b\u7f3a\u4e4f\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u74f6\u9888\u4e3b\u8981\u5728\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u800c\u975e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u901a\u8fc7\u63d0\u4f9b\u660e\u786e\u7684\u56e0\u679c\u76f8\u5173\u7ed3\u6784\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u8868\u73b0\u3002"}}
{"id": "2602.20918", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20918", "abs": "https://arxiv.org/abs/2602.20918", "authors": ["Hyewon Jang", "Nikolai Ilinykh", "Sharid Lo\u00e1iciga", "Jey Han Lau", "Shalom Lappin"], "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts", "comment": null, "summary": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.", "AI": {"tldr": "\u7814\u7a76\u89c6\u89c9\u4e0a\u4e0b\u6587\u5bf9\u53e5\u5b50\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u7684\u5f71\u54cd\uff1a\u4eba\u7c7b\u51e0\u4e4e\u4e0d\u53d7\u89c6\u89c9\u56fe\u50cf\u5f71\u54cd\uff0c\u800cLLMs\u8868\u73b0\u51fa\u538b\u7f29\u6548\u5e94\uff0c\u4e14\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u6027\u80fd\u7565\u964d\uff0c\u4e0d\u540cLLMs\u8868\u73b0\u5404\u5f02", "motivation": "\u63a2\u7d22\u89c6\u89c9\u4e0a\u4e0b\u6587\uff08\u56fe\u50cf\uff09\u5bf9\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53e5\u5b50\u53ef\u63a5\u53d7\u6027\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u6587\u672c\u4e0a\u4e0b\u6587\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u4e0d\u540c\u4f5c\u7528\uff0c\u4ee5\u53ca\u4eba\u7c7b\u4e0eLLMs\u5728\u8de8\u6a21\u6001\u5904\u7406\u4e2d\u7684\u5f02\u540c", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4eba\u7c7b\u548c\u4e0d\u540cLLMs\u5728\u6709\u65e0\u89c6\u89c9\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u5bf9\u53e5\u5b50\u53ef\u63a5\u53d7\u6027\u7684\u5224\u65ad\uff0c\u5206\u6790LLMs\u7684\u9884\u6d4b\u51c6\u786e\u5ea6\u3001\u5224\u65ad\u5206\u5e03\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5176\u751f\u6210\u9884\u6d4b\u4e0e\u5185\u90e8\u5bf9\u6570\u6982\u7387\u7684\u76f8\u5173\u6027", "result": "1) \u89c6\u89c9\u56fe\u50cf\u5bf9\u4eba\u7c7b\u53ef\u63a5\u53d7\u6027\u8bc4\u5206\u51e0\u4e4e\u6ca1\u6709\u5f71\u54cd\uff1b2) LLMs\u8868\u73b0\u51fa\u5148\u524d\u7814\u7a76\u4e2d\u4eba\u7c7b\u5728\u6587\u6863\u4e0a\u4e0b\u6587\u4e2d\u7684\u538b\u7f29\u6548\u5e94\uff1b3) LLMs\u9884\u6d4b\u4eba\u7c7b\u5224\u65ad\u51c6\u786e\u5ea6\u9ad8\uff0c\u4f46\u53bb\u9664\u89c6\u89c9\u4e0a\u4e0b\u6587\u65f6\u6027\u80fd\u7565\u597d\uff1b4) \u4e0d\u540cLLMs\u5224\u65ad\u5206\u5e03\u4e0d\u540c\uff0cQwen\u6700\u63a5\u8fd1\u4eba\u7c7b\u6a21\u5f0f\uff1b5) LLM\u751f\u6210\u9884\u6d4b\u4e0e\u5176\u5bf9\u6570\u6982\u7387\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u76f8\u5173\u6027\u964d\u4f4e", "conclusion": "\u4eba\u7c7b\u4e0eLLMs\u5728\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u5b58\u5728\u6709\u8da3\u5f02\u540c\uff1a\u4eba\u7c7b\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e0d\u654f\u611f\uff0c\u800cLLMs\u53d7\u5176\u5f71\u54cd\u4e14\u5185\u90e8\u8868\u793a\u4e0e\u751f\u6210\u9884\u6d4b\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u5b58\u5728\u66f4\u5927\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u8bed\u8a00\u5904\u7406\u7684\u590d\u6742\u6027"}}
{"id": "2602.20926", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20926", "abs": "https://arxiv.org/abs/2602.20926", "authors": ["Yuqi Huang", "Ning Liao", "Kai Yang", "Anning Hu", "Shengchao Hu", "Xiaoxing Wang", "Junchi Yan"], "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG", "comment": null, "summary": "Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\\times$ speedup over leading Graph-based RAG baselines.", "AI": {"tldr": "HELP\u6846\u67b6\u901a\u8fc7\u8d85\u8282\u70b9\u6269\u5c55\u548c\u903b\u8f91\u8def\u5f84\u5f15\u5bfc\u7684\u8bc1\u636e\u5b9a\u4f4d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u77e5\u8bc6\u5b8c\u6574\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u56fe\u589e\u5f3a\u68c0\u7d22\u7684\u6548\u7387\uff0c\u5b9e\u73b028.8\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u77e5\u8bc6\u8fb9\u754c\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9762\u4e34\u56fe\u904d\u5386\u6210\u672c\u9ad8\u548c\u8bed\u4e49\u566a\u58f0\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faHELP\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1\uff09\u8d85\u8282\u70b9\u6269\u5c55\uff1a\u5c06\u77e5\u8bc6\u4e09\u5143\u7ec4\u8fed\u4ee3\u94fe\u63a5\u4e3a\u8fde\u8d2f\u63a8\u7406\u8def\u5f84\u5e76\u62bd\u8c61\u4e3a\u8d85\u8282\u70b9\uff0c\u6355\u6349\u590d\u6742\u7ed3\u6784\u4f9d\u8d56\uff1b2\uff09\u903b\u8f91\u8def\u5f84\u5f15\u5bfc\u7684\u8bc1\u636e\u5b9a\u4f4d\uff1a\u5229\u7528\u9884\u8ba1\u7b97\u7684\u56fe-\u6587\u672c\u76f8\u5173\u6027\uff0c\u5c06\u8def\u5f84\u76f4\u63a5\u6620\u5c04\u5230\u8bed\u6599\u5e93\uff0c\u907f\u514d\u6602\u8d35\u7684\u968f\u673a\u6e38\u8d70\u3002", "result": "\u5728\u591a\u4e2a\u7b80\u5355\u548c\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u76f8\u6bd4\u9886\u5148\u7684\u57fa\u4e8e\u56fe\u7684RAG\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8fbe28.8\u500d\u7684\u52a0\u901f\u3002", "conclusion": "HELP\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8d85\u8282\u70b9\u6269\u5c55\u548c\u903b\u8f91\u8def\u5f84\u5f15\u5bfc\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u56fe\u589e\u5f3a\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u5b9e\u9645\u6548\u7387\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bed\u4e49\u5931\u771f\u548c\u5ef6\u8fdf\u95ee\u9898\u3002"}}
{"id": "2602.20934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20934", "abs": "https://arxiv.org/abs/2602.20934", "authors": ["ChengYou Li", "XiaoDong Liu", "XiangBao Meng", "XinYu Zhao"], "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence", "comment": "16 pages,9 figures", "summary": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AgentOS\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53d7\u7ed3\u6784\u5316\u64cd\u4f5c\u7cfb\u7edf\u903b\u8f91\u7ba1\u7406\u7684\"\u63a8\u7406\u5185\u6838\"\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7cfb\u7edf\u7ea7\u534f\u8c03\u6765\u6784\u5efa\u52a8\u6001\u81ea\u4e3b\u8ba4\u77e5\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u4f46\u5fae\u89c2\u4ee4\u724c\u5904\u7406\u4e0e\u5b8f\u89c2\u7cfb\u7edf\u667a\u80fd\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881\u4ecd\u7136\u96f6\u6563\u3002\u9700\u8981\u4ece\u9759\u6001\u63a8\u7406\u5f15\u64ce\u5411\u52a8\u6001\u81ea\u4e3b\u8ba4\u77e5\u7cfb\u7edf\u8fdb\u884c\u6839\u672c\u6027\u8f6c\u53d8\u3002", "method": "\u63d0\u51faAgentOS\u6982\u5ff5\u6846\u67b6\uff0c\u5c06LLM\u89c6\u4e3a\"\u63a8\u7406\u5185\u6838\"\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u64cd\u4f5c\u7cfb\u7edf\u903b\u8f91\u3002\u6838\u5fc3\u662f\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5c06\u4e0a\u4e0b\u6587\u7a97\u53e3\u6982\u5ff5\u5316\u4e3a\u53ef\u5bfb\u5740\u8bed\u4e49\u7a7a\u95f4\u800c\u975e\u88ab\u52a8\u7f13\u51b2\u533a\u3002\u5f15\u5165\u8bed\u4e49\u5207\u7247\u548c\u65f6\u95f4\u5bf9\u9f50\u673a\u5236\u6765\u51cf\u8f7b\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e2d\u7684\u8ba4\u77e5\u6f02\u79fb\u3002", "result": "\u901a\u8fc7\u5c06\u7ecf\u5178\u64cd\u4f5c\u7cfb\u7edf\u62bd\u8c61\uff08\u5982\u5185\u5b58\u5206\u9875\u3001\u4e2d\u65ad\u5904\u7406\u548c\u8fdb\u7a0b\u8c03\u5ea6\uff09\u6620\u5c04\u5230LLM\u539f\u751f\u6784\u9020\uff0c\u4e3a\u6784\u5efa\u5f39\u6027\u3001\u53ef\u6269\u5c55\u548c\u81ea\u6211\u8fdb\u5316\u7684\u8ba4\u77e5\u73af\u5883\u63d0\u4f9b\u4e86\u4e25\u8c28\u8def\u7ebf\u56fe\u3002", "conclusion": "AGI\u53d1\u5c55\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u5728\u4e8e\u7cfb\u7edf\u7ea7\u534f\u8c03\u7684\u67b6\u6784\u6548\u7387\uff0cAgentOS\u6846\u67b6\u4e3a\u5b9e\u73b0\u4ece\u79bb\u6563\u5e8f\u5217\u5230\u8fde\u8d2f\u8ba4\u77e5\u72b6\u6001\u7684\u8f6c\u53d8\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.21044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21044", "abs": "https://arxiv.org/abs/2602.21044", "authors": ["Yanrui Wu", "Lingling Zhang", "Xinyu Zhang", "Jiayu Chang", "Pengyu Li", "Xu Jiang", "Jingtao Hu", "Jun Liu"], "title": "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification", "comment": "24 pages, 17 figures", "summary": "Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.", "AI": {"tldr": "LogicGraph\uff1a\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u591a\u8def\u5f84\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u6784\u5efa\uff0c\u63ed\u793a\u6a21\u578b\u503e\u5411\u4e8e\u8fc7\u65e9\u56fa\u5b9a\u5355\u4e00\u63a8\u7406\u8def\u5f84\u800c\u65e0\u6cd5\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6536\u655b\u6027\u903b\u8f91\u63a8\u7406\uff08\u4ea7\u751f\u5355\u4e00\u6b63\u786e\u8bc1\u660e\uff09\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u8bb8\u591a\u63a8\u7406\u95ee\u9898\u5b58\u5728\u591a\u79cd\u6709\u6548\u63a8\u5bfc\u8def\u5f84\uff0c\u9700\u8981\u6a21\u578b\u63a2\u7d22\u591a\u6837\u5316\u7684\u903b\u8f91\u8def\u5f84\u800c\u975e\u56fa\u5b88\u5355\u4e00\u8def\u7ebf", "method": "\u63d0\u51faLogicGraph\u57fa\u51c6\uff0c\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u5411\u540e\u903b\u8f91\u751f\u6210\u548c\u8bed\u4e49\u5b9e\u4f8b\u5316\u6784\u5efa\u6c42\u89e3\u5668\u9a8c\u8bc1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u5305\u542b\u9ad8\u6df1\u5ea6\u591a\u8def\u5f84\u63a8\u7406\u548c\u56fa\u6709\u903b\u8f91\u5e72\u6270\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u5173\u8054\u4e00\u7ec4\u6700\u5c0f\u8bc1\u660e\u96c6\u5408", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5171\u540c\u5c40\u9650\uff1a\u503e\u5411\u4e8e\u8fc7\u65e9\u56fa\u5b9a\u5355\u4e00\u63a8\u7406\u8def\u5f84\u800c\u65e0\u6cd5\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u968f\u7740\u63a8\u7406\u6df1\u5ea6\u589e\u52a0\uff0c\u8986\u76d6\u5dee\u8ddd\u663e\u8457\u6269\u5927", "conclusion": "LogicGraph\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6536\u655b\u548c\u53d1\u6563\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8def\u5f84\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3"}}
{"id": "2602.21061", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21061", "abs": "https://arxiv.org/abs/2602.21061", "authors": ["David Koplow", "Tomer Galanti", "Tomaso Poggio"], "title": "Tool Building as a Path to \"Superintelligence\"", "comment": null, "summary": "The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $\u03b3$. In this work, we design a benchmark to measure $\u03b3$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $\u03b3$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2aGF(2)\u7535\u8def\u91cd\u5efa\u57fa\u51c6\u6765\u6d4b\u91cfLLMs\u5728\u903b\u8f91\u5206\u5e03\u5916\u63a8\u7406\u4e2d\u7684\u6b65\u9aa4\u6210\u529f\u6982\u7387\u03b3\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u90e8\u5206\u9c81\u68d2\u6027\uff0c\u800c\u5de5\u5177\u8c03\u7528\u7cbe\u5ea6\u662f\u5b9e\u73b0\u901a\u7528\u8d85\u667a\u80fd\u7684\u5173\u952e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30Diligent Learner\u6846\u67b6\u4e2dLLMs\u901a\u8fc7\u6d4b\u8bd5\u65f6\u641c\u7d22\u5b9e\u73b0\u8d85\u667a\u80fd\u7684\u53ef\u80fd\u6027\uff0c\u9700\u8981\u6d4b\u91cf\u6b65\u9aa4\u6210\u529f\u6982\u7387\u03b3\u3002\u5177\u4f53\u5173\u6ce8\u903b\u8f91\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u968f\u7740\u63a8\u7406\u6b65\u9aa4\u589e\u52a0\u800c\u53d8\u96be\uff0c\u4e14\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u770b\u5fc5\u987b\u6574\u5408\u6240\u6709\u4fe1\u606f\u624d\u80fd\u53ef\u9760\u89e3\u51b3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u51c6\u6765\u6d4b\u91cf\u03b3\u503c\uff0c\u6784\u5efa\u4e86GF(2)\u7535\u8def\u91cd\u5efa\u4efb\u52a1\u7c7b\u522b\u3002\u8fd9\u4e9b\u4efb\u52a1\u968f\u7740\u63a8\u7406\u6b65\u9aa4\u589e\u52a0\u800c\u53d8\u96be\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u8981\u6c42\u6a21\u578b\u5fc5\u987b\u4ed4\u7ec6\u6574\u5408\u6240\u6709\u63d0\u4f9b\u7684\u4fe1\u606f\u3002\u901a\u8fc7\u8be5\u57fa\u51c6\u5206\u6790\u4e0d\u540c\u89c4\u6a21LLMs\u7684\u03b3\u503c\u53d8\u5316\u8d8b\u52bf\u3002", "result": "\u5c0f\u578bLLMs\u7684\u03b3\u503c\u968f\u6df1\u5ea6\u589e\u52a0\u5448\u8d85\u7ebf\u6027\u4e0b\u964d\uff0c\u800c\u524d\u6cbf\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u90e8\u5206\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u6210\u529f\u7684\u89c4\u6a21\u5316\u63a8\u7406\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5de5\u5177\u8bbe\u8ba1\u88ab\u786e\u5b9a\u4e3aLLMs\u901a\u8fc7Diligent Learner\u6846\u67b6\u5b9e\u73b0\u901a\u7528\u8d85\u667a\u80fd\u7684\u5173\u952e\u80fd\u529b\u3002", "conclusion": "\u5de5\u5177\u8bbe\u8ba1\u662fLLMs\u901a\u8fc7Diligent Learner\u6846\u67b6\u5b9e\u73b0\u901a\u7528\u8d85\u667a\u80fd\u7684\u5173\u952e\u80fd\u529b\u3002\u867d\u7136\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u524d\u6cbf\u6a21\u578b\u663e\u793a\u51fa\u4e00\u5b9a\u7684\u9c81\u68d2\u6027\uff0c\u7cbe\u786e\u7684\u5de5\u5177\u8c03\u7528\u5bf9\u4e8e\u6210\u529f\u7684\u89c4\u6a21\u5316\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.21064", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21064", "abs": "https://arxiv.org/abs/2602.21064", "authors": ["Mehdi Acheli", "Walid Gaaloul"], "title": "Motivation is Something You Need", "comment": null, "summary": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.", "AI": {"tldr": "\u63d0\u51fa\u53d7\u60c5\u611f\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u53cc\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5927\u8111\u7684\"\u5bfb\u6c42\"\u52a8\u673a\u72b6\u6001\uff0c\u8ba9\u5c0f\u6a21\u578b\u6301\u7eed\u8bad\u7ec3\uff0c\u5927\u6a21\u578b\u5728\u7279\u5b9a\"\u52a8\u673a\u6761\u4ef6\"\u4e0b\u95f4\u6b47\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u53d7\u60c5\u611f\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\uff0c\u7279\u522b\u662f\u5927\u8111\u4e2d\u60c5\u7eea\u4e0e\u8ba4\u77e5\u7684\u76f8\u4e92\u4f5c\u7528\u4ee5\u53ca\"\u5bfb\u6c42\"\u52a8\u673a\u72b6\u6001\u3002\u4eba\u7c7b\u5728\u597d\u5947\u5fc3\u548c\u5956\u52b1\u9884\u671f\u589e\u5f3a\u65f6\uff0c\u4f1a\u6fc0\u6d3b\u66f4\u5e7f\u6cdb\u8111\u533a\u6765\u63d0\u5347\u8ba4\u77e5\u8868\u73b0\u3002\u4f5c\u8005\u5e0c\u671b\u5c06\u8fd9\u4e00\u539f\u7406\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u8bbe\u8ba1\u53cc\u6a21\u578b\u6846\u67b6\uff1a\u8f83\u5c0f\u7684\u57fa\u7840\u6a21\u578b\u6301\u7eed\u8bad\u7ec3\uff0c\u8f83\u5927\u7684\u52a8\u673a\u6a21\u578b\u5728\u9884\u5b9a\u4e49\u7684\"\u52a8\u673a\u6761\u4ef6\"\u4e0b\u95f4\u6b47\u6fc0\u6d3b\u3002\u5229\u7528\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u5927\u6a21\u578b\u6269\u5c55\u5c0f\u6a21\u578b\uff0c\u5b9e\u73b0\u6743\u91cd\u5171\u4eab\u548c\u7f51\u7edc\u5bb9\u91cf\u9009\u62e9\u6027\u6269\u5c55\u3002\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u65b9\u6848\uff0c\u5728\u91cd\u8981\u8bad\u7ec3\u6b65\u9aa4\u65f6\u6fc0\u6d3b\u5927\u6a21\u578b\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff1a1\uff09\u4ea4\u66ff\u8bad\u7ec3\u65b9\u6848\u76f8\u6bd4\u4f20\u7edf\u65b9\u6848\u80fd\u66f4\u9ad8\u6548\u5730\u589e\u5f3a\u57fa\u7840\u6a21\u578b\uff1b2\uff09\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u52a8\u673a\u6a21\u578b\u5c3d\u7ba1\u6bcf\u4e2aepoch\u770b\u5230\u7684\u6570\u636e\u66f4\u5c11\uff0c\u4f46\u4ecd\u80fd\u8d85\u8d8a\u5176\u72ec\u7acb\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\uff1b3\uff09\u80fd\u540c\u65f6\u8bad\u7ec3\u4e24\u4e2a\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u7ea6\u675f\u7684\u6a21\u578b\uff0c\u4fdd\u6301\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e\u5355\u72ec\u8bad\u7ec3\u5927\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u5927\u8111\u7684\u52a8\u673a\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53cc\u6a21\u578b\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8fd8\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u4e3a\u540c\u65f6\u8bad\u7ec3\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u9700\u6c42\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.21066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21066", "abs": "https://arxiv.org/abs/2602.21066", "authors": ["Claire McNamara", "Lucy Hederman", "Declan O'Sullivan"], "title": "The Initial Exploration Problem in Knowledge Graph Exploration", "comment": "13 pages", "summary": "Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u521d\u59cb\u63a2\u7d22\u95ee\u9898(IEP)\"\u7406\u8bba\uff0c\u89e3\u91ca\u975e\u4e13\u4e1a\u7528\u6237\u5728\u9996\u6b21\u63a5\u89e6\u77e5\u8bc6\u56fe\u8c31\u65f6\u9762\u4e34\u7684\u4e09\u5927\u969c\u788d\uff1a\u8303\u56f4\u4e0d\u786e\u5b9a\u6027\u3001\u672c\u4f53\u4e0d\u900f\u660e\u6027\u548c\u67e5\u8be2\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u754c\u9762\u8bbe\u8ba1\u7f3a\u4e4f\u6709\u6548\u7684\u8303\u56f4\u63ed\u793a\u4ea4\u4e92\u539f\u8bed\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u867d\u7136\u80fd\u6574\u5408\u590d\u6742\u4fe1\u606f\uff0c\u4f46\u5176\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u7ed3\u6784\u590d\u6742\u6027\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u8bbe\u7f6e\u4e86\u5de8\u5927\u969c\u788d\u3002\u7528\u6237\u5728\u9996\u6b21\u63a5\u89e6\u964c\u751f\u77e5\u8bc6\u56fe\u8c31\u65f6\u9762\u4e34\u72ec\u7279\u7684\u5b9a\u5411\u6311\u6218\uff1a\u4e0d\u77e5\u9053\u53ef\u4ee5\u95ee\u4ec0\u4e48\u95ee\u9898\u3001\u77e5\u8bc6\u5982\u4f55\u7ec4\u7ec7\u3001\u5982\u4f55\u5f00\u59cb\u63a2\u7d22\u3002\u8fd9\u79cd\"\u521d\u59cb\u63a2\u7d22\u95ee\u9898\"\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u8bba\u5316\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u884c\u4e3a\u548c\u4eba\u673a\u4ea4\u4e92\u7406\u8bba\uff08\u5305\u62ecASK\u3001\u63a2\u7d22\u5f0f\u641c\u7d22\u3001\u4fe1\u606f\u89c5\u98df\u548c\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff09\uff0c\u6784\u5efaIEP\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5206\u6790\u5176\u4e09\u5927\u76f8\u4e92\u4f9d\u8d56\u7684\u969c\u788d\u3002\u901a\u8fc7\u5206\u6790\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\u754c\u9762\u7684\u4ea4\u4e92\u539f\u8bed\uff0c\u63ed\u793a\u5f53\u524d\u7cfb\u7edf\u5728\u9996\u6b21\u63a5\u89e6\u65f6\u7684\u8ba4\u77e5\u5047\u8bbe\u7f3a\u9677\u3002", "result": "\u8bc6\u522b\u5e76\u7406\u8bba\u5316\u4e86IEP\u73b0\u8c61\uff0c\u63d0\u51fa\u5176\u4e09\u5927\u6838\u5fc3\u969c\u788d\uff1a\u8303\u56f4\u4e0d\u786e\u5b9a\u6027\uff08\u4e0d\u77e5\u9053KG\u5305\u542b\u4ec0\u4e48\uff09\u3001\u672c\u4f53\u4e0d\u900f\u660e\u6027\uff08\u4e0d\u7406\u89e3\u77e5\u8bc6\u7ed3\u6784\uff09\u3001\u67e5\u8be2\u80fd\u529b\u4e0d\u8db3\uff08\u65e0\u6cd5\u6709\u6548\u63d0\u95ee\uff09\u3002\u53d1\u73b0\u5f53\u524dKG\u754c\u9762\u8bbe\u8ba1\u5b58\u5728\u7ed3\u6784\u6027\u7a7a\u767d\uff1a\u7f3a\u4e4f\"\u8303\u56f4\u63ed\u793a\"\u4ea4\u4e92\u539f\u8bed\uff0c\u65e0\u6cd5\u5728\u4e0d\u8981\u6c42\u7528\u6237\u67e5\u8be2\u6216\u7406\u89e3\u672c\u4f53\u7684\u524d\u63d0\u4e0b\u4f20\u8fbeKG\u5185\u5bb9\u3002", "conclusion": "IEP\u7406\u8bba\u4e3a\u8bc4\u4f30\u77e5\u8bc6\u56fe\u8c31\u754c\u9762\u63d0\u4f9b\u4e86\u7406\u8bba\u89c6\u89d2\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u652f\u6301\u521d\u59cb\u63a2\u7d22\u7684\u5165\u53e3\u811a\u624b\u67b6\u6307\u660e\u4e86\u65b9\u5411\u3002\u672a\u6765\u8bbe\u8ba1\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u4ea4\u4e92\u539f\u8bed\u6765\u63ed\u793aKG\u8303\u56f4\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u9996\u6b21\u63a5\u89e6\u65f6\u514b\u670d\u4e09\u5927\u969c\u788d\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u63a2\u7d22\u3002"}}
{"id": "2602.21143", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21143", "abs": "https://arxiv.org/abs/2602.21143", "authors": ["Debjit Paul", "Daniel Murphy", "Milan Gritta", "Ronald Cardenas", "Victor Prokhorov", "Lena Sophia Bolliger", "Aysim Toker", "Roy Miles", "Andreea-Maria Oncescu", "Jasivan Alex Sivakumar", "Philipp Borchert", "Ismail Elezi", "Meiru Zhang", "Ka Yiu Lee", "Guchun Zhang", "Jun Wang", "Gerasimos Lampouras"], "title": "A Benchmark for Deep Information Synthesis", "comment": "Accepted at ICLR 2026", "summary": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.", "AI": {"tldr": "DEEPSYNTH\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b120\u4e2a\u8de87\u4e2a\u9886\u57df\u300167\u4e2a\u56fd\u5bb6\u7684\u4efb\u52a1\uff0c\u8981\u6c42\u4fe1\u606f\u6536\u96c6\u3001\u7efc\u5408\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5f53\u524dSOTA\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff08F1\u6700\u9ad8\u4ec58.97-17.5\uff09\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u89e3\u51b3\u9700\u8981\u4ece\u591a\u6e90\u4fe1\u606f\u5408\u6210\u548c\u8d85\u8d8a\u7b80\u5355\u4e8b\u5b9e\u68c0\u7d22\u7684\u63a8\u7406\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u6784\u5efaDEEPSYNTH\u57fa\u51c6\uff1a\u6536\u96c6\u5b98\u65b9\u6570\u636e\u6e90\u3001\u521b\u5efa\u5047\u8bbe\u3001\u8fdb\u884c\u624b\u52a8\u5206\u6790\u3001\u8bbe\u8ba1\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u4efb\u52a1\uff0c\u5305\u542b120\u4e2a\u8de87\u4e2a\u9886\u57df\u548c67\u4e2a\u56fd\u5bb6\u7684\u4efb\u52a1\u3002", "result": "\u5728DEEPSYNTH\u4e0a\u8bc4\u4f3011\u4e2aSOTA LLM\u548c\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\uff0c\u6700\u5927F1\u5206\u6570\u4ec58.97\uff08LLM-judge\u6307\u6807\u4e3a17.5\uff09\uff0c\u8868\u660e\u57fa\u51c6\u96be\u5ea6\u5f88\u9ad8\uff0c\u5f53\u524d\u667a\u80fd\u4f53\u5728\u5e7b\u89c9\u548c\u5927\u4fe1\u606f\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "DEEPSYNTH\u662f\u4e00\u4e2a\u5173\u952e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5e7b\u89c9\u548c\u590d\u6742\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2602.21154", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21154", "abs": "https://arxiv.org/abs/2602.21154", "authors": ["Ziwei Niu", "Hao Sun", "Shujun Bian", "Xihong Yang", "Lanfen Lin", "Yuxin Liu", "Yueming Jin"], "title": "CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning", "comment": "Accepted by ICASSP 2026", "summary": "Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.", "AI": {"tldr": "CG-DMER\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u591a\u6a21\u6001\u5fc3\u7535\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u63a9\u7801\u5efa\u6a21\u548c\u8868\u793a\u89e3\u8026\u5bf9\u9f50\u7b56\u7565\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5ffd\u7565\u5bfc\u8054\u95f4\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5fc3\u7535\u56fe\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5728\u6a21\u6001\u5185\u90e8\uff0c\u73b0\u6709\u6a21\u578b\u4ee5\u5bfc\u8054\u65e0\u5173\u7684\u65b9\u5f0f\u5904\u7406\u5fc3\u7535\u56fe\uff0c\u5ffd\u7565\u4e86\u5bfc\u8054\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5efa\u6a21\u7ec6\u7c92\u5ea6\u8bca\u65ad\u6a21\u5f0f\u7684\u80fd\u529b\uff1b2\uff09\u5728\u6a21\u6001\u4e4b\u95f4\uff0c\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5c06\u5fc3\u7535\u56fe\u4fe1\u53f7\u4e0e\u4e34\u5e8a\u62a5\u544a\u5bf9\u9f50\uff0c\u7531\u4e8e\u62a5\u544a\u7684\u81ea\u7531\u6587\u672c\u6027\u8d28\u5f15\u5165\u4e86\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u3002", "method": "\u63d0\u51faCG-DMER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1\uff09\u7a7a\u95f4-\u65f6\u95f4\u63a9\u7801\u5efa\u6a21\uff1a\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5e94\u7528\u63a9\u7801\u5e76\u91cd\u5efa\u7f3a\u5931\u4fe1\u606f\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u548c\u5bfc\u8054\u95f4\u7a7a\u95f4\u4f9d\u8d56\uff1b2\uff09\u8868\u793a\u89e3\u8026\u548c\u5bf9\u9f50\u7b56\u7565\uff1a\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u548c\u6a21\u6001\u5171\u4eab\u7f16\u7801\u5668\uff0c\u786e\u4fdd\u6a21\u6001\u4e0d\u53d8\u548c\u6a21\u6001\u7279\u5b9a\u8868\u793a\u4e4b\u95f4\u7684\u6e05\u6670\u5206\u79bb\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u566a\u58f0\u548c\u6a21\u6001\u7279\u5b9a\u504f\u5dee\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCG-DMER\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CG-DMER\u901a\u8fc7\u89e3\u51b3\u5fc3\u7535\u56fe\u5206\u6790\u4e2d\u7684\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u5fc3\u7535\u56fe\u4fe1\u53f7\u7684\u65f6\u7a7a\u7279\u5f81\u5e76\u51cf\u5c11\u6a21\u6001\u504f\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.21172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21172", "abs": "https://arxiv.org/abs/2602.21172", "authors": ["Ishaan Rawal", "Shubh Gupta", "Yihan Hu", "Wei Zhan"], "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning", "comment": "Accepted to CVPR 2026", "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no reasoning annotations, resulting in 3$\\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \\modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \\modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.", "AI": {"tldr": "NoRD\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709VLA\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u4ec5\u9700\u4e0d\u523060%\u7684\u8bad\u7ec3\u6570\u636e\u548c\u65e0\u63a8\u7406\u6807\u6ce8\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u9762\u4e34\u4e24\u5927\u6602\u8d35\u9700\u6c42\uff1a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u548c\u5bc6\u96c6\u63a8\u7406\u6807\u6ce8\u3002\u8fd9\u4e9b\u8981\u6c42\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faNoRD\u6a21\u578b\uff0c\u4f7f\u7528\u5c0f\u4e8e60%\u7684\u6570\u636e\u548c\u65e0\u63a8\u7406\u6807\u6ce8\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5f15\u5165Dr.GRPO\u7b97\u6cd5\u6765\u514b\u670d\u6807\u51c6GRPO\u5728\u5c0f\u89c4\u6a21\u65e0\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u89e3\u51b3\u96be\u5ea6\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5728Waymo\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u8bad\u7ec3token\u6570\u91cf\u51cf\u5c11\u4e863\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "NoRD\u6a21\u578b\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u9700\u6c42\u548c\u6d88\u9664\u63a8\u7406\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.21201", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21201", "abs": "https://arxiv.org/abs/2602.21201", "authors": ["Tony Feng", "Junehyuk Jung", "Sang-hyun Kim", "Carlo Pagano", "Sergei Gukov", "Chiang-Chiang Tsai", "David Woodruff", "Adel Javanmard", "Aryan Mokhtari", "Dawsen Hwang", "Yuri Chervonyi", "Jonathan N. Lee", "Garrett Bingham", "Trieu H. Trinh", "Vahab Mirrokni", "Quoc V. Le", "Thang Luong"], "title": "Aletheia tackles FirstProof autonomously", "comment": "34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia", "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.", "AI": {"tldr": "Aletheia\u6570\u5b66\u7814\u7a76\u4ee3\u7406\u5728FirstProof\u6311\u6218\u4e2d\u81ea\u4e3b\u89e3\u51b3\u4e8610\u4e2a\u95ee\u9898\u4e2d\u76846\u4e2a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30Aletheia\u6570\u5b66\u7814\u7a76\u4ee3\u7406\u5728\u771f\u5b9e\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u6311\u6218\u4e2d\u7684\u8868\u73b0\uff0c\u6d4b\u8bd5\u5176\u81ea\u4e3b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528Gemini 3 Deep Think\u9a71\u52a8\u7684Aletheia\u4ee3\u7406\u53c2\u4e0eFirstProof\u6311\u6218\uff0c\u5728\u9650\u5b9a\u65f6\u95f4\u5185\u81ea\u4e3b\u89e3\u51b3\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "Aletheia\u6210\u529f\u89e3\u51b3\u4e8610\u4e2a\u95ee\u9898\u4e2d\u76846\u4e2a\uff08\u95ee\u98982\u30015\u30017\u30018\u30019\u300110\uff09\uff0c\u5176\u4e2d\u95ee\u98988\u7684\u4e13\u5bb6\u8bc4\u4f30\u5b58\u5728\u5206\u6b67\uff0c\u5176\u4ed6\u95ee\u9898\u83b7\u5f97\u591a\u6570\u4e13\u5bb6\u8ba4\u53ef\u3002", "conclusion": "Aletheia\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u80fd\u529b\uff0c\u80fd\u591f\u81ea\u4e3b\u89e3\u51b3\u590d\u6742\u7684\u6570\u5b66\u95ee\u9898\uff0c\u4e3aAI\u6570\u5b66\u7814\u7a76\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
