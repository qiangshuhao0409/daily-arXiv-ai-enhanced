<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things](https://arxiv.org/abs/2510.21103)
*Zongyang Yuan,Lailong Luo,Qianzhen Zhang,Bangbang Ren,Deke Guo,Richard T. B. Ma*

Main category: cs.NI

TL;DR: 提出了一种名为Senses的在线节能解决方案，通过多智能体强化学习在边缘物联网网络中实现数据去重和能耗优化，能节省11.37%控制设备能耗并延长20%网络运行时间。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备数量增长和应用场景丰富，传感器数据爆炸式增加，但大量数据在计算和传输过程中消耗大量能量。不合理的重复数据覆盖会导致"蝴蝶效应"，因此需要解决数据冗余和能耗问题。

Method: 采用多智能体强化学习(MARL)，在传感器层面动态调整传感器覆盖范围实现数据去重；在控制器层面进行数据分区和冗余数据消除；在全局层面考虑设备异构性，平衡设备运行时间以延长整体网络寿命。

Result: 通过测试台实验和仿真评估，Senses能够节省控制设备11.37%的能耗，并将物联网设备网络的整体运行时间延长20%。

Conclusion: Senses是一种有效的边缘物联网网络在线节能解决方案，通过多层次的数据去重和能耗优化策略，显著提升了网络能效和运行寿命。

Abstract: As the number of Internet of Things (IoT) devices continuously grows and
application scenarios constantly enrich, the volume of sensor data experiences
an explosive increase. However, substantial data demands considerable energy
during computation and transmission. Redundant deployment or mobile assistance
is essential to cover the target area reliably with fault-prone sensors.
Consequently, the ``butterfly effect" may appear during the IoT operation,
since unreasonable data overlap could result in many duplicate data. To this
end, we propose Senses, a novel online energy saving solution for edge IoT
networks, with the insight of sensing and storing less at the network edge by
adopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data
de-duplication by dynamically adjusting sensor coverage at the sensor level.
For exceptional cases where sensor coverage cannot be altered, Senses conducts
data partitioning and eliminates redundant data at the controller level.
Furthermore, at the global level, considering the heterogeneity of IoT devices,
Senses balances the operational duration among the devices to prolong the
overall operational duration of edge IoT networks. We evaluate the performance
of Senses through testbed experiments and simulations. The results show that
Senses saves 11.37% of energy consumption on control devices and prolongs 20%
overall operational duration of the IoT device network.

</details>


### [2] [Enhanced Evolutionary Multi-Objective Deep Reinforcement Learning for Reliable and Efficient Wireless Rechargeable Sensor Networks](https://arxiv.org/abs/2510.21127)
*Bowei Tong,Hui Kang,Jiahui Li,Geng Sun,Jiacheng Wang,Yaoqi Yang,Bo Xu,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出一种增强的进化多目标深度强化学习算法，用于无线可充电传感器网络中的移动充电优化，同时最大化节点生存率和充电能量效率。


<details>
  <summary>Details</summary>
Motivation: 传统电池供电传感器网络在偏远环境中寿命有限且维护频繁，无线可充电传感器网络虽能延长网络寿命，但面临节点生存率与充电能量效率之间的权衡挑战。

Method: 集成LSTM策略网络进行时序模式识别、多层感知机前瞻增量模型进行未来状态预测，以及时变Pareto策略评估方法进行动态偏好适应。

Result: 算法在平衡节点生存率和能量效率方面显著优于现有方法，LSTM增强策略网络收敛速度快25%，时变评估方法能有效适应动态条件。

Conclusion: 所提算法能有效解决无线可充电传感器网络中移动充电的多目标优化问题，生成多样化的Pareto最优解，适应动态操作环境。

Abstract: Despite rapid advancements in sensor networks, conventional battery-powered
sensor networks suffer from limited operational lifespans and frequent
maintenance requirements that severely constrain their deployment in remote and
inaccessible environments. As such, wireless rechargeable sensor networks
(WRSNs) with mobile charging capabilities offer a promising solution to extend
network lifetime. However, WRSNs face critical challenges from the inherent
trade-off between maximizing the node survival rates and maximizing charging
energy efficiency under dynamic operational conditions. In this paper, we
investigate a typical scenario where mobile chargers move and charge the
sensor, thereby maintaining the network connectivity while minimizing the
energy waste. Specifically, we formulate a multi-objective optimization problem
that simultaneously maximizes the network node survival rate and mobile charger
energy usage efficiency across multiple time slots, which presents NP-hard
computational complexity with long-term temporal dependencies that make
traditional optimization approaches ineffective. To address these challenges,
we propose an enhanced evolutionary multi-objective deep reinforcement learning
algorithm, which integrates a long short-term memory (LSTM)-based policy
network for temporal pattern recognition, a multilayer perceptron-based
prospective increment model for future state prediction, and a time-varying
Pareto policy evaluation method for dynamic preference adaptation. Extensive
simulation results demonstrate that the proposed algorithm significantly
outperforms existing approaches in balancing node survival rate and energy
efficiency while generating diverse Pareto-optimal solutions. Moreover, the
LSTM-enhanced policy network converges 25% faster than conventional networks,
with the time-varying evaluation method effectively adapting to dynamic
conditions.

</details>


### [3] [A Confidence-Constrained Cloud-Edge Collaborative Framework for Autism Spectrum Disorder Diagnosis](https://arxiv.org/abs/2510.21130)
*Qi Deng,Yinghao Zhang,Yalin Liu,Bishenghui Tao*

Main category: cs.NI

TL;DR: 提出C3EKD框架，在边缘设备执行大部分推理，仅上传低置信度样本到云端进行知识蒸馏，解决ASD诊断中的隐私和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 学校环境中ASD诊断系统依赖物联网摄像头，但纯云处理存在隐私和延迟问题，纯边缘推理准确率有限。

Method: 采用分层框架，边缘执行主要推理，选择性上传低置信度样本到云端；云端生成温度缩放软标签，通过跨学校聚合的全局损失蒸馏回边缘模型。

Result: 在两个公共ASD面部图像数据集上达到87.4%的准确率。

Conclusion: 该框架在现实应用中具有可扩展部署潜力，能在不集中原始数据的情况下提高泛化能力。

Abstract: Autism Spectrum Disorder (ASD) diagnosis systems in school environments
increasingly relies on IoT-enabled cameras, yet pure cloud processing raises
privacy and latency concerns while pure edge inference suffers from limited
accuracy. We propose Confidence-Constrained Cloud-Edge Knowledge Distillation
(C3EKD), a hierarchical framework that performs most inference at the edge and
selectively uploads only low-confidence samples to the cloud. The cloud
produces temperature-scaled soft labels and distils them back to edge models
via a global loss aggregated across participating schools, improving
generalization without centralizing raw data. On two public ASD facial-image
datasets, the proposed framework achieves a superior accuracy of 87.4\%,
demonstrating its potential for scalable deployment in real-world applications.

</details>


### [4] [TURBOTEST: Learning When Less is Enough through Early Termination of Internet Speed Tests](https://arxiv.org/abs/2510.21141)
*Haarika Manda,Manshi Sagar,Yogesh,Kartikay Singh,Cindy Zhao,Tarun Mangla,Phillipa Gill,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: TURBOTEST是一个基于机器学习的速度测试终止框架，通过解耦吞吐量预测和测试终止决策，显著减少测试数据量同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有速度测试平台（如Ookla、M-Lab、Fast.com）每月产生PB级流量，单次高速测试可传输数百MB数据，造成巨大成本负担。需要在不牺牲准确性的前提下提前终止测试。

Method: 采用两阶段框架：第一阶段训练回归器从部分测量值预测最终吞吐量；第二阶段训练分类器决定何时积累足够证据可以停止测试。利用传输层特征（RTT、重传、拥塞窗口）和吞吐量数据。

Result: 在17.3万次M-Lab NDT速度测试上的评估显示，TURBOTEST相比基于BBR信号的方法实现2-4倍数据节省，同时降低中位误差。

Conclusion: 基于机器学习的自适应终止方法能够提供准确、高效且可部署的大规模速度测试解决方案。

Abstract: Internet speed tests are indispensable for users, ISPs, and policymakers, but
their static flooding-based design imposes growing costs: a single high-speed
test can transfer hundreds of megabytes, and collectively, platforms like
Ookla, M-Lab, and Fast.com generate petabytes of traffic each month. Reducing
this burden requires deciding when a test can be stopped early without
sacrificing accuracy. We frame this as an optimal stopping problem and show
that existing heuristics-static thresholds, BBR pipe-full signals, or
throughput stability rules from Fast.com and FastBTS-capture only a narrow
portion of the achievable accuracy-savings trade-off. This paper introduces
TURBOTEST, a systematic framework for speed test termination that sits atop
existing platforms. The key idea is to decouple throughput prediction (Stage 1)
from test termination (Stage 2): Stage 1 trains a regressor to estimate final
throughput from partial measurements, while Stage 2 trains a classifier to
decide when sufficient evidence has accumulated to stop. Leveraging richer
transport-level features (RTT, retransmissions, congestion window) alongside
throughput, TURBOTEST exposes a single tunable parameter for accuracy tolerance
and includes a fallback mechanism for high-variability cases. Evaluation on
173,000 M-Lab NDT speed tests (2024-2025) shows that TURBOTEST achieves nearly
2-4x higher data savings than an approach based on BBR signals while reducing
median error. These results demonstrate that adaptive ML-based termination can
deliver accurate, efficient, and deployable speed tests at scale.

</details>


### [5] [Quality of Coverage (QoC): A New Paradigm for Quantifying Cellular Network Coverage Quality, Usability and Stability](https://arxiv.org/abs/2510.21162)
*Varshika Srinivasavaradhan,Morgan Vigil-Hayes,Ellen Zegura,Elizabeth Belding*

Main category: cs.NI

TL;DR: 提出了一种新的多维度蜂窝网络覆盖质量(QoC)指标，通过实际测量的性能质量、可用性和稳定性来更精细地表征蜂窝网络覆盖，相比现有简单带宽指标能更好地反映时空可用性和弹性。


<details>
  <summary>Details</summary>
Motivation: 当前蜂窝网络覆盖表示过于简化，仅提供最低带宽水平，未能纳入网络在时空上的稳定性这一关键可用性组件。由于无线传播特性和网络负载与容量关系，蜂窝覆盖质量很复杂，需要更精细的表征。

Method: 定义了一套QoC关键性能指标(KPIs)，使用三个不同的数据集分析这些指标表征网络行为的能力，验证QoC比现有指标能提供更精细有用的蜂窝覆盖表示。

Result: QoC KPIs能够有效表征网络行为，相比当前指标能够提供更细粒度和更有用的蜂窝网络覆盖表示，更好地捕捉时空可用性和弹性。

Conclusion: QoC提供了一种新颖的多维度方法来表征蜂窝网络覆盖质量，通过实际测量的性能、可用性和稳定性指标，能够比传统简单带宽指标更全面地反映网络覆盖的实际用户体验。

Abstract: Current representations of cellular coverage are overly simplistic; they
state only the minimal level of available bandwidth (i.e., 35/3Mbps
download/upload speed for 5G) and fail to incorporate a critical component of
usability: network stability over space and time. Cellular coverage quality is
complex given wireless propagation characteristics and relationships between
network load and (often limited) network capacity. A more fine-grained
characterization is essential. We introduce Quality of Coverage (QoC), a novel
multi-dimensional set of key performance indicators (KPIs) that reflect actual
measured performance quality, usability and stability. This representation of
the coverage of the cellular network more fully captures temporal and spatial
usability and resilience. We motivate and define a set of QoC KPIs and use
three distinct datasets to analyze the ability of the KPIs to characterize
network behavior, demonstrating the ability of QoC to offer a more fine-grained
and useful representation of cellular coverage than possible with current
metrics.

</details>


### [6] [Source-Coded Online Algorithm for Multicast Subgraph Construction](https://arxiv.org/abs/2510.21580)
*Tomas Lestayo Martinez,Manuel Fernandez Veiega Veiga*

Main category: cs.NI

TL;DR: 提出了一种基于最大流分解的源编码组播框架，通过构建多条不相交或部分重叠路径，结合路径重定向机制，在保证无环路传输的同时最大化吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统组播树存在路径冗余和网络资源利用效率低的问题，而网络编码虽然能达到容量上限但计算开销大且部署困难，需要一种更实用的高吞吐量组播解决方案。

Method: 利用最大流分解构建从源到所有接收者的多条路径，引入路径重定向机制在流首次相交时重新调整下游路径，开发了路径构建、重叠检测和组播子图迭代优化算法。

Result: 在合成和真实网络拓扑上的评估表明，该方法接近网络编码的吞吐量但编码解码复杂度显著降低，同时在公平性、链路故障鲁棒性和传输效率方面明显优于传统组播树。

Conclusion: 源编码组播是下一代网络高吞吐量自适应群组通信的实用且可扩展的解决方案。

Abstract: Multicast remains a fundamental mechanism for scalable content distribution,
yet existing approaches face critical limitations. Traditional multicast trees
suffer from path redundancy and inefficient utilization of network resources,
while network coding, although capacity-achieving, incurs significant
computational overhead and deployment challenges. In this paper, we introduce a
source-coded multicast framework that exploits maximum-flow decomposition to
construct multiple disjoint or partially overlapping paths from the source to
all receivers. Our scheme incorporates a novel path redirection mechanism: when
multiple overlaps occur between receiver flows, downstream paths are realigned
at the first intersection, ensuring loop-free delivery while maximizing overall
throughput. We develop algorithms for path construction, overlap detection, and
iterative refinement of multicast subgraphs, and analyze their computational
complexity. Through extensive evaluation on synthetic and real network
topologies, we demonstrate that the proposed method consistently approaches the
throughput of network coding with substantially lower encoding and decoding
complexity, while significantly outperforming multicast tree constructions in
terms of fairness, robustness to link failures, and delivery efficiency. These
results position source-coded multicast as a practical and scalable solution
for next-generation networks requiring high-throughput and adaptive group
communication.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM](https://arxiv.org/abs/2510.20838)
*Abir Khan Ratul,Sanjay Acharjee,Somin Park,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 该研究开发了一个人机交互流程，将手绘平面图转换为语义一致的3D BIM模型，使用多模态大语言模型和多代理框架，通过反馈迭代提高准确性。


<details>
  <summary>Details</summary>
Motivation: 使BIM创建对专家和非专家都更易访问，仅使用手绘草图即可生成3D BIM模型。

Method: 采用多模态大语言模型的多代理框架，结合感知提取、人工反馈、模式验证和自动化BIM脚本生成，将草图迭代优化为结构化JSON布局，再转换为可执行脚本。

Result: 在10个不同平面图上实验显示：门窗检测初始准确率高，墙体检测从83%开始，经几次反馈迭代后接近完美对齐；所有类别的精确率、召回率和F1分数均超过0.83，几何误差通过反馈修正逐步降至零。

Conclusion: MLLM驱动的多代理推理能够使BIM创建对专家和非专家都变得可访问，仅需使用手绘草图。

Abstract: This study introduces a human-in-the-loop pipeline that converts unscaled,
hand-drawn floor plan sketches into semantically consistent 3D BIM models. The
workflow leverages multimodal large language models (MLLMs) within a
multi-agent framework, combining perceptual extraction, human feedback, schema
validation, and automated BIM scripting. Initially, sketches are iteratively
refined into a structured JSON layout of walls, doors, and windows. Later,
these layouts are transformed into executable scripts that generate 3D BIM
models. Experiments on ten diverse floor plans demonstrate strong convergence:
openings (doors, windows) are captured with high reliability in the initial
pass, while wall detection begins around 83% and achieves near-perfect
alignment after a few feedback iterations. Across all categories, precision,
recall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)
progressively decrease to zero through feedback corrections. This study
demonstrates how MLLM-driven multi-agent reasoning can make BIM creation
accessible to both experts and non-experts using only freehand sketches.

</details>


### [8] [Cultural Alien Sampler: Open-ended art generation balancing originality and coherence](https://arxiv.org/abs/2510.20849)
*Alejandro H. Artiles,Hiromu Yakura,Levin Brinkmann,Mar Canet Sola,Hassan Abu Alhaija,Ignacio Serna,Nasim Rahaman,Bernhard Schölkopf,Iyad Rahwan*

Main category: cs.AI

TL;DR: 提出Cultural Alien Sampler (CAS)方法，通过分离概念组合的连贯性和文化典型性，生成既具有内部一致性又偏离文化惯例的创意想法


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型在开放领域创意生成中要么固守熟悉的文化模式，要么在追求新颖性时牺牲连贯性的问题

Method: 使用两个在WikiArt概念上微调的GPT-2模型：概念连贯性模型评估概念在艺术品中共同出现的合理性，文化背景模型估计这些组合在艺术家作品中的典型性

Result: 在人类评估中优于随机选择和GPT-4o基线，在原创性和和谐度方面与艺术专业学生表现相当，定量研究显示比GPT-4o产生更多样化的输出和更广泛的概念空间探索

Conclusion: 人工文化异化可以释放自主代理的创造潜力，CAS方法能生成既保持内部一致性又偏离文化惯例的创意想法

Abstract: In open-ended domains like art, autonomous agents must generate ideas that
are both original and internally coherent, yet current Large Language Models
(LLMs) either default to familiar cultural patterns or sacrifice coherence when
pushed toward novelty. We address this by introducing the Cultural Alien
Sampler (CAS), a concept-selection method that explicitly separates
compositional fit from cultural typicality. CAS uses two GPT-2 models
fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether
concepts plausibly co-occur within artworks, and a Cultural Context Model that
estimates how typical those combinations are within individual artists' bodies
of work. CAS targets combinations that are high in coherence and low in
typicality, yielding ideas that maintain internal consistency while deviating
from learned conventions and embedded cultural context. In a human evaluation
(N = 100), our approach outperforms random selection and GPT-4o baselines and
achieves performance comparable to human art students in both perceived
originality and harmony. Additionally, a quantitative study shows that our
method produces more diverse outputs and explores a broader conceptual space
than its GPT-4o counterpart, demonstrating that artificial cultural alienness
can unlock creative potential in autonomous agents.

</details>


### [9] [Fuzzy numbers revisited: operations on extensional fuzzy numbers](https://arxiv.org/abs/2510.20861)
*Krzysztof Siminski*

Main category: cs.AI

TL;DR: 本文提出了一种新的模糊数表示方法——外延模糊数，以解决传统模糊数运算中的计算复杂性和结果形状变化问题，并定义了相关运算和关系运算符。


<details>
  <summary>Details</summary>
Motivation: 传统模糊数使用模糊集表示，但运算存在计算复杂度高、结果形状变化和模糊度扩散等问题，限制了模糊数的应用范围。

Method: 提出了外延模糊数的概念，定义了外延模糊数上的运算（如加法、乘法等）和关系运算符（=、>、>=、<、<=），并通过C++实现验证。

Result: 外延模糊数方法能够避免传统模糊数运算中的问题，保持运算结果的形状特征，减少计算复杂度，并通过应用示例展示了有效性。

Conclusion: 外延模糊数为模糊数运算提供了一种更高效和实用的替代方案，能够克服传统方法的局限性，扩展了模糊数的应用潜力。

Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to
better represent imprecise data. However, operations on fuzzy numbers are not
as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension
rule is applied to elaborate a result. This can produce two problems: (1) high
computational complexity and (2) for some fuzzy sets and some operations the
results is not a fuzzy set with the same features (eg. multiplication of two
triangular fuzzy sets does not produce a triangular fuzzy set). One more
problem is the fuzzy spread -- fuzziness of the result increases with the
number of operations. These facts can severely limit the application field of
fuzzy numbers. In this paper we would like to revisite this problem with a
different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines
operations on extensional fuzzy numbers and relational operators (=, >, >=, <,
<=) for them. The proposed approach is illustrated with several applicational
examples. The C++ implementation is available from a public GitHub repository.

</details>


### [10] [Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems](https://arxiv.org/abs/2510.21027)
*Zhe Fei,Mehmet Yigit Turali,Shreyas Rajesh,Xinyang Dai,Huyen Pham,Pavan Holur,Yuhui Zhu,Larissa Mooney,Yih-Ing Hser,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出了一个基于开源大语言模型的框架，用于从异构电子健康记录中提取阿片类药物使用障碍治疗处方信息，并计算标准化的药物覆盖天数指标。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录系统中药物数据标准化的问题，特别是在阿片类药物使用障碍治疗监测中，处方关键属性分散在不同格式字段和自由文本中。

Method: 定制开源大语言模型（Llama、Qwen、Gemma、MedGemma）从异构数据中提取统一处方属性，通过JSON模式处理记录，进行轻量级标准化和跨字段一致性检查。

Result: 在5个诊所的25,605条记录上评估，Qwen2.5-32B达到93.4%覆盖率和93.0%准确率，MedGemma-27B达到93.1%/92.2%。

Conclusion: 该方法消除了脆弱的站点特定ETL流程，支持本地隐私保护部署，实现了真实环境中MOUD暴露、依从性和保留率的一致跨站点分析。

Abstract: Harmonizing medication data across Electronic Health Record (EHR) systems is
a persistent barrier to monitoring medications for opioid use disorder (MOUD).
In heterogeneous EHR systems, key prescription attributes are scattered across
differently formatted fields and freetext notes. We present a practical
framework that customizes open source large language models (LLMs), including
Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription
attributes (prescription date, drug name, duration, total quantity, daily
quantity, and refills) from heterogeneous, site specific data and compute a
standardized metric of medication coverage, \emph{MOUD days}, per patient. Our
pipeline processes records directly in a fixed JSON schema, followed by
lightweight normalization and cross-field consistency checks. We evaluate the
system on prescription level EHR data from five clinics in a national OUD study
(25{,}605 records from 1{,}257 patients), using a previously annotated
benchmark of 10{,}369 records (776 patients) as the ground truth. Performance
is reported as coverage (share of records with a valid, matchable output) and
record-level exact-match accuracy. Larger models perform best overall:
Qwen2.5-32B achieves \textbf{93.4\%} coverage with \textbf{93.0\%} exact-match
accuracy across clinics, and MedGemma-27B attains
\textbf{93.1\%}/\textbf{92.2\%}. A brief error review highlights three common
issues and fixes: imputing missing dosage fields using within-drug norms,
handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from
the documented schedule, and adding unit checks to prevent mass units (e.g.,
``250 g'') from being misread as daily counts. By removing brittle,
site-specific ETL and supporting local, privacy-preserving deployment, this
approach enables consistent cross-site analyses of MOUD exposure, adherence,
and retention in real-world settings.

</details>


### [11] [Epistemic Deference to AI](https://arxiv.org/abs/2510.21043)
*Benjamin Lange*

Main category: cs.AI

TL;DR: 该论文探讨了何时应该优先考虑AI输出而非人类专家判断，提出了AI优先主义及其替代方案——全证据观点，认为AI输出应作为贡献性理由而非完全取代人类独立认知。


<details>
  <summary>Details</summary>
Motivation: 基于社会认识论，探讨AI系统作为人工认知权威(AEAs)的资格，以及AI输出与人类判断之间的关系，旨在为AI依赖提供原则性指导。

Method: 通过分析AI优先主义的经典反对意见（如不加批判的依赖、认知固化和认知基础脱节），并发展全证据观点作为替代方案。

Result: 全证据观点具有三个关键优势：减轻专业知识萎缩、为有意义的人类监督提供认知依据、解释AI不可靠时的合理不信任。

Conclusion: 全证据观点为确定何时AI依赖是合理的提供了原则性方法，特别适用于需要严格可靠性的高风险情境。

Abstract: When should we defer to AI outputs over human expert judgment? Drawing on
recent work in social epistemology, I motivate the idea that some AI systems
qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated
reliability and epistemic superiority. I then introduce AI Preemptionism, the
view that AEA outputs should replace rather than supplement a user's
independent epistemic reasons. I show that classic objections to preemptionism
- such as uncritical deference, epistemic entrenchment, and unhinging epistemic
bases - apply in amplified form to AEAs, given their opacity, self-reinforcing
authority, and lack of epistemic failure markers. Against this, I develop a
more promising alternative: a total evidence view of AI deference. According to
this view, AEA outputs should function as contributory reasons rather than
outright replacements for a user's independent epistemic considerations. This
approach has three key advantages: (i) it mitigates expertise atrophy by
keeping human users engaged, (ii) it provides an epistemic case for meaningful
human oversight and control, and (iii) it explains the justified mistrust of AI
when reliability conditions are unmet. While demanding in practice, this
account offers a principled way to determine when AI deference is justified,
particularly in high-stakes contexts requiring rigorous reliability.

</details>


### [12] [From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL](https://arxiv.org/abs/2510.21045)
*Ali Khosravi Kazazi,Zhenlong Li,M. Naser Lessani,Guido Cervone*

Main category: cs.AI

TL;DR: 提出了一个多智能体框架，用于将自然语言问题准确转换为空间SQL查询，通过专门化的智能体协作和程序化验证来提高空间查询的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决SQL复杂性和PostGIS等工具中地理空间函数的专业化特性带来的障碍，使非专家也能进行空间数据分析。单智能体方法在处理空间查询的语义和句法复杂性方面存在困难。

Method: 采用多智能体框架，包括知识库（程序化模式分析和语义增强）、上下文检索嵌入、协作多智能体管道（实体提取、元数据检索、查询逻辑制定、SQL生成）以及执行程序化和语义验证的审查智能体。

Result: 在非空间KaggleDBQA基准测试中达到81.2%的总体准确率（272个问题中的221个）；在空间查询中达到87.7%的总体准确率（90个问题中的79个），相比没有审查智能体的76.7%有明显提升。

Conclusion: 该工作使空间分析更加易于访问，为空间文本到SQL系统提供了稳健、可推广的基础，推动了自主GIS的发展。

Abstract: The complexity of Structured Query Language (SQL) and the specialized nature
of geospatial functions in tools like PostGIS present significant barriers to
non-experts seeking to analyze spatial data. While Large Language Models (LLMs)
offer promise for translating natural language into SQL (Text-to-SQL),
single-agent approaches often struggle with the semantic and syntactic
complexities of spatial queries. To address this, we propose a multi-agent
framework designed to accurately translate natural language questions into
spatial SQL queries. The framework integrates several innovative components,
including a knowledge base with programmatic schema profiling and semantic
enrichment, embeddings for context retrieval, and a collaborative multi-agent
pipeline as its core. This pipeline comprises specialized agents for entity
extraction, metadata retrieval, query logic formulation, SQL generation, and a
review agent that performs programmatic and semantic validation of the
generated SQL to ensure correctness (self-verification). We evaluate our system
using both the non-spatial KaggleDBQA benchmark and a new, comprehensive
SpatialQueryQA benchmark that includes diverse geometry types, predicates, and
three levels of query complexity. On KaggleDBQA, the system achieved an overall
accuracy of 81.2% (221 out of 272 questions) after the review agent's review
and corrections. For spatial queries, the system achieved an overall accuracy
of 87.7% (79 out of 90 questions), compared with 76.7% without the review
agent. Beyond accuracy, results also show that in some instances the system
generates queries that are more semantically aligned with user intent than
those in the benchmarks. This work makes spatial analysis more accessible, and
provides a robust, generalizable foundation for spatial Text-to-SQL systems,
advancing the development of autonomous GIS.

</details>


### [13] [MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning](https://arxiv.org/abs/2510.21093)
*Siyong Chen,Jinbo Wen,Jiawen Kang,Tenghui Huang,Xumin Huang,Yuanjia Su,Hudan Pan,Zishao Zhong,Dusit Niyato,Shengli Xie,Dong In Kim*

Main category: cs.AI

TL;DR: MedAlign是一个用于医疗视觉问答的新框架，通过多模态直接偏好优化、检索感知专家混合架构和联邦治理机制，解决大型视觉语言模型在医疗应用中的幻觉问题、固定深度推理效率低下和多机构协作困难。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗临床服务部署中面临三个关键挑战：产生无视觉依据的幻觉答案、固定深度推理效率低下、多机构协作困难。

Method: 提出多模态直接偏好优化(mDPO)目标，设计检索感知专家混合(RA-MoE)架构，采用联邦治理机制实现自适应推理和多机构协作。

Result: 在三个代表性Med-VQA数据集上达到最先进性能，比强检索增强基线F1分数提升高达11.85%，平均推理长度比固定深度CoT方法减少51.60%。

Conclusion: MedAlign框架有效解决了LVLM在医疗应用中的关键挑战，实现了视觉准确、高效推理和多机构协作的医疗视觉问答。

Abstract: Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.

</details>


### [14] [Confounding Robust Deep Reinforcement Learning: A Causal Approach](https://arxiv.org/abs/2510.21110)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.AI

TL;DR: 提出了一种新的深度强化学习算法，能够处理观测数据中的混淆偏差，在存在未观测混淆变量的复杂高维环境中优于标准DQN


<details>
  <summary>Details</summary>
Motivation: 研究在存在未观测混淆变量的复杂高维环境中进行离策略学习的问题，传统方法如Q学习在这种情况下可能失效

Method: 基于深度Q网络(DQN)，提出了一种鲁棒算法，寻找与观测兼容的最坏情况环境下的安全策略

Result: 在12个存在混淆的Atari游戏中测试，在所有存在行为策略与目标策略输入不匹配且存在未观测混淆变量的游戏中，该方法始终优于标准DQN

Conclusion: 所提出的算法能够有效处理观测数据中的混淆偏差，在存在未观测混淆变量的复杂环境中具有更好的性能

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [15] [DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance](https://arxiv.org/abs/2510.21117)
*Chunghyun Han,Alfio Gliozzo,Junkyu Lee,Agostino Capponi*

Main category: cs.AI

TL;DR: 该研究首次实证分析了AI代理在去中心化治理中作为自主决策者的表现，通过构建AI投票代理在真实DAO环境中评估其与人类决策的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理能否在去中心化治理中作为自主决策者，增强集体决策过程，并为去中心化金融系统设计可解释且经济严谨的AI代理。

Method: 使用3000多个主要协议的提案，构建AI投票代理，通过模块化可组合程序工作流程解释提案背景、检索历史审议数据并独立确定投票立场，在基于可验证区块链数据的真实金融模拟环境中运行。

Result: AI代理的决策与人类和代币加权结果高度一致，通过精心设计的评估指标显示出强对齐性。

Conclusion: AI代理能够通过产生可解释、可审计且基于实证的信号来增强集体决策，在现实DAO治理环境中表现出色，为去中心化金融系统的可解释AI代理设计提供了重要贡献。

Abstract: This paper presents a first empirical study of agentic AI as autonomous
decision-makers in decentralized governance. Using more than 3K proposals from
major protocols, we build an agentic AI voter that interprets proposal
contexts, retrieves historical deliberation data, and independently determines
its voting position. The agent operates within a realistic financial simulation
environment grounded in verifiable blockchain data, implemented through a
modular composable program (MCP) workflow that defines data flow and tool usage
via Agentics framework. We evaluate how closely the agent's decisions align
with the human and token-weighted outcomes, uncovering strong alignments
measured by carefully designed evaluation metrics. Our findings demonstrate
that agentic AI can augment collective decision-making by producing
interpretable, auditable, and empirically grounded signals in realistic DAO
governance settings. The study contributes to the design of explainable and
economically rigorous AI agents for decentralized financial systems.

</details>


### [16] [PanicToCalm: A Proactive Counseling Agent for Panic Attacks](https://arxiv.org/abs/2510.21143)
*Jihyun Lee,Yejin Min,San Kim,Yejin Jeon,SungJun Yang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.AI

TL;DR: 本文提出了PACE数据集和PACER模型，用于在恐慌发作时提供心理急救支持。PACER通过监督学习和模拟偏好对齐训练，在恐慌场景中优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 恐慌发作时需要及时、适当的干预，但由于伦理和后勤问题，训练此类模型的合适数据集稀缺。

Method: 引入PACE数据集（基于第一人称叙事构建的高危情境数据），训练PACER咨询模型（提供共情和指导性支持），使用监督学习和模拟偏好对齐优化，并提出PanicEval多维评估框架。

Result: 实验结果显示PACER在咨询师侧指标和客户情感改善方面均优于强基线模型。人类评估进一步证实其实用价值，在恐慌场景中一致优于通用、CBT和GPT-4驱动的模型。

Conclusion: PACER模型在恐慌发作干预中表现出色，为心理急救提供了有效的技术支持。

Abstract: Panic attacks are acute episodes of fear and distress, in which timely,
appropriate intervention can significantly help individuals regain stability.
However, suitable datasets for training such models remain scarce due to
ethical and logistical issues. To address this, we introduce PACE, which is a
dataset that includes high-distress episodes constructed from first-person
narratives, and structured around the principles of Psychological First Aid
(PFA). Using this data, we train PACER, a counseling model designed to provide
both empathetic and directive support, which is optimized through supervised
learning and simulated preference alignment. To assess its effectiveness, we
propose PanicEval, a multi-dimensional framework covering general counseling
quality and crisis-specific strategies. Experimental results show that PACER
outperforms strong baselines in both counselor-side metrics and client affect
improvement. Human evaluations further confirm its practical value, with PACER
consistently preferred over general, CBT-based, and GPT-4-powered models in
panic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm
).

</details>


### [17] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 提出了NeuroGenPoisoning攻击框架，通过LLM内部神经元归因和遗传优化生成对抗性外部知识，在RAG系统中实现高效知识投毒。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击主要关注检索内容或提示结构，忽略了模型内部表示动态和神经元级敏感性，且未充分考虑知识冲突问题。

Method: 首先识别与上下文投毒知识强相关的毒物响应神经元，然后使用遗传算法进化对抗性段落以最大化激活这些神经元，并重用有潜力的知识变体。

Result: 在多个模型和数据集上实验，持续实现超过90%的人口覆盖成功率，同时保持流畅性，有效解决知识冲突。

Conclusion: NeuroGenPoisoning通过神经元引导的投毒能有效解决知识冲突，实现大规模生成有效的RAG投毒知识。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [18] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出了EGO-Prompt框架，通过进化图优化自动设计更好的提示和推理过程，在领域特定任务中显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，为LLMs设计领域特定任务的最优提示和推理过程是必要且具有挑战性的，需要解决如何整合领域知识、提升推理效率等问题。

Method: EGO-Prompt从专家构建的初始语义因果图开始，通过因果引导的文本梯度过程分两步优化：生成确定性推理指导，并让LLM适应使用该指导。使用迭代优化算法精炼语义因果图和推理机制。

Result: 在公共卫生、交通和人类行为任务上测试，EGO-Prompt比前沿方法F1分数提高7.32%-12.61%，让小模型以不到20%的成本达到大模型性能，并输出可解释的领域特定语义因果图。

Conclusion: EGO-Prompt是一个有效的自动化框架，能够显著提升LLM在领域特定任务中的性能，同时提供更好的可解释性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [19] [String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation](https://arxiv.org/abs/2510.21150)
*Kou Misaki,Takuya Akiba*

Main category: cs.AI

TL;DR: 提出了String Seed of Thought (SSoT)方法，通过让LLM先生成随机字符串来增加熵，然后从中提取随机性来选择答案，显著改善了概率指令跟随(PIF)任务的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要单一确定性答案的任务上表现出色，但在需要从预定义选项集中按特定概率分布选择答案的PIF任务上表现不佳，这限制了在人类行为模拟、内容多样化和多人游戏等需要非确定性行为的应用。

Method: SSoT方法首先指示LLM输出一个随机字符串以生成足够的熵，然后通过操作这个字符串来提取随机性，从而在保持多样性的同时遵循特定约束条件来选择最终答案。

Result: SSoT显著提高了LLMs在PIF任务上的性能，接近伪随机数生成器的理想性能。在NoveltyBench上的实验还表明SSoT能够增强开放端任务的响应多样性。

Conclusion: SSoT是一种简单有效的提示方法，能够有效解决LLMs在概率指令跟随任务中的偏差问题，提高输出多样性，适用于需要非确定性行为的各种应用场景。

Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs
that improves Probabilistic Instruction Following (PIF). We define PIF as a
task requiring an LLM to select its answer from a predefined set of options,
each associated with a specific probability, such that the empirical
distribution of the generated answers aligns with the target distribution when
prompted multiple times. While LLMs excel at tasks with single, deterministic
answers, they often fail at PIF, exhibiting biases problematic for applications
requiring non-deterministic behaviors, such as human-behavior simulation,
content diversification, and multiplayer games. It also harms the diversity of
generated responses, a crucial factor in test-time scaling, by causing the
outputs to collapse into a limited set of answers. To address this, we propose
SSoT, a simple prompting method that instructs an LLM to first output a random
string to generate sufficient entropy. SSoT also instructs the LLM to extract
randomness by manipulating this string to derive a final answer, thereby
preserving diversity while adhering to specific constraints. We demonstrate
that SSoT significantly improves the PIF performance of LLMs, approaching the
ideal performance of a pseudo-random number generator. Furthermore, our
experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks
to open-ended tasks by enhancing response diversity.

</details>


### [20] [Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2510.21175)
*Yujin Jo,Taesup Kim*

Main category: cs.AI

TL;DR: NuSA-CL是一个轻量级无记忆的持续学习框架，通过低秩适应和零空间约束来保护预训练视觉语言模型的零样本能力，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在现实部署中面临分布偏移和新任务挑战，静态零样本能力不足，需要持续学习方法来适应环境变化同时避免遗忘已有知识。

Method: 采用低秩适应技术，将任务特定的权重更新约束在模型当前参数的近似零空间内，最小化对已学知识的干扰。

Result: 实验表明该框架不仅有效保护零样本迁移能力，还在持续学习基准上取得了极具竞争力的性能。

Conclusion: NuSA-CL为现实应用中持续演化的零样本视觉语言模型提供了一个实用且可扩展的解决方案。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.

</details>


### [21] [Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints](https://arxiv.org/abs/2510.21181)
*Shuo Li,Keqin Xu,Jie Liu,Dan Ye*

Main category: cs.AI

TL;DR: Shylock是一种新颖的因果发现方法，专门针对多变量时间序列(MTS)设计，在少样本和正常数据情况下都能有效工作。它使用分组扩张卷积和共享核来指数级减少参数数量，同时学习带时间延迟的变量表示。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法依赖人工经验、统计方法或图形准则方法，容易出错、受限于理想化假设且需要大量数据。多变量时间序列存在严重的数据缺口，现有方法容易过拟合。

Method: Shylock使用分组扩张卷积和共享核来减少参数数量，结合全局约束和局部约束实现网络间信息共享。还设计了数据生成方法来生成带时间延迟的MTS数据。

Result: 在常用基准测试和生成数据集上的广泛实验表明，Shylock在少样本和正常MTS上都优于两种现有最先进方法。

Conclusion: Shylock成功解决了MTS因果发现中的数据缺口问题，开发了Tcausal库并部署在EarthDataMiner平台上，便于使用。

Abstract: Causal relationship discovery has been drawing increasing attention due to
its prevalent application. Existing methods rely on human experience,
statistical methods, or graphical criteria methods which are error-prone, stuck
at the idealized assumption, and rely on a huge amount of data. And there is
also a serious data gap in accessing Multivariate time series(MTS) in many
areas, adding difficulty in finding their causal relationship. Existing methods
are easy to be over-fitting on them. To fill the gap we mentioned above, in
this paper, we propose Shylock, a novel method that can work well in both
few-shot and normal MTS to find the causal relationship. Shylock can reduce the
number of parameters exponentially by using group dilated convolution and a
sharing kernel, but still learn a better representation of variables with time
delay. By combing the global constraint and the local constraint, Shylock
achieves information sharing among networks to help improve the accuracy. To
evaluate the performance of Shylock, we also design a data generation method to
generate MTS with time delay. We evaluate it on commonly used benchmarks and
generated datasets. Extensive experiments show that Shylock outperforms two
existing state-of-art methods on both few-shot and normal MTS. We also
developed Tcausal, a library for easy use and deployed it on the EarthDataMiner
platform

</details>


### [22] [OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series](https://arxiv.org/abs/2510.21244)
*Pengyu Xu,Shijia Li,Ao Sun,Feng Zhang,Yahan Li,Bo Wu,Zhanyu Ma,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Rui Wang,Yang Liu,Xiaobo Hu,Fan Yang,Jia Zheng,Guanghua Yao*

Main category: cs.AI

TL;DR: 提出了OutboundEval基准测试，用于评估大语言模型在专业级智能外呼场景中的表现，解决了现有方法的三个关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在数据集多样性不足、用户模拟不真实、评估指标不准确等问题，需要建立一个更全面的专业应用评估标准。

Method: 设计了涵盖6个主要业务领域和30个子场景的结构化框架，开发了大模型驱动的用户模拟器，并引入了动态评估方法。

Result: 在12个最先进的大语言模型上进行的实验揭示了专家级任务完成度和交互流畅性之间的权衡关系。

Conclusion: OutboundEval为专业应用中大语言模型的基准测试建立了一个实用、可扩展且面向领域的标准。

Abstract: We propose OutboundEval, a comprehensive benchmark for evaluating large
language models (LLMs) in expert-level intelligent outbound calling scenarios.
Unlike existing methods that suffer from three key limitations - insufficient
dataset diversity and category coverage, unrealistic user simulation, and
inaccurate evaluation metrics - OutboundEval addresses these issues through a
structured framework. First, we design a benchmark spanning six major business
domains and 30 representative sub-scenarios, each with scenario-specific
process decomposition, weighted scoring, and domain-adaptive metrics. Second,
we develop a large-model-driven User Simulator that generates diverse,
persona-rich virtual users with realistic behaviors, emotional variability, and
communication styles, providing a controlled yet authentic testing environment.
Third, we introduce a dynamic evaluation method that adapts to task variations,
integrating automated and human-in-the-loop assessment to measure task
execution accuracy, professional knowledge application, adaptability, and user
experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct
trade-offs between expert-level task completion and interaction fluency,
offering practical insights for building reliable, human-like outbound AI
systems. OutboundEval establishes a practical, extensible, and domain-oriented
standard for benchmarking LLMs in professional applications.

</details>


### [23] [Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems](https://arxiv.org/abs/2510.21254)
*Victoria J. Hodge,Colin Paterson,Ibrahim Habli*

Main category: cs.AI

TL;DR: 这篇综述论文分析了自主系统中OOD检测技术在安全保证方面的应用，探讨了OOD的成因、安全保证挑战，并识别了在ML开发生命周期中可用的技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主系统能力的扩展，在安全关键领域需要严格证明其安全性，OOD检测对于处理系统生命周期中的新颖和不确定情况至关重要。

Method: 采用全面的文献综述方法，定义相关概念，调查OOD成因，分析安全保证挑战，识别ML开发生命周期中可用的OOD检测技术。

Result: 识别了一系列可在ML开发生命周期中使用的OOD检测技术，并建议了在生命周期中支持安全保证论证的应用领域。

Conclusion: 总结了将OOD检测集成到系统生命周期中需要注意的注意事项，并概述了跨领域自主系统安全开发和运营的挑战与未来研究方向。

Abstract: The operational capabilities and application domains of AI-enabled autonomous
systems have expanded significantly in recent years due to advances in robotics
and machine learning (ML). Demonstrating the safety of autonomous systems
rigorously is critical for their responsible adoption but it is challenging as
it requires robust methodologies that can handle novel and uncertain situations
throughout the system lifecycle, including detecting out-of-distribution (OoD)
data. Thus, OOD detection is receiving increased attention from the research,
development and safety engineering communities. This comprehensive review
analyses OOD detection techniques within the context of safety assurance for
autonomous systems, in particular in safety-critical domains. We begin by
defining the relevant concepts, investigating what causes OOD and exploring the
factors which make the safety assurance of autonomous systems and OOD detection
challenging. Our review identifies a range of techniques which can be used
throughout the ML development lifecycle and we suggest areas within the
lifecycle in which they may be used to support safety assurance arguments. We
discuss a number of caveats that system and safety engineers must be aware of
when integrating OOD detection into system lifecycles. We conclude by outlining
the challenges and future work necessary for the safe development and operation
of autonomous systems across a range of domains and applications.

</details>


### [24] [Investigating Scale Independent UCT Exploration Factor Strategies](https://arxiv.org/abs/2510.21275)
*Robin Schmöcker,Christoph Schnell,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文评估了UCT算法中自适应选择探索常数λ的各种策略，提出了五种新策略，并推荐使用2σ作为λ值，其中σ是搜索树中所有状态-动作对Q值的经验标准差。


<details>
  <summary>Details</summary>
Motivation: UCT算法对游戏奖励尺度不鲁棒，许多游戏具有密集奖励和手动选择的奖励尺度，导致不同游戏中节点的Q值跨度不同。

Method: 评估文献中提出的λ策略以及五种新策略，包括选择λ为2σ的方法，其中σ是搜索树中所有状态-动作对Q值的经验标准差。

Result: 新提出的2σ策略在广泛任务中优于现有λ策略，无论是在单一参数值还是优化所有可用参数获得的峰值性能方面。

Conclusion: 推荐使用新提出的λ策略，即选择λ为2σ，该方法在各种任务中表现最佳。

Abstract: The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the
reward scale of the game it is applied to. For zero-sum games with the sparse
rewards of $\{-1,0,1\}$ at the end of the game, this is not a problem, but many
games often feature dense rewards with hand-picked reward scales, causing a
node's Q-value to span different magnitudes across different games. In this
paper, we evaluate various strategies for adaptively choosing the UCT
exploration constant $\lambda$, called $\lambda$-strategies, that are agnostic
to the game's reward scale. These $\lambda$-strategies include those proposed
in the literature as well as five new strategies. Given our experimental
results, we recommend using one of our newly suggested $\lambda$-strategies,
which is to choose $\lambda$ as $2 \cdot \sigma$ where $\sigma$ is the
empirical standard deviation of all state-action pairs' Q-values of the search
tree. This method outperforms existing $\lambda$-strategies across a wide range
of tasks both in terms of a single parameter value and the peak performances
obtained by optimizing all available parameters.

</details>


### [25] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 该论文提出Chain-of-Guardrail (CoG)框架，通过重组或回溯不安全的推理步骤来解决大型推理模型的安全性与推理能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但存在严重的安全风险，包括有害内容生成和越狱攻击。现有缓解策略依赖注入启发式安全信号，往往会抑制推理能力，无法解决安全性与推理能力之间的权衡。

Method: 通过分析推理轨迹发现Self-Jailbreak现象，提出Chain-of-Guardrail (CoG)训练框架，重组或回溯不安全的推理步骤，引导模型回到安全轨迹同时保留有效推理链。

Result: 在多个推理和安全基准测试上的广泛实验表明，CoG显著提高了当前大型推理模型的安全性，同时保持了相当的推理能力，显著优于先前存在严重安全-推理权衡的方法。

Conclusion: CoG框架有效解决了大型推理模型的安全性与推理能力之间的权衡问题，通过重组不安全推理步骤来提升安全性而不损害推理性能。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [26] [Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles](https://arxiv.org/abs/2510.21293)
*Siddharth Mehrotra,Jin Huang,Xuelong Fu,Roel Dobbe,Clara I. Sánchez,Maarten de Rijke*

Main category: cs.AI

TL;DR: 本文通过范围综述分析了AIES和FAccT会议中关于可信AI的研究现状，发现当前研究过于技术中心化，忽视了社会技术维度，提出了结合技术严谨性与社会文化因素的跨学科方法。


<details>
  <summary>Details</summary>
Motivation: 当前可信AI研究主要关注技术属性如可靠性、鲁棒性和公平性，而忽视了理解真实世界环境中AI可信度所需的社会技术维度，需要全面审视AIES和FAccT社区如何概念化、测量和验证AI可信度。

Method: 对AIES和FAccT会议论文集进行范围综述，系统分析可信度在不同研究领域中的定义、操作化和应用方式，重点关注概念化方法、测量方法、验证技术、应用领域和基础价值观。

Result: 研究发现虽然在定义透明度、问责制和鲁棒性等技术属性方面取得显著进展，但当前研究往往过度强调技术精度而牺牲社会伦理考量，AI系统的社会技术性质仍较少被探索，可信度成为由有权定义者塑造的争议概念。

Conclusion: 需要结合技术严谨性与社会、文化和制度考量的跨学科方法推进可信AI发展，为AI伦理社区提出可操作措施，采用真正解决AI系统与社会复杂互动的整体框架，促进惠及所有利益相关者的负责任技术发展。

Abstract: Background: Trustworthy AI serves as a foundational pillar for two major AI
ethics conferences: AIES and FAccT. However, current research often adopts
techno-centric approaches, focusing primarily on technical attributes such as
reliability, robustness, and fairness, while overlooking the sociotechnical
dimensions critical to understanding AI trustworthiness in real-world contexts.
  Objectives: This scoping review aims to examine how the AIES and FAccT
communities conceptualize, measure, and validate AI trustworthiness,
identifying major gaps and opportunities for advancing a holistic understanding
of trustworthy AI systems.
  Methods: We conduct a scoping review of AIES and FAccT conference proceedings
to date, systematically analyzing how trustworthiness is defined,
operationalized, and applied across different research domains. Our analysis
focuses on conceptualization approaches, measurement methods, verification and
validation techniques, application areas, and underlying values.
  Results: While significant progress has been made in defining technical
attributes such as transparency, accountability, and robustness, our findings
reveal critical gaps. Current research often predominantly emphasizes technical
precision at the expense of social and ethical considerations. The
sociotechnical nature of AI systems remains less explored and trustworthiness
emerges as a contested concept shaped by those with the power to define it.
  Conclusions: An interdisciplinary approach combining technical rigor with
social, cultural, and institutional considerations is essential for advancing
trustworthy AI. We propose actionable measures for the AI ethics community to
adopt holistic frameworks that genuinely address the complex interplay between
AI systems and society, ultimately promoting responsible technological
development that benefits all stakeholders.

</details>


### [27] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出了一种结合符号验证和交互验证的神经符号具身任务规划框架，通过探索性代码主动与环境交互获取缺失观测，在动态和部分可观测环境中显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码即策略方法在动态或部分可观测环境中存在环境接地不足的问题，导致代码生成错误或不完整，影响任务成功率。

Method: 神经符号具身任务规划框架，包含显式符号验证和交互验证过程，在验证阶段生成探索性代码主动与环境交互获取缺失观测，同时保持任务相关状态。

Result: 在RLBench和真实世界动态部分可观测场景中，任务成功率比Code-as-Policies基线提升46.2%，任务相关动作可执行性达到86.8%以上。

Conclusion: 该框架通过增强生成代码的环境接地性，显著提高了动态环境中任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [28] [CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation](https://arxiv.org/abs/2510.21324)
*Jinhui Lou,Yan Yang,Zhou Yu,Zhenqi Fu,Weidong Han,Qingming Huang,Jun Yu*

Main category: cs.AI

TL;DR: CXRAgent是一个基于LLM的导演编排多阶段智能体，用于胸部X光片分析，通过工具协调、多阶段推理和团队协作来提升诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR分析模型难以适应新的诊断任务和复杂推理场景，现有智能体依赖单一诊断流程且缺乏工具可靠性评估机制。

Method: 采用导演编排的三阶段方法：工具调用（包含证据驱动验证器）、诊断规划（组建专家团队）、协作决策（整合上下文记忆）。

Result: 在多种CXR解释任务上表现出色，能提供视觉证据并良好适应不同复杂度的临床任务。

Conclusion: CXRAgent通过多阶段协作和证据验证机制，显著提升了CXR分析的适应性、可靠性和诊断能力。

Abstract: Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety
of task-specific and foundation models have been developed for automatic CXR
interpretation. However, these models often struggle to adapt to new diagnostic
tasks and complex reasoning scenarios. Recently, LLM-based agent models have
emerged as a promising paradigm for CXR analysis, enhancing model's capability
through tool coordination, multi-step reasoning, and team collaboration, etc.
However, existing agents often rely on a single diagnostic pipeline and lack
mechanisms for assessing tools' reliability, limiting their adaptability and
credibility. To this end, we propose CXRAgent, a director-orchestrated,
multi-stage agent for CXR interpretation, where a central director coordinates
the following stages: (1) Tool Invocation: The agent strategically orchestrates
a set of CXR-analysis tools, with outputs normalized and verified by the
Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual
evidence to support reliable downstream diagnosis; (2) Diagnostic Planning:
Guided by task requirements and intermediate findings, the agent formulates a
targeted diagnostic plan. It then assembles an expert team accordingly,
defining member roles and coordinating their interactions to enable adaptive
and collaborative reasoning; (3) Collaborative Decision-making: The agent
integrates insights from the expert team with accumulated contextual memories,
synthesizing them into an evidence-backed diagnostic conclusion. Experiments on
various CXR interpretation tasks show that CXRAgent delivers strong
performance, providing visual evidence and generalizes well to clinical tasks
of different complexity. Code and data are valuable at this
\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.

</details>


### [29] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan是一个基于蒙特卡洛树搜索(MCTS)的创新生成框架，通过语义罗盘和景观感知价值函数引导LLM探索概念空间，解决了LLM在生成创新想法时的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成创新想法时往往陷入训练数据的"重力井"，倾向于产生高概率的熟悉概念。现有的搜索方法如思维树(ToT)依赖于无原则的自评估启发式方法，存在根本性限制。

Method: 使用蒙特卡洛树搜索(MCTS)配合分层引导系统：长程方向由语义罗盘向量通过正交投影引导搜索；局部决策由景观感知价值函数替代有缺陷的自评估，平衡内在连贯性、外在新颖性和叙事进展。

Result: 广泛实验表明，Magellan在生成科学想法方面显著优于ReAct和ToT等强基线方法，在合理性和创新性方面表现更优。

Conclusion: 对于创造性发现，有原则的引导搜索比无约束的自主性更有效，为LLM成为创新合作伙伴铺平了道路。

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [30] [Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning](https://arxiv.org/abs/2510.21398)
*Ravindra Aribowo Tarunokusumo,Rafael Fernandes Cunha*

Main category: cs.AI

TL;DR: 提出结合强化学习(RL)的框架，提升1.5B模型在数学推理中的性能，相比仅使用监督微调(SFT)的方法，在GSM8K数据集上获得更高准确率，同时显著减少40%以上的token使用量。


<details>
  <summary>Details</summary>
Motivation: 解决预算强制方法依赖长上下文推理轨迹的监督微调导致小模型性能下降的问题，通过强化学习提高token效率并恢复因长上下文训练造成的性能损失。

Method: 集成强化学习框架，仅使用1.5K训练样本，结合监督微调(SFT)和强化学习(RL)来优化1.5B模型的数学推理能力。

Result: SFT+RL模型在GSM8K数据集上表现更好，在不同计算预算下获得更高准确率，同时token使用量相比SFT模型减少超过40%。

Conclusion: 强化学习能够有效恢复长上下文训练造成的损失，显著提高数学推理性能，同时大幅提升token效率。

Abstract: Test-time scaling methods have seen a rapid increase in popularity for its
computational efficiency and parameter-independent training to improve
reasoning performance on Large Language Models. One such method is called
budget forcing, a decoding intervention strategy which allocates extra compute
budget for thinking and elicits the inherent self-correcting behavior of the
model. However, this relies on supervised fine-tuning (SFT) on long-context
reasoning traces which causes performance degradation on smaller models due to
verbose responses. For this reason, we offer a framework integrating
reinforcement learning (RL) to improve token efficiency and boost the
performance of a 1.5B model for mathematical reasoning. We demonstrate this
using only 1.5K training samples and found that our SFT+RL model performed
better on the GSM8K dataset with varying compute budgets. Our main findings
showed an overall higher accuracy while significantly reducing its token usage
by over 40% compared to the SFT model, revealing how RL can recover the losses
due to long-context training and altogether improving performance in
mathematical reasoning.

</details>


### [31] [Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI](https://arxiv.org/abs/2510.21425)
*Maneeha Rani,Bhupesh Kumar Mishra,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 该论文提出了一种新的符号AI集成到LLMs的分类框架和路线图，旨在解决LLMs透明度不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键领域表现出色但缺乏透明度，现有神经符号AI方法主要针对传统神经网络，不适用于LLMs的特性，需要系统性地理解符号AI如何有效集成到LLMs中。

Method: 首先回顾已建立的神经符号AI方法，然后提出LLMs中符号集成的新分类法，并制定将符号技术与LLMs融合的路线图，包括四个维度的分类框架。

Result: 提出了一个包含LLM不同阶段符号集成、耦合机制、架构范式以及算法和应用层面视角的新分类框架，系统整理了现有文献。

Conclusion: 通过突出最新进展和文献中的显著差距，为在LLMs中实现符号集成框架以增强透明度提供了实用见解和未来研究方向。

Abstract: LLMs have demonstrated highly effective learning, human-like response
generation,and decision-making capabilities in high-risk sectors. However,
these models remain black boxes because they struggle to ensure transparency in
responses. The literature has explored numerous approaches to address
transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI
approaches were primarily developed for conventional neural networks and are
not well-suited to the unique features of LLMs. Consequently, there is a
limited systematic understanding of how symbolic AI can be effectively
integrated into LLMs. This paper aims to address this gap by first reviewing
established NeSy AI methods and then proposing a novel taxonomy of symbolic
integration in LLMs, along with a roadmap to merge symbolic techniques with
LLMs. The roadmap introduces a new categorisation framework across four
dimensions by organising existing literature within these categories. These
include symbolic integration across various stages of LLM, coupling mechanisms,
architectural paradigms, as well as algorithmic and application-level
perspectives. The paper thoroughly identifies current benchmarks, cutting-edge
advancements, and critical gaps within the field to propose a roadmap for
future research. By highlighting the latest developments and notable gaps in
the literature, it offers practical insights for implementing frameworks for
symbolic integration into LLMs to enhance transparency.

</details>


### [32] [AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving](https://arxiv.org/abs/2510.21436)
*Ankur Sinha,Shobhit Arora,Dhaval Pujara*

Main category: cs.AI

TL;DR: AutoOpt-11k是一个包含11,000多个手写和打印数学优化模型图像的数据集，配套开发了AutoOpt框架，通过深度学习模型自动识别数学表达式并转换为优化建模语言，最终使用双层优化分解方法求解优化问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决数学优化问题求解过程中需要人工建模和编程的繁琐过程，开发一个能够直接从图像自动求解优化问题的端到端框架。

Method: 开发了AutoOpt框架，包含三个模块：M1使用深度学习模型进行数学表达式识别生成LaTeX代码；M2使用微调的小型LLM将LaTeX转换为PYOMO脚本；M3使用双层优化分解方法求解优化问题。

Result: MER模型在BLEU得分上优于ChatGPT、Gemini和Nougat；BOBD方法在复杂测试问题上比内点算法和遗传算法表现更好。

Conclusion: AutoOpt-11k数据集和AutoOpt框架为自动化求解优化问题提供了有效的解决方案，在数学表达式识别和复杂优化问题求解方面表现出色。

Abstract: This study presents AutoOpt-11k, a unique image dataset of over 11,000
handwritten and printed mathematical optimization models corresponding to
single-objective, multi-objective, multi-level, and stochastic optimization
problems exhibiting various types of complexities such as non-linearity,
non-convexity, non-differentiability, discontinuity, and high-dimensionality.
The labels consist of the LaTeX representation for all the images and modeling
language representation for a subset of images. The dataset is created by 25
experts following ethical data creation guidelines and verified in two-phases
to avoid errors. Further, we develop AutoOpt framework, a machine learning
based automated approach for solving optimization problems, where the user just
needs to provide an image of the formulation and AutoOpt solves it efficiently
without any further human intervention. AutoOpt framework consists of three
Modules: (i) M1 (Image_to_Text)- a deep learning model performs the
Mathematical Expression Recognition (MER) task to generate the LaTeX code
corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-
a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling
language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization
based Decomposition (BOBD) method solves the optimization formulation described
in the PYOMO script. We use AutoOpt-11k dataset for training and testing of
deep learning models employed in AutoOpt. The deep learning model for MER task
(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method
(M3), which is a hybrid approach, yields better results on complex test
problems compared to common approaches, like interior-point algorithm and
genetic algorithm.

</details>


### [33] [Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP](https://arxiv.org/abs/2510.21453)
*Yuxin Pan,Zhiguang Cao,Chengyang Gu,Liu Liu,Peilin Zhao,Yize Chen,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出了MoSES框架，通过状态可分解MDP和潜在空间扩展，让统一求解器能够感知VRP变体的共享组件特性，重用基础求解器，避免神经网络求解器数量指数增长。


<details>
  <summary>Details</summary>
Motivation: 现有多任务车辆路径问题神经方法通常学习统一求解器，但未能充分利用VRP变体的组合结构，每个变体都可以从一组基础VRP变体派生而来。

Method: 引入状态可分解MDP将状态空间表示为与基础VRP变体相关的基础状态空间的笛卡尔积，开发潜在空间扩展结合最优基础策略和可学习混合函数，实现MoSES求解器使用专用LoRA专家实现基础策略。

Result: 在多个VRP变体上的广泛实验表明MoSES优于先前方法。

Conclusion: 该框架通过重用基础求解器有效解决了统一求解器未能充分利用VRP组合结构的问题，在多个VRP变体上表现出优越性能。

Abstract: Existing neural methods for multi-task vehicle routing problems (VRPs)
typically learn unified solvers to handle multiple constraints simultaneously.
However, they often underutilize the compositional structure of VRP variants,
each derivable from a common set of basis VRP variants. This critical oversight
causes unified solvers to miss out the potential benefits of basis solvers,
each specialized for a basis VRP variant. To overcome this limitation, we
propose a framework that enables unified solvers to perceive the
shared-component nature across VRP variants by proactively reusing basis
solvers, while mitigating the exponential growth of trained neural solvers.
Specifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates
VRPs by expressing the state space as the Cartesian product of basis state
spaces associated with basis VRP variants. More crucially, this formulation
inherently yields the optimal basis policy for each basis VRP variant.
Furthermore, a Latent Space-based SDMDP extension is developed by incorporating
both the optimal basis policies and a learnable mixture function to enable the
policy reuse in the latent space. Under mild assumptions, this extension
provably recovers the optimal unified policy of SDMDP through the mixture
function that computes the state embedding as a mapping from the basis state
embeddings generated by optimal basis policies. For practical implementation,
we introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes
basis policies through specialized Low-Rank Adaptation (LoRA) experts, and
implements the mixture function via an adaptive gating mechanism. Extensive
experiments conducted across VRP variants showcase the superiority of MoSES
over prior methods.

</details>


### [34] [EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law](https://arxiv.org/abs/2510.21524)
*Ilija Lichkovski,Alexander Müller,Mariam Ibrahim,Tiwai Mhundwa*

Main category: cs.AI

TL;DR: EU-Agent-Bench是一个可验证的人工策划基准，用于评估LLM代理在欧盟立法背景下执行非法行为的潜在倾向，涵盖数据保护、偏见/歧视和科学诚信等多个类别。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在各种环境中部署，它们可能表现出不可预测的行为，包括采取不良和/或不安全的行动。需要测量LLM代理在欧盟立法背景下执行非法行为的潜在倾向。

Method: 通过将模型的函数调用与详尽引用相关立法的评分标准进行比较，评估前沿LLM的法律合规性，并研究在代理系统提示中提供相关立法摘录和明确合规指令的合规效果。

Result: 评估了前沿LLM的法律合规性，并研究了提供立法摘录对合规性的影响。发布了供研究社区使用的公共预览集，同时保留私有测试集以防止数据污染。

Conclusion: 鼓励未来的工作将代理安全基准扩展到不同的法律管辖区，以及多轮和多语言交互。代码已公开发布。

Abstract: Large language models (LLMs) are increasingly deployed as agents in various
contexts by providing tools at their disposal. However, LLM agents can exhibit
unpredictable behaviors, including taking undesirable and/or unsafe actions. In
order to measure the latent propensity of LLM agents for taking illegal actions
under an EU legislative context, we introduce EU-Agent-Bench, a verifiable
human-curated benchmark that evaluates an agent's alignment with EU legal norms
in situations where benign user inputs could lead to unlawful actions. Our
benchmark spans scenarios across several categories, including data protection,
bias/discrimination, and scientific integrity, with each user request allowing
for both compliant and non-compliant execution of the requested actions.
Comparing the model's function calls against a rubric exhaustively supported by
citations of the relevant legislature, we evaluate the legal compliance of
frontier LLMs, and furthermore investigate the compliance effect of providing
the relevant legislative excerpts in the agent's system prompt along with
explicit instructions to comply. We release a public preview set for the
research community, while holding out a private test set to prevent data
contamination in evaluating upcoming models. We encourage future work extending
agentic safety benchmarks to different legal jurisdictions and to multi-turn
and multilingual interactions. We release our code on
\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.

</details>


### [35] [Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts](https://arxiv.org/abs/2510.21557)
*Hongwei Zhang,Ji Lu,Shiqing Jiang,Chenxiang Zhu,Li Xie,Chen Zhong,Haoran Chen,Yurui Zhu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.AI

TL;DR: Co-Sight通过冲突感知元验证和可信推理结构化事实机制，将推理转化为可证伪和可审计的过程，解决了LLM智能体长程推理中的验证不足问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长程推理中失败往往不是因为生成能力弱，而是因为中间推理验证不足。需要将推理转化为可证伪和可审计的过程。

Method: 采用两种互补机制：冲突感知元验证（CAMV）将验证重构为冲突识别和针对性证伪，只在专家智能体间的不一致热点分配计算；可信推理结构化事实（TRSF）通过结构化事实模块持续组织、验证和同步证据。

Result: 在GAIA上达到84.4%的准确率，Humanity's Last Exam上达到35.5%，Chinese-SimpleQA上达到93.8%的强结果。消融研究证实结构化事实基础和冲突感知验证的协同作用驱动了这些改进。

Conclusion: Co-Sight为LLM智能体中的可靠长程推理提供了一个可扩展的范式，通过形成封闭验证循环实现透明可信的推理。

Abstract: Long-horizon reasoning in LLM-based agents often fails not from generative
weakness but from insufficient verification of intermediate reasoning. Co-Sight
addresses this challenge by turning reasoning into a falsifiable and auditable
process through two complementary mechanisms: Conflict-Aware Meta-Verification
(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV
reformulates verification as conflict identification and targeted
falsification, allocating computation only to disagreement hotspots among
expert agents rather than to full reasoning chains. This bounds verification
cost to the number of inconsistencies and improves efficiency and reliability.
TRSF continuously organizes, validates, and synchronizes evidence across agents
through a structured facts module. By maintaining verified, traceable, and
auditable knowledge, it ensures that all reasoning is grounded in consistent,
source-verified information and supports transparent verification throughout
the reasoning process. Together, TRSF and CAMV form a closed verification loop,
where TRSF supplies structured facts and CAMV selectively falsifies or
reinforces them, yielding transparent and trustworthy reasoning. Empirically,
Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last
Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies
confirm that the synergy between structured factual grounding and
conflict-aware verification drives these improvements. Co-Sight thus offers a
scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code
is available at
https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.

</details>


### [36] [Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning](https://arxiv.org/abs/2510.21560)
*Yuxuan Yang,Hussein Sibai*

Main category: cs.AI

TL;DR: 该论文提出了一种使用模仿学习训练神经控制屏障函数的方法，无需显式指定失败状态集，仅通过专家演示数据就能学习安全约束，在四个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在关键领域自主系统中，安全是基本要求。传统方法需要显式定义失败状态集，但这往往难以形式化指定（如自动驾驶中的跟车问题），而专家演示数据更容易获取。

Method: 使用模仿学习训练约束函数来分类系统状态为安全或不安全，然后用该函数标注模拟轨迹来训练神经控制屏障函数，避免了对失败集的显式定义需求。

Result: 在四个不同环境中的实证评估表明，该方法优于现有基线，并且与使用真实安全标签训练的神经控制屏障函数性能相当。

Conclusion: 该方法提供了一种数据驱动的安全约束学习方法，无需显式指定失败状态集，仅通过专家演示就能有效训练神经控制屏障函数，为自主系统的安全设计提供了新思路。

Abstract: Safety is a fundamental requirement for autonomous systems operating in
critical domains. Control barrier functions (CBFs) have been used to design
safety filters that minimally alter nominal controls for such systems to
maintain their safety. Learning neural CBFs has been proposed as a data-driven
alternative for their computationally expensive optimization-based synthesis.
However, it is often the case that the failure set of states that should be
avoided is non-obvious or hard to specify formally, e.g., tailgating in
autonomous driving, while a set of expert demonstrations that achieve the task
and avoid the failure set is easier to generate. We use ICL to train a
constraint function that classifies the states of the system under
consideration to safe, i.e., belong to a controlled forward invariant set that
is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to
the complement of that set. We then use that function to label a new set of
simulated trajectories to train our neural CBF. We empirically evaluate our
approach in four different environments, demonstrating that it outperforms
existing baselines and achieves comparable performance to a neural CBF trained
with the same data but annotated with ground-truth safety labels.

</details>


### [37] [Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine](https://arxiv.org/abs/2510.21614)
*Wenyi Wang,Piotr Piękos,Li Nanbo,Firas Laakom,Yimeng Chen,Mateusz Ostaszewski,Mingchen Zhuge,Jürgen Schmidhuber*

Main category: cs.AI

TL;DR: 本文提出了Huxley-Gödel Machine (HGM)方法来解决自改进编码代理中元生产力与性能不匹配的问题，通过估计代理的自我改进潜力来指导代码修改树的搜索，在多个基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自改进编码代理基于软件工程基准性能来选择代码修改，但作者发现代理的自我改进潜力（元生产力）与其编码基准性能之间存在不匹配问题，需要更好的指标来指导自我改进过程。

Method: 提出CMP指标来聚合代理后代的基准性能作为其自我改进潜力的指示器，并基于此构建Huxley-Gödel Machine (HGM)框架，通过估计CMP来指导自修改树的搜索过程。

Result: 在SWE-bench Verified和Polyglot基准测试中，HGM超越了先前的自改进编码代理开发方法，同时使用了更少的实际时间。优化后的代理在SWE-bench Lite上达到了人类水平性能。

Conclusion: HGM方法通过正确估计代理的自我改进潜力，能够更有效地指导自改进过程，在多个编码数据集和大语言模型上表现出强大的迁移能力，实现了人类水平的编码性能。

Abstract: Recent studies operationalize self-improvement through coding agents that
edit their own codebases. They grow a tree of self-modifications through
expansion strategies that favor higher software engineering benchmark
performance, assuming that this implies more promising subsequent
self-modifications. However, we identify a mismatch between the agent's
self-improvement potential (metaproductivity) and its coding benchmark
performance, namely the Metaproductivity-Performance Mismatch. Inspired by
Huxley's concept of clade, we propose a metric ($\mathrm{CMP}$) that aggregates
the benchmark performances of the descendants of an agent as an indicator of
its potential for self-improvement. We show that, in our self-improving coding
agent development setting, access to the true $\mathrm{CMP}$ is sufficient to
simulate how the G\"odel Machine would behave under certain assumptions. We
introduce the Huxley-G\"odel Machine (HGM), which, by estimating $\mathrm{CMP}$
and using it as guidance, searches the tree of self-modifications. On SWE-bench
Verified and Polyglot, HGM outperforms prior self-improving coding agent
development methods while using less wall-clock time. Last but not least, HGM
demonstrates strong transfer to other coding datasets and large language
models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and
evaluated on SWE-bench Lite with GPT-5 achieves human-level performance,
matching the best officially checked results of human-engineered coding agents.
Our code is available at https://github.com/metauto-ai/HGM.

</details>


### [38] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: DeepAgent是一个端到端的深度推理智能体，通过自主思考、工具发现和行动执行在单一连贯推理过程中完成任务。它引入自主记忆折叠机制来压缩交互历史，并使用ToolPO强化学习策略来高效教授通用工具使用。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架通常遵循预定义工作流程，限制了自主和全局任务完成能力。现实世界任务需要外部工具和长时程交互，但现有方法面临上下文长度爆炸和交互历史积累的问题。

Method: 提出DeepAgent框架，包含自主记忆折叠机制（压缩过往交互为结构化记忆）和ToolPO强化学习策略（利用LLM模拟API并通过工具调用优势归因分配细粒度信用）。

Result: 在8个基准测试（包括通用工具使用任务和下游应用）上的广泛实验表明，DeepAgent在标记工具和开放集工具检索场景中均优于基线方法。

Conclusion: 这项工作朝着为现实世界应用构建更通用和有能力智能体的方向迈出了一步。

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [39] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: 提出了AstaBench基准套件，用于更严格地评估AI科学代理的能力，包含2400+个涵盖整个科学发现过程的问题，并提供了可重现的科学研究环境和基线代理。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估基准存在多方面不足：缺乏真实世界科学研究的整体衡量、缺少可重现的代理工具、未考虑模型成本和工具访问等混杂变量、缺乏标准化接口、缺少全面的基线代理。

Method: 定义了更严格的代理基准原则和工具，开发了AstaBench套件，包含多科学领域的2400+问题，提供生产级搜索工具的科学研究环境，以及9类科学优化代理和多个基线。

Result: 对57个代理在22个代理类别的广泛评估显示，尽管在某些方面取得有意义进展，但AI在科学研究辅助方面仍远未解决挑战。

Conclusion: 需要更严格的基准来准确评估AI科学代理的能力，AstaBench为此提供了首个全面的科学发现过程评估框架，揭示了当前AI在科学研究辅助方面的局限性。

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


### [40] [CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning](https://arxiv.org/abs/2510.21656)
*Marta Contreiras Silva,Daniel Faria,Catia Pesquita*

Main category: cs.AI

TL;DR: CMOMgen是首个端到端的复杂多本体匹配策略，能够生成完整且语义正确的映射，无需限制目标本体或实体的数量。


<details>
  <summary>Details</summary>
Motivation: 构建全面的知识图谱需要使用多个本体来将数据完全情境化到特定领域。简单的成对等价映射无法提供相关但不相交本体的完整语义集成，需要更复杂的多本体匹配方法。

Method: 使用检索增强生成方法选择相关类别来组成映射，并过滤匹配的参考映射作为示例来增强上下文学习。

Result: 在三个生物医学任务中，CMOMgen在类别选择方面优于基线方法，F1分数至少达到63%，在两个任务中优于所有基线和消融版本，在第三个任务中排名第二。手动评估显示46%的非参考映射获得最高分。

Conclusion: CMOMgen能够构建语义正确的映射，证明了其作为复杂多本体匹配策略的有效性。

Abstract: Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.

</details>


### [41] [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://arxiv.org/abs/2510.21679)
*Gaku Morio,Harri Rowlands,Dominik Stammbach,Christopher D. Manning,Peter Henderson*

Main category: cs.AI

TL;DR: 提出了一个用于评估视觉语言模型的多模态基准数据集，包含专家标注的视频广告框架分析，重点关注能源公司的绿色创新框架识别。


<details>
  <summary>Details</summary>
Motivation: 企业公关活动存在言行不一的问题，如石油公司的"漂绿"行为。需要大规模理解框架及其变化来识别公关活动的真实目标。

Method: 构建了从Facebook和YouTube获取的专家标注视频广告数据集，包含13种框架类型，覆盖50多家公司和20个国家，专门用于视觉语言模型评估。

Result: 基线实验显示GPT-4.1能检测环境信息，F1分数79%，但最佳模型在识别绿色创新框架方面仅达到46% F1分数。

Conclusion: 该数据集有助于能源部门战略沟通的多模态分析研究，同时揭示了视觉语言模型在处理隐性框架、不同长度视频和文化背景方面的挑战。

Abstract: Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil & gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.

</details>


### [42] [A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics](https://arxiv.org/abs/2510.21695)
*Edward Holmberg,Elias Ioup,Mahdi Abdelguerfi*

Main category: cs.AI

TL;DR: 提出了一种基于知识图谱的框架，作为智能翻译层来弥合高层任务目标与低层规划器输入之间的语义鸿沟，通过双平面架构将声明性事实编译为任务感知的世界观和物理感知的遍历规则。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中自主智能体协调时高层任务目标与低层规划器输入之间的语义鸿沟问题。

Method: 引入以知识图谱为中心的框架，采用双平面架构将声明性事实编译为每个智能体的任务感知"世界观"和物理感知的遍历规则，使任务语义与领域无关的规划器解耦。

Result: 在墨西哥湾的自主水下航行器案例研究中，视觉展示了端到端过程，并定量证明不同的声明性策略能产生独特且高性能的结果。

Conclusion: 该工作确立了知识图谱不仅是数据存储库，更是创建自适应和可解释自主系统的强大、有状态编排器。

Abstract: The coordination of autonomous agents in dynamic environments is hampered by
the semantic gap between high-level mission objectives and low-level planner
inputs. To address this, we introduce a framework centered on a Knowledge Graph
(KG) that functions as an intelligent translation layer. The KG's two-plane
architecture compiles declarative facts into per-agent, mission-aware
``worldviews" and physics-aware traversal rules, decoupling mission semantics
from a domain-agnostic planner. This allows complex, coordinated paths to be
modified simply by changing facts in the KG. A case study involving Autonomous
Underwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the
end-to-end process and quantitatively proves that different declarative
policies produce distinct, high-performing outcomes. This work establishes the
KG not merely as a data repository, but as a powerful, stateful orchestrator
for creating adaptive and explainable autonomous systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [43] [Information Theoretic Learning for Diffusion Models with Warm Start](https://arxiv.org/abs/2510.20903)
*Yirong Shen,Lu Gan,Cong Ling*

Main category: cs.IT

TL;DR: 提出一个更紧的似然下界来改进噪声驱动生成模型的准确性和效率，扩展了KL散度与Fisher信息的关系到任意噪声扰动，在多个数据集上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 基于扰动的似然估计模型虽然实用，但面临收敛慢和理论理解有限的问题。需要改进最大似然学习的准确性和效率。

Method: 将经典KL散度与Fisher信息的关系扩展到任意噪声扰动，超越高斯假设，允许使用结构化噪声分布。将扩散过程视为高斯信道，表达数据与模型之间的失配熵。

Result: 在CIFAR-10上取得竞争性负对数似然(NLL)，在ImageNet多个分辨率上达到SOTA结果，且无需数据增强。框架自然扩展到离散数据。

Conclusion: 提出的方法为噪声驱动生成模型提供了更紧的似然下界，提高了学习效率和准确性，具有广泛适用性。

Abstract: Generative models that maximize model likelihood have gained traction in many
practical settings. Among them, perturbation based approaches underpin many
strong likelihood estimation models, yet they often face slow convergence and
limited theoretical understanding. In this paper, we derive a tighter
likelihood bound for noise driven models to improve both the accuracy and
efficiency of maximum likelihood learning. Our key insight extends the
classical KL divergence Fisher information relationship to arbitrary noise
perturbations, going beyond the Gaussian assumption and enabling structured
noise distributions. This formulation allows flexible use of randomized noise
distributions that naturally account for sensor artifacts, quantization
effects, and data distribution smoothing, while remaining compatible with
standard diffusion training. Treating the diffusion process as a Gaussian
channel, we further express the mismatched entropy between data and model,
showing that the proposed objective upper bounds the negative log-likelihood
(NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and SOTA
results on ImageNet across multiple resolutions, all without data augmentation,
and the framework extends naturally to discrete data.

</details>


### [44] [Overlapped-repetition Shor codes achieving fourfold asymptotic rate](https://arxiv.org/abs/2510.21030)
*En-Jui Chang*

Main category: cs.IT

TL;DR: 通过重叠少量重复码，将标准Shor码的渐近码率提高四倍，在最小距离d=3的情况下将开销从[[9,1,3]]减少到更高效的[[7,1,3]]配置。


<details>
  <summary>Details</summary>
Motivation: 标准Shor码使用两个重复码作为内外码，结构简单但码率相对较低。

Method: 通过重叠少量重复码来增强渐近码率。

Result: 在最小距离d=3的情况下，将开销从[[9,1,3]]减少到[[7,1,3]]配置。

Conclusion: 这种构造方法显著提高了量子纠错码的效率，减少了资源开销。

Abstract: The standard Shor code employs two repetition codes as inner and outer codes,
yielding a simple structure but a relatively low code rate. By overlapping a
small number of repetition codes, we enhance the asymptotic code rate fourfold.
In the minimal-distance case $d = 3$, this construction reduces the overhead
from $[[9,1,3]]$ to the more efficient $[[7,1,3]]$ configuration.

</details>


### [45] [Complex DNA Synthesis Sequences](https://arxiv.org/abs/2510.21253)
*Boaz Moav,Ryan Gabrys,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了一种混合DNA合成框架，在受限核苷酸子集上进行并行合成，建立了复杂合成序列的新概念，分析了最大信息率和渐近行为，并设计了动态规划算法求解最优合成序列。


<details>
  <summary>Details</summary>
Motivation: DNA存储具有高密度和耐久性，但现有合成方法要么允许无约束的核苷酸添加，要么强制所有链进行相同添加，限制了可扩展性。需要一种混合方法来平衡灵活性和并行性。

Method: 引入混合合成框架：每个周期从受限核苷酸子集中选择核苷酸进行并行合成。扩展了信息率定义，分析了删除球的类似概念，设计了动态规划算法计算最优复杂合成序列。

Result: 推导了最大信息率及其渐近行为的紧致表达式，建立了受限模型与理想化设置之间的理论桥梁。对于已知链的情况，设计了计算最优合成序列的动态规划算法。

Conclusion: 建立了一个全面统一的DNA合成理论框架，包含了先前模型，为未来该领域的进展奠定了基础。

Abstract: DNA-based storage offers unprecedented density and durability, but its
scalability is fundamentally limited by the efficiency of parallel strand
synthesis. Existing methods either allow unconstrained nucleotide additions to
individual strands, such as enzymatic synthesis, or enforce identical additions
across many strands, such as photolithographic synthesis. We introduce and
analyze a hybrid synthesis framework that generalizes both approaches: in each
cycle, a nucleotide is selected from a restricted subset and incorporated in
parallel. This model gives rise to a new notion of a complex synthesis
sequence. Building on this framework, we extend the information rate definition
of Lenz et al. and analyze an analog of the deletion ball, defined and studied
in this setting, deriving tight expressions for the maximal information rate
and its asymptotic behavior. These results bridge the theoretical gap between
constrained models and the idealized setting in which every nucleotide is
always available. For the case of known strands, we design a dynamic
programming algorithm that computes an optimal complex synthesis sequence,
highlighting structural similarities to the shortest common supersequence
problem. We also define a distinct two-dimensional array model with synthesis
constraints over the rows, which extends previous synthesis models in the
literature and captures new structural limitations in large-scale strand
arrays. Additionally, we develop a dynamic programming algorithm for this
problem as well. Our results establish a new and comprehensive theoretical
framework for constrained DNA, subsuming prior models and setting the stage for
future advances in the field.

</details>


### [46] [Text-Guided Diffusion Model-based Generative Communication for Wireless Image Transmission](https://arxiv.org/abs/2510.21299)
*Shengkang Chen,Tong Wu,Zhiyong Chen,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于扩散模型的生成式通信框架，在极低传输速率下通过JSCC和语义引导重建实现可靠图像传输，优于传统编码方案。


<details>
  <summary>Details</summary>
Motivation: 解决极低传输速率下传统压缩和信道编码方案无法保持足够视觉质量的挑战，专注于在严重受限速率下保持语义内容。

Method: 结合JSCC与预训练生成模型，发送端通过JSCC编码图像并传输文本提示，接收端使用Stable Diffusion和ControlNet融合损坏的低速率表示进行重建。

Result: 在极端带宽限制下仍能产生感知上令人信服的图像，在各种信道条件下始终优于传统编码方案和深度学习基线。

Conclusion: 所提框架通过利用生成先验和语义指导，在严重受限速率下实现了卓越的感知质量和鲁棒性。

Abstract: Reliable image transmission over wireless channels is particularly
challenging at extremely low transmission rates, where conventional compression
and channel coding schemes fail to preserve adequate visual quality. To address
this issue, we propose a generative communication framework based on diffusion
models, which integrates joint source channel coding (JSCC) with
semantic-guided reconstruction leveraging a pre-trained generative model.
Unlike conventional architectures that aim to recover exact pixel values of the
original image, the proposed method focuses on preserving and reconstructing
semantically meaningful visual content under severely constrained rates,
ensuring perceptual plausibility and faithfulness to the scene intent.
Specifically, the transmitter encodes the source image via JSCC and jointly
transmits it with a textual prompt over the wireless channel. At the receiver,
the corrupted low-rate representation is fused with the prompt and
reconstructed through a Stable Diffusion model with ControlNet, enabling
high-quality visual recovery. Leveraging both generative priors and semantic
guidance, the proposed framework produces perceptually convincing images even
under extreme bandwidth limitations. Experimental results demonstrate that the
proposed method consistently outperforms conventional coding-based schemes and
deep learning baselines, achieving superior perceptual quality and robustness
across various channel conditions.

</details>


### [47] [Low-Complexity MIMO Channel Estimation with Latent Diffusion Models](https://arxiv.org/abs/2510.21386)
*Xiaotian Fan,Xingyu Zhou,Le Liang,Shi Jin*

Main category: cs.IT

TL;DR: 提出PSLD-CE算法，基于潜在扩散模型进行信道估计，通过轻量级LDM架构捕获复杂信道分布，在保持低计算复杂度的同时显著提升性能


<details>
  <summary>Details</summary>
Motivation: 深度生成模型能够学习无线信道的复杂先验分布，为传统信道估计方法提供强大替代方案

Method: 设计轻量级潜在扩散模型架构作为生成先验，引入似然项有效近似和变分自编码器潜在空间的自一致性约束来增强扩散后验采样

Result: 实验结果表明PSLD-CE在广泛对比方法中表现最优，在保持低计算复杂度和快速推理速度的同时获得显著性能提升

Conclusion: 该方法为下一代无线系统提供了高度有前景且实用的信道估计解决方案

Abstract: Deep generative models offer a powerful alternative to conventional channel
estimation by learning the complex prior distribution of wireless channels.
Capitalizing on this potential, this paper proposes a novel channel estimation
algorithm based on latent diffusion models (LDMs), termed posterior sampling
with latent diffusion for channel estimation (PSLD-CE). The core of our
approach is a lightweight LDM architecture specifically designed for channel
estimation, which serves as a powerful generative prior to capture the
intricate channel distribution. Furthermore, we enhance the diffusion posterior
sampling process by introducing an effective approximation for the likelihood
term and a tailored self-consistency constraint on the variational autoencoder
latent space. Extensive experimental results demonstrate that PSLD-CE
consistently outperforms a wide range of existing methods. Notably, these
significant performance gains are achieved while maintaining low computational
complexity and fast inference speed, establishing our method as a highly
promising and practical solution for next-generation wireless systems.

</details>


### [48] [Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication](https://arxiv.org/abs/2510.21414)
*Hoang Ly,Emina Soljanin*

Main category: cs.IT

TL;DR: 提出了一个简单、与码无关的框架，将最大似然解码的最坏情况复杂度从q^k n降低到q^k，适用于线性和非线性分组码，通过向量-矩阵乘法和Mailman算法实现。


<details>
  <summary>Details</summary>
Motivation: 最大似然解码对于任意分组码在计算上仍然困难，最坏情况时间复杂度与穷举搜索相同，需要q^k n次操作，这在实际应用中是不可接受的。

Method: 利用条件概率可以表示为两个向量内积的洞察，将码本中所有码字的似然评估简化为单个向量-矩阵乘法，然后选择结果向量中的最大值。使用Mailman算法降低计算成本。

Result: 将最坏情况复杂度降低了n倍，从q^k n减少到q^k次操作，适用于各种信道和解码场景，但需要O(q^{k+1} n)的空间复杂度来存储预计算的码本矩阵。

Conclusion: 该框架显著降低了最大似然解码的计算复杂度，使其更实用，但以高空间复杂度为代价，为实际应用提供了可行的解决方案。

Abstract: Maximum-likelihood (ML) decoding for arbitrary block codes remains
fundamentally hard, with worst-case time complexity-measured by the total
number of multiplications-being no better than straightforward exhaustive
search, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper
introduces a simple, code-agnostic framework that reduces the worst-case
complexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable
reduction in practice. The result holds for both linear and nonlinear block
codes over general memoryless channels and under both hard-decision and
soft-decision decoding. It naturally extends to intersymbol-interference (ISI)
channels and ML list decoding with only a negligible increase in complexity.
Our core insight is that, upon receipt of each sequence at the receiver, the
conditional probability of that sequence for each codeword in the codebook
(i.e., the \emph{likelihood}) can be expressed as the inner product of two
carefully constructed vectors -- the first depending on the received sequence,
and the second on that codeword itself. As a result, evaluating the likelihoods
for all codewords in the codebook reduces to a single vector-matrix
multiplication, and ML decoding (MLD) becomes the simple task of picking the
maximum entry in the resulting vector. The only non-trivial cost lies in the
vector-matrix product. However, our matrix construction allows the use of the
Mailman algorithm to reduce this cost. This time reduction is achieved at the
cost of high space complexity, requiring $\mathcal{O}(q^{k+1} n)$ space to
store the pre-computed codebook matrix.

</details>


### [49] [Resilient Radio Access Networks: AI and the Unknown Unknowns](https://arxiv.org/abs/2510.21587)
*Bho Matthiesen,Armin Dekorsy,Petar Popovski*

Main category: cs.IT

TL;DR: 本文探讨了在5G网络中设计AI以实现弹性通信系统的挑战，特别关注未预料到的罕见中断情况，并指出当前统计学习方法在弹性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 5G网络虽然可靠，但在面对意外情况时仍可能失效。弹性系统能够适应现实世界的复杂性，包括系统设计时完全未预料到的运行条件，这对通信系统至关重要。

Method: 通过理论分析，研究了为弹性无线接入网络设计AI的挑战，特别关注未预料和罕见的中断情况。

Result: 理论结果表明当前统计学习方法在实现弹性方面存在严重局限性，并建议与在线学习和因果推断建立联系。

Conclusion: AI在提供通信系统弹性方面将发挥重要作用，但需要超越传统的统计学习方法，结合在线学习和因果推断等方法来应对未预料到的中断。

Abstract: 5G networks offer exceptional reliability and availability, ensuring
consistent performance and user satisfaction. Yet they might still fail when
confronted with the unexpected. A resilient system is able to adapt to
real-world complexity, including operating conditions completely unanticipated
during system design. This makes resilience a vital attribute for communication
systems that must sustain service in scenarios where models are absent or too
intricate to provide statistical guarantees. Such considerations indicate that
artifical intelligence (AI) will play a major role in delivering resilience. In
this paper, we examine the challenges of designing AIs for resilient radio
access networks, especially with respect to unanticipated and rare disruptions.
Our theoretical results indicate strong limitations of current statistical
learning methods for resilience and suggest connections to online learning and
causal inference.

</details>
