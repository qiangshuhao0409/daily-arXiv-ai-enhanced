<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments](https://arxiv.org/abs/2511.08851)
*Po-Heng Chou,Da-Chih Lin,Hung-Yu Wei,Walid Saad,Yu Tsao*

Main category: cs.NI

TL;DR: 提出了一个基于测量的框架，用于在5G非独立组网铁路环境中预测早期无线链路故障。通过比较六种模型在不同观察窗口和预测时间下的性能，发现TimesNet在3秒观察窗口和3秒预测时间下表现最佳，CNN在2秒预测时间下提供良好的精度-延迟平衡。


<details>
  <summary>Details</summary>
Motivation: 在5G铁路系统中，需要提前预测无线链路故障以采取主动措施（如冗余和自适应切换），提高系统可靠性。

Method: 使用10Hz地铁列车轨迹数据，包含服务小区和邻小区指标，比较了CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST和TimesNet六种模型在不同观察窗口和预测时间下的性能。

Result: 当观察窗口为3秒时，TimesNet在3秒预测时间下获得最高F1分数，CNN在2秒预测时间下提供良好的精度-延迟平衡，能够提前几秒预测可靠性下降。

Conclusion: 深度时序模型能够使用商用设备上的轻量级特征提前几秒预测可靠性下降，为5G铁路系统提供了实用的早期预警控制路径。

Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.

</details>


### [2] [Content-based Fine-grained Flow Management Supporting Out-of-Path Transparent Add-ons](https://arxiv.org/abs/2511.08976)
*Anna Ishizaki,Takuma Fukui,Hiroaki Nishi*

Main category: cs.NI

TL;DR: 提出了一种基于内容的细粒度流管理方法和旁路透明附加架构，用于在边缘域实现网络透明的数据包处理，支持智能社区中多样化的服务需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于数据包的流控制缺乏灵活性，无法满足智能社区中多样化服务需求，需要实现网络透明的细粒度内容管理。

Method: 提出内容级细粒度流管理方法，支持对数据包内单个内容段的控制；采用旁路透明附加架构解决传统透明附加组件需要主路径处理资源的限制；实现选择性内容掩码和两种旁路匿名化方法；开发动态重写TCP Ack和Seq号机制以保持会话完整性。

Result: 在Mininet上实现并评估，结果显示能够在保持网络透明性的同时，以最小网络延迟影响实现有效的流管理。

Conclusion: 所提出的方法能够有效实现边缘域的网络透明数据包处理，支持细粒度内容管理，同时保持TCP会话完整性并最小化网络延迟影响。

Abstract: This study aims to realize a mechanism for packet processing in the edge domain while maintaining network transparency, in order to accommodate diverse service requirements in smart communities. Since conventional flow control, which operates on a per-packet basis, lacks flexibility, we propose a content-based fine-grained flow management method that enables control at the level of individual content segments within packets. In addition, we introduce an out-of-path transparent add-on architecture to address the limitations of conventional transparent add-ons, which assume the presence of processing resources on the main path. The proposed system implements one approach for selective content masking and two approaches for out-of-path anonymization. Furthermore, we develop a mechanism for dynamically rewriting Ack and Seq numbers to preserve TCP session integrity. The proposed approaches were implemented and evaluated on Mininet, and the results demonstrate that effective flow management can be achieved with minimal impact on network delay while maintaining network transparency.

</details>


### [3] [Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems](https://arxiv.org/abs/2511.09006)
*Ameneh Zarei,Mahmood Ahmadi,Farhad Mardukhi*

Main category: cs.NI

TL;DR: 本文探讨了云、雾和边缘计算在物联网中的互补作用，提出了HIPA框架来动态分配计算任务，以降低延迟、提高可扩展性和确保数据隐私。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的海量数据和实时需求对传统云计算架构构成挑战，需要探索更高效的计算范式来支持物联网生态系统。

Method: 提出了分层物联网处理架构（HIPA），使用机器学习在云、雾和边缘层之间动态分配计算任务。

Result: 通过整合当前研究和引入HIPA框架，展示了这些计算范式如何创建高效、安全和可扩展的物联网生态系统。

Conclusion: 云、雾和边缘计算的协同作用能够显著提升物联网性能，HIPA框架为实现这一目标提供了有效解决方案。

Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.

</details>


### [4] [Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks](https://arxiv.org/abs/2511.09087)
*Vijay K Shah,Cong Shen*

Main category: cs.NI

TL;DR: Tele-LLM-Hub是一个用户友好的低代码解决方案，用于快速原型设计和部署面向5G及未来网络的上下文感知多智能体LLM系统。


<details>
  <summary>Details</summary>
Motivation: 随着电信无线网络日益复杂，智能LLM应用需要共享对网络状态的领域特定理解，以应对网络复杂性挑战。

Method: 提出TeleMCP（电信模型上下文协议）实现智能体间的结构化通信，通过低代码界面支持智能体创建、工作流组合以及与srsRAN等软件栈的交互。

Result: 开发了包含直接聊天界面、预建系统库、基于RANSTRUCT框架的智能体制作器和多智能体工作流组合器的完整平台。

Conclusion: Tele-LLM-Hub旨在民主化上下文感知多智能体系统的设计，加速下一代无线网络的创新。

Abstract: This paper introduces Tele-LLM-Hub, a user friendly low-code solution for rapid prototyping and deployment of context aware multi-agent (MA) Large Language Model (LLM) systems tailored for 5G and beyond. As telecom wireless networks become increasingly complex, intelligent LLM applications must share a domainspecific understanding of network state. We propose TeleMCP, the Telecom Model Context Protocol, to enable structured and context-rich communication between agents in telecom environments. Tele-LLM-Hub actualizes TeleMCP through a low-code interface that supports agent creation, workflow composition, and interaction with software stacks such as srsRAN. Key components include a direct chat interface, a repository of pre-built systems, an Agent Maker leveraging finetuning with our RANSTRUCT framework, and an MA-Maker for composing MA workflows. The goal of Tele-LLM-Hub is to democratize the design of contextaware MA systems and accelerate innovation in next-generation wireless networks.

</details>


### [5] [Experimenting with Energy-Awareness in Edge-Cloud Containerized Application Orchestration](https://arxiv.org/abs/2511.09116)
*Dalal Ali,Rute C. Sofia*

Main category: cs.NI

TL;DR: 该论文探索了在异构边缘云基础设施中部署应用时的能源感知策略，提出了在现有调度方法中注入计算和网络层面的能源指标，以优化资源分配并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和边缘计算的发展，能源效率成为关键问题。现有调度方法缺乏对能源消耗的考虑，导致资源分配不够优化，需要将能源指标纳入调度过程以实现更可持续的计算。

Method: 在现有调度方法中注入计算和网络层面的能源指标，使用基于ARM设备的真实测试平台进行实验评估，与标准Kubernetes调度进行比较。

Result: 实验结果显示在能源效率方面取得了持续改进，特别是在高负载场景下，证明了将能源感知纳入编排过程的潜力。

Conclusion: 将能源感知策略整合到编排过程中能够显著提高云原生计算的可持续性，特别是在异构边缘云基础设施中部署应用时。

Abstract: This paper explores the role of energy-awareness strategies into the deployment of applications across heterogeneous Edge-Cloud infrastructures. It proposes methods to inject into existing scheduling approaches energy metrics at a computational and network level, to optimize resource allocation and reduce energy consumption. The proposed approach is experimentally evaluated using a real-world testbed based on ARM devices, comparing energy consumption and workload distribution against standard Kubernetes scheduling. Results demonstrate consistent improvements in energy efficiency, particularly under high-load scenarios, highlighting the potential of incorporating energy-awareness into orchestration processes for more sustainable cloud-native computing.

</details>


### [6] [LLP-V2X: Low Latency-Power Vehicular Networking Towards 6G V2X](https://arxiv.org/abs/2511.09182)
*Zhaoyu Liu,Liu Cao,Lyutianyang Zhang,Dongyu Wei,Ye Hu,Weizheng Wang*

Main category: cs.NI

TL;DR: 提出了一种用于6G车联网的多跳多路径网络方案，联合优化车辆流量分配和传输功率，实现低延迟低功耗通信，并设计了可根据需求在最小延迟和最小功耗模式间切换的调度算法。


<details>
  <summary>Details</summary>
Motivation: 由于6G车联网对服务质量要求更加严格，能量和延迟预算之间的权衡变得至关重要，但目前对具有新V2X特性的6G车联网中能量与延迟权衡的全面研究仍不足。

Method: 提出多跳多路径车联网架构，联合优化候选路由的车辆流量分配和每链路传输功率；形式化两个互补问题（最小延迟和最小功率），设计可根据需求在固定功率最小延迟模式和固定延迟最小功率模式间切换的LLP MHMP调度算法。

Result: 通过精心设计的仿真评估了所提方案的性能，验证了其在6G V2X网络中的有效性。

Conclusion: 该研究为6G车联网提供了有效的能量-延迟权衡解决方案，通过智能调度算法实现了在不同需求场景下的最优性能。

Abstract: The trade-off between energy and latency budgets is becoming significant due to the more stringent QoS requirements in 6G vehicular networks. However, comprehensively studying the trade-off between energy and latency budgets for 6G vehicular network with new Vehicle-to-Everything (V2X) features is still under-explored. This paper proposes a novel multi-hop, multi-path vehicular networking that jointly optimizes vehicular traffic splitting across candidate routes and per-link transmit power to achieve low-latency and low-power communications. Afterwards, we formalize two complementary problem formulations (minimum latency and minimum power) based on the proposed 6G V2X architecture and provide sufficient conditions. The performance of the proposed scheme is evaluated via well-designed simulations. Based on these theories, we design algorithm (LLP MHMP Scheduler) that switches on demand between a fixed-power minimum-latency mode and a fixed-latency minimum-power mode.

</details>


### [7] [Digital Co-Founders: Transforming Imagination into Viable Solo Business via Agentic AI](https://arxiv.org/abs/2511.09533)
*Farhad Rezazadeh,Pegah Bonehgazy*

Main category: cs.NI

TL;DR: 本文提出了一个框架，帮助个体创业者利用AI代理将创意转化为成功的单人企业，包含想象力塑造、现实测试和现实扩展三个阶段。


<details>
  <summary>Details</summary>
Motivation: 研究在AI时代，个体创业者如何将个人创意转化为成功企业，探索AI作为数字联合创始人的作用，解决资源有限、决策责任集中等挑战。

Method: 基于创业学、创造力和创新研究，提出三阶段框架：想象力塑造（目标转化为价值主张）、现实测试（低成本实验和反馈）、现实扩展（可重复流程和规模化策略）。

Result: 明确了AI代理在各阶段的具体支持作用，包括市场扫描、概念生成、原型制作、内容创作、客户互动和数据分析等任务自动化。

Conclusion: 该框架为个体创业者提供了系统的方法论，强调心理适应性、有效规划和成功的人机协作，同时应对不确定性和认知过载等持续挑战。

Abstract: This paper investigates how individual entrepreneurs can turn creative ideas into successful solo businesses in an era increasingly shaped by Artificial Intelligence (AI) agents. It highlights the key steps that connect personal vision, structured experimentation, and lasting value creation, and shows how AI agents can act as digital co-founders throughout this journey. Building on research in entrepreneurship, creativity, and innovation, we present a framework with three key stages: (1) Imagination shaping, where vague goals become clear value propositions, supported by AI agents that help with market scanning, idea refinement, and rapid concept generation; (2) Reality testing, where these ideas are tested through low-cost experiments, structured feedback loops, and efficient execution, with AI agents automating tasks such as prototyping, content creation, customer interaction, and data analysis; and (3) Reality scaling, where successful ideas are transformed into repeatable processes, scalable market strategies, and long-term business models, increasingly operated and optimized by autonomous or semi-autonomous AI workflows. We focus on the specific context of solopreneurship, characterized by limited human resources, complete accountability for decision-making, and a strong association between the founder's identity and the business. The framework clearly identifies key enabling factors such as mental adaptability, effective planning, and successful human-AI collaboration within digital ecosystems. It also thoughtfully addresses ongoing challenges, like uncertainty and cognitive overload, which are heightened by our constant connectivity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 提出了一个双层可靠性监控框架，用于检测智能AI系统在运行期间的不可靠性，包括分布外检测层和AI透明度层，为人类操作员提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统在自主运行时存在可靠性不足的问题，特别是在高风险领域如医疗保健和流程工业中，不可靠的系统会带来意外行为的风险，需要缓解技术。

Method: 基于智能AI系统的特性推导运行期间的主要可靠性挑战，提出双层可靠性监控框架：分布外检测层用于检测新输入，AI透明度层用于揭示内部操作。

Result: 该框架为人类操作员提供了决策支持，帮助他们判断输出是否可能不可靠并进行干预。

Conclusion: 该框架为开发缓解技术提供了基础，以减少运行期间因可靠性不确定而产生的风险。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [9] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 提出了一种使用LLM和AMR图将无约束英语翻译成ASP程序的新方法，用于解决逻辑谜题，生成完整的ASP程序。


<details>
  <summary>Details</summary>
Motivation: ASP是一种强大的组合问题解决工具，但需要学习语法。随着非编程人员与代码交互的需求增加，需要更易用的自然语言到ASP的转换方法。

Method: 使用LLM简化自然语言句子、识别关键词和生成简单事实，然后通过AMR图解析简化语言并系统生成ASP约束。

Result: 系统成功创建了完整的ASP程序来解决组合逻辑问题，相比完全依赖LLM的方法，本系统最小化了LLM的作用。

Conclusion: 这是创建轻量级、可解释的自然语言到复杂逻辑问题解决系统的重大第一步。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [10] [BarrierBench : Evaluating Large Language Models for Safety Verification in Dynamical Systems](https://arxiv.org/abs/2511.09363)
*Ali Taheri,Alireza Taban,Sadegh Soudjani,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的智能体框架，用于自动合成动态系统的屏障证书，通过自然语言推理来提出、优化和验证候选证书，结合LLM驱动的模板发现和SMT验证，支持屏障-控制器协同合成。


<details>
  <summary>Details</summary>
Motivation: 传统屏障证书合成方法存在可扩展性差、依赖精心设计的模板、需要大量手动专业知识等问题，需要理论知识和实践经验，通常通过语言推理而非形式化方法分享。

Method: 使用LLM智能体框架进行自然语言推理，集成LLM驱动的模板发现与SMT验证，支持屏障-控制器协同合成，采用检索增强生成和智能体协调策略。

Result: 在BarrierBench基准测试中（包含100个动态系统），框架在生成有效证书方面达到超过90%的成功率。

Conclusion: 该工作展示了语言模型能够捕获和操作化专家推理，为动态系统中语言推理与形式验证的集成建立了社区测试平台。

Abstract: Safety verification of dynamical systems via barrier certificates is essential for ensuring correctness in autonomous applications. Synthesizing these certificates involves discovering mathematical functions with current methods suffering from poor scalability, dependence on carefully designed templates, and exhaustive or incremental function-space searches. They also demand substantial manual expertise--selecting templates, solvers, and hyperparameters, and designing sampling strategies--requiring both theoretical and practical knowledge traditionally shared through linguistic reasoning rather than formalized methods.
  This motivates a key question: can such expert reasoning be captured and operationalized by language models? We address this by introducing an LLM-based agentic framework for barrier certificate synthesis. The framework uses natural language reasoning to propose, refine, and validate candidate certificates, integrating LLM-driven template discovery with SMT-based verification, and supporting barrier-controller co-synthesis to ensure consistency between safety certificates and controllers.
  To evaluate this capability, we introduce BarrierBench, a benchmark of 100 dynamical systems spanning linear, nonlinear, discrete-time, and continuous-time settings. Our experiments assess not only the effectiveness of LLM-guided barrier synthesis but also the utility of retrieval-augmented generation and agentic coordination strategies in improving its reliability and performance. Across these tasks, the framework achieves more than 90% success in generating valid certificates. By releasing BarrierBench and the accompanying toolchain, we aim to establish a community testbed for advancing the integration of language-based reasoning with formal verification in dynamical systems.
  The benchmark is publicly available at https://hycodev.com/dataset/barrierbench

</details>


### [11] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 提出了一种基于向量符号代数(VSA)的认知合理ARC-AGI求解器，结合系统1直觉和系统2推理，在ARC-AGI基准测试中取得初步成功。


<details>
  <summary>Details</summary>
Motivation: 尽管人类能轻松解决ARC-AGI问题，但现有AI系统仍难以应对。受神经科学和心理学中人类智能建模方法的启发，旨在开发认知合理的解决方案。

Method: 使用基于向量符号代数的神经符号方法，集成系统1直觉和系统2推理，通过面向对象的程序合成，利用VSA表示抽象对象、指导解决方案搜索并实现样本高效的神经学习。

Result: 在ARC-AGI-1-Train上得分10.8%，ARC-AGI-1-Eval上得分3.0%；在Sort-of-ARC上得分94.5%，1D-ARC上得分83.1%，后者以极小计算成本超越GPT-4。

Conclusion: 这是首个将VSA应用于ARC-AGI的方法，开发了迄今为止最认知合理的ARC-AGI求解器，为人工智能通用智能提供了新的研究方向。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [12] [Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning](https://arxiv.org/abs/2511.08749)
*Mehrdad Zakershahrak*

Main category: cs.AI

TL;DR: QDIN是一种将强化学习系统构建为可查询推理引擎的统一架构，通过专门化神经网络模块处理不同类型的查询（策略、可达性、路径、比较等），实现了推理准确性与控制性能的解耦。


<details>
  <summary>Details</summary>
Motivation: 挑战传统强化学习仅关注最大化奖励的范式，探索将RL系统设计为能够回答环境多样化查询的推理引擎，利用训练好的智能体隐含的环境知识（可达性、距离、价值、动态等）。

Method: 引入查询条件确定性推理网络（QDIN），为不同类型的推理模式设计专门的神经模块，将查询作为一等公民处理，构建统一的推理架构。

Result: 实验显示推理准确性可达接近完美水平（99%可达性IoU），而控制性能仍处于次优状态（31%回报），表明世界知识表示与最优控制所需表示存在根本性解耦。专门化架构在推理性能上优于统一模型和后验提取方法，同时保持竞争力的控制性能。

Conclusion: 这项工作确立了将RL系统从设计之初就构建为可查询知识库的研究议程，对可解释性、验证和人机协作具有重要意义。

Abstract: Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.

</details>


### [13] [Neural Value Iteration](https://arxiv.org/abs/2511.08825)
*Yang You,Ufuk Çakır,Alex Schutz,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 该论文提出了一种新的POMDP规划算法——神经价值迭代，用神经网络替代传统的α向量来表示价值函数，解决了大规模POMDP中计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于α向量的POMDP求解器在大规模问题上计算成本过高，因为每个α向量都是|S|维的，贝尔曼备份操作变得难以处理。

Method: 利用PWLC特性，将POMDP价值函数表示为有限个神经网络的集合，结合神经网络的泛化能力和经典价值迭代框架，提出神经价值迭代算法。

Result: 该方法在现有离线求解器无法处理的大规模POMDP中仍能获得接近最优的解。

Conclusion: 神经网络表示为大规模POMDP规划提供了可行的替代方案，结合了神经网络的泛化优势和传统价值迭代的框架。

Abstract: The value function of a POMDP exhibits the piecewise-linear-convex (PWLC) property and can be represented as a finite set of hyperplanes, known as $α$-vectors. Most state-of-the-art POMDP solvers (offline planners) follow the point-based value iteration scheme, which performs Bellman backups on $α$-vectors at reachable belief points until convergence. However, since each $α$-vector is $|S|$-dimensional, these methods quickly become intractable for large-scale problems due to the prohibitive computational cost of Bellman backups. In this work, we demonstrate that the PWLC property allows a POMDP's value function to be alternatively represented as a finite set of neural networks. This insight enables a novel POMDP planning algorithm called \emph{Neural Value Iteration}, which combines the generalization capability of neural networks with the classical value iteration framework. Our approach achieves near-optimal solutions even in extremely large POMDPs that are intractable for existing offline solvers.

</details>


### [14] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 提出UCO方法解决LLM作为智能导师时的动态适应问题，通过多轮交互强化学习结合认知进步奖励和支架奖励，在数学教育基准上超越同等规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM监督微调方法只能学习表面教学模式，缺乏动态适应能力；强化学习方法面临无法区分学生真实理解与答案回显、无法感知学生认知状态演变的挑战。

Method: UCO方法采用多轮交互强化学习范式，包含两个协同奖励函数：认知进步奖励评估学生从困惑到理解的转变，支架奖励动态识别学生的最近发展区并鼓励教师在此区域内教学。

Result: 在BigMath和MathTutorBench基准测试中，UCO模型优于所有同等规模模型，性能与先进闭源模型相当。

Conclusion: UCO方法通过认知优化有效解决了LLM作为智能导师的动态适应问题，为教育AI提供了新的强化学习范式。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [15] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是首个能够在3D开放世界中实时完成数小时复杂任务的通用智能体，采用端到端的视觉语言模型统一感知、推理和行动，在Genshin Impact中训练后能完成5小时主线剧情，并具有零样本跨游戏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂3D开放世界中完成长时间任务的通用智能体，解决现有方法在实时性、任务复杂度和泛化能力方面的局限性。

Method: 采用类似人类的交互范式，使用视觉语言模型端到端处理原始像素（5Hz），生成精确的键盘鼠标动作（30Hz），仅在必要时进行推理。在Genshin Impact中训练。

Result: 成功完成Genshin Impact中5小时Mondstadt主线剧情，效率达到人类水平；在Wuthering Waves中完成100分钟任务，在Honkai: Star Rail中完成5小时第一章，均无需微调。

Conclusion: Lumine在开放世界环境和不同交互动态中表现出色，标志着向开放环境通用智能体迈出了具体一步。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [16] [The Double Contingency Problem: AI Recursion and the Limits of Interspecies Understanding](https://arxiv.org/abs/2511.08927)
*Graham L. Bishop*

Main category: cs.AI

TL;DR: 本文探讨了生物声学AI系统与其他物种递归通信过程相遇时的双重偶然性问题，主张从通用模式识别转向不同递归认知形式之间的外交性相遇。


<details>
  <summary>Details</summary>
Motivation: 当前生物声学AI系统虽然取得了跨物种的优异表现，但忽视了AI系统自身递归认知特性可能对其他物种通信结构的系统性遮蔽或扭曲，这构成了一个根本性问题。

Method: 基于哲学家Yuk Hui关于递归性和偶然性的理论框架，分析AI系统作为递归认知代理与其他物种递归通信过程相遇时的双重偶然性问题。

Result: 揭示了AI系统不是中立的模式检测器，而是具有自身递归处理特性的认知代理，其信息处理可能系统性地模糊或扭曲其他物种的通信结构。

Conclusion: 解决这一挑战需要将生物声学AI重新概念化为不同递归认知形式之间的外交性相遇，这对模型设计、评估框架和研究方法都有重要影响。

Abstract: Current bioacoustic AI systems achieve impressive cross-species performance by processing animal communication through transformer architectures, foundation model paradigms, and other computational approaches. However, these approaches overlook a fundamental question: what happens when one form of recursive cognition--AI systems with their attention mechanisms, iterative processing, and feedback loops--encounters the recursive communicative processes of other species? Drawing on philosopher Yuk Hui's work on recursivity and contingency, I argue that AI systems are not neutral pattern detectors but recursive cognitive agents whose own information processing may systematically obscure or distort other species' communicative structures. This creates a double contingency problem: each species' communication emerges through contingent ecological and evolutionary conditions, while AI systems process these signals through their own contingent architectural and training conditions. I propose that addressing this challenge requires reconceptualizing bioacoustic AI from universal pattern recognition toward diplomatic encounter between different forms of recursive cognition, with implications for model design, evaluation frameworks, and research methodologies.

</details>


### [17] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 构建了融合人工智能与大数据的业务流程优化模型，通过三层架构实现流程全生命周期智能管理，显著提升企业运营效率。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型深入，业务流程优化成为提升企业竞争力的关键，需要智能化管理手段来应对复杂业务场景。

Method: 采用包含数据处理、AI算法和业务逻辑的三层架构模型，结合分布式计算和深度学习技术，实现实时流程监控与优化。

Result: 实验验证显示：流程处理时间缩短42%，资源利用率提升28%，运营成本降低35%，系统在高并发负载下保持99.9%可用性。

Conclusion: 研究成果对企业数字化转型具有重要理论价值和实践意义，为提升企业运营效率提供了新思路。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [18] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast是一个结合人类智慧与LLM智能的交互式时间序列预测框架，通过两阶段协作过程提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法缺乏交互性、推理能力和适应性，限制了在复杂现实环境中的实用性。

Method: 采用两阶段框架：1)自动预测准备，构建多源认知基础；2)生成推理和反思优化，通过元推理循环实现持续自我修正。

Result: 在短期和长期数据集上的广泛实验表明，AlphaCast在预测准确性上持续优于最先进的基线方法。

Conclusion: AlphaCast通过人类智慧与LLM智能的协同推理，将预测重新定义为交互过程，显著提升了时间序列预测的性能。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [19] [Heterogeneous Graph Neural Networks for Assumption-Based Argumentation](https://arxiv.org/abs/2511.08982)
*Preesha Gehlot,Anna Rapberger,Fabrizio Russo,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了首个基于图神经网络（GNN）的方法来近似计算ABA框架中的可信接受问题，通过依赖图表示和两种GNN架构在ICCMA基准上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 基于假设的论证（ABA）是一种强大的结构化论证形式，但在稳定语义下精确计算扩展对于大型框架是难以处理的，需要开发近似方法来提高可扩展性。

Method: 将ABA框架建模为依赖图，编码假设、声明和规则作为节点，使用异质边标签区分支持、推导和攻击关系。提出了ABAGCN和ABAGAT两种GNN架构，分别使用残差异质卷积层和注意力层来学习节点嵌入。

Result: ABAGCN和ABAGAT在ICCMA 2023基准上优于从抽象论证文献中适应的最先进GNN基线，节点级F1得分最高达到0.71。扩展重构算法在小ABAF上F1超过0.85，在大型框架上保持约0.58的F1。

Conclusion: 这项工作为结构化论证中的可扩展近似推理开辟了新途径，证明了GNN在ABA框架中近似推理的有效性。

Abstract: Assumption-Based Argumentation (ABA) is a powerful structured argumentation formalism, but exact computation of extensions under stable semantics is intractable for large frameworks. We present the first Graph Neural Network (GNN) approach to approximate credulous acceptance in ABA. To leverage GNNs, we model ABA frameworks via a dependency graph representation encoding assumptions, claims and rules as nodes, with heterogeneous edge labels distinguishing support, derive and attack relations. We propose two GNN architectures - ABAGCN and ABAGAT - that stack residual heterogeneous convolution or attention layers, respectively, to learn node embeddings. Our models are trained on the ICCMA 2023 benchmark, augmented with synthetic ABAFs, with hyperparameters optimised via Bayesian search. Empirically, both ABAGCN and ABAGAT outperform a state-of-the-art GNN baseline that we adapt from the abstract argumentation literature, achieving a node-level F1 score of up to 0.71 on the ICCMA instances. Finally, we develop a sound polynomial time extension-reconstruction algorithm driven by our predictor: it reconstructs stable extensions with F1 above 0.85 on small ABAFs and maintains an F1 of about 0.58 on large frameworks. Our work opens new avenues for scalable approximate reasoning in structured argumentation.

</details>


### [20] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 论文提出通过结构化多智能体管道实现渐进式搜索来增强LLM推理能力，并通过递归精炼方法验证了复杂模型在政治议题分析上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在流畅性方面表现出色，但仍需提升其推理能力。基于搜索的计算解释，本研究旨在探索通过结构化多智能体管道实现渐进式搜索来增强LLM推理。

Method: 采用递归精炼方法——包含自我批评、对抗性压力测试和关键反馈整合的迭代过程。设计了简单线性管道与复杂结构化管道的对比实验，使用三位美国开国元勋历史人物构建多智能体模型，分析三个当代政治议题。

Result: 复杂模型在所有九个测试案例中均优于简单模型，平均仲裁得分88.3 vs 71.7。复杂模型的论证在分析深度、结构细微差别和战略框架方面更优越。

Conclusion: 递归精炼是通过渐进式搜索增强LLM推理的强大架构特征，结构化多智能体管道能有效提升推理质量。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [21] [Solving a Million-Step LLM Task with Zero Errors](https://arxiv.org/abs/2511.09030)
*Elliot Meyerson,Giuseppe Paolo,Roberto Dailey,Hormoz Shahrzad,Olivier Francon,Conor F. Hayes,Xin Qiu,Babak Hodjat,Risto Miikkulainen*

Main category: cs.AI

TL;DR: MAKER系统首次实现了超过100万步LLM推理的零错误任务执行，通过极端任务分解和微代理机制解决了LLM在长程任务中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然在推理、洞察和工具使用方面取得突破，但在执行类似人类组织和社会规模的扩展过程时存在持续错误率问题，无法实现长程任务的可靠执行。

Method: 采用极端任务分解策略，将任务分解为可由专注微代理处理的子任务，通过高效的多代理投票机制在每一步进行错误校正。

Result: 成功解决了超过100万步LLM推理的任务且零错误，理论上可扩展到更高水平，证明了大规模分解代理过程的可行性。

Conclusion: 相比持续改进现有LLMs，大规模分解代理过程可能为组织和社会层面问题的高效解决提供可行路径。

Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.

</details>


### [22] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 提出了一个名为Argus的运行时弹性框架，用于增强端到端自动驾驶系统的安全性，通过持续监控轨迹和危险缓解来防止安全违规。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在公共道路上部署时面临各种驾驶危险，需要具备持续监控危险和自适应响应潜在安全违规的能力，以保持复杂驾驶场景中的稳健驾驶行为。

Method: Argus框架持续监控ADS生成的轨迹，当EGO车辆被认为不安全时，通过危险缓解器无缝接管控制。该框架与TCP、UniAD和VAD三种最先进的端到端ADS集成。

Result: 评估显示Argus有效提升了ADS的弹性，平均驾驶分数提高了150.30%，防止了64.38%的违规，且时间开销很小。

Conclusion: Argus框架能够高效增强自动驾驶系统的弹性，显著提升驾驶性能并减少安全违规，为复杂驾驶场景下的安全运行提供了有效解决方案。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [23] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了下一代自动驾驶车辆在应急服务中的优化策略，重点分析了从传统强化学习向扩散模型增强强化学习和大语言模型辅助上下文学习的范式转变。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆有潜力通过更快、更安全、更高效的响应来革新应急服务，但传统强化学习方法在动态应急场景中存在样本效率低和适应性差的问题。

Method: 分析了从传统强化学习到扩散模型增强强化学习的转变（通过合成数据生成增强策略鲁棒性），以及新兴的大语言模型辅助上下文学习范式（提供轻量级、可解释的替代方案）。

Result: 通过综述自动驾驶智能、扩散模型增强强化学习和LLM辅助上下文学习的最新技术，为理解下一代自主应急响应系统提供了关键框架。

Conclusion: 从生成式AI的角度，本文为下一代自主应急响应系统的开发提供了重要见解和框架，强调了不同AI方法在提升自动驾驶车辆应急响应能力方面的潜力和权衡。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [24] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1是一个数据高效的训练框架，用于自动化优化建模和求解。它通过监督微调和测试时组相对策略优化，仅需1/10的数据就能达到67.7%的平均求解准确率，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的优化建模方法需要大量标注或合成数据，成本高且可扩展性差。需要开发数据效率更高的自动化解决方案。

Method: 采用两阶段训练：1）监督微调从有限标注数据学习问题建模和代码生成；2）测试时组相对策略优化利用大量未标注数据提升能力和一致性。

Result: 平均求解准确率达67.7%，仅需ORLM方法1/10的合成数据，准确率提升达4.2%。在仅100个样本时仍优于ORLM 2.4%。TGRPO带来额外3.1%-6.4%的准确率提升。

Conclusion: OR-R1提供了一个稳健、可扩展且成本效益高的自动化OR优化问题建模和求解方案，显著降低了工业应用的专业知识和数据壁垒。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [25] [History-Aware Reasoning for GUI Agents](https://arxiv.org/abs/2511.09127)
*Ziwei Wang,Leyang Yang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: 提出了历史感知推理框架(HAR)，通过反思错误和获取情景推理知识来增强GUI代理的短期记忆能力，解决现有方法对历史交互无感知的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在显式推理中表现出弱短期记忆，将链式交互视为离散的屏幕理解，缺乏对情景中历史交互的感知，这限制了GUI自动化的性能。

Method: 构建反思学习场景、合成定制修正指南、设计混合强化学习奖励函数，开发了端到端模型HAR-GUI-3B，将推理模式从历史无感知转变为历史感知。

Result: 在多个GUI相关基准测试上的综合评估表明，该方法在有效性和泛化性方面表现出色。

Conclusion: HAR框架成功增强了GUI代理的短期记忆能力和屏幕细节感知，提升了长时程GUI任务的执行性能。

Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.

</details>


### [26] [ProBench: Benchmarking GUI Agents with Accurate Process Information](https://arxiv.org/abs/2511.09157)
*Leyang Yang,Ziwei Wang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: 提出了ProBench移动GUI代理基准测试，包含200多个任务，扩展了传统状态评估，引入了过程相关任务评估方法，通过过程提供器自动捕获过程信息，揭示了当前GUI代理在真实场景中的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理基准测试仅通过最终屏幕状态评估任务完成情况，但GUI操作任务包含多个链式步骤，且关键信息不一定呈现在最后几页。虽然已有研究开始考虑中间步骤，但准确自动捕获过程信息仍是一个挑战。

Method: 提出了ProBench移动基准测试，包含200多个挑战性GUI任务，扩展了状态相关任务评估，增加了过程相关任务，并设计了专门的评估方法。通过新引入的过程提供器自动提供准确的过程信息，实现精确的代理性能评估。

Result: 对先进GUI代理的评估揭示了在真实GUI场景中的显著局限性。这些缺陷普遍存在于各种模型中，包括大规模通用模型和较小的GUI专用模型。详细错误分析进一步暴露了几个普遍问题。

Conclusion: 该研究为GUI代理的未来改进指明了具体方向，强调了过程评估在GUI代理性能评估中的重要性，并提供了能够准确捕获操作过程的评估框架。

Abstract: With the deep integration of artificial intelligence and interactive technology, Graphical User Interface (GUI) Agent, as the carrier connecting goal-oriented natural language and real-world devices, has received widespread attention from the community. Contemporary benchmarks aim to evaluate the comprehensive capabilities of GUI agents in GUI operation tasks, generally determining task completion solely by inspecting the final screen state. However, GUI operation tasks consist of multiple chained steps while not all critical information is presented in the final few pages. Although a few research has begun to incorporate intermediate steps into evaluation, accurately and automatically capturing this process information still remains an open challenge. To address this weakness, we introduce ProBench, a comprehensive mobile benchmark with over 200 challenging GUI tasks covering widely-used scenarios. Remaining the traditional State-related Task evaluation, we extend our dataset to include Process-related Task and design a specialized evaluation method. A newly introduced Process Provider automatically supplies accurate process information, enabling presice assessment of agent's performance. Our evaluation of advanced GUI agents reveals significant limitations for real-world GUI scenarios. These shortcomings are prevalent across diverse models, including both large-scale generalist models and smaller, GUI-specific models. A detailed error analysis further exposes several universal problems, outlining concrete directions for future improvements.

</details>


### [27] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了一种训练简洁性奖励模型(CRM)的管道，通过新颖的简洁性奖励函数(CRF)来解决大型推理模型中的过度思考问题，在减少响应长度的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型如DeepSeek-R1和OpenAI o1经常产生包含冗余推理步骤的冗长响应(过度思考现象)，显著增加计算成本。现有方法在奖励函数中加入长度惩罚，但经常出现长度崩溃和训练崩溃问题。

Method: 提出训练简洁性奖励模型(CRM)的管道，对推理路径的简洁性进行评分；引入新颖的简洁性奖励函数(CRF)，明确结果奖励与简洁性评分之间的依赖关系。

Result: 在五个数学基准数据集上的实验显示，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应token长度减少，并且在Llama和Mistral等其他LLM上泛化良好。

Conclusion: 该方法从理论角度证明了新奖励在方差减少和改进收敛性方面的优越性，从实践角度验证了方法的有效性和token效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [28] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: MedFuse是一个用于不规则临床时间序列的框架，通过乘法嵌入融合模块(MuFuse)改进特征交互建模，在多个真实世界数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的临床时间序列具有不规则性、异步采样、缺失值和异质特征动态等问题。现有嵌入策略通常通过加法操作结合特征身份和值嵌入，限制了捕捉值依赖特征交互的能力。

Method: 提出MedFuse框架，核心是MuFuse模块，通过乘法调制融合值和特征嵌入，在保留特征特定信息的同时建模跨特征的高阶依赖关系。

Result: 在涵盖重症和慢性护理的三个真实世界数据集上的实验表明，MedFuse在关键预测任务上持续优于最先进的基线方法。学习表示的分析进一步证明乘法融合增强了表达能力并支持跨数据集预训练。

Conclusion: MedFuse为建模不规则临床时间序列提供了一个可泛化的方法，乘法融合机制显著提升了特征交互建模能力。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [29] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: 提出了HyperD框架，通过将交通数据解耦为周期性和残差分量来改进交通预测，分别处理规律性模式和异常波动，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临复杂空间依赖性和多尺度周期性模式与不规则波动共存的双重挑战，现有方法难以同时有效处理这两类特征。

Method: HyperD框架包含混合周期性表示模块（处理周期性分量）和频率感知残差表示模块（处理非周期性波动），并引入双视图对齐损失来强制语义分离。

Result: 在四个真实世界交通数据集上的实验表明，HyperD实现了最先进的预测精度，同时在干扰下具有更好的鲁棒性和更高的计算效率。

Conclusion: 通过解耦周期性模式和异常波动，HyperD能够更有效地建模交通数据的复杂时空特性，为智能交通系统提供更可靠的预测能力。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


### [30] [From Model Training to Model Raising -- A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 提出从"模型训练"转向"模型培养"的新范式，将价值观对齐融入模型开发全过程，通过重构训练语料库实现知识与价值观的内在融合


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法只在模型核心能力建立后进行价值观对齐，导致模型易错位且缺乏深层次价值体系。在大模型能力开始超越人类的背景下，需要从根本上改变训练方式

Method: 重新设计训练语料库：采用第一人称视角重构训练数据、将信息重新情境化为生活经验、模拟社会互动、对训练数据进行有序搭建

Result: 预期这种训练语料库重构将实现从第一个训练token开始的早期价值观承诺，使知识、技能和价值观内在难以分离

Conclusion: 在大模型能力超越人类任务的生态系统中，这种从训练到培养的范式转变是至关重要的需求

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [31] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 本文主张开发专门为定性研究设计的AI系统，以解决当前通用AI工具在定性研究中存在的偏见、不透明、不可重复和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要推动了定量方法的发展，而定性研究这一对意义建构和全面科学理解至关重要的维度却被忽视。定性研究者只能依赖通用AI工具，但这些工具存在明显局限性。

Method: 通过文献回顾，分析现有自动化发现流程如何通过增强定性能力来改进，并确定安全定性AI在多学科和混合方法研究中的关键机会。

Result: 识别了开发透明、可重复且保护隐私的定性AI系统的必要性，以及这种系统对促进多学科和混合方法研究的潜力。

Conclusion: 需要从零开始构建专门用于解释性研究的定性AI系统，这些系统必须透明、可重复且保护隐私，以弥补当前AI在定性研究中的关键空白。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [32] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: 2025年最新评估显示，GPT-5在标准PDDL规划任务上的表现与专业规划器LAMA相当，但在混淆测试中所有LLM性能均下降，不过下降幅度小于之前的模型。


<details>
  <summary>Details</summary>
Motivation: 评估前沿大语言模型在端到端规划任务中的推理能力，特别是与专业规划器的性能对比。

Method: 使用国际规划竞赛学习赛道中的PDDL领域和任务子集，评估DeepSeek R1、Gemini 2.5 Pro、GPT-5和LAMA规划器，包括标准PDDL和混淆PDDL两种测试场景。

Result: 在标准PDDL领域，GPT-5解决任务的能力与LAMA相当；在混淆测试中所有LLM性能下降但幅度较小，相比前代模型有显著改进。

Conclusion: 前沿LLM在规划任务上取得了实质性进步，缩小了与专业规划器在挑战性基准测试中的性能差距。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [33] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 提出了一种基于潜在流匹配和分类器自由引导的新方法，通过明确分离条件信息与残差表示来解缠潜在子空间，实现在高维数据中访问有意义的特征。


<details>
  <summary>Details</summary>
Motivation: 在科学发现中访问学习表示中的信息对于高维领域至关重要，需要一种机制来分析、控制和重新利用潜在表示。

Method: 基于潜在流匹配与分类器自由引导，明确分离条件信息与残差表示，解缠潜在子空间。

Result: 在三个实验（合成2D高斯问题、彩色MNIST、Galaxy10天文数据集）中证明该方法能够访问高维数据的有意义特征。

Conclusion: 该方法为使用生成模型进行科学探索提供了一条简单而强大的途径，特别是对于未被捕获、考虑或编目的信息。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [34] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: CrochetBench是一个评估多模态大语言模型在钩针编织领域进行细粒度、低层次程序推理能力的基准测试，强调从描述转向执行，要求模型识别针法、选择结构适当的指令并生成可编译的钩针程序。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注高层次描述或视觉问答，缺乏对实际执行能力的评估。CrochetBench旨在填补这一空白，通过评估模型在真实世界创造性领域中的程序能力。

Method: 采用CrochetPARADE DSL作为中间表示，支持结构验证和通过执行进行功能评估。基准测试涵盖针法分类、指令接地、自然语言到DSL翻译和图像到DSL翻译等任务。

Result: 在所有任务中，当评估从表面相似性转向可执行正确性时，模型性能急剧下降，暴露出在长距离符号推理和3D感知程序合成方面的局限性。

Conclusion: CrochetBench为评估多模态模型的程序能力提供了新视角，并揭示了在真实世界创造性领域中表面理解与可执行精度之间的差距。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [35] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 提出一种基于多模型聚合的AI安全方法，通过共识采样算法从k个模型中选择最安全的s个子集，在模型间达成足够共识时输出结果，否则弃权。


<details>
  <summary>Details</summary>
Motivation: 现有的AI安全检查方法主要依赖模型输出或激活的检测，但某些风险无法仅通过检测发现，需要一种架构无关的补充安全方法。

Method: 使用共识采样算法，利用模型计算输出概率的能力，当足够多的安全模型达成共识时输出结果，否则弃权。算法受Vyas等人(2023)的可证明版权保护算法启发。

Result: 该方法能够实现与k个模型中最安全的s个模型平均风险相当的安全性，并在安全模型数量足够且共识充分时限制弃权概率。

Conclusion: 该方法提供了一种新的模型无关的AI安全方法，通过放大集合中未知安全子集的安全保证来构建单个可靠模型。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [36] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: 本文从科学和系统角度阐述了物理人工智能的基本原理，旨在为智能系统的物理体现、感知、行动、学习和情境敏感性建立统一的理论框架，将智能视为身体、环境和经验之间真实交互的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 传统AI依赖符号处理和数据驱动模型，而Physical AI将智能理解为身体、环境和经验之间真实交互的涌现现象，需要建立新的理论基础来描述物理体现的智能系统。

Method: 提出了六个基本原理：体现、感知、运动行动、学习、自主性和情境敏感性，这些原理构成一个闭环控制系统，其中能量、信息、控制和情境持续交互。

Result: 建立了物理AI的理论框架，表明这六个原理不是松散的功能模块，而是构成一个闭环控制系统，使系统能够从物理经验而非数据库中生成意义。

Conclusion: 物理AI将学习理解为智能体与环境之间结构耦合的变化，而非参数调整，这种范式转变将智能理解为物理体现的过程，通过康复机器人实例验证了理论模型的有效性。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>


### [37] [Robust and Diverse Multi-Agent Learning via Rational Policy Gradient](https://arxiv.org/abs/2511.09535)
*Niklas Lauffer,Ameesh Shah,Micah Carroll,Sanjit A. Seshia,Stuart Russell,Michael Dennis*

Main category: cs.AI

TL;DR: 提出了Rationality-preserving Policy Optimization (RPO)框架和Rational Policy Gradient (RPG)算法，解决对抗优化在合作设置中导致智能体自毁的问题，确保智能体保持理性。


<details>
  <summary>Details</summary>
Motivation: 对抗优化算法在多智能体设置中能有效找到鲁棒和多样化的策略，但在合作设置中会导致智能体非理性自毁，阻碍任务完成和学习进程。

Method: 开发了RPG算法，在修改后的原始游戏版本中训练智能体最大化自身奖励，使用对手塑造技术优化对抗目标，确保智能体策略相对于某些可能的伙伴策略保持最优。

Result: RPG能够扩展多种现有对抗优化算法，不再受自毁限制，可以找到对抗样本、提高鲁棒性和适应性、学习多样化策略，在多个合作和一般和环境中表现优异。

Conclusion: RPO框架和RPG算法成功解决了对抗优化在合作设置中的自毁问题，使对抗优化方法能够有效应用于更广泛的多智能体环境。

Abstract: Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io.

</details>


### [38] [Breadth-First Search vs. Restarting Random Walks for Escaping Uninformed Heuristic Regions](https://arxiv.org/abs/2511.09549)
*Daniel Platnick,Dawson Tomasz,Eamon Earl,Sourena Khanzadeh,Richard Valenzano*

Main category: cs.AI

TL;DR: 本文比较了广度优先搜索(BrFS)和重启随机游走(RRWs)两种逃离无信息启发式区域(UHRs)的方法，推导了它们的期望运行时间，并开发了EHC-RRW变体来改进标准EHC算法。


<details>
  <summary>Details</summary>
Motivation: 贪婪搜索方法如GBFS和EHC在面对启发式局部极小值或平台等无信息启发式区域(UHRs)时表现不佳，需要有效的逃离机制。

Method: 理论推导BrFS和RRWs逃离UHRs的期望运行时间，开发EHC-RRW变体使用RRWs替代BrFS，并在PDDL规划基准上进行实验验证。

Result: 确定了RRWs比BrFS更快的条件，EHC-RRW在EHC有效的场景下具有更强的期望运行时间保证，实验验证了这些方法逃离UHRs的相对有效性。

Conclusion: RRWs在某些情况下比BrFS更有效地逃离UHRs，EHC-RRW为EHC算法提供了理论保证和实际改进。

Abstract: Greedy search methods like Greedy Best-First Search (GBFS) and Enforced Hill-Climbing (EHC) often struggle when faced with Uninformed Heuristic Regions (UHRs) like heuristic local minima or plateaus. In this work, we theoretically and empirically compare two popular methods for escaping UHRs in breadth-first search (BrFS) and restarting random walks (RRWs). We first derive the expected runtime of escaping a UHR using BrFS and RRWs, based on properties of the UHR and the random walk procedure, and then use these results to identify when RRWs will be faster in expectation than BrFS. We then evaluate these methods for escaping UHRs by comparing standard EHC, which uses BrFS to escape UHRs, to variants of EHC called EHC-RRW, which use RRWs for that purpose. EHC-RRW is shown to have strong expected runtime guarantees in cases where EHC has previously been shown to be effective. We also run experiments with these approaches on PDDL planning benchmarks to better understand their relative effectiveness for escaping UHRs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [39] [Design and Performance Analysis of Hybrid FSO/THz Relay with Aerial RIS for Future NTN-Integrated 6G Wireless Communications](https://arxiv.org/abs/2511.08756)
*Al Nahian Mugdho,Md. Ibrahim,A. S. M. Badrudduza,Md. Abdur Rakib,Imran Shafique Ansari*

Main category: cs.IT

TL;DR: 提出了一种新型双跳无线网络，第一跳采用混合FSO/THz链路，第二跳采用空中RIS的RF链路，比较了硬切换和软切换策略，性能比传统RF-FSO框架提升52.54%。


<details>
  <summary>Details</summary>
Motivation: 在6G无线网络背景下，利用RIS智能控制电磁波传播并增强回程通信性能，解决传统框架的性能限制问题。

Method: 构建双跳网络模型，第一跳为混合FSO/THz链路，第二跳为空中RIS-RF链路，推导关键性能指标的闭式表达式，进行渐近分析和蒙特卡洛仿真验证。

Result: 混合模型比传统RF-FSO性能提升52.54%，集成空中RIS使系统性能提升41.39%，将RIS置于较低高度且与两端等距放置可显著改善性能。

Conclusion: 提出的混合网络架构在6G背景下具有显著性能优势，空中RIS的优化部署策略对系统性能有重要影响。

Abstract: In the context of emerging sixth-generation (6G) wireless networks, reconfigurable intelligent surfaces (RISs) are gaining prominence for their ability to intelligently control electromagnetic wave propagation and enhance backhaul communication performance. In this paper, we propose a novel dual-hop wireless network, where the first hop consists of a hybrid free-space optics (FSO) / terahertz (THz) link, and the second hop incorporates an aerial RIS-based radio frequency (RF) link. To provide a comprehensive performance evaluation, a comparative analysis of two switching strategies is conducted: (1) hard switching and (2) soft switching. Novel closed-form expressions are derived for key performance metrics, including outage probability and bit error rate. These expressions are then utilized to investigate the impact of various system parameters. Our proposed hybrid model demonstrates a 52.54% performance improvement over the traditional RF-FSO framework. Moreover, the integration of an aerial RIS in the second hop enhances system performance by 41.39%. Numerical findings suggest that strategically placing the aerial RIS at a lower altitude and maintaining an equal, shorter distance from both communication endpoints significantly improves overall system performance. To analyze the response under high signal-to-noise ratio (SNR) conditions, asymptotic analysis is performed, and the diversity order of the system is determined. Finally, the analytical results are validated through a Monte Carlo simulation.

</details>


### [40] [Tracing AG Codes: Toward Meeting the Gilbert-Varshamov Bound](https://arxiv.org/abs/2511.08788)
*Gil Cohen,Dean Doron,Noam Goldgraber,Tomer Manket*

Main category: cs.IT

TL;DR: 该论文通过使用代数几何码的迹来尝试匹配Gilbert-Varshamov界，提出了一种新的字母表缩减方法，并建立了适用于分析迹-代数几何码的Hasse-Weil型定理。


<details>
  <summary>Details</summary>
Motivation: 解决编码理论中最古老的问题之一：用显式二进制码匹配Gilbert-Varshamov界。利用代数几何码在较大常数域上超越GV界的现象，通过取迹操作将这种优势转移到二进制域。

Method: 采用代数几何码的迹（TAG码）作为字母表缩减方法，而不是传统的级联方法。建立了专门适用于分析TAG码的Hasse-Weil型定理，并推导了新的指数和估计。

Result: 虽然目前没有获得改进的构造，但证明了常数因子加强的界就足够了。在高距离机制下，TAG码劣于码级联。新的Hasse-Weil型定理具有比分析TAG码所需更广泛的适用性。

Conclusion: TAG码提供了一种有前景的字母表缩减方法，其分析需要专门的代数工具。虽然在高距离机制下不如级联方法，但该方法为匹配GV界提供了新的研究方向。

Abstract: One of the oldest problems in coding theory is to match the Gilbert-Varshamov bound with explicit binary codes. Over larger-yet still constant-sized-fields, algebraic-geometry codes are known to beat the GV bound. In this work, we leverage this phenomenon by taking traces of AG codes. Our hope is that the margin by which AG codes exceed the GV bound will withstand the parameter loss incurred by taking the trace from a constant field extension to the binary field. In contrast to concatenation, the usual alphabet-reduction method, our analysis of trace-of-AG (TAG) codes uses the AG codes' algebraic structure throughout - including in the alphabet-reduction step.
  Our main technical contribution is a Hasse-Weil-type theorem that is well-suited for the analysis of TAG codes. The classical theorem (and its Grothendieck trace-formula extension) are inadequate in this setting. Although we do not obtain improved constructions, we show that a constant-factor strengthening of our bound would suffice. We also analyze the limitations of TAG codes under our bound and prove that, in the high-distance regime, they are inferior to code concatenation. Our Hasse-Weil-type theorem holds in far greater generality than is needed for analyzing TAG codes. In particular, we derive new estimates for exponential sums.

</details>


### [41] [Policy-Guided MCTS for near Maximum-Likelihood Decoding of Short Codes](https://arxiv.org/abs/2511.09054)
*Y. Tian,C. Yue,P. Cheng,G. Pang,B. Vucetic,Y. Li*

Main category: cs.IT

TL;DR: 提出了一种基于策略引导的蒙特卡洛树搜索解码器，用于短块码的近最大似然解码，无需高斯消元，复杂度降低95%，在高信噪比下延迟更低。


<details>
  <summary>Details</summary>
Motivation: 传统有序统计解码需要高斯消元，而非高斯消元OSD复杂度较高，需要一种既能达到近最大似然性能又降低复杂度的解码方法。

Method: 使用蒙特卡洛树搜索在接收信息位中搜索测试错误模式，通过神经网络策略引导搜索过程，找到正确的TEP后重新编码得到码字候选。

Result: 相比非高斯消元OSD，搜索复杂度降低95%，在高信噪比下解码延迟低于OSD和非高斯消元OSD，达到近最大似然解码性能。

Conclusion: 该方法为短块码提供了一种高效的最大似然解码方案，避免了高斯消元操作，显著降低了计算复杂度。

Abstract: In this paper, we propose a policy-guided Monte Carlo Tree Search (MCTS) decoder that achieves near maximum-likelihood decoding (MLD) performance for short block codes. The MCTS decoder searches for test error patterns (TEPs) in the received information bits and obtains codeword candidates through re-encoding. The TEP search is executed on a tree structure, guided by a neural network policy trained via MCTS-based learning. The trained policy guides the decoder to find the correct TEPs with minimal steps from the root node (all-zero TEP). The decoder outputs the codeword with maximum likelihood when the early stopping criterion is satisfied. The proposed method requires no Gaussian elimination (GE) compared to ordered statistics decoding (OSD) and can reduce search complexity by 95\% compared to non-GE OSD. It achieves lower decoding latency than both OSD and non-GE OSD at high SNRs.

</details>


### [42] [Color Multiset Codes based on Sunmao Construction](https://arxiv.org/abs/2511.09070)
*Wing Shing Wong,Chung Shue Chen,Yuan-Hsun Lo*

Main category: cs.IT

TL;DR: 本文提出了一种基于多重集而非有序序列的编码方案，适用于符号顺序无法保持或观察的场景，特别是在传感器网络中的移动目标跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于传感器网络中移动目标跟踪问题，以及那些无法维持或观察码字符号顺序的应用场景。

Method: 采用多维度整数网格分解方法，将原始源数据网格分解为子网格，在每个子网格上解决多重集编码问题，然后拼接形成最终解，这种方法被称为'榫卯构造'。

Result: 提出了辫子码作为具体解决方案，辫子码易于在多维网格上定义，对于给定码集大小和多重集基数的编码，其编码效率在所需不同符号数量方面具有渐近最优性，并具有内在的纠错特性。

Conclusion: 辫子码通过榫卯构造在多维网格上实现了高效的多重集编码，具有渐近最优的编码效率和良好的纠错能力。

Abstract: We present results on coding using multisets instead of ordered sequences. The study is motivated by a moving object tracking problem in a sensor network and can find applications in settings where the order of the symbols in a codeword cannot be maintained or observed. In this paper a multiset coding scheme is proposed on source data that can be organized as a flat or cyclic multi-dimensional integer lattice (grid). A fundamental idea in the solution approach is to decompose the original source data grid into sub-grids. The original multiset coding problem can then be restricted to each of the sub-grid. Solutions for the sub-grids are subsequently piece together to form the desired solution. We name this circle of idea as sunmao construction in reference to woodwork construction method with ancient origin. Braid codes are specific solutions defined using the sunmao construction. They are easy to define for multi-dimensional grids. Moreover for a code of a given code set size and multiset cardinality, if we measure coding efficiency by the number of distinct symbols required, then braid codes have asymptotic order equal to those that are optimal. We also show that braid codes have interesting inherent error correction properties.

</details>


### [43] [Learning Binary Autoencoder-Based Codes with Progressive Training](https://arxiv.org/abs/2511.09221)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 提出了一种简化的两阶段训练方法，用于学习二进制纠错码的自动编码器架构，无需梯度近似技术即可实现稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 传统自动编码器方法在强制二进制码字时面临困难，因为离散化会破坏梯度流并导致不稳定收敛。

Method: 采用两阶段训练：连续预训练阶段，然后直接二值化和微调，不使用梯度近似技术。

Result: 在(7,4)块配置下，学习的编码器-解码器对学会了最优汉明码的旋转版本（陪集码），自然恢复了其线性和距离特性，在最大似然解码下实现了相同的块错误率。

Conclusion: 紧凑的自动编码器架构可以通过稳定且简单的训练有效学习结构化的、代数最优的二进制码。

Abstract: Error correcting codes play a central role in digital communication, ensuring that transmitted information can be accurately reconstructed despite channel impairments. Recently, autoencoder (AE) based approaches have gained attention for the end-to-end design of communication systems, offering a data driven alternative to conventional coding schemes. However, enforcing binary codewords within differentiable AE architectures remains difficult, as discretization breaks gradient flow and often leads to unstable convergence. To overcome this limitation, a simplified two stage training procedure is proposed, consisting of a continuous pretraining phase followed by direct binarization and fine tuning without gradient approximation techniques. For the (7,4) block configuration over a binary symmetric channel (BSC), the learned encoder-decoder pair learns a rotated version (coset code) of the optimal Hamming code, naturally recovering its linear and distance properties and thereby achieving the same block error rate (BLER) with maximum likelihood (ML) decoding. These results indicate that compact AE architectures can effectively learn structured, algebraically optimal binary codes through stable and straightforward training.

</details>


### [44] [Generic Construction of Optimal-Access Binary MDS Array Codes with Smaller Sub-packetization](https://arxiv.org/abs/2511.09251)
*Lan Ma,Qifu Tyler Sun,Shaoteng Liu,Liyang Zhou*

Main category: cs.IT

TL;DR: 提出了两种通用构造方法（通用构造I和II）来构建具有单节点故障最优修复带宽的二进制MDS阵列码，显著降低了子分组化程度。


<details>
  <summary>Details</summary>
Motivation: 解决二进制MDS阵列码在分布式存储系统中的单节点故障修复问题，特别是降低修复带宽和子分组化程度。

Method: 通用构造I：基于任意子分组化为m的二进制MDS阵列码，构造(k+r,k,ms^⌈(k+r)/s⌉)码，修复时连接d=k+s-1个辅助节点；通用构造II：针对偶数r≥4且s=r/2的情况，构造(k+r,k,ms^{(k+r)/(s+1)})码，其中部分节点具有最优访问特性。

Result: 构造的码C1具有最优访问带宽，C2具有最优修复带宽且子分组化程度是目前已知二进制MDS阵列码中最小的。

Conclusion: 提出的两种通用构造方法有效解决了二进制MDS阵列码的修复问题，在降低子分组化程度的同时保证了最优修复性能。

Abstract: A $(k+r,k,l)$ binary array code of length $k+r$, dimension $k$, and sub-packetization $l$ is composed of $l\times(k+r)$ matrices over $\mathbb{F}_2$, with every column of the matrix stored on a separate node in the distributed storage system and viewed as a coordinate of the codeword. It is said to be maximum distance separable (MDS) if any $k$ out of $k+r$ coordinates suffice to reconstruct the whole codeword. The repair problem of binary MDS array codes has drawn much attention, particularly for single-node failures. In this paper, given an arbitrary binary MDS array code with sub-packetization $m$ as the base code, we propose two generic approaches (Generic Construction I and II) for constructing binary MDS array codes with optimal access (or repair) bandwidth for single-node failures. For every $s\leq r$, a $(k+r,k,ms^{\lceil \frac{k+r}{s}\rceil})$ code $\mathcal{C}_1$ with optimal access bandwidth can be constructed by Generic Construction I. Repairing a failed node of $\mathcal{C}_1$ requires connecting to $d = k+s-1$ helper nodes, in which $s-1$ helper nodes are designated and $k$ are free to select. $\mathcal{C}_1$ generally achieves smaller sub-packetization and provides greater flexibility in the selection of its coefficient matrices. For even $r\geq4$ and $s=\frac{r}{2}$ such that $s+1$ divides $k+r$, a $(k+r, k,ms^{\frac{k+r}{s+1}})$ code $\mathcal{C}_2$ with optimal repair bandwidth can be constructed by Generic Construction II, with $\frac{s}{s+1}(k+r)$ out of $k+r$ nodes having the optimal access property. To the best of our knowledge, $\mathcal{C}_2$ possesses the smallest sub-packetization among existing binary MDS array codes with optimal repair bandwidth known to date.

</details>


### [45] [Enabling Smart Radio Environments in the Frequency Domain With Movable Signals](https://arxiv.org/abs/2511.09384)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 提出了一种在频率域实现智能无线电环境的新方法——可移动信号，通过动态调整信号频谱在频率轴上的位置来克服传统可重构智能表面和柔性天线的实现挑战。


<details>
  <summary>Details</summary>
Motivation: 传统智能无线电环境技术（如可重构智能表面和柔性天线）依赖电子可重构或可移动组件，存在实现挑战，可能阻碍商业化。需要寻找新的实现域来克服这些限制。

Method: 提出在频率域实现智能无线电环境，通过可移动信号动态调整信号频谱位置。分析了在视距和非视距条件下多输入单输出系统中的性能，并研究了使用固定智能表面辅助的系统。

Result: 在视距条件下，可移动信号比量化等增益传输获得更高的平均接收功率。在非视距条件下，使用固定智能表面辅助的可移动信号系统比使用固定频率信号的可重构智能表面系统最高可获得四倍的接收功率。

Conclusion: 频率域的可移动信号是实现智能无线电环境的有前景的新方法，能够克服传统技术的实现挑战，在多种传播条件下都能提供显著的性能提升。

Abstract: Smart radio environments (SREs) enhance wireless communications by allowing control over the channel. They have been enabled through surfaces with reconfigurable electromagnetic (EM) properties, known as reconfigurable intelligent surfaces (RISs), and through flexible antennas, which can be viewed as realizations of SREs in the EM domain and space domain, respectively. However, these technologies rely on electronically reconfigurable or movable components, introducing implementation challenges that could hinder commercialization. To overcome these challenges, we propose a new domain to enable SREs, the frequency domain, through the concept of movable signals, where the signal spectrum can be dynamically moved along the frequency axis. We first analyze movable signals in multiple-input single-output (MISO) systems under line-of-sight (LoS) conditions, showing that they can achieve higher average received power than quantized equal gain transmission (EGT). We then study movable signals under non-line-of-sight (NLoS) conditions, showing that they remain effective by leveraging reflections from surfaces made of uniformly spaced elements with fixed EM properties, denoted as fixed intelligent surfaces (FISs). Analytical results reveal that a FIS-aided system using movable signals can achieve up to four times the received power of a RIS-aided system using fixed-frequency signals.

</details>


### [46] [Computability of the Optimizer for Rate Distortion Functions](https://arxiv.org/abs/2511.09412)
*Jonathan E. W. Huffmann,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究了率失真函数中优化测试信道的可计算性问题，发现虽然率失真函数本身通常可计算，但优化器在一般情况下不可计算，即使对于简单的失真度量也是如此。


<details>
  <summary>Details</summary>
Motivation: 率失真理论在通信和信息理论中具有重要理论和实践价值，研究其优化器的可计算性有助于理解信息论问题的计算复杂性。

Method: 通过分析率失真函数优化问题的计算特性，与已知的其他信息论问题优化器结果进行比较研究。

Result: 研究发现率失真函数通常可计算，但优化测试信道在一般情况下不可计算，即使对于简单失真度量也是如此。

Conclusion: 率失真函数的优化器存在计算复杂性限制，这一发现与信息论中其他优化问题的可计算性结果相似。

Abstract: Rate distortion theory treats the problem of encoding a source with minimum codebook size while at the same time allowing for a certain amount of errors in the reconstruction measured by a fidelity criterion and distortion level. Similar to the channel coding problem the optimal rate of the codebook with respect to the blocklength is given by a convex optimization problem involving information theoretic quantities like mutual information. The value of the rate in dependence of the distortion level as well as the optimizer used in the codebook construction are of theoretical and practical importance in communication and information theory. In this paper the behavior of the rate distortion function regarding the computability of the optimizing test channel is investigated. We find that comparable with known results about the optimizer for other information theoretic problems a similar result is found to be true also regarding the computability of the optimizer for rate distortion functions.
  It turns out that while the rate distortion function is usually computable the optimizer for this problem is in general non-computable even for simple distortion measures.

</details>
