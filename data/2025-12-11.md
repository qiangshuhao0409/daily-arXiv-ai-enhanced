<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping](https://arxiv.org/abs/2512.09312)
*Ziheng Yang,Kun Qiu,Zhe Chen,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: Tyche框架使用MCTS-BH算法计算波束跳变照明模式，结合滑动窗口和剪枝技术，将37个单元的计算时间从300秒降至12秒，吞吐量提升高达98.76%。


<details>
  <summary>Details</summary>
Motivation: 高吞吐量卫星使用波束跳变处理非均匀时变地面流量，但现有算法计算效率低下：遗传算法需300秒计算37个单元，多智能体深度强化学习在超过40个单元时难以收敛，无法满足实际应用需求。

Method: 提出Tyche混合计算框架：1) MCTS-BH算法计算照明模式，结合滑动窗口和剪枝技术加速；2) G-BH算法提供临时解决方案，确保实时计算；3) 后台运行MCTS-BH完成精确计算。

Result: MCTS-BH算法仅需12秒计算37个单元的照明模式（相比遗传算法300秒），吞吐量提升高达98.76%，显著优于现有解决方案。

Conclusion: Tyche框架通过MCTS-BH算法和混合计算策略，有效解决了波束跳变照明模式计算的实时性问题，为大规模卫星通信系统提供了实用解决方案。

Abstract: High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.

</details>


### [2] [Eunomia: A Multicontroller Domain Partitioning Framework in Hierarchical Satellite Network](https://arxiv.org/abs/2512.09345)
*Qi Zhang,Kun Qiu,Zhe Chen,Wenjun Zhu,Xiaofan Xu,Ping Du,Yue Gao*

Main category: cs.NI

TL;DR: Eunomia是一个用于分层卫星网络的三步域划分框架，通过FOV感知分割、谱聚类和Kuhn-Munkres算法优化控制器分配，显著降低控制平面延迟和开销。


<details>
  <summary>Details</summary>
Motivation: 随着巨型卫星星座的兴起，分层非地面和地面网络集成成为6G覆盖增强的关键。然而，LEO卫星的高移动性和视场限制给高效域划分带来根本性挑战：集中控制面临可扩展性瓶颈，分布式架构忽视FOV限制导致信号开销过大，控制器FOV外的LEO卫星需要额外5跳，响应时间增加10.6倍。

Method: 提出Eunomia三步域划分框架：1) 利用地面站和MEO卫星组成的混合控制平面，进行移动感知的FOV分割，将域限制在FOV感知区域内，确保单跳信令；2) 在控制开销关系图上进行谱聚类以平衡流量负载；3) 使用Kuhn-Munkres算法优化控制器分配。在Plotinus仿真平台上实现，采用真实星座参数。

Result: 实验结果表明，Eunomia相比现有最优方案，请求丢失率降低高达58.3%，控制开销降低高达50.3%，算法执行时间减少77.7%。

Conclusion: Eunomia通过FOV感知域划分和混合控制平面，有效解决了分层卫星网络中高移动性和视场限制带来的挑战，显著提升了网络性能和可扩展性。

Abstract: With the rise of mega-satellite constellations, the integration of hierarchical non-terrestrial and terrestrial networks has become a cornerstone of 6G coverage enhancements. In these hierarchical satellite networks, controllers manage satellite switches within their assigned domains. However, the high mobility of LEO satellites and field-of-view (FOV) constraints pose fundamental challenges to efficient domain partitioning. Centralized control approaches face scalability bottlenecks, while distributed architectures with onboard controllers often disregard FOV limitations, leading to excessive signaling overhead. LEO satellites outside a controller's FOV require an average of five additional hops, resulting in a 10.6-fold increase in response time. To address these challenges, we propose Eunomia, a three-step domain-partitioning framework that leverages movement-aware FOV segmentation within a hybrid control plane combining ground stations and MEO satellites. Eunomia reduces control plane latency by constraining domains to FOV-aware regions and ensures single-hop signaling. It further balances traffic load through spectral clustering on a Control Overhead Relationship Graph and optimizes controller assignment via the Kuhn-Munkres algorithm. We implement Eunomia on the Plotinus emulation platform with realistic constellation parameters. Experimental results demonstrate that Eunomia reduces request loss by up to 58.3%, control overhead by up to 50.3\%, and algorithm execution time by 77.7% significantly outperforming current state-of-the-art solutions.

</details>


### [3] [BlockFLEX: An Adaptive and Survivable Architecture with Hierarchical Routing for LEO Satellite Networks](https://arxiv.org/abs/2512.09453)
*Xiangtong Wang*

Main category: cs.NI

TL;DR: BlockFLEX是一种自适应、可生存的LEO卫星网络架构，采用分层路由方案，通过将卫星组织成自治区块来应对动态拓扑变化和严重链路故障，在30%链路故障下可达性提升2倍，路由可用性接近100%。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星网络面临动态拓扑变化和严重链路故障的挑战，现有方案难以同时保证路由效率、弹性和稳定性，需要设计能够适应网络波动并提供稳定覆盖视图的架构。

Method: 将卫星组织成自治区块，建立可生存的底层网络；采用分层路由方案，集成无收敛地理路由和隔离收敛路由；自适应切换有状态和无状态转发模式；使用专用保护机制和优化的源卫星选择算法。

Result: 在30%随机链路故障下，可达性比现有领先方案提升2倍，路由可用性接近100%；控制消息和FIB更新开销低于OSPF的0.2%；路由计算时间减少≥36%，延迟抖动降低≥50%。

Conclusion: BlockFLEX通过区块化组织和分层路由方案，有效解决了LEO卫星网络的动态性和故障问题，在保持低开销的同时显著提升了网络的可达性、稳定性和性能。

Abstract: This paper presents \textbf{BlockFLEX}, an adaptive and survivable architecture with a hierarchical routing scheme for Low Earth Orbit satellite networks, designed to address dynamic topology changes and severe link failures.
  By organizing satellites into autonomous blocks, BlockFLEX establishes a survivable underlay network that masks network volatility and offers a stable overlay view. The architecture employs a hierarchical routing scheme integrating both convergence-free geographic routing and convergence-isolated routing. Furthermore, BlockFLEX adaptively switches between stateful and stateless forwarding modes, enabling efficient, resilient, and stable routing via a dedicated protection mechanism and an optimized source satellite selection algorithm.
  Experimental evaluations on current operational LEO satellite networks (LSNs) demonstrate that under scenarios with up to 30\% random link failures, the proposed method achieves a $2\times$ improvement in reachability compared to current leading schemes, while maintaining near-100\% routing availability. Moreover, the overhead of control messages and forwarding information base (FIB) updates remains below $0.2\%$ of that in OSPF, accompanied by a $\geq 36\%$ reduction in routing computation time and a $\geq 50\%$ decrease in latency jitter.

</details>


### [4] [M3Net: A Multi-Metric Mixture of Experts Network Digital Twin with Graph Neural Networks](https://arxiv.org/abs/2512.09797)
*Blessed Guda,Carlee Joe-Wong*

Main category: cs.NI

TL;DR: M3Net是一个基于图神经网络的多指标专家混合网络数字孪生系统，能够从扩展的网络状态数据中准确预测多个性能指标，显著提升了流延迟预测精度。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络技术的发展带来了自动驾驶、虚拟现实等应用，导致连接设备激增和网络管理复杂化。这些应用对延迟、可靠性等性能指标有严格且异构的要求。传统网络建模方法（如离散事件模拟器和仿真）难以平衡准确性和可扩展性，而现有网络数字孪生模型通常只关注单一性能指标或模拟数据。

Method: 提出M3Net（Multi-Metric Mixture-of-experts Network Digital Twin），采用图神经网络架构，从扩展的网络状态数据中估计多个性能指标。该方法结合专家混合（MoE）模型，能够处理多种场景下的网络性能预测。

Result: M3Net显著提升了流延迟预测的准确性，将MAPE（平均绝对百分比误差）从20.06%降低到17.39%。同时，在抖动和丢包预测方面分别达到66.47%和78.7%的准确率。

Conclusion: M3Net作为一个多指标专家混合网络数字孪生系统，通过图神经网络架构有效解决了传统方法在准确性和可扩展性方面的不足，能够为5G/6G网络中的异构性能要求提供更全面的性能预测能力。

Abstract: The rise of 5G/6G network technologies promises to enable applications like autonomous vehicles and virtual reality, resulting in a significant increase in connected devices and necessarily complicating network management. Even worse, these applications often have strict, yet heterogeneous, performance requirements across metrics like latency and reliability. Much recent work has thus focused on developing the ability to predict network performance. However, traditional methods for network modeling, like discrete event simulators and emulation, often fail to balance accuracy and scalability. Network Digital Twins (NDTs), augmented by machine learning, present a viable solution by creating virtual replicas of physical networks for real- time simulation and analysis. State-of-the-art models, however, fall short of full-fledged NDTs, as they often focus only on a single performance metric or simulated network data. We introduce M3Net, a Multi-Metric Mixture-of-experts (MoE) NDT that uses a graph neural network architecture to estimate multiple performance metrics from an expanded set of network state data in a range of scenarios. We show that M3Net significantly enhances the accuracy of flow delay predictions by reducing the MAPE (Mean Absolute Percentage Error) from 20.06% to 17.39%, while also achieving 66.47% and 78.7% accuracy on jitter and packets dropped for each flow

</details>


### [5] [Link-Sharing Backpressure Routing In Wireless Multi-Hop Networks](https://arxiv.org/abs/2512.09902)
*Zhongyuan Zhao,Yujun Ming,Ananthram Swami,Kevin Chan,Fikadu Dagefu,Santiago Segarra*

Main category: cs.NI

TL;DR: 提出一种最大效用（MaxU）链路共享方法，改进传统背压路由的排他性商品选择问题，缓解最后一包问题并提升网络容量


<details>
  <summary>Details</summary>
Motivation: 传统背压路由采用排他性商品选择机制，导致最后一包问题和带宽利用率不足。虽然最近的最短路径偏置背压路由（SP-BP）改善了启动慢和随机游走问题，但排他性选择机制仍然存在性能瓶颈

Method: 通过重新审视支撑背压路由的Lyapunov漂移理论，发现传统排他性商品选择并非必需，提出最大效用（MaxU）链路共享方法，允许多个商品共享链路资源，而不增加控制消息开销

Result: 数值结果显示，MaxU SP-BP显著缓解了最后一包问题，并略微扩展了网络容量区域

Conclusion: MaxU链路共享方法能够在不增加控制开销的情况下扩展背压路由的性能边界，改善网络性能

Abstract: Backpressure (BP) routing and scheduling is an established resource allocation method for wireless multi-hop networks, noted for its fully distributed operation and maximum queue stability. Recent advances in shortest path-biased BP routing (SP-BP) mitigate shortcomings such as slow startup and random walks, yet exclusive link-level commodity selection still causes last-packet problem and bandwidth underutilization. By revisiting the Lyapunov drift theory underlying BP, we show that the legacy exclusive commodity selection is unnecessary, and propose a Maximum Utility (MaxU) link-sharing method to expand its performance envelope without increasing control message overhead. Numerical results show that MaxU SP-BP substantially mitigates the last-packet problem and slightly expands the network capacity region.

</details>


### [6] [Towards Practical and Usable In-network Classification](https://arxiv.org/abs/2512.09809)
*Di Zhu,Jianxi Chen,Hyojoon Kim*

Main category: cs.NI

TL;DR: ACORN是一个端到端系统，用于在网络硬件上自动部署机器学习模型，支持决策树、随机森林和SVM等模型，比现有方案支持2-4倍更多特征。


<details>
  <summary>Details</summary>
Motivation: 现有网络内机器学习方案受限于硬件约束、资源稀缺和可用性差，难以被ML开发者和云运营商实际采用。

Method: ACORN提供全自动流水线，使用ILP规划器生成优化部署方案，采用新颖的数据平面表示支持决策树、随机森林和SVM模型，在P4中实现原型并在可编程硬件上运行。

Result: ACORN能够部署比现有方案多2-4倍特征的分类ML模型，同时对网络性能和流量影响可忽略。

Conclusion: ACORN解决了网络内机器学习部署的实用性问题，使ML模型能够在网络硬件上高效运行，相关工具将开源。

Abstract: In-network machine learning enables real-time classification directly on network hardware, offering consistently low inference latency. However, current solutions are limited by strict hardware constraints, scarce on-device resources, and poor usability, making them impractical for ML developers and cloud operators. To this end, we propose ACORN, an end-to-end system that automates the distributed deployment of practical machine learning models across the network. ACORN provides a fully automated pipeline that loads and deploys Python ML models on network devices using an optimized deployment plan from an ILP planner. To support larger models under hardware constraints and allow runtime programmability, ACORN adopts a novel data plane representation for Decision Tree, Random Forest, and Support Vector Machine models. We implement ACORN prototype in P4 and run it on real programmable hardware. Our evaluation shows ACORN can deploy classification ML models with 2-4x more features than state-of-the-art solutions, while imposing negligible overhead on network performance and traffic. We will make our data plane program, model translator, optimizer, and all related scripts publicly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 研究大语言模型幻觉如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，并识别直觉作为新的信任因素。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型产生的幻觉（事实错误但看似合理）如何影响用户对LLM的信任以及用户与LLM的互动，探索日常使用中的信任动态。

Method: 采用定性研究方法，对192名参与者进行研究，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架进行分析。

Result: 幻觉不会导致全面不信任，而是引发情境敏感的信任校准；确认期望、先前经验、用户专业知识为信任因素，并识别直觉作为新的信任因素；信任动态还受感知风险和决策风险等情境因素影响。

Conclusion: 验证了Blöbaum提出的递归信任校准过程，并扩展了直觉作为用户相关信任因素；基于研究发现提出了负责任和反思性LLM使用的实践建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [8] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决AI治理三大挑战：用例风险评估不足、现有框架缺乏可操作控制、规模化治理机制缺失


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键问题：1) 缺乏针对具体用例的风险评估，导致类似Humana集体诉讼的严重后果；2) ISO 42001和NIST AI RMF等框架停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化实施治理的机制，无法将可信AI实践嵌入开发生命周期

Method: 提出AI TIPS 2.0（人工智能可信集成支柱可持续发展2.0），这是对2019年开发的全面操作框架的更新，比NIST AI风险管理框架早四年，直接针对上述挑战提供解决方案

Result: AI TIPS 2.0框架能够直接解决当前AI治理的三大挑战，提供可操作的治理方案

Conclusion: 需要像AI TIPS 2.0这样的操作框架来解决当前AI治理框架的不足，确保AI系统部署的可信性和可持续性

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [9] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出形式化范畴框架分析人类和LLM如何将内容转化为关于可能世界W的真值命题，论证LLM并非解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 研究人类和大型语言模型(LLM)如何将内容转化为关于可能世界状态空间的真值命题，探讨LLM是否真正解决了符号接地问题

Method: 使用形式化、范畴论的框架来分析内容到真值命题的转换过程，比较人类和LLM在该过程中的差异

Result: LLM实际上绕过了符号接地问题，而不是真正解决了该问题

Conclusion: 论文通过形式化分析表明，LLM在生成真值命题时采用了与人类不同的机制，从而避免了符号接地问题的核心挑战

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [10] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源的Python工具包，集成了对话生成、评估和机制可解释性，用于构建和分析基于LLM的对话系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的框架来系统性地构建、评估和理解基于LLM的对话系统。研究人员需要能够同时处理对话生成、评估和机制分析的工具。

Method: 围绕标准化的Dialog表示构建，提供：1）基于角色的多智能体模拟和可组合编排；2）结合语言指标、LLM作为评判和功能正确性验证的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整音频生成。

Result: SDialog集成了所有主要的LLM后端，支持统一API下的混合后端实验，为研究人员提供了系统构建、基准测试和理解对话系统的工具。

Conclusion: 通过将生成、评估和可解释性耦合在对话中心架构中，SDialog使研究人员能够更系统地构建、基准测试和理解对话系统。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [11] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 该论文对比人类与AI系统在模糊视觉刺激下的图像标注表现，分析两者在表征、推理和置信度校准方面的异同，为未来神经符号架构提供认知基础。


<details>
  <summary>Details</summary>
Motivation: 理解人类与AI系统如何解释模糊视觉刺激，对于揭示感知、推理和决策的本质至关重要。通过对比两者的处理方式，可以推动构建更可解释、认知对齐的AI系统。

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类参与者和深度神经网络在低分辨率、感知退化刺激下的表现。使用Grad-CAM可视化模型注意力，并通过ACT-R和Soar等认知架构解释人类行为。

Result: 研究发现人类与AI系统在表征、推理和置信度校准方面存在关键相似点和差异。人类采用类比推理、形状识别和置信度调节等策略，而AI主要依赖特征处理。在不确定性下，人类表现出分层和启发式决策策略。

Conclusion: 分析结果支持开发融合结构化符号推理与连接主义表征的神经符号架构。这种受具身性、可解释性和认知对齐原则指导的架构，有望实现既高性能又可解释、认知基础的AI系统。

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [12] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文认为AI代理和生成式AI的可靠性主要是一种架构属性，通过组件化、接口规范和显式控制循环来实现可靠性保证。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统变得越来越复杂和自主，确保其可靠性成为关键挑战。传统软件工程方法不足以应对AI系统的动态性和不确定性，需要新的架构方法来保证AI代理在现实世界中的可靠运行。

Method: 提出基于架构的可靠性方法：1) 定义代理系统为闭环的目标导向工具使用者；2) 通过组件化（目标管理器、规划器、工具路由器、执行器、内存、验证器、安全监控器、遥测）实现可靠性；3) 建立规范的接口（模式约束、验证、最小权限工具调用）；4) 引入显式的控制和保证循环。

Result: 建立了实用的分类法：工具使用代理、记忆增强代理、规划与自我改进代理、多代理系统、具身或网络代理。分析了每种模式如何重塑可靠性范围和故障模式，并提炼了设计指导原则。

Conclusion: AI代理的可靠性本质上是一个架构问题，通过系统化的组件设计、接口规范和运行时治理机制可以实现可靠的AI系统。提出的架构方法和设计原则为构建可靠的AI代理系统提供了实用框架。

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [13] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个结合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效、可解释的闭环靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法耗时昂贵且成功率低，现有生成模型存在泛化能力不足、可解释性差、过度关注结合亲和力而忽视其他关键药理学性质等问题，限制了其实际应用价值。

Method: Trio框架整合三个关键组件：1）基于片段的分子语言建模实现上下文感知的片段组装；2）强化学习确保物理化学和合成可行性；3）蒙特卡洛树搜索平衡新颖化学型探索和蛋白质结合口袋中有前景中间体的利用。

Result: 实验结果显示Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时将分子多样性扩展四倍以上。

Conclusion: Trio提供了一个有效且可解释的闭环靶向分子设计框架，通过整合多种技术解决了现有生成模型的局限性，在药物发现中展现出优越的性能和实用性。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [14] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 提出一个由验证器支持的端到端规划框架，使用LLM将自然语言规范转换为PDDL模型，自动解决规划需求、歧义和矛盾，生成可读的自然语言计划。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在规划任务中的局限性，特别是处理复杂规划问题（如Blocksworld、汉诺塔）时的困难，实现无需人工干预的端到端自动规划系统。

Method: 使用编排器接收自然语言规范，通过LLM驱动的子模块迭代精炼PDDL域和问题，解决时间约束、最优性、歧义和矛盾，然后传递给外部规划引擎生成计划，最后将计划翻译回自然语言。

Result: 框架在多个领域和任务中表现出灵活性和有效性，包括Google NaturalPlan基准、PlanBench以及Blocksworld和汉诺塔等规划问题，可与各种PDDL规划引擎集成。

Conclusion: 该框架代表了LLM辅助端到端规划的重要进展，能够处理LLM难以解决的复杂规划问题，且完全自动化无需人工干预。

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [15] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 提出一种使用高斯过程回归聚合MCTS多线程统计信息的方法，在连续动作空间中优于现有策略


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，如何有效聚合MCTS多线程统计信息是一个重要但未被充分探索的问题

Method: 使用高斯过程回归为未在环境中试验的有前景动作获取价值估计

Result: 在6个不同领域进行系统评估，证明该方法优于现有聚合策略，推理时间增加有限

Conclusion: 高斯过程回归是聚合MCTS多线程统计信息的有效方法，在连续动作空间中表现优异

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [16] [Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation](https://arxiv.org/abs/2512.09736)
*Jingtian Yan,Zhifei Li,William Kang,Stephen F. Smith,Jiaoyang Li*

Main category: cs.AI

TL;DR: 该研究系统分析了多智能体路径规划(MAPF)算法在实际部署中的关键设计因素，包括解最优性与执行性能的关系、运动学建模误差的敏感性、以及模型精度与规划最优性的相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF评估框架通常基于简化的机器人模型，导致算法基准测试与实际性能之间存在显著差距。虽然SMART等新框架引入了运动学建模，但缺乏对关键设计选择如何影响实际执行性能的系统研究。

Method: 基于SMART框架的运动学建模能力，系统研究三个基本因素：1)解最优性与执行性能的关系；2)系统性能对运动学建模误差的敏感性；3)模型精度与规划最优性的相互作用。通过实证分析这些因素在实际场景中的影响。

Result: 通过实证研究揭示了关键设计选择对实际性能的具体影响，为MAPF算法在实际部署中的优化提供了指导。

Conclusion: 研究指出了MAPF领域面临的实际挑战和研究方向，推动社区向实用、现实世界部署方向发展，强调了在实际约束下平衡算法最优性与执行鲁棒性的重要性。

Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.

</details>


### [17] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT使用强化学习自动发现最小化高影响故障场景，加速AI加速器故障评估，相比进化方法提速2.2倍，相比随机故障注入减少99%测试向量，同时提供更好的故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法面临计算成本过高、关键故障模式覆盖率差的问题，需要更高效的故障评估框架。

Method: 将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化高影响测试套件。

Result: 在十亿参数大语言模型工作负载上评估，使用NVIDIA A100 GPU，相比进化方法获得2.2倍加速，相比随机故障注入减少99%测试向量，同时实现更好的故障覆盖率。RIFT指导的选择性错误校正码比均匀三模冗余保护成本效益提高12.8倍。

Conclusion: RIFT提供了一个可扩展的框架，能够自动发现最小化高影响故障场景，显著提高故障评估效率，同时生成可直接集成到商业RTL验证流程的UVM兼容验证工件。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [18] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架，用于建模认知异质智能体之间的信念、动机和影响。通过个性化价值空间表示智能体，信念作为结构化向量传播，其可理解性取决于线性解释映射的零空间条件。


<details>
  <summary>Details</summary>
Motivation: 传统信念传播模型通常假设智能体共享相同的认知结构或理性标准，但现实中智能体具有不同的认知框架和价值维度。需要一种能够处理认知异质性的理论框架，解释信念如何在具有不同认知几何的智能体之间传播、变形或消失。

Method: 1. 将每个智能体表示为个性化价值空间（向量空间），编码其内部解释和评估意义的维度
2. 将信念形式化为结构化向量（抽象存在）
3. 信念传播通过线性解释映射实现
4. 引入"无零空间领导条件"作为领导力的代数特征
5. 使用纯代数约束解释信念扭曲、动机漂移和相互理解的限制

Result: 1. 信念只有在避免解释映射的零空间时才能在传播中存活
2. 领导力被表征为表征可达性而非说服或权威
3. 解释了抽象存在如何在多样认知几何中传播、突变或消失
4. 提供了信念扭曲、动机漂移和反事实评估的结构化解释

Conclusion: 该认知几何视角将概念空间、社会认识论和AI价值对齐的见解统一起来，将意义保存建立在结构兼容性而非共享信息或理性的基础上。这一框架阐明了人类和人工系统中影响的认知边界，为分析异质智能体间的信念动态提供了通用基础。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [19] [Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing](https://arxiv.org/abs/2512.09882)
*Justin W. Lin,Eliot Krzysztof Jones,Donovan Julian Jasper,Ethan Jun-shen Ho,Anna Wu,Arnold Tianyi Yang,Neil Perry,Andy Zou,Matt Fredrikson,J. Zico Kolter,Percy Liang,Dan Boneh,Daniel E. Ho*

Main category: cs.AI

TL;DR: AI安全代理ARTEMIS在真实企业环境中评估，发现9个有效漏洞，击败9/10人类专家，成本仅$18/小时，但存在高误报率和GUI任务处理困难


<details>
  <summary>Details</summary>
Motivation: 首次在真实企业环境中全面评估AI代理与人类网络安全专家的性能对比，了解AI在网络安全攻防中的实际能力和局限性

Method: 在包含约8000台主机的大学网络中，评估10名网络安全专家、6个现有AI代理和新的多代理框架ARTEMIS。ARTEMIS具有动态提示生成、任意子代理和自动漏洞分类功能

Result: ARTEMIS总体排名第二，发现9个有效漏洞，有效提交率82%，击败了10名人类参与者中的9名。现有框架如Codex和CyAgent表现不如大多数人类参与者。AI代理在系统枚举、并行利用和成本方面有优势（$18/小时 vs $60/小时），但存在高误报率和GUI任务处理困难

Conclusion: AI安全代理在真实环境中展现出与顶尖人类专家相当的技术能力和提交质量，具有成本效益和并行处理优势，但仍需解决高误报率和GUI交互等关键能力差距

Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.

</details>


### [20] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和众包的人类参与循环，支持材料科学领域的元数据词汇开发，成功生成19个AI定义，验证了AI-HILT模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇对FAIR和FARR数据原则至关重要，但开发受到人力资源有限和标准化实践不一致的限制。

Method: 开发MatSci-YAMZ平台，整合AI和人类参与循环（包括众包），在材料科学领域进行概念验证，6名参与者提供术语定义和示例来促进AI定义精炼。

Result: 成功生成19个AI生成的定义，迭代反馈循环证明了AI-HILT精炼的可行性，确认了概念验证成功、与FAIR原则一致、建立了研究协议并展示了跨领域扩展潜力。

Conclusion: MatSci-YAMZ模型能够增强语义透明度，减少共识构建和元数据词汇开发所需时间，具有跨领域扩展的潜力。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [21] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅用于初始化预训练轻量级学生模型，相比依赖持续LLM查询的方法，显著提高了效率（推理时间从164.4秒降至3.0秒），在TextCraft环境中达到0.56成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在两个主要问题：1) 训练和推理期间需要频繁查询LLM，计算成本高且部署困难；2) 使用预训练且参数固定的LLM，无法针对目标任务进行适应。需要更高效的规划方法。

Method: SCOPE是一种一次性分层规划器，仅在初始化时使用LLM生成的子目标来预训练轻量级学生模型。与之前方法不同，它直接从示例轨迹中推导子目标，避免了训练期间重复查询LLM的需求，大大提高了效率。

Result: 在TextCraft环境中，SCOPE达到0.56的成功率，优于LLM分层代理ADaPT的0.52成功率。更重要的是，推理时间从164.4秒大幅减少到仅3.0秒，效率显著提升。

Conclusion: 尽管LLM生成的子目标可能不是最优的，但仍能为文本规划任务中的分层目标分解提供良好的起点。SCOPE展示了在保持性能的同时显著提高效率的可行性，为实际部署提供了更实用的解决方案。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


### [22] [Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective](https://arxiv.org/abs/2512.09908)
*Antonio Lorenzin,Fabio Zanasi*

Main category: cs.AI

TL;DR: 本文提出一个范畴论框架，将贝叶斯网络和马尔可夫网络之间的道德化与三角化转换建模为函子，并通过函子前复合在语法层面定义这些操作。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型理论中，道德化和三角化是两种重要的图转换方法，但缺乏统一的数学框架来描述这些操作及其语法/语义区别。本文旨在建立范畴论视角，为这些转换提供形式化基础。

Method: 1. 定义贝叶斯网络和马尔可夫网络的范畴，其中对象是网络（表示为从语法域到语义域的函子）。2. 将道德化和三角化建模为这两个范畴之间的函子。3. 通过函子前复合在语法层面归纳定义这些转换。4. 重新解释变量消除算法为函子，将三角化过程分解为纯语法和纯语义两部分。

Result: 成功建立了概率图模型的函子视角，将道德化（纯语法操作）和三角化（依赖语义的操作）统一在范畴论框架中。变量消除算法被重构为函子，清晰分离了语法和语义层面的操作。

Conclusion: 范畴论为概率图模型提供了有力的形式化工具，能够清晰区分语法和语义层面的修改。这种框架不仅统一了现有转换方法，还为未来图模型理论的发展提供了新的数学基础。

Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [SURA: Secure Unsourced Random Access](https://arxiv.org/abs/2512.09104)
*Mohammad Javad Ahmadi,Rafael F. Schaefer,H. Vincent Poor*

Main category: cs.IT

TL;DR: 提出一种基于物理层技术的无源随机接入安全方案，利用反馈信号生成密钥和人工噪声，在不改变原系统结构的前提下实现保密通信。


<details>
  <summary>Details</summary>
Motivation: 无源随机接入（URA）系统通常缺乏安全机制，需要在不增加开销、不改变原有结构和操作特性的情况下，为URA提供保密通信能力。

Method: 利用基站广播的反馈信号（依赖BS-用户信道）为每个用户生成私密密钥和人工噪声序列。通过三个步骤实现安全传输：1) 用密钥加密数据；2) 仅发送LDPC编码密钥的校验位；3) 用人工噪声掩盖校验位。设计了合法接收者的接收算法，并分析了窃听者的信息泄露。

Result: 仿真结果表明，该方案在不修改URA结构的情况下实现了有意义的保密性，对标准性能影响可忽略不计。

Conclusion: 成功将物理层安全技术应用于无源随机接入系统，在保持URA低成本、低延迟、最小信令开销优势的同时，提供了有效的保密通信。

Abstract: This work introduces security for unsourced random access (URA) by employing wiretap-inspired physical layer techniques. To achieve confidentiality, the proposed system opportunistically exploits intrinsic features of feedback-aided URA without adding any overhead or altering its original structure or operational characteristics. As a result, the proposed system preserves the low-cost advantages of URA, including low delay and minimal signaling overhead, while providing secure communication. To secure transmission, each user generates a secret key and an artificial noise sequence from the feedback signal that the BS broadcasts in previous transmission rounds. This feedback depends on the BS-user channel, making it a private signal for each user. The secure transmission is performed by three actions: encrypting the data using the secret key, sending only the parity bits of the LDPC encoded secret key to allow the legitimate receiver to recover it, and masking these parity bits with the artificial noise. For reception, a receiver algorithm is designed for the legitimate user, and a leakage analysis is provided to quantify the information available to the eavesdropper. The simulation results show that meaningful secrecy is achieved in URA without modifying its structure and with negligible impact on standard performance.

</details>


### [24] [$t$-Fold $s$-Blocking Sets and $s$-Minimal Codes](https://arxiv.org/abs/2512.09457)
*Hao Chen,Xu Pan,Conghui Xie*

Main category: cs.IT

TL;DR: 本文研究了阻塞集与极小码的新下界，推广了Ashikhmin-Barg条件，并构造了多个满足/违反该条件的s-极小码无限族


<details>
  <summary>Details</summary>
Motivation: 研究阻塞集与极小码之间的关系，改进经典下界，探索s-极小码的性质与构造

Method: 1. 建立无t≤q条件的t-fold s-阻塞集新下界；2. 推广Ashikhmin-Barg条件到s-极小码；3. 构造满足/违反该条件的无限族；4. 证明(s+1)-极小码必然是s-极小码

Result: 1. 获得了比Beutelspacher(1983)更强的阻塞集下界；2. 得到了射影s-极小码长度的下界；3. 构造了多个满足和违反广义Ashikhmin-Barg条件的s-极小码无限族；4. 给出了二进制极小码但不是2-极小码的例子

Conclusion: 本文在阻塞集和极小码理论中取得了多项进展，包括改进下界、推广经典条件、构造新码族，为相关领域提供了新的理论工具和实例

Abstract: Blocking sets and minimal codes have been studied for many years in projective geometry and coding theory. In this paper, we provide a new lower bound on the size of $t$-fold $s$-blocking sets without the condition $t \leq q$, which is stronger than the classical result of Beutelspacher in 1983. Then a lower bound on lengths of projective $s$-minimal codes is also obtained. It is proved that $(s+1)$-minimal codes are certainly $s$-minimal codes. We generalize the Ashikhmin-Barg condition for minimal codes to $s$-minimal codes. Many infinite families of $s$-minimal codes satisfying and violating this generalized Ashikhmin-Barg condition are constructed. We also give several examples which are binary minimal codes, but not $2$-minimal codes.

</details>


### [25] [Binary and Non-Binary Self-Dual Sequences and Maximum Period Single-Track Gray Codes](https://arxiv.org/abs/2512.09655)
*Tuvi Etzion*

Main category: cs.IT

TL;DR: 论文研究了二进制和非二进制自对偶序列的结构与递归构造，建立了与最大周期单轨格雷码的联系，并构造了周期为p^{p^t}的最大周期非二进制单轨格雷码


<details>
  <summary>Details</summary>
Motivation: 受单轨格雷码构造的启发，研究二进制和非二进制自对偶序列的结构与递归构造，探讨生成这些序列的反馈移位寄存器，建立这些序列与最大周期单轨码之间的联系

Method: 分析自对偶序列的结构特性，提出递归构造方法，讨论生成这些序列的反馈移位寄存器，建立与单轨格雷码的理论联系，构造长度为p^t、周期为p^{p^t}的最大周期非二进制单轨格雷码

Result: 提出了自对偶序列的递归构造方法，建立了自对偶序列与单轨格雷码的理论联系，构造了第一个无限族的最大周期非二进制单轨格雷码，这些码的长度为p^t，周期为p^{p^t}

Conclusion: 论文成功建立了自对偶序列与单轨格雷码之间的理论联系，提出了有效的递归构造方法，并构造了首个无限族的最大周期非二进制单轨格雷码，为相关领域的研究提供了新的理论框架和构造方法

Abstract: Binary self-dual sequences have been considered and analyzed throughout the years, and they were used for various applications. Motivated by a construction for single-track Gray codes, we examine the structure and recursive constructions for binary and non-binary self-dual sequences. The feedback shift registers that generate such sequences are discussed. The connections between these sequences and maximum period single-track codes are discussed. Maximum period non-binary single-track Gray codes of length $p^t$ and period $p^{p^t}$ are constructed. These are the first infinite families of maximum period codes presented in the literature.

</details>


### [26] [Typical Solutions of Multi-User Linearly-Decomposable Distributed Computing](https://arxiv.org/abs/2512.09858)
*Ali Khalesi,Mohammad Reza Deylam Salehi*

Main category: cs.IT

TL;DR: 本文解决了典型意义下的多发送者线性可分解分布式计算问题，分析了真实值编码器/解码器和需求矩阵，通过阈值图编辑距离评估结构保真度，获得了闭式二阶矩风险、GED与范数误差的确定性联系、高斯替代模型等结果。


<details>
  <summary>Details</summary>
Motivation: 解决tessellated分布式计算中提出的多发送者线性可分解分布式计算问题，在典型情况下分析真实值编码器/解码器和需求矩阵的性能，通过图编辑距离评估计算产品与需求支持之间的结构保真度。

Method: 采用真实值编码器/解码器和需求矩阵建模，使用阈值图编辑距离(GED)评估需求支持与计算产品两跳支持之间的结构差异，分析spike-and-slab集合下的二阶矩风险，建立GED与范数误差的确定性联系，提出高斯替代模型，并设计计算上限的系统。

Result: 获得了spike-and-slab集合下的闭式二阶矩(Frobenius)风险；建立了阈值GED与范数误差的确定性联系；提出了具有次指数尾的高斯替代模型，揭示了显式召回线；证明了GED的集中性和算子范数控制；设计了具有可见拐点的计算上限系统；并将规则映射到航空和卫星网络。

Conclusion: 本文成功解决了典型意义下的多发送者线性可分解分布式计算问题，建立了结构保真度与数值误差之间的理论联系，提出了实用的系统设计方法，并将结果应用于实际网络场景，为分布式计算系统的性能评估和设计提供了理论框架。

Abstract: We solve, in the typical-case sense, the multi-sender linearly-decomposable distributed computing problem introduced by tessellated distributed computing. We model real-valued encoders/decoders and demand matrices, and assess structural fidelity via a thresholded graph edit distance between the demand support and the two-hop support of the computed product. Our analysis yields: a closed-form second-moment (Frobenius) risk under spike-and-slab ensembles; deterministic links between thresholded GED and norm error; a Gaussian surrogate with sub-exponential tails that exposes explicit recall lines; concentration of GED and operator-norm control; and a compute-capped design with a visible knee. We map the rules to aeronautical and satellite networks.

</details>
