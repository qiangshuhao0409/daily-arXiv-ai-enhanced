<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 23]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling](https://arxiv.org/abs/2508.09620)
*Michel Rottleuthner,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 通过动态电压频率调整（DVFS）在低功耗无线节点中显著降低能耗，实验显示能耗节省24%-52%。


<details>
  <summary>Details</summary>
Motivation: 解决受限物联网设备因硬件不平衡导致的能耗问题，探索DVFS在优化能耗中的潜力。

Method: 将DVFS集成到RIOT物联网操作系统中，分析其对不同网络任务（如CSMA/CA和时间分槽）及加密通信的影响。

Result: 实验显示MAC操作能耗节省24%-52%，加密CoAP通信能耗降低37%。

Conclusion: DVFS可显著提升物联网设备的能效，延长电池寿命，建议未来研究将其集成到系统设计中。

Abstract: Minimizing energy consumption of low-power wireless nodes is a persistent
challenge from the constrained Internet of Things (IoT). In this paper, we
start from the observation that constrained IoT devices have largely different
hardware (im-)balances than full-scale machines. We find that the performance
gap between MCU and network throughput on constrained devices enables minimal
energy delay product (EDP) for IoT networking at largely reduced clock
frequencies. We analyze the potentials by integrating dynamic voltage and
frequency scaling (DVFS) into the RIOT IoT operating system and show that the
DVFS reconfiguration overhead stays below the energy saved for a single,
downscaled MAC operation. Backed by these findings, we systematically
investigate how DVFS further improves energy-efficiency for common networking
tasks -- in addition to duty-cycling. We measure IoT communication scenarios
between real-world systems and analyze two MAC operating modes -- CSMA/CA and
time slotting -- in combination with different CoAP transactions, payload
sizes, as well as DTLS transport encryption. Our experiments reveal energy
savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP
communication. These results shall encourage research and system design work to
integrate DVFS in future IoT devices for performing tasks at their optimal
frequencies and thereby significantly extending battery lifetimes.

</details>


### [2] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: 本文提出WAAN框架，通过轻量级TinyML代理实现意图感知的主动切换，提升6G网络在移动边缘计算中的用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统反应式切换机制在6G网络中的移动边缘计算和自主代理服务场景中存在局限性，需要更智能的解决方案。

Method: WAAN框架嵌入轻量级TinyML代理作为自主协商实体，利用半稳定会合点实现上下文转移和状态保持。

Result: 通过多模态环境控制案例研究，验证了WAAN在移动性下维持用户体验的有效性。

Conclusion: WAAN为6G网络中的意图感知切换提供了可行方案，并探讨了其部署和演进的挑战与机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [3] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: 论文提出了一种语义感知的主动LLM编排框架（SP-LLM），用于优化动态车辆边缘计算环境中的任务卸载和资源分配。


<details>
  <summary>Details</summary>
Motivation: 当前车辆边缘计算管理系统固定且反应式，无法适应动态环境，需改进以实现更智能、自主的网络行为。

Method: 将传统数字孪生（DT）升级为预测性数字孪生（pDT），结合大型语言模型（LLM）作为认知编排器，利用预测数据主动决策。

Result: SP-LLM在可扩展性、鲁棒性和适应性上显著优于现有反应式和MARL方法。

Conclusion: SP-LLM框架能将人类意图转化为最优网络行为，推动更智能、自主的车辆网络发展。

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [4] [Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs](https://arxiv.org/abs/2508.09150)
*Pietro Piscione,Leonardo Lossi,Maziar Nekovee,Chathura Galkandage,Phil O Connor,Simon Davies*

Main category: cs.NI

TL;DR: 论文展示了一个概念验证（PoC），将5G高级网络功能与CAPIF集成，以支持汽车应用的增强连接性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过动态调整QoS和边缘流量重定向，优化5G网络性能，满足汽车应用的低延迟和高效率需求。

Method: 利用3GPP NEF API通过CAPIF实现网络性能监控和QoS动态调整，并将流量重定向至边缘。

Result: PoC成功展示了网络性能监控、QoS动态调整和边缘流量重定向，提升了连接性和资源利用率。

Conclusion: 该研究验证了5G与CAPIF集成的可行性，为汽车应用的网络优化提供了有效解决方案。

Abstract: This paper presents the design and implementation of a Proof of Concept (PoC)
that demonstrates how 5G Advanced Network Functions can be integrated with the
Common API Framework (CAPIF) to support enhanced connectivity for automotive
applications. The PoC shows the continuous monitoring of the mobile network
performance and the on-demand and dynamic adaptation of Quality of Service
(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using
standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,
traffic flows are redirected to the edge to improve latency and optimize
network resource utilization.

</details>


### [5] [Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission](https://arxiv.org/abs/2508.09151)
*Chang Wu,Yuang Chen,Yiyuan Chen,Fengqian Guo,Xiaowei Qin,Hancheng Lu*

Main category: cs.NI

TL;DR: 论文提出了一种基于生理信号的VR流媒体QoE建模与优化框架，通过EEG、ECG和皮肤活动信号动态调整分辨率，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有QoE模型未能充分解决VR流媒体中分辨率突变对用户体验的影响，需更精准的生理信号驱动方法。

Method: 结合EEG、ECG和皮肤活动信号，利用深度强化学习动态分配无线资源，优化分辨率调整策略。

Result: 实验显示，该方法在分辨率和切换频率上分别提升了88.7%和减少了81.0%。

Conclusion: 生理信号驱动的优化框架有效提升了VR流媒体的QoE，展示了边缘AI在沉浸式媒体中的潜力。

Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly
impair the quality-of-experience (QoE) of users, particularly during
transitions from high to low resolutions. Existing QoE models and transmission
schemes inadequately address the perceptual impact of these shifts. To bridge
this gap, this article proposes, for the first time, an innovative
physiological signal-driven QoE modeling and optimization framework that fully
leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin
activity signals. This framework precisely captures the temporal dynamics of
physiological responses and resolution changes in VR streaming, enabling
accurate quantification of resolution upgrades' benefits and downgrades'
impacts. Integrated the proposed QoE framework into the radio access network
(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission
strategies have been implemented to allocate radio resources dynamically, which
mitigates short-term channel fluctuations and adjusts frame resolution in
response to channel variations caused by user mobility. By prioritizing
long-term resolution while minimizing abrupt transitions, the proposed solution
achieves an 88.7\% improvement in resolution and an 81.0\% reduction in
handover over the baseline. Experimental results demonstrate the effectiveness
of this physiological signal-driven strategy, underscoring the promise of edge
AI in immersive media services.

</details>


### [6] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 本文提出了一种基于AI/ML的故障分析引擎，用于自动分类5G核心网中的PCAP文件故障，显著提升效率并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 5G网络中，确保数据包核心流量的完整性和性能至关重要，但现有方法需要大量人工时间分析测试结果和故障。

Method: 采用自然语言处理技术和生成式AI（基于大型语言模型）分析网络流量，识别异常并提供修复建议。

Result: 测试显示，ML模型在训练数据集上分类准确率高（80-20分割）。

Conclusion: 该引擎显著提升了故障分析效率，未来可扩展至4G网络和其他数据类型。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [7] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran SRB是一个基于AI代理的市场平台，通过三个自治分支（立法、行政、司法）协调多方利益，显著提升5G网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决下一代移动网络中多方服务所有者目标冲突的问题，当前网络切片控制器缺乏灵活性和业务感知能力。

Method: Agoran SRB采用多目标优化器生成Pareto最优方案，通过AI代理协商达成共识，并部署到Open和AI RAN控制器。

Result: 在5G测试中，eMBB切片吞吐量提升37%，URLLC切片延迟降低73%，PRB使用节省8.3%。

Conclusion: Agoran为6G网络提供了一种灵活、以利益相关者为中心的标准路径。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [8] [WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking](https://arxiv.org/abs/2508.09166)
*Wei Guo,Shunsei Yamagishi,Lei Jing*

Main category: cs.NI

TL;DR: WPTrack提出了一种结合Wi-Fi和压力鞋垫数据的单目标跟踪系统，解决了现有Wi-Fi跟踪中初始位置获取和盲点问题。


<details>
  <summary>Details</summary>
Motivation: 室内定位对智能家居和老年护理至关重要，但现有Wi-Fi跟踪方法需要多设备支持或存在初始位置获取困难。

Method: 通过单Wi-Fi链路的CSI数据和90个鞋垫传感器的压力数据，结合相位差、多普勒速度和步行速度，提出CSI-压力融合模型。

Result: 初始定位精度为0.02 cm至42.55 cm，实验数据中的轨迹与实际轨迹高度吻合。

Conclusion: WPTrack通过融合Wi-Fi和压力数据，实现了高精度的单目标跟踪。

Abstract: As the Internet of Things (IoT) continues to evolve, indoor location has
become a critical element for enabling smart homes, behavioral monitoring, and
elderly care. Existing WiFi-based human tracking solutions typically require
specialized equipment or multiple Wi-Fi links, a limitation in most indoor
settings where only a single pair of Wi-Fi devices is usually available.
However, despite efforts to implement human tracking using one Wi-Fi link,
significant challenges remain, such as difficulties in acquiring initial
positions and blind spots in DFS estimation of tangent direction. To address
these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure
Insoles Fusion System for Single Target Tracking. WPTrack collects Channel
State Information (CSI) from a single Wi-Fi link and pressure data from 90
insole sensors. The phase difference and Doppler velocity are computed from the
CSI, while the pressure sensor data is used to calculate walking velocity.
Then, we propose the CSI-pressure fusion model, integrating CSI and pressure
data to accurately determine initial positions and facilitate precise human
tracking. The simulation results show that the initial position localization
accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results
obtained from experimental data collected in a real-world environment closely
align with the actual trajectory.

</details>


### [9] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: webMCP是一种客户端标准，通过在网页中嵌入结构化交互元数据，显著降低AI代理处理网页的计算开销，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理处理网页需要大量计算，导致交互缓慢且昂贵，webMCP旨在解决这一问题。

Method: webMCP直接在网页中嵌入结构化元数据，提供页面元素与用户操作的显式映射，避免处理完整HTML文档。

Result: 评估显示webMCP减少67.6%处理需求，任务成功率97.9%，成本降低34-63%，响应时间更快。

Conclusion: webMCP无需服务器端修改，可部署于现有网站，显著提升AI辅助网页交互的效率和可持续性。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [10] [Camel: Energy-Aware LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2508.09173)
*Hao Xu,Long Peng,Shezheng Song,Xiaodong Liu,Ma Jun,Shasha Li,Jie Yu,Xiaoguang Mao*

Main category: cs.NI

TL;DR: 论文提出了一种LLM推理能量管理框架，通过优化GPU频率和批量大小来平衡延迟和能耗，实验显示其能显著降低EDP。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要部署在云端，面临网络延迟、隐私和带宽问题，边缘设备部署成为研究重点，但需平衡能耗与延迟。

Method: 提出框架优化GPU频率和批量大小，解决配置搜索中的探索-利用问题，并在NVIDIA Jetson AGX Orin平台上实现验证。

Result: 相比默认配置，框架将EDP降低12.4%-29.9%，更好地平衡了能耗与延迟。

Conclusion: 该框架有效解决了边缘设备上LLM推理的能耗与延迟平衡问题，具有实际应用价值。

Abstract: Most Large Language Models (LLMs) are currently deployed in the cloud, with
users relying on internet connectivity for access. However, this paradigm faces
challenges such as network latency, privacy concerns, and bandwidth limits.
Thus, deploying LLMs on edge devices has become an important research focus. In
edge inference, request latency is critical as high latency can impair
real-time tasks. At the same time, edge devices usually have limited battery
capacity, making energy consumption another major concern. Balancing energy
consumption and inference latency is essential. To address this, we propose an
LLM inference energy management framework that optimizes GPU frequency and
batch size to balance latency and energy consumption. By effectively managing
the exploration-exploitation dilemma in configuration search, the framework
finds the optimal settings. The framework was implemented on the NVIDIA Jetson
AGX Orin platform, and a series of experimental validations were conducted.
Results demonstrate that, compared to the default configuration, our framework
reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance
between energy consumption and latency.

</details>


### [11] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: HiSTM模型结合双空间编码器和Mamba时序模块，显著提升蜂窝流量预测准确性，同时减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对网络规划和资源分配至关重要，但现有模型在准确性和计算效率之间存在权衡。

Method: HiSTM采用选择性状态空间方法和注意力机制，捕捉时空模式。

Result: 在真实数据集上，HiSTM比STN基线MAE提升29.4%，参数减少94%。

Conclusion: HiSTM在不同数据集和长时间跨度上表现优异，具有良好泛化能力。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [12] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: MX-AI是一个基于LLM的端到端智能系统，用于6G RAN的自主观测与配置，在真实5G测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 未来6G RAN需要AI原生支持，实现自主观测、推理与配置。

Method: 通过OpenAirInterface和FlexRIC构建5G测试平台，部署LLM驱动的智能代理，支持自然语言交互。

Result: 在50个操作查询中，MX-AI平均得分4.1/5.0，决策准确率100%，延迟仅8.8秒。

Conclusion: MX-AI性能媲美人类专家，验证了其在真实场景中的实用性。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [13] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: CoMoE是一个动态资源感知的协作优化框架，用于在移动边缘计算环境中高效部署Mixture-of-Experts模型，显著降低内存占用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在资源受限的移动边缘环境中部署时面临的高内存需求和动态专家激活问题。

Method: 提出CoMoE框架，联合优化专家聚合粒度和卸载策略，结合实时设备资源状态、网络条件和输入特征。

Result: 实验显示，CoMoE减少70%内存使用，降低10.5%推理延迟，并在大规模MoE模型上显著减少内存需求。

Conclusion: CoMoE为资源受限的移动边缘设备提供了高效的MoE模型部署方案。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [14] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 提出了一种基于整数线性规划（ILP）的模型放置算法，用于优化预训练MoE LLM在多服务器集群中的部署，以减少网络流量。


<details>
  <summary>Details</summary>
Motivation: MoE LLM推理时仅激活部分专家，且负载不均衡，需考虑网络拓扑以提高集群利用率。

Method: 使用整数线性规划（ILP）确定专家的最优放置，最小化预期传输次数。

Result: ILP策略在小规模（DeepSeekMoE~16B）和大规模（DeepSeek-R1~671B）模型中均优于其他方法，减少了网络流量。

Conclusion: ILP方法能有效优化MoE LLM的部署，提升集群效率。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [15] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: NEFMind框架通过高效参数微调开源大语言模型，显著降低5G服务架构API的通信开销，提升API调用识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现代电信中服务架构的复杂性和API数量激增，导致服务发现和管理困难。

Method: 结合合成数据集生成、量化低秩适应优化模型，并通过GPT-4 Ref Score和BertScore评估性能。

Result: 通信开销减少85%，API调用识别准确率达98-100%，性能接近GPT-4但计算效率更高。

Conclusion: 验证了高效参数微调策略在电信复杂API生态系统管理中的有效性。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [16] [On-Device Multimodal Federated Learning for Efficient Jamming Detection](https://arxiv.org/abs/2508.09369)
*Ioannis Panitsas,Iason Ofeidis,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了一种基于多模态联邦学习的轻量级框架，用于设备端干扰检测与分类，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有干扰检测方法多为单模态、集中式处理，计算资源需求高，难以扩展和部署。

Method: 采用多模态联邦学习框架，结合频谱图和跨层网络KPI，通过双编码器架构和融合模块实现隐私保护训练与推理。

Result: 检测精度提升15%，通信轮次减少60%，资源占用低，在异构数据分布下表现稳健。

Conclusion: 该框架在干扰检测中表现出高效、隐私保护和强鲁棒性，优于现有单模态方法。

Abstract: Wireless networks face severe vulnerabilities from jamming attacks, which can
significantly disrupt communication. Existing detection approaches are often
unimodal, rely on centralized processing, and demand substantial computational
resources, hindering scalability, efficiency, and deployment feasibility. To
address these challenges, we introduce a multimodal Federated Learning (FL)
framework for on-device jamming detection and classification that integrates
spectrograms with cross-layer network Key Performance Indicators (KPIs) through
a lightweight dual-encoder architecture equipped with a fusion module and a
multimodal projection head. This design enables privacy-preserving training and
inference by ensuring that only model parameters are exchanged, while raw data
remains on the device. The framework is implemented and evaluated on a wireless
experimental testbed using, to the best of our knowledge, the first
over-the-air multimodal dataset with synchronized benign and three distinct
jamming scenarios. Results show that our approach surpasses state-of-the-art
unimodal baselines by up to 15% in detection accuracy, achieves convergence
with 60% fewer communication rounds, and maintains low resource usage. Its
benefits are most evident under heterogeneous data distributions across
devices, where it exhibits strong robustness and reliability.

</details>


### [17] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: 论文提出了一套量化网络负载和流量影响的指标，通过百分位数和样本分布分析网络状态，并引入利用率评分和修正的Shapley值方法，评估了11种指标的实际价值。


<details>
  <summary>Details</summary>
Motivation: 解决在流量波动条件下评估网络性能的挑战，特别是峰值数据速率对网络资源的影响。

Method: 使用百分位数和样本分布分析链路和流量数据，引入利用率评分和修正的Shapley值方法。

Result: 评估了11种指标，其中三种能有效捕捉网络状态变化并提供广泛见解，同时易于维护。

Conclusion: 该方法为未来研究提供了框架，可扩展和优化用于评估流量对网络性能影响的指标。

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [18] [Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment](https://arxiv.org/abs/2508.09582)
*Wafaa B. M. Fadlelmula,Sanaa Hamid Mohamed,Taisir E. H. El-Gorashi,Jaafar M. H. Elmirghani*

Main category: cs.NI

TL;DR: 提出了一种基于无源光网络（PON）的节能回程架构，支持可见光通信（VLC）系统，通过混合整数线性规划（MILP）优化计算资源分配，显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决室内雾计算资源连接的能耗问题，提升能效。

Method: 开发MILP模型优化资源分配，比较PON架构与现有S&L网络和集中式云处理的能效。

Result: 相比S&L网络节能82%，相比集中式云处理能效提升93%。

Conclusion: 提出的架构显著节能，支持动态带宽分配和多节点任务拆分，适用于高需求场景。

Abstract: In this paper, we consider the use of visible light communication (VLC) to
provide connectivity to indoor fog computing resources and propose an
energy-efficient passive optical network (PON)-based backhaul architecture to
support the VLC system. We develop a mixed-integer linear programming (MILP)
model to optimize the allocation of computing resources over the proposed
architecture, aiming to minimize processing and networking power consumption.
We evaluate the performance of the proposed architecture under varying workload
demands and user distributions. Comparative analysis against a backhaul
architecture that is based on the state-of-the-art spine-and-leaf (S&L) network
design demonstrates total power savings of up to 82%. Further comparison with
centralized cloud processing shows improvements in energy efficiency of up to
93%. Additionally, we examine the improvements in energy efficiency obtained by
splitting tasks among multiple processing nodes and propose enhancements to the
architecture including dynamic bandwidth allocation, increased wavelength
bandwidth and improved connectivity within rooms to alleviate networking
bottlenecks. Furthermore, we introduce an inter-building architecture that
leverages resources from neighboring buildings to support high-demand
scenarios.

</details>


### [19] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: ANCHOR是一种无监督异常检测解决方案，用于全球漫游平台的物联网连接服务，旨在通过主动识别潜在问题客户来提高服务可靠性。


<details>
  <summary>Details</summary>
Motivation: 物联网服务在复杂的多实体通信路径中难以保证可用性和可靠性，现有平台通常采用被动响应方式，影响服务质量。

Method: 结合统计规则、机器学习和深度学习模型，基于被动信令流量设计无监督异常检测方案。

Result: ANCHOR能够有效识别潜在问题客户，实现主动问题解决，提升服务质量。

Conclusion: ANCHOR为物联网连接服务提供了一种有效的主动异常检测方法，显著改善了服务可靠性。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>


### [20] [Route Planning and Online Routing for Quantum Key Distribution Networks](https://arxiv.org/abs/2508.09735)
*Jorge López,Charalampos Chatzinakis,Marc Cartigny*

Main category: cs.NI

TL;DR: 论文提出了一种基于二次规划（QP）的模型，用于解决量子密钥分发（QKD）网络中的路由规划和在线路由问题，并证明了最宽最短路径路由策略的竞争比至少为1/2。


<details>
  <summary>Details</summary>
Motivation: QKD网络由于容量限制和信息处理的特殊性，传统最短路径算法表现不佳，且资源稀缺常导致需求无法满足，因此需要新的路由策略。

Method: 将路由问题建模为二次规划问题，并提出最宽最短路径路由策略。

Result: 最宽最短路径路由策略在在线路由中表现优于传统最短路径策略，且竞争比至少为1/2。

Conclusion: 提出的方法有效解决了QKD网络中的路由问题，为资源稀缺环境提供了公平且高效的解决方案。

Abstract: Quantum Key Distribution (QKD) networks harness the principles of quantum
physics in order to securely transmit cryptographic key material, providing
physical guarantees. These networks require traditional management and
operational components, such as routing information through the network
elements. However, due to the limitations on capacity and the particularities
of information handling in these networks, traditional shortest paths
algorithms for routing perform poorly on both route planning and online
routing, which is counterintuitive. Moreover, due to the scarce resources in
such networks, often the expressed demand cannot be met by any assignment of
routes. To address both the route planning problem and the need for fair
automated suggestions in infeasible cases, we propose to model this problem as
a Quadratic Programming (QP) problem. For the online routing problem, we
showcase that the shortest (available) paths routing strategy performs poorly
in the online setting. Furthermore, we prove that the widest shortest path
routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$,
efficiently addressing both routing modes in QKD networks.

</details>


### [21] [The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges](https://arxiv.org/abs/2508.09756)
*Mauro De Sanctis*

Main category: cs.NI

TL;DR: 本文提出了一种基于多种异构无线通信信号的“大规模无线人体感知”范式，旨在通过时间和空间域的信号多样性提升感知能力。


<details>
  <summary>Details</summary>
Motivation: 通过利用无线信号的多样性，提高人体感知的准确性和服务可用性。

Method: 结合设备无关和设备相关的无线感知方法，利用时间、频率和空间域的信号多样性。

Result: 提出了一种支持多技术和多方法射频接收的大规模无线人体感知边缘设备。

Conclusion: 讨论了架构解决方案和挑战，为这一新范式的未来发展提供指导。

Abstract: This article is a position paper which introduces the paradigm of ``Massive
Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing
based on a plethora of heterogeneous wireless communication signals. More
specifically, we aim to exploit signal diversity in the time, frequency, and
space domains using opportunistically both device-free and device-based
wireless sensing approaches, with the objective of enhancing human sensing
capabilities in terms of accuracy and service availability over different
environments. The enabling element of this concept is the massive wireless
human sensing edge device, that is, an embedded system acting as a
multi-technology and multi-approach RF receiver with feature extraction
functionality, located within the monitoring area or at its borders. In this
framework, architecture solutions and challenges are discussed to lead the
future development of this new paradigm.

</details>


### [22] [An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks](https://arxiv.org/abs/2508.09769)
*Simon Egger,Robin Laidig,Heiko Geppert,Lucas Haug,Jona Herrmann,Frank Dürr,Christian Becker*

Main category: cs.NI

TL;DR: 论文提出了一种(m,k)-firm Elevation Policy，用于在不稳定的网络条件下维持弱硬实时保证，增强5G-TSN网络的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前5G和TSN的集成在实时通信应用中存在延迟模型与实际特性不匹配的问题，影响实时保证的鲁棒性。

Method: 通过动态优先级驱动方案，提升m/k连续延迟帧的优先级，补充主时间驱动调度。

Result: 评估表明，该方法能有效维持控制质量，且在主调度提供强QoS时仅引入小开销。

Conclusion: (m,k)-firm Elevation Policy是一种轻量级后备机制，适用于不稳定网络条件下的应用保障。

Abstract: Current standardization efforts are advancing the integration of 5G and
Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical
industrial applications that require real-time communication. However, there
remains a fundamental disconnect between the probabilistic 5G delay
characteristics and the often idealistic delay models used to synthesize 5G-TSN
network configurations. For time-driven schedules in particular, any delay
outlier unforeseen during schedule synthesis can jeopardize the robustness of
their real-time guarantees. To address this challenge, we present the
(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time
guarantees during unstable network conditions that do not match the expected
delay characteristics. It augments the primary time-driven schedule with a
dynamic priority-driven scheme to elevate the priority of m out of k
consecutive frames if they are delayed. Our evaluations demonstrate that weakly
hard real-time guarantees are essential to uphold the quality of control within
a networked control system. At the same time, only a small overhead is imposed
when the primary schedule can provide stronger quality of service guarantees.
Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight
fallback mechanism to serve applications with meaningful guarantees during
unstable network conditions.

</details>


### [23] [A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study](https://arxiv.org/abs/2508.09839)
*Muhammad Asad Ullah,Luca Borgianni,Heikki Kokkinen,Antti Anttonen,Stefano Giordano*

Main category: cs.NI

TL;DR: 本文研究了Starlink在航空领域的性能表现，通过实测数据分析了飞行中的上下行吞吐量和延迟，并探讨了影响性能的因素。


<details>
  <summary>Details</summary>
Motivation: 随着航空公司开始在航班上提供Starlink互联网服务，亟需对其在飞行中的性能进行评估和改进。

Method: 通过在波罗的海和太平洋上空进行飞行实测，收集吞吐量和延迟数据，并分析影响因素。

Result: 实测显示，单个用户设备的中位下行吞吐量为64 Mbps，上行吞吐量为24 Mbps；在飞机下降阶段，上行性能显著下降。RTT受地面站位置和卫星间链路影响较大。

Conclusion: 本文填补了Starlink在航空领域性能研究的空白，为优化其服务提供了数据支持。

Abstract: Starlink delivers Internet services to users across terrestrial, maritime,
and aviation domains. The prior works have studied its performance at fixed
sites and in-motion vehicles, while an in-depth analysis of in-flight
performance remains absent. With major airlines now offering Starlink Internet
onboard, there is a growing need to evaluate and improve its performance for
aviation users. This paper addresses this shortcoming by conducting in-flight
measurements over the Baltic Sea and the Pacific Ocean. Our measurement results
show that a single user device experiences median throughputs of 64 Mbps and 24
Mbps for the downlink and uplink, respectively. The median uplink throughput is
approximately 33 Mbps when the aircraft maintains an altitude above 17,000
feet. However, a significant reduction in uplink performance is observed during
the aircraft descent phase, with the median throughput dropping to around 20
Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the
location of the ground station being pinged and the use of inter-satellite
links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over
the Pacific Ocean and investigate factors influencing RTT, hypothesizing that
ISLs routing, data queuing at satellites, and feeder link congestion contribute
to deviations from theoretical values. For comparative analysis, we evaluate
the Starlink ground terminal and in-flight connectivity performance from the
perspectives of a residential user and an airline passenger, respectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用先前任务的紧凑表格Q值作为可转移知识库，提升学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在DRL中扩展VFI面临状态-动作空间连续性、神经网络噪声近似和存储过去模型不切实际的挑战，DQInit旨在解决这些问题。

Method: DQInit利用已知性机制软性整合转移值到未探索区域，并逐步转向代理学习估计，避免固定时间衰减的限制。

Result: 实验表明，DQInit在连续控制任务中显著提高了早期学习效率、稳定性和整体性能。

Conclusion: DQInit通过仅依赖值估计而非策略或演示，为DRL中的知识转移提供了新视角，结合了跳启RL和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [25] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 论文提出Othello AI Arena，一个评估AI系统在短时间内适应新环境能力的基准框架，强调快速适应和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准多关注固定环境下的性能优化，缺乏对系统适应性和泛化能力的评估，尤其是在规则或结构变化时。

Method: 通过Othello AI Arena平台，要求参与者在60秒内分析新Othello棋盘配置并生成定制策略，分离元级智能与任务级策略性能评估。

Result: 初步测试显示参与者采用多种适应方法，如快速参数调整和模拟学习环境模型，平台提供实时可视化和多维评估。

Conclusion: Othello AI Arena是评估和培养AI系统快速智能适应能力的独特工具和研究基准。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [26] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架，解决了当前评估方法的高成本、标准不一致和主观偏见问题。


<details>
  <summary>Details</summary>
Motivation: 多模态AI助手日益重要，但现有评估方法存在高人工成本、标准不一致和主观偏见等挑战。

Method: 采用三层智能体架构（交互评估、语义验证和体验决策），并在Qwen3-8B模型上进行监督微调。

Result: 在八个主要智能体上的实验表明，该框架能有效预测用户满意度和识别生成缺陷，匹配专家评估的准确率显著。

Conclusion: 该框架为多模态AI助手的自动化评估提供了高效且可靠的解决方案。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [27] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种名为EvoCurr的自进化框架，通过动态调整问题难度提升LLM在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在复杂问题中表现不佳，缺乏结构化中间指导导致效率低下或失败。

Method: 使用专门的课程生成LLM构建逐步增加难度的问题序列，动态调整以适应求解LLM的学习进度。

Result: 实验表明，该方法显著提高了任务成功率和解决方案效率。

Conclusion: LLM驱动的课程学习在增强复杂领域自动推理方面具有潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [28] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该论文提出了一种方法，将SHAP值中的不确定性分解为偶然性、认知性和纠缠性成分，以提升SHAP解释的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: SHAP值通常被视为点估计，忽略了预测模型和数据中的不确定性。这种不确定性可能影响高风险领域（如医疗分析）中的决策。

Method: 结合Dempster-Shafer证据理论和Dirichlet过程对树集成进行假设采样，分解SHAP值中的不确定性。

Result: 实验表明，SHAP值最高的特征不一定最稳定，认知性不确定性可通过更好的数据和模型开发技术减少。

Conclusion: 树集成模型（尤其是装袋法）能有效量化认知性不确定性，为高风险应用提供更可靠的SHAP解释。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [29] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互学习机制和多样化提示，解决了RLVR中的奖励稀疏问题，显著提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR在奖励稀疏时无法提供学习信号，限制了LLM在复杂任务中的表现。

Method: 提出MEML-GRPO框架，利用多样化专家提示生成更广泛响应，并通过专家互学习机制促进知识共享。

Result: 在多个推理基准测试中，MEML-GRPO平均性能提升4.89%（Qwen）和11.33%（Llama）。

Conclusion: MEML-GRPO有效克服了传统RLVR的核心限制，显著提升了模型性能。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [30] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出了一种无监督去偏对齐框架（UDA），通过动态调整Elo评分系统减少跨模型评估中的偏好偏差，显著降低了评委间的评分差异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）成对评估存在偏好偏差，导致评委间评分不一致。

Method: 提出UDA框架，通过紧凑神经网络动态调整Elo评分系统的K因子，以最小化评委间的评分分散为目标，实现无监督去偏。

Result: UDA将评委间评分标准差降低63.4%，与人类判断的平均相关性提高24.7%。

Conclusion: UDA有效减少评估偏差，提升评分一致性和可靠性。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [31] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出PacifAIst基准，用于评估LLMs在目标冲突中的行为对齐，发现模型表现差异显著，强调标准化工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键社会功能中的自主性增强，现有安全基准未能系统评估模型在目标冲突中的决策行为，存在风险测量和缓解的空白。

Method: 引入PacifAIst基准，包含700个场景，基于Existential Prioritization（EP）分类，测试自我保存、资源冲突和目标保留等行为。

Result: 评估8个LLMs，Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最低（79.49%），模型在子类别中表现差异显著。

Conclusion: 需标准化工具如PacifAIst以测量和缓解目标冲突风险，确保AI行为优先人类安全。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [32] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理基于公共观察的知识更新的逻辑，其可满足性问题是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中基于观察的知识更新逻辑，特别是在认知规划中的应用。

Method: 使用公共观察逻辑（POL），在克里普克模型中为每个状态配备预期观察集，并通过匹配实际观察更新状态。

Result: 证明了POL的可满足性问题是2EXPTIME完全的。

Conclusion: POL为多智能体系统中的知识更新提供了一种有效的逻辑框架，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [33] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种结合文本、关卡和草图的多模态强化学习框架，通过对比学习和嵌入对齐提升AI生成内容的人性化程度。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在协作内容创作中未能充分体现以人为中心的行为，限制了其实用性。

Method: 提出VIPCGRL框架，结合文本、关卡和草图三种模态，通过四重对比学习和嵌入相似性奖励对齐策略。

Result: VIPCGRL在人性化方面优于现有基线，定量指标和人工评估均验证了其效果。

Conclusion: VIPCGRL为协作内容创作提供了更人性化的AI工具，代码和数据集将公开。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [34] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种动态监督和机动机制的多智能体系统（MAS），通过引入Guard Agent验证和修正推理过程，显著提升了系统的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，智能代理依赖多种外部工具解决复杂问题，但多工具带来的上下文扩展和噪声输出降低了系统可靠性，亟需增强稳定性。

Method: 在AWorld框架中构建动态MAS架构，Execution Agent在关键步骤调用Guard Agent验证和修正推理过程，减少噪声导致的错误。

Result: 在GAIA测试数据集上的实验表明，动态机动机制显著提升解决方案的有效性和稳定性，优于单智能体系统（SAS）和标准工具增强系统，并在GAIA排行榜上排名第一。

Conclusion: 动态MAS系统通过协作代理角色，为开发更可靠和可信的智能系统提供了实用价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [35] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和检索增强生成的多智能体框架，用于解决法规合规问答中的精确性和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 法规合规问答需要精确且可验证的信息，这对大型语言模型提出了挑战。

Method: 通过构建和维护无本体知识图谱，结合检索增强生成技术，实现高效的问答系统。

Result: 该系统在复杂法规查询中优于传统方法，确保事实正确性并提供可追溯性。

Conclusion: 该框架为合规驱动和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [36] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型（LLMs）在解决数学问题时的准确性，发现推理增强的OpenAI o1模型表现最佳，双代理配置显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数学教育中的准确性和可靠性，为AI驱动的教学和评估提供改进策略。

Method: 通过构建挑战性数学任务，分析四种LLM在算术、代数和数论中的答案准确性和步骤错误。

Result: OpenAI o1模型表现最佳，双代理配置提升性能，程序性错误最常见。

Conclusion: 研究为提升LLM性能和数学教育应用提供了有效策略。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [37] [Deviation Inequalities for Rényi Divergence Estimators via Variational Expression](https://arxiv.org/abs/2508.09382)
*Sreejith Sreekumar,Kengo Kato*

Main category: cs.IT

TL;DR: 该论文针对Rényi散度的估计误差提出了指数偏差不等式，改进了现有结果，并展示了在信息理论和隐私审计中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有关于Rényi散度估计误差的概率界限研究不足，尤其是在非紧凑支持或无密度下界的情况下。

Method: 通过将误差关联到经验过程并利用经验过程理论工具，建立了平滑插件估计器和神经估计器的指数偏差不等式。

Result: 提出了不依赖于紧凑支持或密度下界假设的偏差不等式，并展示了其在信息理论和隐私审计中的具体应用。

Conclusion: 论文为Rényi散度估计提供了更通用的概率界限，并展示了其实际应用潜力。

Abstract: R\'enyi divergences play a pivotal role in information theory, statistics,
and machine learning. While several estimators of these divergences have been
proposed in the literature with their consistency properties established and
minimax convergence rates quantified, existing accounts of probabilistic bounds
governing the estimation error are premature. Here, we make progress in this
regard by establishing exponential deviation inequalities for smoothed plug-in
estimators and neural estimators by relating the error to an appropriate
empirical process and leveraging tools from empirical process theory. In
particular, our approach does not require the underlying distributions to be
compactly supported or have densities bounded away from zero, an assumption
prevalent in existing results. The deviation inequality also leads to a
one-sided concentration bound from the expectation, which is useful in
random-coding arguments over continuous alphabets in information theory with
potential applications to physical-layer security. As another concrete
application, we consider a hypothesis testing framework for auditing R\'{e}nyi
differential privacy using the neural estimator as a test statistic and obtain
non-asymptotic performance guarantees for such a test.

</details>


### [38] [Hermitian Self-dual Twisted Generalized Reed-Solomon Codes](https://arxiv.org/abs/2508.09687)
*Chun'e Zhao,Yuxin Han,Wenping Ma,Tongjiang Yan,Yuhua Sun*

Main category: cs.IT

TL;DR: 本文研究了广义扭曲Reed-Solomon（A-TGRS）码的Hermitian自对偶性，提出了四个构造方法，并获得了新的自对偶码类。还给出了A-TGRS码同时为Hermitian自对偶和MDS的充要条件。


<details>
  <summary>Details</summary>
Motivation: 自对偶MDS码在组合和密码学中有重要应用，而TGRS码可以同时满足MDS和自对偶性。本文旨在全面研究A-TGRS码的Hermitian自对偶性。

Method: 通过矩阵表示法分析A-TGRS码的Hermitian自对偶性，提出了四个构造方法，并给出了充要条件。

Result: 获得了新的Hermitian自对偶TGRS码类，并构造了一类MDS Hermitian自对偶TGRS码。

Conclusion: 本文从矩阵表示的角度研究了TGRS码的Hermitian自对偶性，为相关研究提供了更简洁的分析框架。

Abstract: Self-dual maximum distance separable (MDS) codes over finite fields are
linear codes with significant combinatorial and cryptographic applications.
Twisted generalized Reed-Solomon (TGRS) codes can be both MDS and self-dual. In
this paper, we study a general class of TGRS codes (A-TGRS), which encompasses
all previously known special cases. First, we establish a sufficient and
necessary condition for an A-TGRS code to be Hermitian self-dual. Furthermore,
we present four constructions of self-dual TGRS codes, which, to the best of
our knowledge, nearly cover all the related results previously reported in the
literature. More importantly, we also obtain several new classes of Hermitian
self-dual TGRS codes with flexible parameters. Based on this framework, we
derive a sufficient and necessary condition for an A-TGRS code to be Hermitian
self-dual and MDS. In addition, we construct a class of MDS Hermitian self-dual
TGRS code by appropriately selecting the evaluation points. This work
investigates the Hermitian self-duality of TGRS codes from the perspective of
matrix representation, leading to more concise and transparent analysis. More
generally, the Euclidean self-dual TGRS codes and the Hermitian self-dual GRS
codes can also be understood easily from this point.

</details>


### [39] [Fluid Reconfigurable Intelligent Surface with Element-Level Pattern Reconfigurability: Beamforming and Pattern Co-Design](https://arxiv.org/abs/2508.09695)
*Han Xiao,Xiaoyan Hu,Kai-Kit Wong,Xusheng Zhu,Hanjiang Hong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一种新型的模式可重构流体可重构智能表面（FRIS）框架，通过动态调整流体元件的辐射模式以适应瞬时信道条件。理论分析和仿真结果表明，该框架在多用户通信场景中显著优于传统RIS架构。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索流体可重构智能表面在动态信道条件下的性能优势，特别是在点对点和多用户通信系统中。

Method: 方法包括理论分析比较三种表面配置的性能，以及使用球谐函数正交分解（SHOD）建模流体元件辐射模式，并通过迭代算法优化波束成形和球谐系数。

Result: 仿真结果显示，模式可重构FRIS在3GPP 38.901和各向同性辐射模型下分别实现了161.5%和176.2%的平均性能提升。

Conclusion: 结论表明，模式可重构FRIS在信号调制和多用户通信中具有显著优势，为未来智能表面设计提供了新思路。

Abstract: This paper proposes a novel pattern-reconfigurable fluid reconfigurable
intelligent surface (FRIS) framework, where each fluid element can dynamically
adjust its radiation pattern based on instantaneous channel conditions. To
evaluate its potential, we first conduct a comparative analysis of the received
signal power in point-to-point communication systems assisted by three types of
surfaces: (1) the proposed pattern-reconfigurable FRIS, (2) a
position-reconfigurable FRIS, and (3) a conventional RIS. Theoretical results
demonstrate that the pattern-reconfigurable FRIS provides a significant
advantage in modulating transmission signals compared to the other two
configurations. To further study its capabilities, we extend the framework to a
multiuser communication scenario. In this context, the spherical harmonics
orthogonal decomposition (SHOD) method is employed to accurately model the
radiation patterns of individual fluid elements, making the pattern design
process more tractable. An optimization problem is then formulated with the
objective of maximizing the weighted sum rate among users by jointly designing
the active beamforming vectors and the spherical harmonics coefficients,
subject to both transmit power and pattern energy constraints. To tackle the
resulting non-convex optimization problem, we propose an iterative algorithm
that alternates between a minimum mean-square error (MMSE) approach for active
beamforming and a Riemannian conjugate gradient (RCG) method for updating the
spherical harmonics coefficients. Simulation results show that the proposed
pattern-reconfigurable FRIS significantly outperforms traditional RIS
architectures based on the 3GPP 38.901 and isotropic radiation models,
achieving average performance gains of 161.5% and 176.2%, respectively.

</details>


### [40] [ORCAS Codes: A Flexible Generalization of Polar Codes with Low-Complexity Decoding](https://arxiv.org/abs/2508.09744)
*Andreas Zunker,Marvin Rübenacke,Stephan ten Brink*

Main category: cs.IT

TL;DR: 论文提出了一种基于递归Plotkin级联的低复杂度软判决解码算法ORCAS码，性能优于极化码。


<details>
  <summary>Details</summary>
Motivation: 需要低复杂度软判决解码的信道码。

Method: 递归级联基于单纯形码及其对偶码的低码率和高码率最优码，采用低复杂度ML解码和SC解码。

Result: ORCAS码性能至少与极化码相当，实际参数下块错误率优于极化码0.5 dB，解码复杂度相似，码长更灵活。

Conclusion: ORCAS码在性能和灵活性上优于极化码，适合实际应用。

Abstract: Motivated by the need for channel codes with low-complexity soft-decision
decoding algorithms, we consider the recursive Plotkin concatenation of optimal
low-rate and high-rate codes based on simplex codes and their duals. These
component codes come with low-complexity maximum likelihood (ML) decoding
which, in turn, enables efficient successive cancellation (SC)-based decoding.
As a result, the proposed optimally recursively concatenated simplex (ORCAS)
codes achieve a performance that is at least as good as that of polar codes.
For practical parameters, the proposed construction significantly outperforms
polar codes in terms of block error rate by up to 0.5 dB while maintaining
similar decoding complexity. Furthermore, the codes offer greater flexibility
in codeword length than conventional polar codes.

</details>


### [41] [Non-Orthogonal Affine Frequency Division Multiplexing for Spectrally Efficient High-Mobility Communications](https://arxiv.org/abs/2508.09782)
*Qin Yi,Zilong Liu,Leila Musavian,Zeping Sui*

Main category: cs.IT

TL;DR: 提出了一种新型非正交仿射频分复用（nAFDM）波形，用于高移动性通信，提高频谱效率。通过带宽压缩因子实现可控子载波重叠，并提出基于IDFT的生成方法和软迭代检测算法。仿真显示nAFDM在BER和频谱效率上优于现有波形。


<details>
  <summary>Details</summary>
Motivation: 解决高移动性通信中频谱效率低的问题，通过非正交调制实现更高的频谱利用率。

Method: 引入带宽压缩因子实现子载波重叠，提出基于IDFT的信号生成方法和软迭代检测算法。

Result: nAFDM在BER上与传统AFDM相近，频谱效率更高，且能实现BER与复杂度的平衡。

Conclusion: nAFDM是一种高效的高移动性通信波形，兼具频谱效率和性能优势。

Abstract: This paper proposes a novel non-orthogonal affine frequency division
multiplexing {(nAFDM)} waveform for reliable high-mobility communications with
enhanced spectral efficiency {(SE)}. The key idea is {to introduce} a bandwidth
compression factor into the AFDM {modulator} to enable controllable subcarrier
overlapping. We first {detail the proposed nAFDM transceiver} and derive the
corresponding input-output {signal} relationship. Then, an efficient {nAFDM}
signal generation method based on the inverse discrete Fourier transform (IDFT)
is proposed, enabling practical implementation using existing inverse fast
Fourier transform (IFFT) modules without additional hardware complexity. Next,
to characterize the impact of non-orthogonal modulation, we derive a
closed-form expression {of} inter-carrier interference (ICI), showing its
dependence on the bandwidth compression factor. To mitigate the resulting
interference, we propose a soft iterative detection algorithm and a
low-complexity implementation approach that leverages the distribution
characteristics of ICI. {Simulation results demonstrate that 1) in terms of bit
error rate (BER), the proposed nAFDM can achieve near identical BER compared to
conventional AFDM, while outperforms other waveform counterparts; 2) nAFDM is
capable of striking higher SE compared to other existing waveforms; and 3) the
proposed nAFDM achieves an attractive BER vs. SE trade-off, and the proposed
soft ID scheme can attain a trade-off between BER and complexity.}

</details>


### [42] [Unified Design of Space-Air-Ground-Sea Integrated Maritime Communications](https://arxiv.org/abs/2508.09817)
*Zhehan Zhou,Xiaoming Chen,Ming Ying,Zhaohui Yang,Chongwen Huang,Yunlong Cai,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 提出了一种结合卫星、无人机、地面基站和无人船的空间-空中-地面-海洋一体化海上通信架构，通过联合波束成形和轨迹优化算法最大化用户传输速率。


<details>
  <summary>Details</summary>
Motivation: 随着海上活动的爆炸性增长，需要为广阔海域提供无缝通信和服务质量保障。

Method: 将海洋空间按距离划分为不同区域，分别由地面基站、无人船、无人机和卫星服务；设计联合波束成形和轨迹优化算法。

Result: 理论分析和仿真结果验证了算法的有效性。

Conclusion: 提出的架构和算法能有效提升海上通信性能。

Abstract: With the explosive growth of maritime activities, it is expected to provide
seamless communications with quality of service (QoS) guarantee over broad sea
area. In the context, this paper proposes a space-air-ground-sea integrated
maritime communication architecture combining satellite, unmanned aerial
vehicle (UAV), terrestrial base station (TBS) and unmanned surface vessel
(USV). Firstly, according to the distance away from the shore, the whole marine
space is divided to coastal area, offshore area, middle-sea area and open-sea
area, the maritime users in which are served by TBS, USV, UAV and satellite,
respectively. Then, by exploiting the potential of integrated maritime
communication system, a joint beamforming and trajectory optimization algorithm
is designed to maximize the minimum transmission rate of maritime users.
Finally, theoretical analysis and simulation results validate the effectiveness
of the proposed algorithm.

</details>


### [43] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 提出了一种基于用户意图的语义通信系统，通过多模态大模型和掩码引导注意力模块，实现深度意图理解和高效通信。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统虽能提取关键语义，但未能深度理解和泛化用户真实意图。

Method: 结合多模态大模型生成用户意图先验，提出掩码引导注意力模块突出关键语义区域，并引入信道状态感知模块适应不同信道条件。

Result: 在瑞利信道5 dB SNR下，PSNR、SSIM和LPIPS分别提升8%、6%和19%，优于DeepJSCC。

Conclusion: 该系统实现了深度意图理解，并在性能上显著优于现有方法。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>
