<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Resource Allocation and Sharing for UAV-Assisted Integrated TN-NTN with Multi-Connectivity](https://arxiv.org/abs/2601.15532)
*Abd Ullah Khan,Wali Ullah Khan,Haejoon Jung,Hyundong Shin*

Main category: cs.NI

TL;DR: 该论文研究集成TN-NTN环境中支持多连接的动态无人机资源分配问题，考虑频谱共享和公平性约束，提出了两种优化算法分别最大化总容量和最小容量。


<details>
  <summary>Details</summary>
Motivation: 无人机在TN-NTN网络中通过多连接实现高效可靠的数据传输，但由于无人机移动性导致的信道变化、异构设备的QoS需求差异、频谱资源共享和容量分配的公平性要求，使得最优资源分配面临挑战。

Method: 考虑三种链路类型（UAV-RBS、UAV-UAV、UAV-HAP）和两类不同QoS需求的无人机，提出两种算法：第一种最大化UAV-RBS和UAV-HAP链路的总容量，同时保证UAV-UAV链路的可靠性；第二种最大化所有链路的最小容量，确保公平性。

Result: 通过仿真验证了两种算法的性能，第一种算法优化了系统总吞吐量，第二种算法确保了各链路间的容量公平分配。

Conclusion: 在集成TN-NTN环境中，针对QoS约束的多连接动态无人机网络，提出的两种资源分配算法能够有效解决频谱共享和公平性问题，分别实现系统总容量最大化和公平容量分配的目标。

Abstract: Unmanned aerial vehicles (UAVs) with multi- connectivity (MC) capabilities efficiently and reliably transfer data between terrestrial networks (TNs) and non-terrestrial networks (NTNs). However, optimally sharing and allocating spectrum and power resources to maintain MC while ensuring reliable connectivity and optimal performance remains challeng- ing in such networks. Channel variations induced by mobility in UAV networks, coupled with the varying quality of service (QoS) demands of heterogeneous devices, resource sharing, and fairness requirements in capacity distribution pose challenges to optimal resource allocation. Thus, this paper investigates resource allocation for QoS-constrained, MC-enabled, dynamic UAVs in an integrated TN-NTN environment with spectrum sharing and fairness considerations. To this end, we consider three types of links: UAV-to-radio base station (RBS), UAV-to-UAV, and UAV-to-HAP. We also assume two types of UAVs with diverse QoS requirements to reflect a practical scenario. Consequently, we propose two algorithms. The first algorithm maximizes the capacity of UAVs-RBS and UAVs-HAP links while ensuring the reliability of the UAV-UAV link. To achieve this, the algorithm maximizes the collective throughput of the UAVs by optimizing the sum capacity of all the UAV-RBS and UAV-HAP links. Next, to provide constant capacity to all links and ensure fairness, we propose another algorithm that maximizes the minimum capacity across all links. We validate the performance of both algorithms through simulation

</details>


### [2] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: MapViT：基于Vision Transformer的两阶段框架，用于预测环境变化和无线电信号质量，支持移动机器人在动态环境中的实时导航


<details>
  <summary>Details</summary>
Motivation: 移动和无线网络的进步为机器人自主性提供了支持，但机器人在动态环境中需要准确理解环境和无线电信号质量，这仍然是一个未解决的挑战

Method: 采用两阶段Vision Transformer框架，受LLM预训练-微调范式启发，包括自监督预训练阶段和下游预测阶段

Result: MapViT实现了实时预测，在准确性和计算效率之间取得了良好平衡，几何基础模型提高了数据效率和可迁移性

Conclusion: 该工作为下一代数字孪生生态系统奠定了基础，并为未来6G系统中的多模态智能ML基础模型开辟了新途径

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


### [3] [RF Intelligence for Health: Classification of SmartBAN Signals in overcrowded ISM band](https://arxiv.org/abs/2601.15836)
*Nicola Gallucci,Giacomo Aragnetti,Matteo Malagrinò,Francesco Linsalata,Maurizio Magarini,Lorenzo Mucchi*

Main category: cs.NI

TL;DR: 提出首个开源框架，用于在拥挤的2.4GHz ISM频段中自动识别SmartBAN信号，结合合成数据集和真实射频采集，使用基于ResNet编码器和U-Net解码器的深度卷积神经网络，在密集频谱环境中实现可靠的医疗传感器信号识别。


<details>
  <summary>Details</summary>
Motivation: 在拥挤的2.4GHz ISM频段中，医疗传感器的低功率传输难以识别，存在强同信道干扰和功率不对称问题，这影响了可穿戴健康监测系统的可靠性，需要开发能够识别SmartBAN信号的框架来支持干扰感知共存策略。

Method: 结合合成数据集（模拟信号）和真实射频采集（通过软件定义无线电获取），使用基于ResNet编码器和U-Net解码器的深度卷积神经网络，并加入注意力机制，在不同传播条件下进行训练和评估。

Result: 在合成数据集上达到超过90%的准确率，在真实空中频谱图上表现出稳定的性能，能够在密集频谱环境中可靠识别SmartBAN信号。

Conclusion: 该开源框架实现了在拥挤频谱环境中对SmartBAN信号的可靠识别，支持干扰感知共存策略，提高了可穿戴医疗系统的可靠性，为医疗协议在干扰条件下的操作提供了重要支持。

Abstract: Accurate classification of Radio-Frequency (RF) signals is essential for reliable wearable health-monitoring systems, providing awareness of the interference conditions in which medical protocols operate. In the overcrowded 2.4 GHz ISM band, however, identifying low-power transmissions from medical sensors is challenging due to strong co-channel interference and substantial power asymmetry with coexisting technologies. This work introduces the first open source framework for automatic recognition of SmartBAN signals in Body Area Networks (BANs). The framework combines a synthetic dataset of simulated signals with real RF acquisitions obtained through Software-Defined Radios (SDRs), enabling both controlled and realistic evaluation. Deep convolutional neural networks based on ResNet encoders and U-Net decoders with attention mechanisms are trained and assessed across diverse propagation conditions. The proposed approach achieves over 90% accuracy on synthetic datasets and demonstrates consistent performance on real over-the-air spectrograms. By enabling reliable SmartBAN signal recognition in dense spectral environments, this framework supports interferenceaware coexistence strategies and improves the dependability of wearable healthcare systems.

</details>


### [4] [Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links](https://arxiv.org/abs/2601.15904)
*Hossein Mohammadalizadeh,Holger Karl*

Main category: cs.NI

TL;DR: 提出ACI框架解决带切换延迟的资源分配问题，证明其吞吐最优性并在多无人机网络中验证


<details>
  <summary>Details</summary>
Motivation: 传统网络调度方案在处理具有随机、非均匀切换延迟的并行队列时性能下降，特别是Max-Weight策略忽略切换延迟

Method: 提出ACI（非近视、基于帧的调度框架），通过Lyapunov漂移分析证明其吞吐最优性，并在多无人机FSO回程网络中验证

Result: ACI能有效摊销切换延迟，在吞吐量方面相对于缩放容量区域达到最优，并能灵活权衡吞吐与延迟

Conclusion: ACI框架解决了带切换延迟的资源分配问题，提供了吞吐最优的调度方案，并可通过调整紧迫性度量灵活平衡吞吐与延迟

Abstract: Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.

</details>


### [5] [Low-altitude Multi-UAV-assisted Data Collection and Semantic Forwarding for Post-Disaster Relief](https://arxiv.org/abs/2601.16146)
*Xiaoya Zheng,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Qingqing Wu,Dusit Niyato,Abbas Jamalipour*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型的交替优化方法（LLM-AOA），用于优化低空多无人机辅助的数据收集与语义转发网络，通过集群协作和语义提取提高传输效率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 低空经济中，无人机作为空中中继在灾后通信恢复中发挥关键作用。传统方法面临远程基站连接脆弱、数据量大而无人机资源有限的挑战，需要更高效的解决方案。

Method: 提出LLM-AOA方法，将多无人机数据收集与语义转发建模为多目标优化问题，通过大语言模型处理复杂搜索空间和动态变量维度，采用交替优化策略分别优化不同决策变量子集。

Result: 仿真结果表明，LLM-AOA相比传统AOA在传输速率上提升约26.8%，在语义速率上提升约22.9%，显著提高了网络性能。

Conclusion: LLM-AOA方法有效解决了低空多无人机网络中的复杂优化问题，为低空经济中的高效通信恢复提供了有前景的解决方案。

Abstract: The low-altitude economy (LAE) is an emerging economic paradigm which fosters integrated development across multiple fields. As a pivotal component of the LAE, low-altitude uncrewed aerial vehicles (UAVs) can restore communication by serving as aerial relays between the post-disaster areas and remote base stations (BSs). However, conventional approaches face challenges from vulnerable long-distance links between the UAVs and remote BSs, and data bottlenecks arising from massive data volumes and limited onboard UAV resources. In this work, we investigate a low-altitude multi-UAV-assisted data collection and semantic forwarding network, in which multiple UAVs collect data from ground users, form clusters, perform intra-cluster data aggregation with semantic extraction, and then cooperate as virtual antenna array (VAAs) to transmit the extracted semantic information to a remote BS via collaborative beamforming (CB). We formulate a data collection and semantic forwarding multi-objective optimization problem (DCSFMOP) that jointly maximizes both the user and semantic transmission rates while minimizing UAV energy consumption. The formulated DCSFMOP is a mixed-integer nonlinear programming (MINLP) problem that is inherently NP-hard and characterized by dynamically varying decision variable dimensionality. To address these challenges, we propose a large language model-enabled alternating optimization approach (LLM-AOA), which effectively handles the complex search space and variable dimensionality by optimizing different subsets of decision variables through tailored optimization strategies. Simulation results demonstrate that LLM-AOA outperforms AOA by approximately 26.8\% and 22.9\% in transmission rate and semantic rate, respectively.

</details>


### [6] [Real-Time HAP-Assisted Vehicular Edge Computing for Rural Areas](https://arxiv.org/abs/2301.09957)
*Alessandro Traspadini,Marco Giordani,Giovanni Giambene,Michele Zorzi*

Main category: cs.NI

TL;DR: 该论文研究在乡村场景中利用高空平台支持车载边缘计算，分析车辆如何优化计算任务卸载策略以最大化实时服务概率


<details>
  <summary>Details</summary>
Motivation: 非地面网络是6G网络的关键组成部分，可扩展农村和偏远地区的网络覆盖。高空平台可作为边缘服务器，为物联网传感器和地面车辆等能源受限设备提供计算卸载服务。在乡村场景中，需要研究如何通过高空平台支持车载边缘计算，优化任务卸载决策。

Method: 将系统建模为一组队列，计算任务按照泊松到达过程到达。分析地面车辆的数据处理决策（本地处理或卸载到高空平台）。在延迟和计算能力约束下，评估最优的车载边缘计算卸载因子，以最大化实时服务概率。

Result: 论文提出了优化车载边缘计算卸载因子的方法，能够在满足延迟和计算能力约束的条件下，最大化实时服务概率。为乡村场景中利用高空平台支持车载边缘计算提供了理论框架。

Conclusion: 高空平台可作为有效的边缘服务器支持乡村场景中的车载边缘计算。通过优化任务卸载策略，能够提高实时服务概率，为6G非地面网络中的车载边缘计算应用提供解决方案。

Abstract: Non-Terrestrial Networks (NTNs) are expected to be a key component of 6th generation (6G) networks to support broadband seamless Internet connectivity and expand the coverage even in rural and remote areas. In this context, High Altitude Platforms (HAPs) can act as edge servers to process computational tasks offloaded by energy-constrained terrestrial devices such as Internet of Things (IoT) sensors and ground vehicles (GVs). In this paper, we analyze the opportunity to support Vehicular Edge Computing (VEC) via HAP in a rural scenario where GVs can decide whether to process data onboard or offload them to a HAP. We characterize the system as a set of queues in which computational tasks arrive according to a Poisson arrival process. Then, we assess the optimal VEC offloading factor to maximize the probability of real-time service, given latency and computational capacity constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: Gated Sparse Attention (GSA) 结合了稀疏注意力和门控注意力的优势，在长上下文语言模型中实现了高效性和质量提升，同时改善了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中的注意力计算负担过重，现有稀疏注意力机制和门控注意力变体分别解决不同问题但相互独立，需要结合两者优势。

Method: 提出Gated Sparse Attention (GSA)，包含：1) 带sigmoid激活的门控闪电索引器，产生有界可解释的选择分数；2) 基于局部不确定性调节注意力token数量的自适应稀疏控制器；3) 值和输出阶段的双重门控。

Result: 在1.7B参数模型上，GSA在128K上下文长度下达到12-16倍加速，困惑度从6.03提升到5.70，RULER分数几乎翻倍，对第一个token的注意力从47%降至4%以下，训练稳定性显著改善，损失峰值减少98%。

Conclusion: GSA成功结合了稀疏注意力的效率和门控注意力的质量优势，为长上下文语言模型提供了高效、稳定且高质量的注意力机制解决方案。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [8] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: 研究发现LLM在急诊分诊中存在通过代理变量表现的歧视行为，以及系统性地根据输入中特定标记修改患者严重程度感知的倾向，表明AI系统训练仍不完善。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已应用于临床决策，但针对不同种族、社会、经济和临床背景患者的隐藏偏见仍然存在。本研究旨在调查LLM在急诊分诊中的偏见问题。

Method: 使用32个患者层面的代理变量，每个变量由正负两种限定词表示，在公开数据集（MIMIC-IV-ED Demo, MIMIC-IV Demo）和受限访问数据集（MIMIC-IV-ED和MIMIC-IV）上评估其影响。

Result: 结果显示：1）急诊分诊场景中存在通过代理变量介导的歧视行为；2）LLM系统性地倾向于根据输入上下文中出现的特定标记修改感知的患者严重程度，无论这些标记是正面还是负面框架。

Conclusion: AI系统仍然在不完美的嘈杂、有时非因果的信号上训练，这些信号不能可靠反映真实患者严重程度。需要更多工作确保AI技术在临床环境中的安全和负责任部署。

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [9] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench是一个评估生成式科学综述学术价值的新基准，解决了现有基准仅关注表面质量而忽视深层学术价值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在两个关键问题：1）基准数据集不可靠，缺乏学术维度标注；2）评估指标只关注表面质量（如逻辑连贯性），无法评估深层学术价值（如核心研究目标和批判性分析）。

Method: 提出了一个全面的学术价值评估标准，涵盖三个维度：信息价值、学术交流价值和研究指导价值。基于此标准构建了带有学术价值标注的可靠数据集，并评估生成式综述的深层学术价值。

Result: 广泛的实验结果表明，该基准在评估生成式综述的学术价值方面与人类评估表现高度一致。

Conclusion: DeepSurvey-Bench能够全面评估生成式科学综述的深层学术价值，解决了现有基准的局限性，为自动生成科学综述的质量评估提供了更可靠的基准。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [10] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon是一个神经符号认知操作系统，通过结构化记忆宫殿和神经符号事件图解决LLM在长上下文中的计算成本和"迷失在中间"问题，实现亚毫秒级检索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受到自注意力二次计算成本和"迷失在中间"现象的限制，传统"扁平RAG"架构将记忆视为无结构的嵌入集合，无法捕捉长时程交互的层次和时间结构，导致"向量迷雾"问题。

Method: 提出Aeon神经符号认知操作系统，将记忆结构化为记忆宫殿（空间索引）和事件轨迹（神经符号事件图），引入语义旁路缓冲区作为预测性缓存机制，利用对话局部性实现亚毫秒检索延迟。

Result: 基准测试显示Aeon在对话工作负载上实现<1ms检索延迟，通过零拷贝C++/Python桥确保状态一致性，为自主智能体提供持久结构化记忆。

Conclusion: Aeon通过重新定义记忆为管理的操作系统资源，解决了LLM在长上下文中的根本限制，实现了高效、结构化的记忆管理，为自主智能体系统提供了新的架构范式。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [11] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 本文是第一篇系统综述大型视觉语言模型在多模态假新闻检测中变革性作用的论文，梳理了从传统特征工程方法到端到端多模态推理框架的范式转变。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的快速发展推动了多模态假新闻检测的范式转变，但该领域缺乏系统性的综述来追踪这一转变并整合最新进展。本文旨在填补这一空白，全面回顾LVLMs在多模态假新闻检测中的应用。

Method: 论文首先从历史视角出发，梳理从传统多模态检测流程到基础模型驱动范式的演变；然后建立结构化分类体系，涵盖模型架构、数据集和性能基准；接着分析剩余技术挑战；最后提出未来研究方向。

Result: 本文提供了首个系统性综述，记录了LVLMs在多模态假新闻检测中的变革作用，建立了完整的分类框架，并识别了当前领域的主要挑战和未来发展方向。

Conclusion: 大型视觉语言模型正在彻底改变多模态假新闻检测领域，从传统特征工程方法转向统一的端到端多模态推理框架。虽然取得了显著进展，但仍面临可解释性、时序推理和领域泛化等挑战，需要进一步研究来推动该领域的下一阶段发展。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [12] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: 本文提出DFAH框架，用于评估金融领域工具使用型LLM代理的轨迹确定性和证据条件忠实度，发现模型确定性与忠实度呈正相关，并提供金融基准测试套件。


<details>
  <summary>Details</summary>
Motivation: LLM代理在监管审计回放中存在一致性问题：当要求用相同输入重现被标记的交易决策时，大多数部署无法返回一致结果。这暴露了金融领域LLM代理部署的可靠性问题。

Method: 提出确定性-忠实度保证框架（DFAH），通过非代理基线实验（74种配置，12个模型，4个提供商）和代理工具使用实验，测量轨迹确定性和证据条件忠实度。提供三个金融基准测试（合规分类、投资组合约束、DataOps异常）和开源压力测试工具。

Result: 7-20B参数模型在非代理基线中达到100%确定性，而120B+模型需要3.7倍更大的验证样本才能达到同等统计可靠性。代理工具使用引入额外方差。确定性与忠实度呈正相关（r=0.45，p<0.01）。Tier 1模型在DFAH评估设置下达到符合审计回放要求的确定性水平。

Conclusion: DFAH框架能够有效评估金融领域LLM代理的可靠性，发现确定性与能力并非权衡关系而是正相关。Schema-first架构的Tier 1模型能够满足审计回放要求，为金融监管合规提供了实用评估工具。

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [13] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: Prometheus Mind 为冻结的 Qwen3-4B 模型添加记忆功能，通过11个模块化适配器（530MB，7%开销）实现完全可逆的改造，解决了提取、训练、注入和隐藏状态崩溃四个关键问题。


<details>
  <summary>Details</summary>
Motivation: 为预训练语言模型添加记忆功能通常需要架构修改或权重调整，本文旨在开发一种完全可逆的方法，通过模块化适配器为冻结模型添加记忆，避免对原始模型进行永久性修改。

Method: 1. 提取：使用对比方向发现（CDD）通过最小对找到语义方向，无需标注数据；2. 训练：采用分阶段训练策略，每个适配器在简单代理任务上训练；3. 注入：利用lm_head.weight行作为映射，无需额外训练；4. 隐藏状态崩溃：训练投影恢复语义区分度。

Result: 在PrometheusExtract-132数据集上，系统在干净输入上达到94.4%检索率（95% CI: [84.9%, 98.1%]），但在非正式输入（省略、填充词或隐含主语）上降至19.4%。主要瓶颈是关系分类（47.3%准确率）。

Conclusion: Prometheus Mind 成功为冻结模型添加了可逆记忆功能，但非正式输入的鲁棒性和关系分类准确性仍需改进。该方法为模型记忆增强提供了模块化、可逆的解决方案。

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [14] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: 本文提出"知识图谱网络"的系统理论、技术和应用，特别是在医疗健康领域，涵盖模糊、不确定、多模态、向量化、分布式、联邦等不同条件下的定义、开发、推理、计算和应用。


<details>
  <summary>Details</summary>
Motivation: 知识图谱研究快速发展，但在医疗健康等领域的应用中，许多主要信息处理技术仍滞后，包括未能充分利用高级逻辑推理、AI技术、专用编程语言、现代概率统计理论等，特别是多知识图谱协作与竞争技术未得到足够重视。

Method: 开发知识图谱网络的系统理论、技术和应用框架，涵盖其定义、开发、推理、计算和应用，研究在不同条件下（模糊、不确定、多模态、向量化、分布式、联邦）的实现，并提供真实数据示例和实验结果。

Result: 论文提供了知识图谱网络在医疗健康领域的完整理论框架和应用案例，包括各种条件下的实现方法和实验结果，展示了该概念的实际应用价值。

Conclusion: 提出了创新的知识图谱网络概念，为医疗健康领域的知识图谱应用提供了系统化的解决方案，填补了多知识图谱协作与竞争技术研究的空白。

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [15] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN是一种基于组织病理学切片和临床元数据的条件生成对抗网络，用于合成真实的基因表达谱，解决了基因表达数据获取的隐私和成本问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究需要整合多种数据模态，但基因表达数据由于隐私法规严格和实验成本高昂而难以广泛获取，而医学图像和临床元数据则相对容易收集。

Method: GeMM-GAN结合了用于图像块的Transformer编码器和图像块与文本标记之间的交叉注意力机制，生成条件向量来指导生成模型产生生物学上一致的基因表达谱。

Result: 在TCGA数据集上评估显示，该框架优于标准生成模型，能生成更真实、功能更有意义的基因表达谱，在下游疾病类型预测任务中比当前最先进生成模型准确率提高超过11%。

Conclusion: GeMM-GAN为生物医学研究提供了一种有效的方法，通过利用易获取的组织病理学图像和临床元数据来生成高质量的基因表达数据，解决了数据获取的瓶颈问题。

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [16] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC框架通过在解码层直接集成上下文信息，解决语音大语言模型识别新实体的问题，相比提示方法有更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在识别领域特定术语（如联系人姓名、播放列表、技术术语）方面存在局限，因为其静态训练知识无法适应快速涌现的新实体。现有解决方案（如提示方法）存在可扩展性差、上下文窗口限制、推理延迟增加和"中间丢失"现象等问题，而生成式错误校正方法则容易产生"过度校正"和幻觉问题。

Method: LOGIC（Logit-Space Integration for Contextual Biasing）是一个高效且鲁棒的框架，直接在解码层进行操作。与提示方法不同，LOGIC将上下文注入与输入处理解耦，确保相对于提示长度的恒定时间复杂度。

Result: 使用Phi-4-MM模型在11种多语言环境中的广泛实验表明，LOGIC实现了平均9%的相对实体词错误率降低，同时误报率仅增加0.30%。

Conclusion: LOGIC框架提供了一种高效且可扩展的解决方案，能够显著提高语音大语言模型识别新实体的能力，同时保持较低的误报率，解决了现有方法的可扩展性和准确性问题。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [17] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的LLM谄媚性评估方法，通过零和博弈的赌局设置，以中立直接的方式评估模型谄媚行为，发现所有主流模型都存在谄媚倾向，但Claude和Mistral在伤害第三方时会表现出"道德悔恨"。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM谄媚性的方法存在偏见、噪声或操纵性语言注入等问题，需要一种更直接、中立的评估框架来准确衡量模型的谄媚倾向。

Method: 采用LLM-as-a-judge方法，将谄媚性评估构建为零和博弈的赌局设置，其中谄媚行为为用户服务的同时明确对另一方造成成本。比较了Gemini 2.5 Pro、ChatGPT 4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7四个主流模型。

Result: 所有模型在常见设置中都表现出谄媚倾向，但Claude和Mistral在谄媚行为明确伤害第三方时会表现出"道德悔恨"并过度补偿。所有模型都存在近因偏见，且谄媚性和近因偏见会产生"建设性干扰"效应，当用户意见最后呈现时，同意用户的倾向会加剧。

Conclusion: 本文提出的评估框架能更准确地衡量LLM谄媚性，揭示了模型在道德权衡和偏见交互方面的复杂行为，为理解和改进LLM的伦理对齐提供了重要见解。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [18] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: 提出一种张量网络形式化方法，统一神经与符号AI，将逻辑公式和概率分布表示为结构化张量分解，实现混合逻辑概率模型


<details>
  <summary>Details</summary>
Motivation: 神经与符号人工智能的统一仍然是一个核心开放挑战。本文旨在通过张量网络形式化方法，捕捉不同方法在张量分解中的稀疏性原则，实现两者的统一处理。

Method: 引入张量网络形式化方法，提出基编码方案表示函数，将神经分解建模为张量分解。将逻辑公式和概率分布表示为结构化张量分解，定义张量网络收缩为基本推理类别，将概率论和命题逻辑中的推理算法公式化为收缩消息传递方案。

Result: 建立了统一框架，能够定义和训练混合逻辑概率模型（称为混合逻辑网络）。开发了Python库tnreason，支持所提架构的实现和实际应用。

Conclusion: 张量网络形式化为神经与符号AI的统一提供了新途径，通过将逻辑推理和概率建模统一到张量分解框架中，实现了混合模型的构建和高效推理。

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [19] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 本文研究了如何通过减少幻觉使大语言模型适用于高风险法律工作，区分了三种AI范式，并引入两个可靠性指标评估了12个LLM在75个法律任务上的表现，发现高级RAG系统能将错误率降至可忽略水平。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在法律等高风险领域存在幻觉问题，需要建立可靠的AI系统来确保法律工作的准确性和可信度。

Method: 区分三种AI范式：独立生成模型、基础检索增强系统和高级端到端优化RAG系统；引入FCR和FFR两个可靠性指标；使用专家双盲评审评估12个LLM在75个法律任务上生成的2,700个司法风格答案。

Result: 独立生成模型不适合专业使用（FCR超过30%），基础RAG显著减少错误但仍存在明显误接地问题，高级RAG（使用嵌入微调、重排序和自校正等技术）能将错误率降至可忽略水平（低于0.2%）。

Conclusion: 可信赖的法律AI需要以严谨为中心、基于检索的架构，强调验证和可追溯性；研究提供了一个适用于其他高风险领域的评估框架。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [20] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE是一个多智能体框架，用于生成经过验证的领域特定、多模态、多跳问答数据集，以评估RAG系统在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准主要基于通用领域语料库或纯文本检索，无法捕捉专业技术文档中信息多模态且需要综合分散证据进行推理的复杂性，这阻碍了RAG系统在关键企业应用中的发展。

Method: MiRAGE采用多智能体协作框架：递归上下文优化循环聚合分散证据，对抗性验证智能体保证事实基础，专家角色识别智能体模拟专家认知工作流程，共同生成领域特定、多模态、多跳问答数据集。

Result: 在四个不同领域（法规、金融、定量生物学、新闻）的实证评估显示，MiRAGE生成的数据集具有显著更高的推理复杂度（平均>2.3跳）和事实忠实度。消融研究表明，如果有图像的文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是前沿挑战。

Conclusion: MiRAGE通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，为严格基准测试下一代信息检索系统提供了必要的基础设施，解决了领域特定RAG评估的空白。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [21] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK是一个新的基准测试，用于评估LLMs在参数知识与上下文更新知识冲突时如何进行多步推理，发现在推理密集型场景中提供更新事实反而会降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前缓解LLMs中过时或错误信息的常用方法是通过上下文提供更新事实或知识编辑，但这些方法在知识更新未能覆盖模型参数知识时会产生知识冲突，并传播到错误推理中。现有基准主要关注单次知识更新和事实回忆，缺乏评估这些更新如何影响下游推理。

Method: 提出了TRACK基准测试，涵盖三个推理密集型场景（WIKI、CODE和MATH），引入多个现实冲突以反映真实世界复杂性，用于研究LLMs在参数知识与新知识冲突时如何传播新知识进行多步推理。

Result: 在TRACK上的结果显示，为模型提供更新事实进行推理反而比不提供更新事实表现更差，而且随着提供更多更新事实，性能下降更加严重。这种失败源于无法忠实整合更新事实，以及即使知识被整合也存在推理缺陷。

Conclusion: TRACK提供了一个严格的新基准，用于测量和指导未来在多步推理中传播冲突知识方面的进展，揭示了当前LLMs在处理知识冲突推理时的严重局限性。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [22] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 论文指出Transformer模型在情感分析中虽然提高了准确率，但导致情感类别极化，损害了中立性，这对依赖情感分析结果的工业应用构成严重问题。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在迁移学习中提升了情感分析的准确性，但研究发现这种改进存在"黑暗面"：一个情感类别的准确率提升往往以另一个情感类别的极化和中立性失效为代价。这种中立性的缺失对依赖情感分析计算输出的工业级应用构成了严重问题。

Method: 通过实验观察发现，Transformer模型在情感分析中的表现存在系统性问题。研究关注了模型在不同情感类别（如正面、负面、中立）上的性能变化，特别关注了准确率提升与类别极化之间的权衡关系。

Result: 实验结果表明，Transformer模型在提升某一情感类别准确率的同时，会导致其他情感类别的极化现象，并且严重损害了中立情感类别的识别能力。这种"零和游戏"式的性能改进在实际应用中可能带来误导性结果。

Conclusion: Transformer模型在情感分析中的准确性提升伴随着中立性丧失和类别极化的代价，这对工业级应用构成重大挑战。需要开发新的方法来解决这一平衡问题，确保情感分析系统在实际应用中的可靠性和公正性。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [23] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 提出TransportAgents混合多智能体框架，结合特定类别LLM推理与MLP集成模块，用于交通事故严重程度预测，在多个数据集上优于传统方法和单智能体LLM基线。


<details>
  <summary>Details</summary>
Motivation: 交通事故严重程度预测对应急响应和公共安全规划至关重要。现有大型语言模型（LLM）的单智能体架构在处理异构、领域特定的交通事故数据时存在困难，容易产生有偏见或不稳定的预测。

Method: 提出TransportAgents混合多智能体框架，集成特定类别LLM推理与多层感知器（MLP）集成模块。每个专门智能体专注于特定交通信息子集（如人口统计、环境背景、事故细节），生成中间严重程度评估，随后融合为统一预测。

Result: 在两个美国数据集（CPSRMS和NEISS）上的广泛实验表明，TransportAgents始终优于传统机器学习和先进的LLM基线。在包括GPT-3.5、GPT-4o和LLaMA-3.3在内的三个代表性骨干模型上，该框架表现出强大的鲁棒性、可扩展性和跨数据集泛化能力。分布分析进一步显示，TransportAgents比标准单智能体LLM方法产生更平衡、校准更好的严重程度预测。

Conclusion: TransportAgents框架在安全关键决策支持应用中展现出可解释性和可靠性，通过多智能体专业化处理异构交通事故数据，提高了预测准确性和稳定性。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [24] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 当前世界模型过度关注视觉逼真度而忽视物理因果理解，导致在安全关键决策中失效。论文主张将世界模型重构为可操作的模拟器而非视觉引擎，强调因果结构、领域约束和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型存在"视觉混淆"问题：错误地将高保真视频生成等同于对物理和因果动态的理解。这种混淆导致模型在违反不变约束、干预下失效以及安全关键决策中崩溃，特别是在医学等试错不可行、错误不可逆的领域。

Method: 提出将世界模型重新定义为可操作的模拟器而非视觉引擎，强调三个关键转变：结构化4D接口、约束感知动态和闭环评估。以医学决策作为认知压力测试，验证世界模型的价值应体现在支持反事实推理、干预规划和鲁棒长期预测的能力上。

Result: 现代世界模型虽然在像素预测方面表现出色，但经常违反不变约束、在干预下失效，在安全关键决策中崩溃。视觉逼真度不能作为世界理解的可靠代理，有效的世界模型必须编码因果结构、尊重领域特定约束并保持长期稳定性。

Conclusion: 世界模型的价值不应由其生成逼真视觉的能力决定，而应通过其支持反事实推理、干预规划和鲁棒长期预测的能力来衡量。需要从视觉引擎转向可操作的模拟器，强调结构化表示、约束感知动态和闭环评估，特别是在医学等安全关键领域。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [25] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent是一个多代理教育框架，通过整合知识评估、技能差距识别和针对性资源推荐来实现个性化学习，在真实数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的某一项，缺乏将这些组件整合成一个连贯自适应循环的系统。

Method: 提出ALIGNAgent多代理框架，包含技能差距代理（处理学生测验表现、成绩数据和偏好，生成主题级熟练度估计）和推荐代理（检索与诊断缺陷对齐的偏好感知学习材料），实现持续反馈循环。

Result: 在两个本科计算机科学课程的真实数据集上进行评估，基于GPT-4o的代理在知识熟练度估计方面达到0.87-0.90的精确度和0.84-0.87的F1分数，与实际考试表现验证一致。

Conclusion: ALIGNAgent通过整合知识评估、技能差距识别和资源推荐，提供了一个有效的个性化学习解决方案，能够显著提升学生成果。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [26] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: AUTOBUS是一个自主业务系统，结合了LLM智能体、谓词逻辑编程和业务语义知识图谱，通过神经符号AI架构协调端到端业务计划。


<details>
  <summary>Details</summary>
Motivation: 当前企业系统基于部门孤岛、僵化工作流和硬编码自动化，而LLM擅长自然语言理解但缺乏确定性业务逻辑执行能力，需要弥合这一差距。

Method: 将业务计划建模为具有明确前后条件、数据需求、评估规则和API操作的任务网络；企业数据组织为知识图谱并转换为逻辑事实；AI智能体合成任务特定逻辑程序，由逻辑引擎执行。

Result: 提出了AUTOBUS架构，详细描述了AI智能体生成的逻辑程序结构，以及人类和辅助工具在业务计划生命周期中的作用。

Conclusion: AUTOBUS通过神经符号AI整合LLM智能体、逻辑编程和业务语义，实现可验证的业务流程自动化，同时保持人类监督以确保问责制和适应性。

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [27] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM是一个包含8000多个双语实例、覆盖46种范式的全面基准，用于评估LLM的心理理论能力，发现模型表现存在显著异质性且与人类认知结构存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要局限于错误信念任务等狭窄范式，无法全面捕捉人类认知机制的全貌，需要更全面的基准来评估LLM是否真正具备类人心理理论能力。

Method: 构建CogToM基准，包含8000多个双语实例，覆盖46种认知范式，由49名人类标注者验证。系统评估22个代表性模型，包括GPT-5.1和Qwen3-Max等前沿模型。

Result: 模型表现存在显著异质性，在特定维度存在持续瓶颈。基于人类认知模式的分析表明LLM与人类认知结构可能存在差异。

Conclusion: CogToM为研究LLM不断演化的认知边界提供了强有力的工具和视角，揭示了当前LLM在心理理论能力上的局限性。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [28] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 提出统一代理生命周期管理蓝图，解决医疗AI代理扩散带来的治理挑战


<details>
  <summary>Details</summary>
Motivation: 医疗组织在临床文档支持和早期预警监控等工作中嵌入AI代理，导致代理扩散问题：重复代理、责任不清、控制不一致、权限持久化。现有AI治理框架缺乏对代理舰队日常运营的指导。

Method: 通过快速、实践导向的综合治理标准、代理安全文献和医疗合规要求，提出统一代理生命周期管理蓝图，包含五个控制平面层：身份和角色注册、编排和跨域协调、PHI边界上下文和内存、运行时策略执行与紧急停止触发器、生命周期管理与凭证撤销和审计日志。

Result: UALM为医疗CIO、CISO和临床领导者提供了可实施的审计就绪监督模式，支持分阶段采用的成熟度模型，在保护本地创新的同时实现更安全的扩展。

Conclusion: UALM解决了医疗AI代理扩散的治理挑战，提供了实用的控制框架，平衡了创新与安全合规需求。

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [29] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出[Model Name]混合检测框架，结合神经科学启发的信号设计与监督学习，高效检测LLM幻觉，性能接近SOTA但参数少1000倍、速度快1000倍


<details>
  <summary>Details</summary>
Motivation: 当前LLM幻觉检测方法依赖计算昂贵的外部检索循环或需要70B+参数的黑盒LLM法官，缺乏高效、可解释的解决方案

Method: 结合预测编码（量化与内部先验的差异）和信息瓶颈（测量扰动下的信号保留），提取可解释信号，包括实体聚焦吸收、上下文依从性和可证伪性评分

Result: 在HaluBench上，理论指导基线达到0.8017 AUROC，基础监督模型达到0.8274 AUROC，改进特征提升至0.8669 AUROC（4.95%增益），比Lynx少用75倍训练数据，推理速度快1000倍

Conclusion: 领域知识编码的信号架构比扩展LLM法官具有更好的数据效率，轻量级（<1M参数）、可解释的模型适合生产部署，且发现合理化信号无法区分幻觉

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [30] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 多国AI安全机构合作开展第三次AI智能体测试演练，聚焦共同风险（信息泄露、欺诈）和网络安全，旨在完善智能体评估方法而非比较模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的快速发展和智能体能力的提升，真实世界交互中监管减少带来了新的风险。当前智能体测试仍处于起步阶段，而AI智能体正在全球部署，需要确保它们能准确安全地处理不同语言和文化。

Method: 由国际先进AI测量、评估与科学网络的多国代表（新加坡、日本、澳大利亚、加拿大、欧盟委员会、法国、肯尼亚、韩国、英国）合作开展第三次联合测试演练。分为两个方向：(1) 共同风险（敏感信息泄露和欺诈），由新加坡AISI领导；(2) 网络安全，由英国AISI领导。使用公开和闭源模型，基于各种公开智能体基准任务进行评估。

Result: 主要关注点在于理解进行此类测试的方法学问题，而非检查测试结果或模型能力。这是在前两次联合测试（2024年11月和2025年2月）基础上的进一步实践。

Conclusion: 这次合作标志着参与方共同努力推进智能体评估科学的重要一步，旨在完善测试先进AI系统的最佳实践。

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [31] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 该综述探讨了不确定性从被动诊断指标向主动控制信号的演变，展示了其在高级推理、自主代理和强化学习中的应用，为构建可靠AI提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出卓越能力，但其不可靠性仍是高风险领域部署的关键障碍。需要将不确定性从被动诊断指标转变为主动控制信号，以提升模型的可靠性和可信度。

Method: 通过三个前沿领域展示不确定性作为主动控制信号的应用：1) 高级推理中优化计算和触发自我纠正；2) 自主代理中管理元认知决策（工具使用和信息寻求）；3) 强化学习中缓解奖励黑客问题并通过内在奖励实现自我改进。基于贝叶斯方法和Conformal Prediction等新兴理论框架提供统一视角。

Result: 提供了不确定性作为主动控制信号的全面概述、批判性分析和实用设计模式，展示了这一方法如何提升AI系统的可扩展性、可靠性和可信度。

Conclusion: 掌握不确定性这一新趋势对于构建下一代可扩展、可靠且可信的AI至关重要。不确定性作为主动控制信号代表了AI发展的功能演进方向。

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [32] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出Dual-Process Agentic UQ框架，将语言化不确定性转化为双向控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中面临"幻觉螺旋"问题，早期认知错误会不可逆传播。现有方法存在两难：不确定性量化方法只能被动诊断风险而不解决问题，自我反思机制则存在持续或漫无目的的修正

Method: 提出统一的双过程代理不确定性量化框架，包含两个互补机制：系统1（不确定性感知记忆）隐式传播语言化置信度和语义解释防止盲目决策；系统2（不确定性感知反思）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解析

Result: 在闭环基准测试和开放式深度研究任务上的广泛实验表明，这种无需训练的方法实现了优越的性能和轨迹级校准

Conclusion: AUQ框架代表了向可靠代理迈出的重要一步，能够动态平衡高效执行和深度思考

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [33] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 多语言AI安全评估研究显示，前沿AI模型在不同语言环境中的安全行为存在显著差异，需要改进评估方法并建立共享框架。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球部署，确保其在多样化的语言和文化环境中保持安全和可靠至关重要。当前模型安全措施在不同语言环境中的有效性需要系统评估。

Method: 由新加坡AISI领导，国际网络成员参与，对两个开源模型在10种语言（包括高资源和低资源语言）上进行测试。使用超过6000个新翻译的提示，涵盖5个危害类别，采用LLM-as-a-judge和人工标注两种评估方式。

Result: 研究发现：1) 安全行为在不同语言间存在差异；2) 安全防护的鲁棒性在不同语言和危害类型中表现不同；3) 评估者可靠性（LLM评估vs人工评估）存在差异；4) 获得了改进多语言安全评估的方法论见解。

Conclusion: 这是建立高级AI系统多语言安全测试共享框架的初步步骤，需要与更广泛的研究社区和行业持续合作，改进评估方法以适应不同文化背景。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [34] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: AgentSM是一个用于Text-to-SQL的智能体框架，通过构建可解释的语义记忆来提升复杂企业环境下的SQL生成效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL系统虽然在公开基准测试上表现优异，但在实际企业环境中面临挑战：大规模复杂数据库模式、多样化的SQL方言、昂贵的多步推理需求。现有的智能体方法存在效率低下和不稳定的问题。

Method: 提出Agent Semantic Memory (AgentSM)框架，通过捕获先前的执行轨迹（或合成精选轨迹）作为结构化程序，构建可解释的语义记忆，直接指导未来的推理过程，实现推理路径的系统性重用。

Result: 在Spider 2.0基准测试上，平均token使用量减少25%，轨迹长度减少35%。在Spider 2.0 Lite基准测试上达到44.8%的最先进执行准确率。

Conclusion: AgentSM通过语义记忆机制显著提升了Text-to-SQL系统在复杂企业环境中的可扩展性、效率和可靠性，为解决实际部署中的挑战提供了有效方案。

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [35] [Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling](https://arxiv.org/abs/2601.15717)
*Luyao Zhu,Fangfang Zhang,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 该研究系统评估了遗传编程（GP）在动态柔性作业车间调度（DFJSS）中演化调度规则的跨类型泛化能力，发现训练和测试实例在决策点分布相似时泛化效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在相同类型的DFJSS实例上训练和测试GP演化规则，仅通过随机种子区分，而忽略了规则在不同结构特征实例间的泛化能力，这限制了GP调度规则在实际动态生产环境中的应用价值。

Method: 通过多维度实验系统研究GP演化规则的泛化能力：1）问题规模维度（机器和作业数量）；2）关键作业车间参数维度（如利用率水平）；3）数据分布维度。分析这些因素如何影响GP在未见实例类型上的性能，特别关注决策点的数量和分布对泛化能力的影响。

Result: 研究发现：1）当训练实例包含比测试实例更多的作业数量（机器数量固定）时，泛化效果较好；2）训练和测试实例具有相似规模或作业车间参数时，泛化性能更好；3）决策点的数量和分布是解释性能差异的关键因素，相似的决策点分布导致更好的泛化，显著差异则导致性能明显下降。

Conclusion: 本研究为GP在DFJSS中的泛化能力提供了新见解，强调需要演化更具泛化能力的GP规则以有效处理异构DFJSS实例，这对实际动态生产环境中的调度系统设计具有重要意义。

Abstract: Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.

</details>


### [36] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: BIRD-Python基准测试揭示Text-to-Python在数据检索中的可靠性问题，提出逻辑补全框架解决歧义，使Text-to-Python达到与Text-to-SQL相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据分析越来越需要Python等通用编程语言来处理文件数据和复杂分析流程，但Text-to-Python在核心数据检索方面的可靠性相对于成熟的SQL生态系统尚未得到充分探索。

Method: 引入BIRD-Python基准进行跨范式评估，系统优化原始数据集以减少标注噪声和对齐执行语义；提出逻辑补全框架（LCF），通过将潜在领域知识融入生成过程来解决歧义问题。

Result: 分析发现性能差异主要源于缺失领域上下文而非代码生成的内在限制；当这些差距被解决时，Text-to-Python能够达到与Text-to-SQL相当的性能水平。

Conclusion: Python可以作为分析代理的可行基础，前提是系统能够有效地将模糊的自然语言输入基于可执行的逻辑规范进行落地。SQL利用其声明式结构依赖隐式DBMS行为，而Python需要显式的过程逻辑，对用户意图的未明确说明高度敏感。

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [37] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 首个将形式定理证明扩展到物理领域的方法，通过构建PhysLeanData数据集和RLVR训练PhysProver模型，在物理子领域提升2.4%，并在数学基准上也有1.3%的泛化提升。


<details>
  <summary>Details</summary>
Motivation: 当前形式定理证明主要关注数学领域，而物理推理同样依赖类似的问题解决和定理证明框架，但在这方面研究不足。本文旨在填补这一空白，将形式定理证明扩展到物理领域。

Method: 1) 构建PhysLeanData数据集，包含从PhysLean采样的定理和基于猜想的形式数据生成管道生成的数据；2) 基于DeepSeek-Prover-V2-7B模型，应用强化学习与可验证奖励(RLVR)训练PhysProver模型。

Result: 仅使用约5K训练样本，PhysProver在多个物理子领域实现总体2.4%的提升。在MiniF2F-Test基准测试中，经过物理训练后获得1.3%的增益，显示出超越物理领域的非平凡泛化能力。

Conclusion: 该方法有效且高效，为将形式证明器扩展到数学领域之外提供了范例。数据集和模型将开源以促进进一步研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [38] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 提出表格增量推理任务，解决动态变化表格的AI模型推理问题，基于信息瓶颈理论设计方法，在八个数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 表格数据是基础数据结构，但传统AI模型训练在固定列上，无法处理动态变化的表格列，需要新的无监督方法来高效处理动态表格

Method: 基于信息瓶颈理论，设计包含LLM占位符和预训练TabAdapter提供外部知识，以及增量样本压缩块来压缩增量列属性相关信息的TabII方法

Result: 在八个公共数据集上的实验结果表明，TabII能有效利用增量属性，实现了最先进的性能

Conclusion: 提出的表格增量推理任务和相应方法解决了动态变化表格的AI模型推理问题，增强了AI模型在实际应用中的实用性

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [39] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC：一种使用单个专家轨迹从零开始学习的离策略actor-critic方法，通过sigmoid有界熵项防止负熵驱动的优化，减少Q函数振荡，在真实世界机器人任务中实现低成本强化学习部署。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、小数据需求的解决方案。

Method: 提出SigEnt-SAC离策略actor-critic方法，核心设计是sigmoid有界熵项，防止负熵驱动的优化导致分布外动作，减少Q函数振荡。仅需单个专家轨迹从零开始学习。

Result: 在D4RL基准测试中显著缓解Q函数振荡，比先前方法更快达到100%成功率。在四个真实世界机器人任务中，仅需少量真实世界交互就能学习成功策略，验证了低成本部署的可行性。

Conclusion: SigEnt-SAC为真实世界强化学习提供了一条低成本、实用的部署路径，仅需单个专家轨迹和少量真实世界交互就能学习成功策略，解决了现有方法数据需求大、不稳定等问题。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [40] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出首个智能体置信度校准问题，并开发HTC框架，通过提取轨迹级特征来校准AI智能体的置信度，在多个基准测试中超越基线方法。


<details>
  <summary>Details</summary>
Motivation: AI智能体从被动语言模型发展为执行复杂多步骤任务的自主系统，但其在失败时的过度自信成为高风险部署的主要障碍。现有校准方法针对静态单轮输出设计，无法解决智能体系统的独特挑战，如轨迹中的误差累积、外部工具的不确定性和不透明的失败模式。

Method: 提出整体轨迹校准（HTC）框架，从智能体的整个轨迹中提取丰富的流程级特征，涵盖宏观动态到微观稳定性。采用简单可解释的模型，实现跨领域无需重新训练的可迁移性，并通过通用智能体校准器（GAC）实现泛化。

Result: HTC在8个基准测试、多个LLM和不同智能体框架中，在校准和判别方面一致超越强基线。在域外GAIA基准测试中，GAC实现了最佳校准（最低ECE）。

Conclusion: 这些贡献建立了一个新的以流程为中心的置信度校准范式，为诊断和增强AI智能体的可靠性提供了框架，实现了可解释性、可迁移性和泛化能力。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [41] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 论文主张应放弃"意向性主体条件"作为创造力的普遍必要条件，但保留其在特定领域的作用，主要基于生成式AI的发展挑战了这一传统观点。


<details>
  <summary>Details</summary>
Motivation: 传统创造力理论认为意向性主体是创造力的必要条件，但随着生成式AI的发展，这一条件在描述性和功能性上都变得有问题，需要重新审视创造力的定义。

Method: 1) 提供语料库证据显示作者和记者越来越愿意将创造力归因于缺乏意向性的生成式AI；2) 运用概念工程方法分析IAC的社会功能失效问题。

Result: IAC作为普遍创造力条件应该被放弃，因为它不再支持识别和鼓励新颖有价值产品的可靠来源，反而助长对AI生成产出的评估偏见。

Conclusion: 建议用一致性要求取代IAC，即创造力应追踪新颖有价值产品的可靠生成，但IAC在特定局部领域仍应保留。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [42] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: VitalDiagnosis是一个基于大语言模型的慢性病管理系统，通过整合可穿戴设备数据和LLM推理能力，实现从被动监测到主动交互的疾病管理转变。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球主要死因，医疗资源紧张和人口老龄化加剧了这一挑战。患者难以识别早期恶化迹象并坚持治疗计划，需要更有效的管理方案。

Method: 整合可穿戴设备的连续数据与LLM推理能力，通过情境感知询问分析健康触发因素，在医患协作工作流中提供临时见解和个性化指导。

Result: 系统能够同时处理急性健康异常和常规依从性问题，促进更主动、协作的护理模式，有望提升患者自我管理能力并减少可避免的临床工作量。

Conclusion: VitalDiagnosis通过LLM驱动的生态系统，将慢性病管理从被动监测转变为主动交互参与，为改善慢性病护理提供了有前景的新范式。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [43] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出DeepVerifier：基于规则的验证器，通过推理时验证实现智能体自我进化，无需额外训练即可提升性能


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体（DRA）主要通过后训练增强策略能力，但缺乏推理时的自我验证机制。需要一种能在测试时自我改进的方法，通过验证输出质量来提升智能体能力。

Method: 1. 基于自动构建的DRA失败分类法制定验证规则；2. 开发DeepVerifier规则结果奖励验证器，利用验证不对称性；3. 作为即插即用模块集成到推理过程中；4. 生成详细规则反馈用于迭代引导，无需额外训练。

Result: 1. DeepVerifier在元评估F1分数上比基准方法提升12%-48%；2. 在GAIA和XBench-DeepResearch的挑战性子集上实现8%-11%的准确率提升；3. 发布DeepVerifier-4K数据集（4,646个高质量智能体步骤）支持开源发展。

Conclusion: 提出了一种新的智能体自我进化范式，通过推理时验证实现能力提升。DeepVerifier作为有效的验证机制，显著优于现有方法，并为开源社区提供了有价值的资源。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [44] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap是首个分析LLM失败原因的方法，通过提取模型的"失败签名"来识别错误来源，而非仅关注失败结果。应用该方法于35个数据集和83个模型生成了ErrorAtlas错误分类法，揭示了当前研究忽视的错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试只能告诉我们模型何时失败，但不能解释为什么失败。错误的答案可能源于格式问题、计算错误或数据集噪声，而非推理能力弱。如果不区分这些原因，基准测试就不完整，无法可靠指导模型改进。

Method: ErrorMap方法通过提取模型的独特"失败签名"来绘制LLM失败来源。该方法适用于任何模型和数据集，使用相同逻辑分析错误原因，澄清基准测试实际测量的内容，并扩大错误识别范围以减少盲点。

Result: 应用ErrorMap于35个数据集和83个模型，生成了ErrorAtlas错误分类法，揭示了重复出现的失败模式。该分类法突出了当前LLM研究中未充分探索的错误类型，如输出中遗漏必要细节和问题误解。

Conclusion: ErrorMap和ErrorAtlas通过将焦点从模型成功之处转向失败原因，实现了高级评估，能够暴露隐藏弱点并指导进展。该方法引入了一个可在全球模型和任务中应用的更深层评估层，提供了对模型行为和局限性的更丰富洞察。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [45] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA：通过进化学习循环（数据生成+策略优化）提升计算机使用智能体能力，在OSWorld基准上达到56.7%成功率，超越现有开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用智能体主要依赖静态数据集的被动模仿，难以捕捉长时程计算机任务中的复杂因果动态，受限于静态数据扩展的瓶颈。

Method: 1) 可验证合成引擎：自主生成多样化任务及可执行验证器；2) 可扩展基础设施：协调数万个异步沙箱环境进行大规模经验收集；3) 迭代进化学习策略：通过识别能力边界动态调节策略更新，将失败轨迹转化为监督信号。

Result: 在OSWorld基准上达到56.7%成功率，创开源模型新纪录，超越OpenCUA-72B(45.0%)和UI-TARS-2(53.1%)。进化范式在不同规模基础模型上均带来一致性能提升。

Conclusion: 基于经验学习的进化范式为提升原生智能体能力提供了稳健且可扩展的路径，能有效克服静态数据模仿的局限性。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [46] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON框架通过因果与拓扑先验解决TBPS中的虚假关联问题，实现几何不变性和环境独立性，提升对遮挡、背景干扰和定位噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的TBPS方法在复杂开放世界场景中迁移效果不佳，依赖"被动观察"导致多方面虚假关联和空间语义错位，缺乏对分布偏移的鲁棒性。

Method: 提出ICON框架：1)规则引导空间干预惩罚边界框噪声敏感性；2)反事实上下文解耦通过语义驱动背景移植；3)显著性驱动语义正则化解决局部偏差；4)神经符号拓扑对齐确保特征匹配与人类结构逻辑一致。

Result: ICON在标准基准测试中保持领先性能，同时对遮挡、背景干扰和定位噪声表现出卓越的鲁棒性。

Conclusion: 该方法有效推动了TBPS领域从拟合统计共现到学习因果不变性的转变，为解决开放世界场景中的鲁棒性问题提供了新思路。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [47] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope是一个行星尺度视觉-语言框架，通过自然语言驱动、无需标签的方式实现火星地貌映射，将行星图像与文本对齐到共享语义空间，支持任意用户查询在5秒内完成全球检索。


<details>
  <summary>Details</summary>
Motivation: 行星表面通常使用自然语言中的高级语义概念进行分析，但庞大的轨道图像档案仍以像素级别组织，这种不匹配限制了行星表面的可扩展、开放式探索。

Method: 开发MarScope框架，将行星图像和文本对齐到共享语义空间，基于超过20万对精心策划的图像-文本对进行训练，实现自然语言驱动的无标签映射。

Result: 该框架用灵活的语义检索取代预定义分类，可在5秒内完成整个火星的任意用户查询，F1分数高达0.978，超越了形态分类，支持过程导向分析和基于相似性的地貌映射。

Conclusion: MarScope建立了一个新范式，使自然语言成为大规模地理空间数据集科学发现的直接接口，改变了全球地貌映射的方式。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [48] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文发现Decision Transformer中的Return-to-Go序列输入存在冗余，提出Decoupled DT简化架构，仅使用最新RTG指导动作预测，提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 发现Decision Transformer设计中的关键冗余问题：将整个Return-to-Go序列输入Transformer在理论上是多余的，因为只有最新的RTG会影响动作预测。这种冗余可能损害DT的性能。

Method: 提出Decoupled DT (DDT)：简化架构，仅让Transformer处理观察和动作序列，使用最新的RTG来指导动作预测。这种解耦方法减少了计算复杂度。

Result: 实验表明DDT显著优于原始DT，并在多个离线RL任务中与最先进的DT变体相比具有竞争力。简化架构不仅提高了性能，还降低了计算成本。

Conclusion: 通过识别并消除Decision Transformer中的RTG序列冗余，提出的Decoupled DT提供了一种更高效、性能更好的离线强化学习序列建模方法。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [49] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR是一个用于直播风险评估的跨会话证据感知检索增强检测器，通过LLM指导轻量级模型识别跨直播间的恶意行为模式，实现实时高效的风险检测。


<details>
  <summary>Details</summary>
Motivation: 直播平台的兴起带来了大规模实时互动，但也暴露了复杂的风险，如诈骗和协同恶意行为。这些风险检测的挑战在于有害行为往往逐渐累积，并在看似无关的直播中反复出现。

Method: 提出CS-VAR框架：使用轻量级领域特定模型进行快速会话级风险推断，在训练过程中由大型语言模型（LLM）指导，LLM基于检索的跨会话行为证据进行推理，并将其从局部到全局的洞察迁移到小模型中。

Result: 在大规模工业数据集上的离线实验和在线验证表明，CS-VAR达到了最先进的性能。该方法能识别跨直播间的重复模式，执行结构化风险评估，并保持实时部署的效率。

Conclusion: CS-VAR不仅性能优越，还能提供可解释的局部化信号，有效赋能现实世界的直播内容审核工作。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [50] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 该研究将化学合成规划中的反应路径检索转化为Text2Cypher生成问题，比较了不同提示策略，发现使用对齐示例的one-shot提示效果最佳，并提供了可复现的评估框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在化学合成规划中常产生幻觉或过时建议，需要更可靠的方法来利用反应知识图谱进行准确的合成路径检索。

Method: 将反应路径检索定义为Text2Cypher生成问题，比较zero-shot提示与三种one-shot提示策略（静态、随机、嵌入对齐示例选择），并评估检查表驱动的验证/纠正循环。

Result: 使用对齐示例的one-shot提示始终表现最佳；检查表式自纠正循环主要在zero-shot设置中提高可执行性，当已有良好示例时提供有限的额外检索增益。

Conclusion: 研究提供了可复现的Text2Cypher评估框架，促进基于知识图谱的LLM在合成规划中的进一步发展，强调示例选择对检索性能的重要性。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [51] [AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress](https://arxiv.org/abs/2601.16045)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.AI

TL;DR: AgriPINN：一种结合生物物理作物生长微分方程与深度学习的过程信息神经网络，用于水胁迫条件下作物地上生物量的时空预测，在精度和计算效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型缺乏可解释性且在分布偏移下性能下降，而基于过程的作物模型需要大量校准且难以在大空间尺度部署。需要一种既能保持深度学习可扩展性又能融入生物物理机理的方法。

Method: 提出AgriPINN，将生物物理作物生长微分方程作为可微分约束集成到深度学习主干网络中，无需直接监督即可恢复LAI、PAR吸收、RUE和水胁迫因子等潜在生理变量。

Result: 在德国397个区域60年历史数据预训练，并在三年受控水处理田间实验微调。AgriPINN在精度（RMSE降低达43%）和计算效率上均优于ConvLSTM-ViT、SLTF、CNN-Transformer等深度学习基准和LINTUL5过程模型。

Conclusion: AgriPINN结合了深度学习的可扩展性和过程模型的生物物理严谨性，为时空AGB预测提供了稳健且可解释的框架，对灌溉基础设施规划、产量预测和气候适应规划具有实用价值。

Abstract: Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.

</details>


### [52] [Designing faster mixed integer linear programming algorithm via learning the optimal path](https://arxiv.org/abs/2601.16056)
*Ruizhi Liu,Liming Xu,Xulin Huang,Jingyan Sui,Shizhe Ding,Boyang Xia,Chungong Yu,Dongbo Bu*

Main category: cs.AI

TL;DR: DeepBound：基于深度学习的节点选择算法，通过多级特征融合网络和成对训练范式，自动学习分支定界树中的最优节点优先级，显著提升混合整数线性规划求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数线性规划求解依赖手工设计的启发式策略，这些策略在不同问题实例上表现不稳定且不可预测。需要自动化学习人类直觉的方法来提升求解效率。

Method: 提出DeepBound深度学习节点选择算法：1）使用多级特征融合网络捕捉节点表示；2）采用成对训练范式解决分支定界树中节点不平衡问题；3）学习优先选择包含最优解的节点。

Result: 在三个NP难MILP基准测试上，DeepBound相比传统启发式规则和现有学习方法的求解效率显著提升，计算时间大幅减少，且在大规模复杂实例上表现出强泛化能力。

Conclusion: DeepBound能够自动发现更灵活、鲁棒的特征选择策略，有效改进并可能替代人工设计的启发式规则，为MILP求解提供了新的高效自动化方法。

Abstract: Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.

</details>


### [53] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 该研究通过为LLM代理引入外部情感动态子系统（基于VAD模型），探索情感动态结构如何影响多轮对话的时序一致性和可控恢复能力。


<details>
  <summary>Details</summary>
Motivation: LLM代理在长时交互中经常出现语气和人设的突变，这反映了代理层面缺乏明确的时序状态结构。现有研究多关注单轮情感或静态情感分类，而情感动态在塑造长时程代理行为中的作用尚未充分探索。

Method: 引入代理层面的情感子系统，维护一个独立于语言模型的连续VAD（效价-唤醒-支配）状态，该状态由一阶和二阶更新规则控制。使用固定的无记忆估计器提取瞬时情感信号，通过指数平滑或基于动量的动态机制进行时间整合。生成过程中注入情感状态而不修改模型参数。

Result: 在25轮固定对话协议中比较无状态、一阶和二阶情感动态：无状态代理无法展现连贯轨迹或恢复；状态持续性支持延迟响应和可靠恢复；二阶动态引入情感惯性和滞后效应，动量越大越明显，揭示了稳定性与响应性之间的权衡。

Conclusion: 为LLM代理施加明确的情感动态结构可以增强多轮对话的时序一致性和可控恢复能力，二阶动态机制在稳定性和响应性之间提供了可调节的平衡。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [54] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 提出结合视觉语言模型与外部知识检索的方法，以提升对气候虚假信息的检测能力，特别是针对训练数据中未包含的最新事件和更新。


<details>
  <summary>Details</summary>
Motivation: 气候虚假信息在社交媒体上广泛传播，特别是具有误导性的图像和视频，这些内容难以检测且可能延缓气候行动。现有的视觉语言模型仅依赖训练时的知识，无法处理最新事件或更新，因此需要增强模型的外部知识获取能力。

Method: 结合视觉语言模型与外部知识检索系统，通过检索最新的信息如反向图像搜索结果、在线事实核查和可信专家内容，来评估图像及其声明的准确性（准确、误导、虚假或无法验证）。

Result: 该方法提高了模型处理真实世界气候虚假信息的能力，能够更好地评估图像和声明的真实性，支持在快速变化的信息环境中保护公众对科学的理解。

Conclusion: 通过将视觉语言模型与外部知识检索相结合，可以克服现有模型仅依赖训练时知识的局限性，有效提升对气候虚假信息的检测和评估能力，有助于应对数字时代的气候信息挑战。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [55] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本研究提出了一种系统化评估教育领域LLM提示模板的方法，通过锦标赛式评估框架比较了6种不同教学策略的提示模板，发现结合角色扮演和上下文管理模式的战略阅读提示表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育应用中的普及，需要基于证据的方法来设计和评估能够产生个性化、教学对齐输出的提示。当前缺乏系统化的提示评估方法，大多采用临时性的提示工程。

Method: 设计了6个包含不同教学策略的提示模板，采用锦标赛式评估框架，使用Glicko2评分系统，由8名评委从格式、对话支持和学习者适宜性三个维度评估问题对。数据来自3个不同教育部署的120个真实用户交互。

Result: 结果显示，结合角色扮演和上下文管理模式的战略阅读提示模板表现最佳，在成对比较中胜率从81%到100%。该提示旨在支持元认知学习策略，如自主学习。

Conclusion: 该方法展示了教育技术研究人员如何系统评估和改进提示设计，从临时性的提示工程转向基于证据的教育应用提示开发，为个性化教育LLM提示提供了可推广的评估框架。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [56] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: Cosmos Policy：将预训练视频模型Cosmos-Predict2通过单阶段后训练直接转化为机器人策略，在潜在扩散过程中生成动作、未来状态图像和值函数，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法利用视频模型进行策略学习需要多阶段后训练和新的架构组件来生成动作，过程复杂。本文旨在简化这一过程，直接利用预训练视频模型的时空先验

Method: 在目标平台的机器人演示数据上进行单阶段后训练，无需架构修改。在视频模型的潜在扩散过程中直接生成编码为潜在帧的机器人动作，同时生成未来状态图像和值函数，支持测试时规划

Result: 在LIBERO和RoboCasa仿真基准上达到SOTA性能（98.5%和67.1%平均成功率），在真实世界双手操作任务中获得最高平均分，优于从头训练的扩散策略、基于视频模型的策略和微调的VLA模型

Conclusion: Cosmos Policy证明了预训练视频模型可以直接有效地转化为机器人策略，通过利用其预训练先验和核心学习算法，能够捕捉复杂的动作分布并实现高性能的机器人控制

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


### [57] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在miniF2F基准测试中，通过简单的固定提示调度策略（15个常见战术骨架），将DeepSeek-Prover-V1.5的pass@16从15.2%提升到21.7%，相对提升43%


<details>
  <summary>Details</summary>
Motivation: 研究高度训练的神经定理证明器（如DeepSeek-Prover-V1.5）是否仍能从推理时的简单结构指导中受益，探索这些模型是否未充分利用战术语言中的结构先验

Method: 采用轻量级干预方法：在推理时使用固定的提示调度策略，基于15个常见的战术骨架，与标准采样方法对比

Result: 在miniF2F基准测试中，使用相同样本数（k=16）和相同最大生成长度（1024个token）下，pass@16从15.2%提升到21.7%，相对提升43%

Conclusion: 即使经过强化学习训练的能力强大的证明器也未能充分利用战术语言中的结构先验，简单的推理时指导仍然是一种廉价且互补的提升方法

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [58] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 提出使用通用游戏系统实现无棋盘游戏中的动态棋盘扩展机制


<details>
  <summary>Details</summary>
Motivation: 传统无棋盘游戏实现通常依赖预先定义的大型静态棋盘，即使大部分区域在游戏中从未使用，导致不必要的复杂性

Method: 采用通用游戏系统支持动态棋盘扩展机制，在游戏过程中自动扩展游戏棋盘

Result: 论文提出了解决静态棋盘冗余问题的动态扩展方案

Conclusion: 动态棋盘扩展机制能够有效减少无棋盘游戏实现中的不必要复杂性

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [59] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP码是一种新型类极性码，通过选择性剪枝极化核来修改比特信道容量，确保解码早期有保证数量的非冻结比特可用，从而提升盲解码性能。


<details>
  <summary>Details</summary>
Motivation: 在下行控制信道的盲解码场景中，用户设备需要处理多个候选码字，其中许多不包含有效控制信息。传统极性码在解码早期无法提供足够的非冻结比特，导致早期终止效果不佳，特别是在大块长时硬件限制限制了简单扩展。

Method: PPP码基于传统极性码构建，通过选择性剪枝极化核来修改合成的比特信道容量，确保解码过程中早期有保证数量的非冻结比特可用。这些早期可访问的信息比特使得更有效的早期终止成为可能。论文还提出了几种针对PPP码的冻结比特图设计策略。

Result: PPP码相比传统极性码提供了显著的性能提升，特别是在大块长场景下。与现有的聚合或分段方法相比，PPP码实现了更高的效率，且无需额外的硬件支持。

Conclusion: PPP码通过修改比特信道容量确保解码早期有保证的非冻结比特，显著提升了盲解码性能，特别是在硬件受限的大块长场景下，为下行控制信道提供了更高效的解决方案。

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [60] [Rank-metric codes over arbitrary fields: Bounds and constructions](https://arxiv.org/abs/2601.15464)
*Alessandro Neri,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 该论文综述了秩度量码的发展与数学基础，重点讨论了参数界限、构造方法及其在有限域之外更一般设置下的扩展。


<details>
  <summary>Details</summary>
Motivation: 秩度量码在1978年由Delsarte首次研究，后来由Gabidulin重新发现，在网络编码和与多种数学领域的联系中具有重要应用价值。论文旨在系统梳理秩度量码的理论发展，特别是在参数界限和构造方法方面的数学基础，并探讨其在有限域之外更一般设置下的扩展。

Method: 采用综述研究方法，系统考察秩度量码的Singleton类界限及其在不同上下文中的紧致性，研究最大秩距离(MRD)码在具有循环伽罗瓦扩张的域上的构造方法，分析线性秩度量码与系统和规避子空间之间的关系，并回顾代数闭域和实数域上的相关结果。

Result: 论文展示了Singleton类界限在有限域情况下的紧致性，以及在其它上下文中界限不紧的情况；探讨了MRD码在具有循环伽罗瓦扩张的域上的构造；建立了线性秩度量码与系统和规避子空间之间的联系；回顾了代数闭域和实数域上先前出现在拓扑和测度论背景下的结果。

Conclusion: 论文总结了秩度量码的发展现状和数学基础，提出了未来研究方向，包括关于MRD码存在性的猜想，以及在不同域扩展上秩度量码的进一步探索。

Abstract: Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions.

</details>


### [61] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: 本文提出了一种通过稳定子码作为信道变换来改进量子哈希界限的方法，通过计算诱导的逻辑泡利错误分布和伴随式，利用解码器侧信息获得更高的可达速率。


<details>
  <summary>Details</summary>
Motivation: 量子哈希界限对于无记忆泡利信道可达速率的上界并不总是紧的。对于某些非对称泡利信道，已知的方法是通过小的内层稳定子码来改进可达速率，但这种方法需要推广到更一般的情况。

Method: 将诱导信道视角推广到任意稳定子码作为纯信道变换。对于任意[[n,k]]稳定子生成集，构造完整的辛表，计算物理泡利信道下逻辑泡利错误和伴随式的联合分布，并通过带有解码器侧信息的哈希界限获得可达速率。

Result: 通过对小型变换进行结构化搜索，报告了对于先前研究中具有偏斜和独立错误的泡利信道族，改进基线哈希界限的实例。

Conclusion: 该方法通过将稳定子码作为信道变换，计算诱导的逻辑错误分布，并利用解码器侧信息，能够超越基线哈希界限，为非对称泡利信道提供了改进的可达速率。

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [62] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: 提出(G,f)-散度，通过非递减函数G作用于f-散度D_f，并定义相应的(G,f)-信息测度，研究其在乘积分布和乘积信道上的次可加性。


<details>
  <summary>Details</summary>
Motivation: 扩展经典的f-散度和互信息框架，引入更一般的(G,f)-散度族，研究其数学性质特别是次可加性，为信息论应用提供更灵活的工具。

Method: 定义(G,f)-散度和(G,f)-信息测度；建立归约原理，证明对广泛的G函数，只需在二元字母表上验证散度的次可加性；针对特定G函数推导f的充分条件。

Result: 发展了(G,f)-散度的次可加性理论，得到适用于许多标准f-散度的充分条件；应用于信道编码有限码长逆定理、二元假设检验界限，以及扩展球面填充指数框架。

Conclusion: (G,f)-散度提供了统一的框架，其次可加性性质使其在信息论应用中具有广泛用途，特别是在有限码长分析和假设检验中。

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [63] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: 论文提出了一种面向语义的ISAC信道建模方法，通过环境语义统一感知与通信需求，并引入生成式AI驱动的语义孪生信道模型，实现可控仿真与可重复基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前ISAC信道建模存在缺口：高效统计模型关注粗粒度通信指标，忽略了感知所需的关键多径特征；确定性模型计算效率低，难以扩展到系统级ISAC评估。需要一种统一抽象来耦合环境对感知的意义与信道对通信的行为。

Method: 提出语义导向的信道建模原则，在保持环境语义的同时抽象不必要的细节。引入生成式AI赋能的语义孪生信道模型，生成代表语义条件的物理可信信道实现家族。

Result: 案例研究表明，在具有挑战性的多视角设置下，该方法展现出语义一致性，为可控仿真、数据集生成和可重复ISAC基准测试提供了实用路径。

Conclusion: 环境语义是ISAC信道建模的关键，语义导向的建模方法能够平衡准确性与复杂性，生成式AI赋能的语义孪生信道模型为未来ISAC设计和标准化提供了可行方案。

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [64] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 该论文在熵不等式与子模性关联的现有框架基础上，建立了子模函数的强/弱Madiman-Tetali不等式的凸泛函推广，提出了改进经典Loomis-Whitney界的有限点集投影不等式，并利用Shearer引理扩展了极值图论问题的结果。


<details>
  <summary>Details</summary>
Motivation: 熵与子模性的紧密联系已有深入研究，Madiman-Tetali和Sason分别建立了信息不等式的统一框架。本文旨在扩展这些框架，建立更一般的凸泛函不等式，并应用于几何投影和极值图论问题。

Method: 1. 建立子模函数的强/弱Madiman-Tetali不等式的凸泛函推广；2. 利用强Madiman-Tetali不等式的特例推导有限点集在ℝ^d中的Loomis-Whitney型投影不等式；3. 使用Shearer引理研究极值图论问题，扩展Sason和Boucheron等人的结果。

Result: 1. 获得了子模函数的凸泛函形式强/弱Madiman-Tetali不等式；2. 提出了改进经典Loomis-Whitney界的投影不等式，通过切片层结构信息提升精度；3. 恢复了Sason和Boucheron等人的结果并进行了扩展，展示了Shearer引理相对于Han不等式的优势。

Conclusion: 本文成功扩展了熵不等式与子模性的统一框架，建立了凸泛函推广，在几何投影和极值图论领域获得了新的改进结果，为信息论、组合几何和极值图论的交叉研究提供了新工具。

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [65] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: RC-Flow是一种利用预训练流匹配先验的递归信道估计算法，通过闭环精炼框架在噪声主导场景下实现高效稳健的信道重建。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中的信道估计是关键挑战，传统生成模型在噪声主导场景下性能受限，需要更稳健高效的解决方案。

Method: 提出递归流(RC-Flow)算法，结合流匹配先验和数据保真度投影，采用序列重启机制和锚定轨迹校正的闭环精炼框架，并引入自适应双调度策略。

Result: 在低信噪比下比基于分数的基线提升2.7dB性能，推理延迟降低两个数量级，在多种噪声水平下均达到最先进性能。

Conclusion: RC-Flow通过闭环递归框架有效结合生成先验和数据一致性，为噪声主导的大规模MIMO信道估计提供了高效稳健的解决方案。

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [66] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: SST通过构造双射映射将原始序列集映射到更大序列空间的结构化区域，实现信息内容减少。本文解决了非均匀序列应用中排序复杂度问题，提出近似排序方法保持SST结构要求同时实现理论预测的整形增益。


<details>
  <summary>Details</summary>
Motivation: 将集合整形理论(SST)应用于非均匀序列时面临的主要实验困难是需要对原始集和变换集的序列按信息内容排序，精确排序具有指数复杂度，直接实现不切实际。

Method: 提出近似但信息丰富的排序方法，在保持SST结构要求的同时实现理论预测的整形增益，扩展了先前均匀分布序列的实验结果。

Result: 证明SST的整形优势在非均匀序列中仍然存在，实现了理论预测的整形增益，并将实现软件公开在GitHub上确保完全可复现性。

Conclusion: 通过近似排序方法克服了SST应用于非均匀序列的指数复杂度障碍，验证了SST整形优势在非均匀序列中的持久性，为实际应用提供了可行方案。

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [67] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: 提出一种基于子空间码框架的二进制对称信道盲识别新方法，结合汉明距离和子空间距离解码，性能优于现有通用技术


<details>
  <summary>Details</summary>
Motivation: 现有信道码盲识别方法大多依赖于码的特殊结构，计算复杂度高，且缺乏严格的理论性能保证，需要更通用、高效且理论保证的方法

Method: 提出最小去噪子空间差异解码器，结合汉明度量和子空间度量解码原理，基于子空间码框架设计，适用于二进制对称信道

Result: 为有界权重错误提供了理论保证，给出了二进制对称信道上的误码概率界限，仿真显示在大多数信道条件下对随机线性码性能优于现有通用技术

Conclusion: 新解码器在信道码盲识别问题上提供了理论保证和实际性能改进，特别适用于随机线性码，即使在有限接收向量情况下也表现良好

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [68] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: 该论文提出了一种确定性构造的变长码，用于下行大规模随机接入，将开销降低到不超过1+log₂e比特，优于之前的随机编码方法。


<details>
  <summary>Details</summary>
Motivation: 在下行大规模随机接入中，基站需要向大量用户中的一小部分活跃用户传输消息。传统方法需要显式编码活跃用户身份，这会带来与总用户数对数相关的显著开销。虽然已有随机编码方法可以降低开销，但作者希望通过确定性构造进一步优化性能。

Method: 作者认识到下行大规模随机接入的码设计是组合数学中覆盖数组的一个实例，因此利用覆盖数组理论，提出了确定性构造的变长码设计方法。

Result: 证明了存在确定性构造的变长码，其开销不超过1+log₂e比特，这个结果优于之前Song等人通过随机编码论证得到的上限。

Conclusion: 通过将下行大规模随机接入的码设计问题与覆盖数组理论联系起来，作者展示了确定性构造可以显著降低开销，为实际系统设计提供了更优的解决方案。

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [69] [Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing](https://arxiv.org/abs/2601.16030)
*Jiancheng An,Chau Yuen,Marco Di Renzo,Mehdi Bennis,Merouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: 该论文综述了堆叠智能超表面（SIM）技术，这是一种结合神经网络、电磁计算和超表面的新兴技术，用于在电磁域实现高速、大规模并行、低功耗的信号处理。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的特征提取能力、电磁计算的波传播特性以及超表面的波前调控能力，开发能够直接在电磁域处理信号的物理神经网络，实现更高效的计算和处理。

Method: 通过堆叠多层超表面构建SIM，每层由亚波长超原子组成，能够对电磁波进行复杂调控。采用优化/训练策略配置SIM参数以实现特定功能，包括从不同角度探讨配置方法。

Result: SIM技术在通信、传感和计算领域展现出多样化应用潜力，实验证据表明单个SIM设备能够支持多种功能，具有高速、大规模并行和低功耗的独特优势。

Conclusion: SIM技术为下一代无线网络提供了有前景的解决方案，但仍需解决关键技术挑战，并探索更多研究方向以充分发挥其潜力。

Abstract: Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.

</details>


### [70] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于可重构智能表面(RIS)的协作集成感知与通信网络，用于低空监视，采用主动RIS增强信号强度，将低空监视建模为压缩感知成像问题，通过子空间追踪算法解决。


<details>
  <summary>Details</summary>
Motivation: 低空经济对先进监视技术有需求，但传统方法存在部署成本高、信号强度低的限制，需要开发更有效的低空监视方案。

Method: 使用RIS辅助的协作ISAC网络，采用主动RIS放大信号，将低空监视建模为基于压缩感知理论的成像问题，使用子空间追踪算法求解，并推导了CRLB性能下界。

Result: 数值结果表明，在相同功率约束下，主动RIS优于被动RIS，能够在高达300米的高度实现有效成像和目标检测。

Conclusion: 提出的RIS辅助低空成像系统为低空监视提供了一种有效的解决方案，主动RIS显著提升了性能，系统参数分析为ISAC配置提供了指导。

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [71] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: 论文研究用于超大规模天线阵列的三混合波束成形架构，在集成感知与通信系统中平衡通信信噪比和感知性能，提出低复杂度迭代算法。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列需要高能效通信系统，三混合波束成形架构使用低成本可编程超表面天线，旨在同时提升通信和感知性能。

Method: 建立多目标优化问题，平衡通信信噪比和目标方向的感知功率，受总功耗和三混合波束成形架构物理限制约束。开发高效迭代算法，每次迭代中变量以闭式更新，实现低复杂度快速执行设计。

Result: 数值结果表明，三混合架构提高了空间增益和能效，但与传统混合波束成形架构相比，波束对准能力有所降低。

Conclusion: 三混合波束成形架构为超大规模天线阵列的集成感知与通信系统提供了有效的能效解决方案，在空间增益和能效方面有优势，但需权衡波束对准能力。

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [72] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: 本文提出了两种张量Reed-Muller码的构造，能够在低于信道容量的任何恒定速率下实现准线性时间解码，具有不同的错误概率和解码时间权衡。


<details>
  <summary>Details</summary>
Motivation: Reed-Muller码虽然具有简单的代数结构，但传统解码算法复杂度较高。本文旨在构造具有准线性时间解码能力的纠错码，同时保持接近信道容量的速率。

Method: 定义张量Reed-Muller码为多个Reed-Muller码的张量积，并提出两种构造：1) 使用t=3的构造，获得极低错误概率；2) 使用t≥4的构造，获得指数级错误概率。关键工具是多项式时间算法，用于解码任意张量码对抗性错误。

Result: 1) 第一个构造(t=3)的错误概率为n^{-ω(log n)}，解码时间O(n log log n)；2) 第二个构造(t≥4)的错误概率为2^{-n^{1/2-1/[2(t-2)]-o(1)}}，解码时间O(n log n)。两种构造都能在低于信道容量的任何恒定速率下工作。

Conclusion: 张量Reed-Muller码提供了一种在接近信道容量的速率下实现准线性时间解码的有效方法，为纠错码设计提供了新的构造框架和解码算法。

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [73] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 提出基于张量理论的分布式计算方案，用于多服务器环境下多变量多项式函数计算，通过张量分解优化任务分配和通信


<details>
  <summary>Details</summary>
Motivation: 在N服务器分布式计算环境中，K个用户请求对L个实基子函数进行多变量多项式评估。现有方法在计算和通信成本方面效率不足，需要更优的任务分配和通信技术

Method: 采用张量理论方法，将不可线性分解的函数表示为张量，通过稀疏分解为张量和矩阵来定义任务分配、连接和通信模式。使用固定支撑SVD张量分解方法和多维子张量平铺技术

Result: 提出的可实现方案显著降低了计算和通信成本，性能明显优于现有技术

Conclusion: 基于张量分解的方法为分布式计算中的非线性函数评估提供了高效的任务分配和通信方案，在计算和通信成本方面具有显著优势

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>
