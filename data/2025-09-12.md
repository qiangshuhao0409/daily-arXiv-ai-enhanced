<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: dMAP是一个远程测量框架，通过行为指纹识别来区分和聚类DPI设备，解决了DPI设备难以测量的问题。


<details>
  <summary>Details</summary>
Motivation: 全球用户面临日益严重的网络干扰（如审查、限速和拦截），DPI设备的普及使得网络运营商都能进行大规模流量干扰，但我们对DPI设备及其部署的了解仍然有限。

Method: 基于差分模糊测试，dMAP系统地发现、选择和部署专门探针，将DPI内部解析行为转化为外部可观察的指纹。

Result: 应用dMAP到全球DPI部署，证明其实用可行性，仅需20-40个区分性探针就能可靠区分各种DPI实现，包括主要国家审查基础设施和商业DPI产品。

Conclusion: 该指纹方法不仅适用于审查，还可推广到其他形式的针对性干扰。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [2] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 这篇论文是一个关于量约理能人工智能在无线通信网络中应用的综述，重点分析了大语言模型和其他量约理方法如何改善网络优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在处理复杂的多步骤决策问题时缺乏结构化的推理能力，需要量约理能的AI技术来动态优化网络运行。

Method: 通过建立分类系统，层层分析从物理层到应用层的量约理方法，重点研究LLM基于的代理能够结合推理、长期规划、记忆和工具利用。

Result: 识别了各层关键挑战，并询明量约理AI方法如何提升无线通信网络的性能，为下一代无线网络提供技术路径。

Conclusion: 量约理能的AI技术将在未来无线通信网络中发挥关键作用，通过结合通信和AI领域的见解，为集成量约理技术到下一代无线网络排定了路线图。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [3] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于机器学习的框架，用于在O-RAN网络中同时优化负载均衡和能源效率，避免传统方法导致的网络负载失衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AI/ML方案依靠将用户设备移除来实现动态睡眠，这会导致网络负载失衡，影响移除用户的速率性能。需要一种方法在保持相同PRB分配的同时提高网络能源效率。

Method: 提出一种基于多类分类的ML框架，通过预测性评估潜在的RU配置来优化能源效率。将网络条件映射到三个负载均衡类别（良好均衡、中度均衡、失衡），并采用多阈值方法（保守、中等、敏锐）来满足不同的运维优先级。

Result: 使用4.26百万个实际网络测量数据进行实验评估，随机森林模型实现了98.3%的F1-macro性能，比传统基准策略提高了195%。

Conclusion: 该研究提供了一种高效的方法，能够在O-RAN环境中同时实现负载均衡和能源效率的优化，充分利用了O-RAN架构的性能监控能力和xApps的动态网络优化潜力。

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [4] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于软件定义网络(SDN)的量子密码分配(QKD)网络架构，通过虚拟化密钥管理系统(vKMS)和量子安全控制器(QuSeC)来解决QKD网络的可扩展性、中继路径发现和密钥管理挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算对通信安全构成了新挑战，QKD技术能够生成无条件安全的密钥。但当前QKD网络在可扩展性和长距离实施方面仍面临挑战，尤其是中继路径确定问题缺乏有效解决方案。

Method: 集成SDN原理，在每个节点建立高层次虚拟KMS(vKMS)，创建量子安全控制器(QuSeC)。vKMS处理用户密钥请求并抽象KMS查找过程，QuSeC基于网络拓扑视图计算端到端中继路径和应用安全策略。

Result: 论文提供了对建议架构的安全分析，识别了架构的安全等级，分析了核心网络安全属性。

Conclusion: 该方案通过SDN化设计有效解决了QKD网络的可扩展性问题，为长距离量子安全通信提供了可行的网络架构支撑。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [5] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: PARROT是一个可复现的Android应用流量采集系统，用于系统化收集应用流量数据，支持SSL/TLS解密和多种捕获模式。通过对比2021年和2025年的数据集，发现TLSv1.3协议占比从6.7%上升到90.0%，QUIC协议采用率大幅增加，DNS通信从非加密Do53转向加密DoT协议。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议的快速演进和当前数据集的有限性限制了应用流量分析的研究。需要开发一个可复现的流量采集系统来支持系统性研究。

Method: 开发PARROT系统，使用Android虚拟设备进行自动化环境设置，支持可配置的Android版本、流量记录管理和带有人机交互的标记捕获提取。集成mitmproxy进行可选流量解密，支持有/无流量拦截的灵活捕获模式。

Result: 收集了80个应用的流量数据集，对比分析显示：TLSv1.3在TCP加密流量中占比从6.7%(2021)上升到90.0%(2025)；QUIC协议采用率显著增加；DNS通信从91.0%的非加密Do53协议转变为81.1%的加密DoT协议。

Conclusion: PARROT系统为研究社区提供了可复现的应用流量采集能力，揭示了应用安全协议的演进趋势，包括向TLSv1.3、QUIC和加密DNS协议的迁移。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值产生精确输出，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理来处理这种不确定性。

Method: 开发了区间二型版本的贝叶斯定理，使用保守方法避免输入不一致性；提出了将专家区间编码为二型模糊隶属函数的新算法。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够处理专家提供的区间输入，避免了传统方法可能产生的无效输出结果。

Conclusion: 该研究为处理现实世界中不确定性信息提供了有效的贝叶斯推断框架，扩展了计算语言学应用中区间编码的方法。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [7] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 通过精调的LLaMA-3模型和多模态LLM技术，将游戏设计文档自动转换为Unity游戏原型，显著提高了代码生成质量和设计遵循度


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的转换问题，简化游戏开发流程，提高效率

Method: 使用细调的LLaMA-3模型专门用于Unity代码生成，结合自定义Unity集成包，构建了一个从GDD解析到代码生成的结构化流程

Result: 在编译成功率、GDD遵循度、最佳实践和代码模块化等指标上显著优于基线模型，获得4.8/5.0的平均评分，支持多游戏类型

Conclusion: 该框架有效填补了AI辅助游戏开发的关键空白，将LLM定位为从游戏设计向实现迁移的价值工具

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [8] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 通过多个专门化的LLM代理按全局约束类型分解任务，各自检测和生成特定约束代码，最后组装成完整的MiniZinc模型，提高了自然语言描述转换为优化模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这一过程常常存在挑战。需要一种更有效的方法来解决这个问题。

Method: 采用多代理模型框架，每个专门化的LLM代理负责检测和生成特定类型的全局约束代码，最后通过一个组装代理将所有约束集成到完整的MiniZinc模型中。

Result: 初步实验显示，该方法在多个LLM上都表现出比基线方法（如一次提示和思维链提示）更好的性能。

Conclusion: 通过任务分解和专门化代理的方式，可以有效降低复杂度并提高模型转换的准确性，为未来工作提供了丰富的改进方向。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [9] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出了截断交叉熵(TCE)损失函数来缓解生成式AI模型在合成数据上递归训练时出现的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型生成合成数据的比例不断增加，预计到203年大部分训练数据将是机器生成的。在合成数据上重复训练会导致模型崩溃现象，使模型性能逐代退化。现有缓解策略有限，需要新的解决方案。

Method: 识别模型对自生成数据的过度自信是崩溃的关键驱动因素，提出置信度感知的损失函数TCE(Truncated Cross Entropy)，在训练过程中降低高置信度预测的权重。

Result: TCE显著延迟了递归训练中的模型崩溃，可将模型崩溃前的保真度间隔延长2.3倍以上，且该方法在不同模态上都具有良好的泛化性。

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具，置信度感知的损失函数是缓解模型崩溃的有效策略。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [10] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究了可解释AI中的不确定性解释和全局解释，通过测试算法在信任校准方面的能力，探讨了复杂但直观的可视化方法是否能提高用户满意度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI(XAI)已被广泛研究，但不确定性解释和全局解释这两个领域相对较少被关注。研究者希望填补这一空白，探索如何通过算法同时处理不确定性、鲁棒性和全局XAI等多个概念，并测试这些方法在信任校准方面的效果。

Method: 选择了一个能够同时涵盖不确定性、鲁棒性和全局XAI概念的算法进行测试，重点评估该算法在提供直观可视化理解方面的能力，尽管算法本身可能较为复杂。通过实验检验算法是否能有效校准用户信任。

Result: 研究发现，尽管算法可能比较复杂，但通过提供更直观的可视化理解，确实能够提高用户满意度和人类可解释性。算法在信任校准方面表现出良好的能力。

Conclusion: 研究表明，即使算法本身较为复杂，通过精心设计的可视化解释方法，仍然可以有效提高XAI系统的用户满意度和可解释性。这为开发更好的不确定性解释和全局解释方案提供了重要启示。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [11] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优样本注入和指令结构调整显著提升了few-shot场景下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户因缺乏历史行为信息而导致推荐效果不佳的问题，探索如何通过提示工程优化大语言模型在低数据环境下的推荐性能。

Method: 提出上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集，R̂是预测的物品排序列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），采用token级对齐和嵌入空间正则化技术。

Result: 实验证明最优样本注入和指令结构调整能显著提高precision@k和NDCG分数，提示组合不仅影响语法结构，还能直接控制注意力尺度和解码器推理行为。

Conclusion: 基于提示的适配方法可以作为解决基于大语言模型的推荐系统中冷启动问题的一种有效途径，特别是在低数据设置下表现出色。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [12] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 人类、LLM和贝叶斯模型在动态协商任务中的性能对比：贝叶斯模型通过突出优化获得最高盈余，LLM采取保守策略，人类则更具战略性和公平性


<details>
  <summary>Details</summary>
Motivation: 评估自主协调代理在动态多代理环境中的性能和协商过程，比较不同类型代理（人类、LLM、贝叶斯模型）的优势和行为差异

Method: 在动态协商环境中进行直接对比实验，涉及216名人类参与者、GPT-4o、Gemini 1.5 Pro和贝叶斯代理，采集结果和行为动态数据

Result: 贝叶斯代理通过突出优化获得最高盈余，但拒绝交易频繁；人类和LLM能达到相似总体盈余，但行为差异：LLM偏向保守进退策略，人类则更具战略性、风险承受和公平导向

Conclusion: 仅仅基于性能平等的评估标准可能隐藏了过程和对齐方面的根本差异，这对于实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [13] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 该论文提出了一种用于反洗钱(AML)的机器学习管道，通过16步设计和统计分析，在银行客户数据上实现了0.961的AUROC，在竞赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 金融机构的反洗钱行动和措施是优先事项，机器学习在这方面显示出巨大潜力。论文旨在开发一个系统化的ML管道来识别高风险银行客户。

Method: 采用16步设计和统计分析确保管道稳健性；将数据构建在SQLite数据库中；开发基于SQL的特征工程算法；连接预训练模型到数据库；提供可解释AI模块分析特征重要性。

Result: 管道实现了0.961的平均AUROC（标准差0.005），在多伦多大学2023-2024大数据与人工智能竞赛中获得第二名。

Conclusion: 提出的综合系统化方法能够有效识别高风险银行客户，为金融机构的反洗钱工作提供了可靠的机器学习解决方案。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [14] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 本文提出基于神经科学原理的计算框架，通过六个核心模块来提升智能体的空间推理能力，填补当前AI系统在空间智能方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在自主任务执行和语言推理方面取得进展，但空间推理能力有限，主要局限于符号和顺序处理，无法像人类那样在非结构化环境中进行灵活、情境感知的决策。

Method: 从计算神经科学角度审视空间神经模型，提出包含六个计算模块的新框架：生物启发的多模态感知、多感官整合、自我中心-异中心转换、人工认知地图、空间记忆和空间推理。

Result: 建立了基于神经科学原理的空间推理框架，对现有方法进行了框架指导的分析，识别了关键差距，并探索了从虚拟到具身系统的应用领域。

Conclusion: 该工作为研究社区提供了神经科学基础视角和结构化路径，有望推动智能体在动态或非结构化环境中的通用空间推理能力发展。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [15] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和多尺度解码策略，解决了多智能体运动预测中交互关系动态演化的挑战，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了多智能体交互关系的动态演化特性，无法准确捕捉未来场景中不断变化的社会交互，这限制了自动驾驶车辆运动预测的准确性。

Method: 提出渐进式多尺度解码策略ProgD：1）使用动态异构图建模场景，显式捕捉演化中的社会交互；2）设计因子化架构处理时空依赖关系；3）采用多尺度解码逐步消除未来运动不确定性。

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准上也达到最优性能，证明了方法的有效性。

Conclusion: ProgD方法通过动态建模交互演化和多尺度渐进解码，显著提升了多智能体运动预测的准确性，为自动驾驶安全规划提供了更可靠的技术支撑。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [16] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多代理协作监管架构，包含行为追踪、声誉评估和恶意行为预测三个核心模块


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型驱动的自治代理在协作中存在的不可预测性、异构能力和监管挑战

Method: 提出三层架构：代理层、区块链数据层和监管应用层，设计了行为追踪仲裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模代理生态系统建立了可信、弹性和可扩展的监管机制基础

Conclusion: 区块链技术能够为多代理系统提供有效的监管解决方案，为未来研究指明了方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [17] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过从实际Jupyter笔记本提取高质量工具基于数据分析任务，构建NbQA数据集，并使用MCTS搜索框架Jupiter提升多步推理能力，在数据分析任务上达到或超过GPT-4o的性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数据科学工作流中在多步推理和工具使用方面仍遇到困难，限制了处理复杂数据分析任务的能力

Method: 1）从真实Jupyter笔记本提取标准化的任务-解决方案对，构建NbQA数据集 2）提出Jupiter框架，将数据分析形式化为搜索问题，使用蒙特卡罗树搜索（MCTS）生成多样化解决轨迹 3）结合价值模型和节点访问计数高效生成可执行的多步计划

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决了InfiAgent-DABench中77.82%和86.38%的任务，性能达到或超过GPT-4o和先进代理框架，在多步推理任务上显示出更好的泛化能力和工具使用推理能力

Conclusion: 通过真实数据分析任务的标准化和MCTS搜索框架，可以有效提升大语言模型在数据科学工作流中的多步推理和工具使用能力，为自动化数据分析提供了可行的解决方案

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [18] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 知识图构建技术对比研究：测试spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种方法在LLM问答中的表现，发现OpenIE覆盖最广，GraphRAG推理能力最强


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂长文本的主题性和整体理解时有限，需要知识图来加强问答系统的深度分析能力

Method: 进行技术寰比研究，对比spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种开源知识图构建方法，评估它们的效果、可行性和适应性

Result: 实验结果显示OpenIE提供了最全面的三元组覆盖，而GraphRAG在推理能力方面表现最优

Conclusion: 论文讨论了各方法的优缺点，并提供了改进知识图基于问答系统的未来发展方向

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [19] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹用于GRPO策略优化，提出基于树结构优势估计的分阶段训练方法，解决优势饱和和奖励信号崩溃问题


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统用于训练价值/奖励模型的MCTS轨迹重新用于改进基于偏好的强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树结构优势估计设置

Result: 结构化优势估计可以稳定更新并更好反映组合推理质量，但仍面临优势饱和和奖励信号崩溃的挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [20] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但设计多功能、鲁棒且高效的智能体部署平台仍面临重大挑战

Method: 提出了LightAgent框架，集成Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，保持极轻量级结构，完全开源并与主流聊天平台无缝集成

Result: 开发了一个轻量级但功能强大的智能体框架，使开发者能够轻松构建自学习智能体

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了实用的部署解决方案

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [21] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究如何为锦标赛中的获胜者提供认证解释，通过识别最小支持子锦标赛来证明候选者为何获胜，针对多种锦标赛规则分析了最小支持的大小并提出了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选者之间的成对优势关系，需要为"为什么获胜者会赢得锦标赛"这个问题提供形式化的可解释AI解释，帮助理解各种锦标赛规则下的获胜原因。

Method: 识别最小支持子锦标赛，即候选者无论锦标赛其余部分如何完成都保证获胜的最小子锦标赛。针对top cycle、uncovered set、Copeland规则、Borda规则、maximin规则和加权uncovered set等常见锦标赛解决方案进行分析。

Result: 确定了每种规则下最小支持的最小尺寸，提出了多项式时间算法来计算除加权uncovered set外的所有规则的最小支持，证明了加权uncovered set的问题是NP完全的。

Conclusion: 最小支持子锦标赛能够提供紧凑、认证且直观的解释，为锦标赛获胜者的解释问题提供了有效的解决方案，有助于增强锦标赛结果的可解释性和可信度。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [22] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究空间协调三个维度（探索多样性、移动专业化、自适应空间接近度）对团队在受限通信环境下协作搜救任务表现的影响


<details>
  <summary>Details</summary>
Motivation: 许多团队（消防、军事、执法、应急响应）需要在没有视觉线索或充分显性沟通的情况下协调物理空间中的移动，但现有研究主要关注共址同步团队或分布式知识工作协调

Method: 分析34个四人团队（136名参与者）在搜救任务中的空间接近度、分布模式和移动对齐等空间协调指标数据

Result: 空间专业化正向预测绩效，自适应空间接近度呈现边际倒U型关系（适度适应最优），这些指标的时序动态能区分高低绩效团队

Conclusion: 研究揭示了基于角色的团队工作中隐性空间协调机制，强调了平衡自适应策略的重要性，对培训和AI辅助团队支持系统有重要意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [23] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的端到端机器学习代理的多样化、真实结构化基准测试，包含150个AutoML任务，具有浏览器自动化采集、基于排行榜的难度建模和多维度评估框架三大创新点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法充分评估LLM代理在真实环境中的端到端ML工作流能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统从Kaggle等平台自动收集结构化ML挑战；2) 基于排行榜的难度建模机制使用参与人数和分数离散度估计任务复杂度；3) 包含性能、格式合规性、约束遵守和任务泛化能力的多维度评估框架。

Result: 构建了包含150个AutoML任务的基准测试，分为Lite(18任务)、Medium和Full三个版本，覆盖多种任务类型和数据模态(表格、文本、图像、图、音频)。

Conclusion: TAM Bench提供了一个全面、可扩展的基准测试，能够更好地评估LLM代理在端到端机器学习工作流中的真实能力。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [24] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型的层级奖励函数和课程学习策略，该方法在保持资源效率的同时实现了更高效的语义探索能力


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习方法在语义探索中的不足，包括认知能力有限、探索效率与语义理解的平衡问题，以及导致人工干预的问题

Method: 设计了一种新的深度强化学习架构，重点是集成视觉-语言模型的常识知识，通过层级奖励函数实现；将VLM查询模型化为专门动作，以便战略性地使用；结合课程学习策略确保稳定学习

Result: 实验结果显示，该方法在物体发现率方面取得显著提升，能够有效导航到语义丰富区域，并学会了战略性地引导环境信息

Conclusion: 该研究提供了一种实用且可扩展的方法，将常识语义推理嵌入自主机器人，为实现全面智能自主探索提供了新的探索方向

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [25] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLMs利用内部推理能力生成响应，无需人工构建few-shot示例，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型内在推理能力的利用，且任务特定的提示构建成本高且存在不一致性问题

Method: 提出Template-Oriented Reasoning (TORSO)方法，激发模型利用内部推理能力生成适当响应，无需手动构建few-shot示例

Result: 实验结果表明TORSO在多样化的LLMs基准测试中实现了强劲性能，并生成了合理的推理过程

Conclusion: TORSO是一种有效的方法，能够在不依赖人工few-shot示例的情况下，引导LLMs发挥其内在推理能力来解决复杂问题

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [26] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 这篇技术报告分析了大语言模型在法律领域中的"u5e7d灵现象"（错误信息）问题，探讨了RAG策略的限制效果和局限性，并建议采用以验证性和可追溯性为优先的"u54a8询式"AI模式


<details>
  <summary>Details</summary>
Motivation: 解决LLM在法律应用中产生假信息的问题，评估现有缓解策略的有效性，探索更好的解决方案

Method: 分析幽灵现象的原因和表现形式，评估RAG策略的效果和局限性，提出整体优化建议

Result: 发现RAG策略有限但存在局限性，人类监督是不可或缺的，需要改变AI应用范式

Conclusion: 解决方案不在于氐步改进生成模型，而是应采用"u54a8询式"AI范式，以验证性和可追溯性为优先，作为扩大专业判断的工具而非替代人类判断

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [27] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中内存管理的噪声积累、内存膨胀和泛化限制问题。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互数据，现有基于向量检索和分层存储的方法存在噪声积累、内存膨胀不可控和跨域泛化能力有限的问题。

Method: SEDM框架包含：1）基于可重现回放的可验证写入准入机制；2）根据经验效用动态排序和整合条目的自调度内存控制器；3）抽象可重用见解的跨域知识扩散机制。

Result: 在基准数据集上的评估显示，SEDM相比强内存基线提高了推理准确性，同时减少了token开销，并能将从事实验证中提取的知识用于增强多跳推理。

Conclusion: SEDM是一个可扩展且可持续的内存机制，适用于开放式多智能体协作，能够将内存从被动存储转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [28] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 该论文探索使用量子变分电路学习组合张量表示，在需要组合泛化的图像描述任务中取得了概念验证结果，性能优于经典组合模型。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具如视觉语言模型缺乏这种能力。先前基于组合张量语义的研究效果不佳，作者推测量子模型更高的训练效率可能改善此类任务的性能。

Method: 将组合张量模型的表示解释在希尔伯特空间中，训练变分量子电路来学习这些表示。使用两种图像编码技术：二进制图像向量的多热编码(MHE)和基于CLIP视觉语言模型的图像向量的角度/幅度编码。

Result: 使用噪声MHE编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍优于经典组合模型。

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在使用适当编码技术时，为量子机器学习在自然语言处理中的应用提供了有前景的方向。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [29] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并提供受控的流水线并行化，显著提升具身AI系统的推理频率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 具身AI系统在动态环境中运行，需要处理高频输入输出需求。传统的顺序计算模式虽然能保证准确性，但难以满足实际应用所需的"思考"频率。

Method: Auras框架将感知和生成模块解耦，为它们提供受控的流水线并行化。通过建立公共上下文让感知和生成共享信息，解决并行化增加时出现的数据陈旧性问题。

Result: 实验结果显示，Auras平均提升吞吐量2.54倍，同时达到原始准确率的102.7%，在保持准确性的同时显著提高了性能。

Conclusion: Auras有效克服了顺序计算的限制，为具身AI代理提供了高吞吐量的推理能力，证明了算法-系统协同设计在优化推理频率方面的有效性。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [30] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 论文研究发现，大语言模型在长序列任务中的执行能力比单步准确性更重要，模型规模扩大能显著提升多步执行能力，但存在自我条件效应导致错误累积，而思维模型能避免这个问题。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在长序列任务中的表现，探索为什么模型在简单任务变长时会失败，以及如何通过提升执行能力来解决复杂推理问题。

Method: 通过隔离执行能力，明确提供解决长序列任务所需的知识和计划，测试不同规模模型在多步任务中的表现，并分析错误模式和自我条件效应。

Result: 发现大模型能正确执行更多步骤，即使小模型单步准确率100%；模型单步准确率随步骤增加而下降，存在自我条件效应；思维模型能避免自我条件效应，单次执行更长任务。

Conclusion: 执行能力是长序列任务成功的关键，模型规模扩展和序列测试时计算能带来巨大收益，思维模型在长任务执行方面表现优异。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [Improved Receiver Chain Performance via Error Location Inference](https://arxiv.org/abs/2509.08869)
*Michael Greenwood,Robert Hunter*

Main category: cs.IT

TL;DR: 使用机器学习模型估计字节级删除概率，在RS解码前标记删除，提升数据恢复能力而无需改变太空舱硬件


<details>
  <summary>Details</summary>
Motivation: 现代太空舱通信系统依靠伪造锐码方案，需要在不改变硬件和编码标准的前提下提升效能

Method: 在解码端使用机器学习模型估计接收数据帧中字节级的误码概率，并将这些估计用于在RS解码前标记删除

Result: 在信号权化条件下实现了收数据恢复能力的提升，获得0.3分贝的增益

Conclusion: 该方法能够在不改变现有太空舱通信系统硬件和标准的情况下，通过智能化的删除标记技术有效提升锐码性能

Abstract: Modern spacecraft communication systems rely on concatenated error correction
schemes, typically combining convolutional and Reed-Solomon (RS) codes. This
paper presents a decoder-side method that uses a machine learning model to
estimate the likelihood of byte-level corruption in received data frames. These
estimates are used to mark erasures prior to RS decoding, enhancing its
correction capacity without requiring changes to spacecraft hardware or
encoding standards. The approach enables improved data recovery under degraded
signal conditions at a gain of 0.3 decibels.

</details>


### [32] [Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?](https://arxiv.org/abs/2509.09411)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Farshad Rostami Ghadi,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本文探讨在流体天线系统(FAS)中使用颜色相关矩阵替代以往的系数相关矩阵来计算高斯套套的优势，并在Nakagami-m衰落下验证了其更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用Jake模型的通道系数相关矩阵来近似高斯套套的协方差矩阵，但本文认为应该使用通道颜色相关矩阵，因为多元正态随机变量是通过变换相关的通道颜色生成的。

Method: 在完全相关的Nakagami-m衰落下，探索使用颜色级相关矩阵的优势，并开发了一种生成这种衰落通道的方法用于Monte Carlo模拟，作为验证理论结果的基准。

Result: 模拟结果证实了所提出的通道建模方法的有效性，并证明了使用颜色级相关矩阵具有更高的准确性，特别是在稀疏端口部署和低断断时制下。

Conclusion: 使用颜色相关矩阵在流体天线系统性能评估中比使用系数相关矩阵更为准确，特别在特定场景下显示出显著优势。

Abstract: Gaussian copula has been employed to evaluate the outage performance of Fluid
Antenna Systems (FAS), with the covariance matrix reflecting the dependence
among multivariate normal random variables (RVs). While prior studies
approximate this matrix using the channel coefficient correlation matrix from
Jake's model, this work instead employs the channel envelope correlation
matrix, motivated by the fact that the multivariate normal RVs are generated by
transforming correlated channel envelopes. This raises an open question of
whether using the coefficient- or envelope-level correlation matrix yields
better accuracy in accessing FAS performance. Toward this end, this paper
explores the benefits of using the envelope-level correlation matrix under
fully correlated Nakagami-m fading, and develops a method for generating such
fading channels for Monte Carlo simulations, which serve as a benchmark for
validating the theoretical results. Simulation results confirm the
effectiveness of the proposed channel modeling approach and demonstrate the
superior accuracy of using the envelope-level correlation matrix, particularly
in sparse port deployment and low-outage regime.

</details>


### [33] [Mixture of Semantics Transmission for Generative AI-Enabled Semantic Communication Systems](https://arxiv.org/abs/2509.09499)
*Junjie Ni,Tong Wu,Zhiyong Chen,Yin Xu,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于生成式AI的语义混合传输策略MoS，通过ROI和RONI分区处理实现无线语义通信中带宽资源的优化分配


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的语义通信方法在信道资源利用效率方面存在不足，需要平衡视觉保真度和语义相关性

Method: 在发送端将图像分为感兴趣区域(ROI)和非感兴趣区域(RONI)，分别提取语义信息并分配不同带宽；在接收端使用扩散模型重建完整图像

Result: 实验结果表明适当的ROI-RONI分配至关重要，MoS在ROI的PSNR和RONI的CLIP分数方面取得了显著性能提升

Conclusion: MoS策略能够更高效地利用信道资源，在保持语义相关性的同时提升视觉质量

Abstract: In this paper, we propose a mixture of semantics (MoS) transmission strategy
for wireless semantic communication systems based on generative artificial
intelligence (AI). At the transmitter, we divide an image into regions of
interest (ROI) and reigons of non-interest (RONI) to extract their semantic
information respectively. Semantic information of ROI can be allocated more
bandwidth, while RONI can be represented in a compact form for transmission. At
the receiver, a diffusion model reconstructs the full image using the received
semantic information of ROI and RONI. Compared to existing generative AI-based
methods, MoS enables more efficient use of channel resources by balancing
visual fidelity and semantic relevance. Experimental results demonstrate that
appropriate ROI-RONI allocation is critical. The MoS achieves notable
performance gains in peak signal-to-noise ratio (PSNR) of ROI and CLIP score of
RONI.

</details>


### [34] [Fast Polarisation-Aware Decoder for Non-Binary Polar Codes](https://arxiv.org/abs/2509.09554)
*Joseph Jabbour,Ali Chamas Al-Ghouwayel,Emmanuel Boutillon*

Main category: cs.IT

TL;DR: 非二进制极化码低复杂度解码器研究，通过核定制和极化感知减少计算复杂度


<details>
  <summary>Details</summary>
Motivation: 优化非二进制极化码解码器的复杂度，提高解码效率

Method: 提出FSC-PA算法，通过离线分析定制每个核，最小化具有相同输入极化水平的奇偶检查节点的计算负荷

Result: 与现有最优算法相比，计算复杂度减少60%域加法和30%实数加法，性能损失仅为0.2dB

Conclusion: FSC-PA算法能够在几乎无性能损失的情况下显著降低非二进制极化码解码器的复杂度

Abstract: The paper investigates the emerging field of low-complexity non-binary polar
code (NB-PC) decoders. It shows that customizing each kernel of an NB-PC
decoder through offline analysis can significantly reduce the overall decoding
complexity. The proposed decoder, referred to as the Fast Successive
Cancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing
the computational load of parity-check nodes that share the same level of input
polarization. The NB polar decoder is developed for both BPSK and CCSK
modulations. Compared to the state-of-the-art extended min-sum algorithm, the
FSC-PA algorithm achieves an overall reduction of 60 percents in field
additions and 30 percents in real additions, while incurring only a negligible
performance loss (less than 0.2 dB degradation).

</details>


### [35] [RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems](https://arxiv.org/abs/2509.09644)
*Chunjie Wang,Xuhui Zhang,Wenchao Liu,Jinke Ren,Shuqiang Wang,Yanyan Shen,Kejiang Ye,Kim Fung Tsang*

Main category: cs.IT

TL;DR: 提出RIS赋能的智能交通系统数据收集增强框架，通过联合优化RIS相移、功率分配、计算资源和时隙分配，最大化最小处理数据量


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中数据收集和处理效率问题，利用RIS技术增强通信性能，提高交通数据处理能力

Method: 采用混合RSMA和TDMA协议，提出基于交替优化和序列秩一约束松弛的高效迭代算法

Result: 大量仿真表明该算法在不同场景下显著优于基线方案，有效提升了智能交通应用的数据处理性能

Conclusion: 所提框架和算法能够有效增强RIS赋能的智能交通系统的数据收集和处理能力，为智能交通应用提供了有效的解决方案

Abstract: This paper investigates the data collection enhancement problem in a
reconfigurable intelligent surface (RIS)-empowered intelligent consumer
transportation system (ICTS). We propose a novel framework where a data center
(DC) provides energy to pre-configured roadside unit (RSU) pairs during the
downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid
rate-splitting multiple access (RSMA) and time-division multiple access (TDMA)
protocol to transmit the processed data to the DC, while simultaneously
performing local data processing using the harvested energy. Our objective is
to maximize the minimal processed data volume of the RSU pairs by jointly
optimizing the RIS downlink and uplink phase shifts, the transmit power of the
DC and RSUs, the RSU computation resource allocation, and the time slot
allocation. To address the formulated non-convex problem, we develop an
efficient iterative algorithm integrating alternating optimization and
sequential rank-one constraint relaxation methods. Extensive simulations
demonstrate that the proposed algorithm significantly outperforms baseline
schemes under diverse scenarios, validating its effectiveness in enhancing the
data processing performance for intelligent transportation applications.

</details>
