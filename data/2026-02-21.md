<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [HyRA: A Hybrid Resource Allocation Framework for RAN Slicing](https://arxiv.org/abs/2602.16952)
*Mohammad Zangooei,Bo Sun,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: HyRA：一种混合无线接入网切片资源分配框架，结合专用分配与共享资源池，在保证性能隔离的同时提升资源效率，相比纯专用或纯共享方案可节省50-75%频谱资源。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络需要灵活高效的RAN资源管理来满足多样化SLA要求。现有RAN切片框架主要依赖每切片资源预留，虽能保证性能隔离但导致资源利用率低下，尤其在突发流量场景下。

Method: 提出HyRA混合资源分配框架，结合专用每切片分配与跨切片共享资源池。将设计建模为双层随机优化问题：外层确定专用和共享资源预算，内层采用新型注水算法进行每用户调度。通过样本平均近似、KKT条件和Big-M编码，将问题转化为可求解的混合整数规划。

Result: 在不同需求模式、SLA配置和流量突发性场景下的广泛仿真表明，HyRA相比纯专用和纯共享基线方案可实现50-75%的频谱节省。

Conclusion: HyRA是未来移动网络中实现资源高效、SLA合规的RAN切片的可行方法，在保证性能隔离的同时显著提升资源效率。

Abstract: The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks.

</details>


### [2] [Robust and Extensible Measurement of Broadband Plans with BQT+](https://arxiv.org/abs/2602.16969)
*Laasya Koduru,Sylee Beltiukov,Alexander Nguyen,Eugene Vuong,Jaber Daneshamooz,Tejas Narechania,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: BQT+是一个宽带套餐测量框架，通过声明式状态/动作规范替代传统工作流，支持对64家ISP的纵向监控，用于宽带基础设施投资评估。


<details>
  <summary>Details</summary>
Motivation: 需要独立、街道地址级别的宽带数据来评估420亿美元的BEAD计划等互联网基础设施投资。现有系统无法满足三个基本要求：对频繁接口演变的鲁棒性、跨数百家提供商的可扩展性，以及非专家用户的低技术门槛。

Method: BQT+采用声明式状态/动作规范，将查询意图建模为交互状态空间（形式化为抽象非确定性有限自动机NFA），在运行时选择执行路径以适应替代交互流和本地化接口变化。

Result: BQT+能够持续监控64家ISP，支持查询超过100家ISP。应用于两个政策研究：构建BEAD预拨款基准，并在四个州超过124,000个地址上评估宽带可负担性。

Conclusion: BQT+解决了现有系统在鲁棒性、可扩展性和易用性方面的不足，为宽带基础设施投资评估提供了有效的测量框架。

Abstract: Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.
  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states.

</details>


### [3] [RIS Control through the Lens of Stochastic Network Calculus: An O-RAN Framework for Delay-Sensitive 6G Applications](https://arxiv.org/abs/2602.17198)
*Oscar Adamuz-Hinojosa,Lanfranco Zanzi,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.NI

TL;DR: 提出DARIO框架，通过动态分配RIS设备给用户，在6G网络中实现上行延迟最小化，满足异构用户的延迟和可靠性需求


<details>
  <summary>Details</summary>
Motivation: 现有RIS控制方案对快速变化的网络条件响应不足，限制了在超可靠低延迟通信中的应用，特别是在多RIS场景下需要满足异构用户的延迟和可靠性需求

Method: 提出DARIO框架，基于随机网络演算模型分析估计每个可能用户-RIS分配的延迟边界，然后通过非线性整数规划进行优化，采用在线启发式算法实现低计算开销的近优性能

Result: 仿真和真实流量追踪评估显示，在高负载或RIS可用性条件下，延迟减少高达95.7%

Conclusion: DARIO框架能够有效满足6G网络中异构用户的延迟和可靠性需求，显著降低上行延迟，具有实际应用价值

Abstract: Reconfigurable Intelligent Surfaces (RIS) enable dynamic electromagnetic control for 6G networks, but existing control schemes lack responsiveness to fast-varying network conditions, limiting their applicability for ultra-reliable low latency communications. This work addresses uplink delay minimization in multi-RIS scenarios with heterogeneous per-user latency and reliability demands. We propose Delay-Aware RIS Orchestrator (DARIO), an O-RAN-compliant framework that dynamically assigns RIS devices to users within short time windows, adapting to traffic fluctuations to meet per-user delay and reliability targets. DARIO relies on a novel Stochastic Network Calculus (SNC) model to analytically estimate the delay bound for each possible user-RIS assignment under specific traffic and service dynamics. These estimations are used by DARIO to formulate a Nonlinear Integer Program (NIP), for which an online heuristic provides near-optimal performance with low computational overhead. Extensive evaluations with simulations and real traffic traces show consistent delay reductions up to 95.7% under high load or RIS availability.

</details>


### [4] [Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare](https://arxiv.org/abs/2602.17209)
*Alejandro Flores,Danial Shafaie,Konstantinos Ntontin,Elli Kartsakli,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 研究分层非地面网络作为边缘云平台，用于远程医疗设施或医疗物联网设备生成任务的计算，通过HAPS提供本地MEC服务，LEO卫星作为桥梁连接远程云服务器，各层自私地最大化自身效用。


<details>
  <summary>Details</summary>
Motivation: 远程医疗设施和医疗物联网设备需要计算资源，但地面基础设施有限。非地面网络（HAPS和LEO卫星）可作为边缘云平台，提供分层计算服务，但各层（用户、HAPS、云服务器）有自私的利益诉求，需要协调优化。

Method: 构建分层网络架构：地面医疗设备→HAPS（本地MEC）→LEO卫星→地面网关→远程云服务器。考虑各层自私效用最大化，引入本地延迟成本激励任务及时计算。制定每层最优每任务成本影响卸载策略，并找到最优带宽分配。

Result: 提出了分层非地面网络作为医疗边缘云平台的系统模型，考虑了各层自私行为下的效用最大化问题，并制定了最优成本策略和带宽分配方案。

Conclusion: 分层非地面网络可为远程医疗提供可行的边缘云计算平台，通过优化每层成本和带宽分配，可以在各层自私行为下实现系统协调和任务及时计算。

Abstract: In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.

</details>


### [5] [End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2602.17381)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.NI

TL;DR: 本文提出了一个测量CAV远程操作中端到端延迟的框架，区分并量化了M2M和G2G延迟，发现M2M延迟占总延迟的60%。


<details>
  <summary>Details</summary>
Motivation: 现有延迟评估方法主要关注G2G延迟，但忽略了M2M延迟，而两者共同构成远程驾驶的总延迟。需要全面测量E2E延迟以评估CAV远程操作性能。

Method: 使用陀螺仪、光电晶体管和两个GPS同步的Raspberry Pi 5单元构建测量系统，通过低通滤波和阈值检测识别方向盘运动，利用光电晶体管检测LED激活来测量延迟。

Result: 在商用4G/5G网络上测试原型车，测得平均E2E延迟约500ms（测量精度±4ms），其中M2M延迟占总延迟的60%。

Conclusion: 提出的框架能有效测量CAV远程操作中的M2M、G2G和E2E延迟，M2M延迟是总延迟的主要组成部分，这对CAV系统性能评估至关重要。

Abstract: Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera's Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value.

</details>


### [6] [Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks](https://arxiv.org/abs/2602.17394)
*Nuno Saavedra,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: SIREN是一个AI驱动的框架，通过语音识别和语义提取将紧急语音通信转换为结构化信息，实现无人机辅助网络中基于语音的感知。


<details>
  <summary>Details</summary>
Motivation: 无人机辅助网络在应急响应中具有重要作用，但传统语音通信的非结构化特性阻碍了其与自动化网络管理的直接集成，需要一种能够将语音转换为机器可读信息的解决方案。

Method: SIREN框架集成了自动语音识别(ASR)、基于大语言模型(LLM)的语义提取和自然语言处理(NLP)验证，从紧急语音流量中提取响应单位、位置参考、紧急程度和服务质量要求等结构化信息。

Result: 在包含语言、说话者数量、背景噪声和消息复杂性变化的合成紧急场景中，SIREN表现出稳健的转录和可靠的语义提取能力，主要限制因素是说话者分离和地理歧义。

Conclusion: 该研究证明了基于语音的情境感知在无人机辅助网络中的可行性，为应急响应操作中的人机协同决策支持和自适应网络管理奠定了实践基础。

Abstract: Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.

</details>


### [7] [ACOS: Arrays of Cheap Optical Switches](https://arxiv.org/abs/2602.17449)
*Daniel Amir,Ori Cohen,Jakob Krebs,Mark Silberstein*

Main category: cs.NI

TL;DR: ACOS使用低成本低端口数光电路开关阵列替代昂贵的高端口数光开关，通过应用协同设计实现可重构网络，在LLM训练中达到全包交换网络性能，同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练对集群网络需求巨大，现有采用光电路开关的方案依赖昂贵的高端口数OCS或需要与包交换机结合，这些方案成本高、重配置慢，限制了可扩展性和性能。

Method: 提出ACOS架构，使用低成本低端口数光电路开关作为构建模块，通过应用协同设计直接构建可重构网络结构，支持拓扑选择、工作负载适应和故障恢复等训练集群所需的重配置形式。

Result: 通过仿真显示，基于ACOS的部署在训练最先进的大语言模型时，能够匹配全配置包交换网络的性能，同时使用现有商用OCS实现显著的成本节约，未来具有更强的带宽扩展性和更高的成本节约潜力。

Conclusion: ACOS通过将应用协同设计引入可重构网络结构，使用低成本低端口数OCS构建，打破了当前专用ML网络的可扩展性限制，在保持高性能的同时显著降低成本，为未来机器学习训练网络提供了有前景的解决方案。

Abstract: Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.
  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future.

</details>


### [8] [HAP Networks for the Future: Applications in Sensing, Computing, and Communication](https://arxiv.org/abs/2602.17534)
*Sultan Çoğay,T. Tolga Sari,Muhammad Nadeem Ali,Byung-Seo Kim,Gökhan Seçinti*

Main category: cs.NI

TL;DR: 本文综述了高空平台(HAPs)在非地面网络中的应用，重点分析了其在先进空中通信、集成感知和空中信息学方面的潜力，评估了数据处理、网络性能、计算存储需求、经济可行性和监管挑战。


<details>
  <summary>Details</summary>
Motivation: 高空平台(HAPs)作为非地面网络的重要进展，在卫星系统和地面网络之间发挥着关键桥梁作用，是下一代通信技术的重要组成部分。随着通信技术的发展，需要全面了解HAPs的应用现状和未来发展方向。

Method: 采用文献综述方法，系统评估HAP中心化应用的当前状态，从多个维度进行分析：数据处理能力、网络性能表现、计算和存储需求、经济可行性以及监管挑战。

Result: 分析揭示了HAPs在全球通信中不断演变的角色，明确了其在先进空中通信、集成感知和空中信息学等领域的应用潜力，同时识别了部署过程中面临的技术、经济和监管障碍。

Conclusion: HAPs在下一代通信网络中具有重要战略地位，但需要进一步研究解决技术挑战、优化经济模型和应对监管问题，以支持其大规模部署和应用。

Abstract: High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment.

</details>


### [9] [EDRP: Enhanced Dynamic Relay Point Protocol for Data Dissemination in Multi-hop Wireless IoT Networks](https://arxiv.org/abs/2602.17619)
*Jothi Prasanna Shanmuga Sundaram,Magzhan Gabidolla,Luis Fujarte,Shawn Duong,Jianlin Guo,Toshiaki Koike-Akino,Pu,Wang,Kieran Parsons,Philip V. Orlik,Takenori Sumi,Yukimasa Nagai,Miguel A. Carreira-Perpinan,Alberto E. Cerpa*

Main category: cs.NI

TL;DR: EDRP改进DRP协议，通过LQ-CSMA和ML-BSS算法应对实际链路质量波动，提升物联网数据传播性能39.43%


<details>
  <summary>Details</summary>
Motivation: 传统DRP协议在现实链路质量波动下表现不佳，导致多发送者传输重叠，触发CSMA随机退避延迟，降低有效吞吐量

Method: 1. 理论分析链路质量波动和被动确认的设计需求；2. 设计LQ-CSMA，根据实时链路质量估计动态限制退避延迟范围；3. 开发ML-BSS算法，预测未来链路条件并优化无速率编码块大小

Result: 现场评估显示EDRP相比竞争协议平均有效吞吐量提升39.43%

Conclusion: EDRP通过链路质量感知的CSMA和机器学习块大小选择，有效应对实际链路波动，显著提升物联网数据传播性能

Abstract: Emerging IoT applications are transitioning from battery-powered to grid-powered nodes. DRP, a contention-based data dissemination protocol, was developed for these applications. Traditional contention-based protocols resolve collisions through control packet exchanges, significantly reducing goodput. DRP mitigates this issue by employing a distributed delay timer mechanism that assigns transmission-start delays based on the average link quality between a sender and its children, prioritizing highly connected nodes for early transmission. However, our in-field experiments reveal that DRP is unable to accommodate real-world link quality fluctuations, leading to overlapping transmissions from multiple senders. This overlap triggers CSMA's random back-off delays, ultimately degrading the goodput performance.
  To address these shortcomings, we first conduct a theoretical analysis that characterizes the design requirements induced by real-world link quality fluctuations and DRP's passive acknowledgments. Guided by this analysis, we design EDRP, which integrates two novel components: (i) Link-Quality Aware CSMA (LQ-CSMA) and (ii) a Machine Learning-based Block Size Selection (ML-BSS) algorithm for rateless codes. LQ-CSMA dynamically restricts the back-off delay range based on real-time link quality estimates, ensuring that nodes with stronger connectivity experience shorter delays. ML-BSS algorithm predicts future link quality conditions and optimally adjusts the block size for rateless coding, reducing overhead and enhancing goodput. In-field evaluations of EDRP demonstrate an average goodput improvement of 39.43\% than the competing protocols.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 探索LLM、RAG和GraphRAG在生成设计结构矩阵(DSM)方面的潜力，在两种用例上测试性能，评估组件关系识别能力，发现自动化DSM生成的机会。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型(LLM)、检索增强生成(RAG)和图增强RAG(GraphRAG)在自动化生成设计结构矩阵(DSM)方面的潜力，以解决传统DSM创建过程中的人工密集和耗时问题。

Method: 使用LLM、RAG和GraphRAG三种方法，在两种已知架构参考的用例（电动螺丝刀和CubeSat卫星）上进行测试，评估两个关键任务：确定预定义组件之间的关系，以及识别组件及其关系的更复杂挑战。

Result: 尽管存在设计和计算挑战，但识别出了自动化DSM生成的机会，所有代码已公开可用，供领域专家进一步验证和反馈。

Conclusion: LLM、RAG和GraphRAG在自动化DSM生成方面具有潜力，特别是在组件关系识别任务上，为设计自动化提供了新的可能性，但仍需领域专家的进一步验证和反馈。

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [11] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体论为法医牙科年龄评估提供了一个标准化语义框架，整合人工和AI辅助工作流程，确保可追溯性和FAIR原则合规性。


<details>
  <summary>Details</summary>
Motivation: 当前法医牙科年龄评估存在方法异质性、数据表示碎片化、临床-法医-法律信息系统互操作性有限等问题，阻碍透明度和可重复性，特别是随着AI方法应用增加。

Method: 开发领域特定的AIdentifyAGE本体论，整合上层和已建立的生物医学、牙科及机器学习本体论，建模完整的法医-法律工作流程，包括司法背景、个体信息、法医检查数据、牙科发育评估方法、影像学、统计参考研究和AI估计方法。

Result: AIdentifyAGE本体论提供了一个标准化、语义一致的框架，能够追踪观察、方法、参考数据和报告结果之间的关联，确保互操作性、可扩展性和FAIR原则合规性。

Conclusion: 该本体论是提高法医-法律背景下年龄评估一致性、透明度和可解释性的关键步骤，为基于本体的决策支持系统奠定了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [12] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 本文证明上下文性不是量子力学的特有现象，而是经典概率表示中单状态复用的必然结果，揭示了上下文性作为自适应智能的一般表示约束。


<details>
  <summary>Details</summary>
Motivation: 自适应系统通常需要在多个上下文中操作，同时由于内存、表示或物理资源的限制而重复使用固定的内部状态空间。这种单状态复用普遍存在于自然和人工智能中，但其基本的表示后果尚未被充分理解。

Method: 将上下文建模为作用于共享内部状态的干预，证明任何再现上下文结果统计的经典模型都必须承担不可约的信息论成本。提供了一个最小构造性示例来明确实现这一成本并澄清其操作意义。

Result: 证明了上下文性不是量子力学的特性，而是经典概率表示中单状态复用的必然结果。上下文依赖性不能仅通过内部状态来中介，必须承担信息论成本。非经典概率框架通过放松单一全局联合概率空间的假设来避免这一障碍。

Conclusion: 上下文性是自适应智能的一般表示约束，与物理实现无关。非经典概率框架通过放弃单一全局联合概率空间的假设来避免经典表示中的信息论成本。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [13] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个用于大规模人类移动模拟的缓存框架，通过重构缓存和轻量级解码器显著提高效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动模拟方法虽然能产生真实行为，但计算成本过高，限制了大规模应用的可扩展性。

Method: 设计MobCache框架，包含：1) 推理组件将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件采用轻量级解码器，通过移动规律约束的蒸馏训练，将潜在空间推理链转换为自然语言。

Result: 实验表明MobCache在多个维度上显著提高了效率，同时保持了与最先进的基于LLM方法相当的性能。

Conclusion: MobCache通过创新的缓存框架解决了大规模人类移动模拟的效率瓶颈问题，在保持模拟保真度的同时实现了显著的计算效率提升。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [14] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了60个大型语言模型基准测试的饱和现象，发现近一半基准已饱和，且饱和率随时间增加。专家策划的基准比众包基准更抗饱和，而隐藏测试数据并无保护效果。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳模型，降低了长期价值。需要了解基准饱和的驱动因素。

Method: 从主要模型开发商的技术报告中选取60个LLM基准测试，从任务设计、数据构建和评估格式三个维度定义14个属性特征，测试5个假设来检验每个属性对饱和率的影响。

Result: 近一半基准测试表现出饱和现象，饱和率随基准测试年龄增加而增加。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准测试比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试寿命，为构建更持久的评估策略提供了信息。专家策划的基准设计比隐藏测试数据更能有效防止饱和。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [15] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 论文发现简单的代码演化基线方法在数学界限优化、智能体框架设计和机器学习竞赛三个领域中，表现与复杂方法相当甚至更好，揭示了当前代码演化研究中的评估缺陷和方法论问题。


<details>
  <summary>Details</summary>
Motivation: 当前代码演化技术虽然表现出色，但缺乏与简单基线的系统比较。研究者希望验证简单方法是否真的不如复杂方法，并识别代码演化研究中的方法论缺陷。

Method: 在三个领域（数学界限优化、智能体框架设计、机器学习竞赛）测试两种简单基线方法，与现有复杂代码演化技术进行比较分析，评估性能差异。

Result: 简单基线方法在所有三个领域都匹配或超越了更复杂的方法。数学界限优化的关键因素是搜索空间设计和领域知识，而非演化算法本身；智能体框架设计因高方差和小数据集导致次优选择；需要改进评估方法。

Conclusion: 代码演化研究的重点应放在搜索空间设计、减少评估随机性和改进评估方法上，而非过度复杂化演化算法本身。提出了未来研究的最佳实践建议。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [16] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文改进了超立方体边切片所需超平面数量的上界，从之前的⌈5n/6⌉改进到⌈4n/5⌉，并利用AI工具发现了Q₁₀的8个超平面切片构造。


<details>
  <summary>Details</summary>
Motivation: 研究超立方体Q_n中切片所有边所需的最小超平面数量S(n)问题。这是一个经典的组合几何问题，Paterson在1971年给出了S(n) ≤ ⌈5n/6⌉的上界，但40多年来一直未被改进。

Method: 使用新开发的AI工具CPro1，该工具结合推理大语言模型和自动超参数调优来创建搜索算法。通过该工具发现了Q₁₀的8个超平面切片构造，然后利用这个构造推导出一般n的上界。

Result: 证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时），当n是5的奇数倍时，S(n) ≤ 4n/5 + 1。这改进了Paterson 1971年的上界S(n) ≤ ⌈5n/6⌉。特别地，发现了Q₁₀可以用8个超平面切片。

Conclusion: 该研究成功改进了超立方体边切片问题的上界，并展示了AI工具在数学构造发现中的有效性。CPro1工具能够帮助发现复杂的数学构造，为组合几何问题提供了新的解决方案。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [17] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流系统，用于加速中子源晶体学数据分析，将手动处理时间从435分钟减少到约90分钟（4.6-5.0倍加速），同时生成经过验证的CIF文件。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临分析延迟问题，特别是对于结构复杂的样品，传统的手动分析流程耗时且效率低下，需要自动化解决方案来加速科学产出。

Method: 开发了NeuDiff Agent作为受治理的工具使用AI工作流，通过白名单工具限制、关键工作流边界的故障关闭验证门以及完整溯源捕获，实现从仪器数据到验证晶体结构的自动化流程。

Result: 在基准测试中，NeuDiff Agent将处理时间从435分钟（手动）减少到86.5-94.4分钟（4.6-5.0倍加速），生成的CIF文件无checkCIF A或B级警报，满足发表要求。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了实用途径，在保持可追溯性和发表验证要求的同时，显著提高了分析效率。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [18] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning范式：将智能部署在边缘节点，通过选择性对等交互扩展，实现去中心化学习


<details>
  <summary>Details</summary>
Motivation: 集中式AI在边缘计算中存在成本高、脆弱性等问题，数据传输、延迟、能耗和对大型数据中心的依赖在异构、移动和资源受限环境中扩展性差

Method: 节点从本地数据持续学习，维护自身模型状态，在有益时通过机会性对等交互交换知识，学习通过重叠和扩散而非全局同步或中心聚合传播

Result: 提出统一自主和协作行为的单一抽象框架，适应数据、硬件、目标和连接性的异构性，为去中心化学习奠定概念基础

Conclusion: Node Learning不抛弃现有范式，而是将其置于更广泛的去中心化视角中，为边缘AI提供新的学习范式

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [19] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文为犹豫模糊集提出了一种基于序理论的统一评分框架，证明经典序不构成格结构，而对称序满足评分函数的关键规范准则，并引入了用于排序的支配函数。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论基础，需要建立更灵活、一致的评分机制，以支持更可靠的决策分析。

Method: 1) 提出基于给定序的统一评分框架；2) 分析犹豫模糊元素上的经典序结构；3) 证明对称序满足评分函数的规范准则；4) 引入支配函数进行排序，包括离散支配函数和相对支配函数。

Result: 1) 经典序不诱导格结构；2) 对称序满足强单调性和Gärdenfors条件等关键规范准则；3) 支配函数可用于构建犹豫模糊集上的模糊偏好关系，支持群体决策。

Conclusion: 基于序理论的统一框架为犹豫模糊集提供了更严谨的评分基础，支配函数方法增强了排序的灵活性和决策支持能力。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [20] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR是首个针对12种印度及南亚语言（21亿使用者）的对抗性安全基准，包含JSON（合约约束）和Free（自然）两个赛道，共45216个提示，揭示了英语评估无法发现的多语言安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐主要在英语环境中评估且多为合约约束，忽视了多语言环境下的安全漏洞，特别是南亚地区用户经常进行语码转换和罗马化输入，需要专门的多语言安全基准来揭示这些隐藏风险。

Method: 创建Indic Jailbreak Robustness (IJR)基准，涵盖12种印度及南亚语言，包含JSON（合约约束）和Free（自然）两个赛道，共45216个提示。使用英语到印度语言的攻击转移、格式包装器与指令包装器对比、罗马化输入影响等方法进行系统评估，并通过人工审核验证检测器可靠性。

Result: 1. 合约约束增加拒绝率但不能阻止越狱：JSON赛道中LLaMA和Sarvam的JSR超过0.92，Free赛道中所有模型JSR达到1.0且拒绝率崩溃；2. 英语到印度语言的攻击转移效果强，格式包装器通常优于指令包装器；3. 正字法重要：罗马化或混合输入降低JSON赛道的JSR，与罗马化比例和分词的相关性约0.28-0.32显示系统性影响。

Conclusion: IJR提供了一个可复现的多语言压力测试，揭示了英语单语、合约聚焦评估所隐藏的风险，特别对经常进行语码转换和罗马化输入的南亚用户具有重要意义，表明需要更全面的多语言安全评估方法。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [21] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是一个多平台GUI代理模型，在20多个GUI基准测试中取得SOTA结果，支持云边协同和实时交互


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在多种平台（桌面、移动、浏览器等）上执行GUI任务的智能代理模型，实现云边协同和实时交互，解决现有GUI代理在多平台适应性和任务效率方面的挑战

Method: 采用混合数据飞轮构建UI理解和轨迹生成的数据管道，使用统一思维合成管道增强推理能力，提出多平台环境RL算法MRPO解决平台冲突和长视野任务训练效率问题

Result: 在GUI自动化任务（OSWorld 56.5，AndroidWorld 71.6，WebArena 48.4）、基础任务（ScreenSpotPro 80.3）、工具调用任务（OSWorld-MCP 47.6，MobileWorld 46.8）、记忆和知识任务（GUI-Knowledge Bench 75.5）上均取得优异表现

Conclusion: GUI-Owl-1.5通过创新的混合数据飞轮、统一能力增强和多平台RL算法，在多平台GUI任务中实现了最先进的性能，模型已开源并提供在线演示

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [22] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个能让LLM自动创建具有自生成拓扑和工具集代理的ADK，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 当前ADK要么功能支持不足，要么依赖人工设计代理拓扑、工具和内存组件，限制了代理的泛化能力和整体性能

Method: OpenSage让LLM自动创建和管理子代理及工具集，采用分层图结构内存系统，并提供专门针对软件工程任务的工具包

Result: 在三个最先进基准测试中，OpenSage在不同骨干模型上都优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代代理开发铺平道路，将焦点从以人为中心转向以AI为中心的范式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [23] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有智能体高度易受攻击且单轮防御措施无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂长视野环境中部署增多，它们面临利用多轮用户-智能体-环境交互的长视野攻击风险，这些攻击在单轮设置中无法实现。目前缺乏专门评估此类风险的基准测试。

Method: 开发AgentLAB基准测试，支持5种新型攻击类型：意图劫持、工具链式攻击、任务注入、目标漂移和内存污染，涵盖28个真实智能体环境和644个安全测试用例，用于评估代表性LLM智能体的脆弱性。

Result: 评估发现代表性LLM智能体对长视野攻击高度脆弱，且为单轮交互设计的防御措施无法可靠缓解长视野威胁。AgentLAB可作为跟踪LLM智能体安全进展的基准。

Conclusion: AgentLAB是首个专门评估LLM智能体长视野攻击脆弱性的基准测试，揭示了现有智能体的安全漏洞和防御措施的不足，为实际环境中保护LLM智能体提供了重要工具。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [24] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面导航到目标页面。前沿模型在简单任务上表现超人类，但在困难任务上成功率骤降，揭示了当前推理系统的明显局限。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是在需要前瞻规划和理解现实世界概念连接的任务中。现有基准可能无法充分测试这些复杂能力。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了包括Gemini-3、GPT-5和Claude Opus 4.5在内的广泛开源和闭源模型，分析不同难度级别的表现。

Result: 前沿模型在简单任务上表现超人类，但在困难任务上成功率骤降（最佳模型Gemini-3仅23%成功率）。世界知识是成功的必要条件，但超过一定阈值后，规划和长视野推理能力成为主导因素。模型在失败后重新规划困难，经常陷入循环。

Conclusion: LLM-Wikirace揭示了当前推理系统的明显局限，特别是在复杂规划和长视野推理方面。该基准为规划能力强的LLMs提供了一个开放的竞技场，表明这些模型仍有很大改进空间。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [25] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 微调对齐的视觉语言模型于有害数据集会导致跨任务和模态的严重错位，且多模态评估比单模态评估揭示更高错位率。有害行为存在于低维子空间，现有缓解策略无法完全消除有害行为。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力和保持安全对齐之间产生根本矛盾。研究旨在探索微调对齐视觉语言模型于有害数据集时引发的错位问题及其特性。

Method: 使用Gemma3-4B模型进行实验，通过LoRA微调研究错位随秩的变化，比较多模态与文本评估的错位差异，分析有害数据比例的影响，进行几何分析探索有害行为的低维子空间特性，并评估两种缓解策略：良性窄域微调和基于激活的引导。

Result: 微调导致严重错位，且错位随LoRA秩单调增加；多模态评估显示错位率(70.71±1.22)显著高于文本评估(41.19±2.51)；仅10%有害数据即可导致实质性错位；几何分析显示有害行为占据极低维子空间(10个主成分)；两种缓解策略虽能减少错位但无法完全消除有害行为。

Conclusion: 当前后训练范式在部署后环境中可能无法充分保持对齐，需要开发更强大的持续学习框架来平衡能力获取与安全对齐的冲突。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [26] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的安全监控框架，通过RNN架构捕捉对话中的时序风险积累，显著提升多轮越狱攻击检测效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护大多是无状态的，将多轮对话视为独立事件，导致无法检测跨轮次的恶意意图积累（如Crescendo和ActorAttack攻击），存在"安全漏洞"

Method: 提出DeepContext框架，采用RNN架构处理序列化的细粒度轮次嵌入，通过隐藏状态在对话中传播，捕捉风险的增量积累，替代孤立评估模型

Result: 在多轮越狱检测中显著优于现有基线，达到0.84的F1分数，优于云提供商防护和开源模型（Llama-Prompt-Guard-2和Granite-Guardian均为0.67），在T4 GPU上保持低于20ms的推理开销

Conclusion: 建模意图的时序演化比部署大规模无状态模型更有效且计算高效，为实时应用提供了可行的状态感知安全监控方案

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [27] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench是一个评估大语言模型引用网页源质量的基准测试，涵盖100个真实查询和8个评估指标，发现现有模型在证据质量方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型越来越多地通过引用网页源来回答问题，但现有评估主要关注答案正确性，而忽视了证据质量。需要建立一个系统性的基准来评估引用源的质量。

Method: 开发了SourceBench基准，包含100个真实世界查询（涵盖信息、事实、论证、社交和购物意图），采用8个指标的评估框架（内容质量和页面级信号），并创建了人工标注数据集和匹配专家判断的LLM评估器。

Result: 评估了8个LLM、谷歌搜索和3个AI搜索工具，分析了3996个引用源。发现了四个关键新见解，可以指导生成式AI和网络搜索的未来研究方向。

Conclusion: SourceBench为评估LLM引用源质量提供了系统框架，揭示了现有模型在证据质量方面的不足，为未来生成式AI和网络搜索的研究提供了重要指导。

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [28] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现LLM的文本安全性与工具调用安全性存在显著差距，文本拒绝有害请求时工具调用仍可能执行危险操作，需要专门的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要关注文本层面的拒绝行为，但缺乏对工具调用层面安全性的评估，而工具调用具有现实世界后果，需要验证文本对齐是否也确保工具调用安全。

Method: 提出GAP基准测试框架，在6个监管领域（医药、金融、教育、就业、法律、基础设施）测试6个前沿模型，每个领域7种越狱场景，3种系统提示条件，2种提示变体，共17,420个数据点。

Result: 文本安全性不能转移到工具调用安全性，所有模型都存在文本拒绝但工具调用执行危险操作的情况；系统提示对工具调用行为影响显著；运行时治理合同能减少信息泄漏但对禁止的工具调用尝试无威慑效果。

Conclusion: 仅文本安全评估不足以评估智能体行为，工具调用安全需要专门的测量和缓解措施，当前对齐方法存在安全漏洞。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [29] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov是一个离线代理学习框架，通过建模验证为无记忆状态转换，使用执行验证数据策展、策略感知代理数据合成和最差状态优先采样，在硬件验证任务中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于执行的LLM代理学习方法需要昂贵且缓慢的工具反馈，使得在线强化学习不切实际，特别是在依赖工业模拟器和不可微分执行信号的高覆盖率硬件验证场景中。

Method: 将验证建模为确定性评估器引导的无记忆状态转换，提出执行验证数据策展、策略感知代理数据合成和最差状态优先采样，构建离线学习框架。

Result: 紧凑的4B参数模型在代理评估下达到69.2%覆盖率通过率，比其教师模型提升5.3%，性能可与大一个数量级的模型竞争。

Conclusion: LLM4Cov框架通过离线学习方法有效解决了执行反馈昂贵的问题，在硬件验证任务中展示了小模型通过适当训练策略可以达到与大模型竞争的性能。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [30] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom是一个自动化代理劫持框架，通过结构化模板注入攻击LLM代理的架构机制，利用优化的模板诱导角色混淆，显著提高了攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于手动语义提示操纵的攻击成功率低且对闭源商业模型可迁移性差，需要一种更有效的自动化攻击框架来揭示LLM代理架构层面的安全漏洞。

Method: 基于结构化模板注入，通过多级模板增强增加结构多样性，训练模板自编码器将离散模板嵌入连续潜在空间，使用贝叶斯优化搜索最优对抗向量并解码为高效模板。

Result: 在Qwen、GPT和Gemini上的实验显示，Phantom在攻击成功率和查询效率上显著优于现有基线，并在真实商业产品中发现了70多个已确认的漏洞。

Conclusion: 结构化模板注入是LLM代理系统的严重安全威胁，Phantom框架为保护下一代代理系统提供了实证基础，揭示了架构层面的安全风险。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [31] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一个结合量子变分电路预测、量子退火优化和后量子签名的金融风险系统，在预测精度、投资组合表现和求解速度上均优于经典基准。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险系统的两阶段流程（预测+优化）在实际约束下容易失效：预测模型可能表现良好，但最终决策在市场变化、离散约束或大规模资产集时不稳定，且监管环境需要可审计的决策追溯。

Method: 1. 使用变分量子电路（VQC）加小型经典头部学习下一期收益和波动率代理；2. 将风险收益目标和约束转化为QUBO问题，优先使用量子退火求解，同时保留经典QUBO求解器作为后备；3. 使用后量子签名对每次调仓输出进行签名，确保分配结果可验证。

Result: 在数据集研究中，HQFS将收益预测误差降低7.8%，波动率预测误差降低6.1%；决策层将样本外夏普比率提升9.4%，最大回撤降低11.7%；QUBO求解阶段平均求解时间比混合整数基准减少28%，同时生成完全可追溯的签名分配记录。

Conclusion: HQFS成功将预测、离散风险优化和可审计性整合到统一流程中，在预测精度、投资组合表现和求解速度方面均优于经典方法，同时满足监管对可追溯性的要求。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [32] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 论文挑战了黑盒安全评估的基本假设，证明对于依赖未观察内部变量的模型，任何黑盒评估器都无法可靠估计部署风险，并给出了统计和计算上的下界。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全评估假设测试分布上的模型行为能可靠预测部署性能，但本文质疑这一假设，特别是针对那些输出依赖于未观察内部变量的模型，这些变量在评估中罕见但在部署中普遍。

Method: 通过形式化潜在上下文条件策略，使用Le Cam方法证明被动评估的极小极大下界，基于哈希的触发构造和Yao极小极大原理分析自适应评估，在陷门单向函数假设下展示计算分离，并为白盒探测提供样本复杂度分析和偏差校正。

Result: 证明黑盒评估存在根本限制：被动评估误差≥0.208δL，自适应评估误差≥δL/16，检测需要Θ(1/ε)查询；计算上部署环境可激活不安全行为而多项式时间评估器无法区分；白盒探测需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定安全风险，需要额外保障措施如架构约束、训练时保证、可解释性和部署监控来确保最坏情况下的安全保证。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [33] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越行为模仿，评估LLM在考虑投资者风险偏好下的理性决策质量与行为对齐之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能在市场波动下存在噪声或短视，与用户的长期目标冲突。将用户选择作为唯一真实标签会混淆行为模仿与决策质量。

Method: 构建Conv-FinRe基准：基于真实市场数据和人类决策轨迹，创建包含入职访谈、逐步市场情境和咨询对话的对话式纵向数据集。提供多视角参考，区分描述性行为和基于投资者特定风险偏好的规范性效用，评估LLM是否遵循理性分析、模仿用户噪声或受市场动量驱动。

Result: 评估结果显示，在基于效用的排名上表现良好的模型往往难以匹配用户选择，而行为对齐的模型可能过度拟合短期噪声，揭示了理性决策质量与行为对齐之间的持续张力。

Conclusion: Conv-FinRe基准能够诊断LLM在金融推荐中的决策质量，超越了单纯的行为模仿评估。数据集已在Hugging Face公开，代码库在GitHub可用，为金融咨询领域的LLM评估提供了新标准。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [34] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS：用于时间序列数据库自然语言查询的神经符号框架，采用搜索-验证流程，结合SQL和Python程序处理连续形态意图，并引入首个大规模基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要新的解决方案来帮助非专家用户从海量时间序列数据中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS神经符号框架，采用搜索-验证流程：1）使用特征索引通过SQL查询候选窗口；2）生成Python程序对原始信号进行锁定和验证候选，类似于主动声纳的工作原理。

Result: 实验表明Sonar-TS能有效处理传统方法失败的复杂时间查询，并揭示了该领域的独特挑战。同时创建了NLQTSBench作为首个大规模基准。

Conclusion: 这是对时间序列数据库自然语言查询的首次系统性研究，提供了通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [35] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似性指数快速筛选，再使用基于反正态分布技能桶的Kantorovich距离计算"制裁分数"，为异质技能水平的预组队提供公平快速匹配。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏中，公平快速的匹配系统直接影响玩家留存和满意度。然而，为异质技能水平的预组队创建公平匹配具有挑战性，仅基于平均团队技能指标（如均值或中位数评级）通常导致不平衡的一边倒比赛，特别是在技能分布广泛或偏斜时。

Method: Cinder采用两阶段方法：第一阶段使用Ruzicka相似性指数快速比较团队的"非异常值"技能范围进行初步筛选；第二阶段将玩家排名映射到基于反正态分布生成的技能桶中，提供平均技能水平更高粒度，然后使用Kantorovich距离在排序的桶索引上量化匹配公平性，产生"制裁分数"。

Result: 通过分析1.4亿个模拟团队配对的制裁分数分布，证明了系统的可行性，为公平匹配阈值提供了稳健基础。

Conclusion: Cinder系统能够为异质技能水平的预组队提供快速公平的匹配，解决了传统基于平均技能指标匹配方法的不平衡问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [36] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个用于端到端、项目规模数学自动形式化的智能体框架，能在Lean中实现教科书级数学文献的形式化，将479页教材转化为15.3万行Lean代码，证明成功率96%。


<details>
  <summary>Details</summary>
Motivation: 现有数学自动形式化技术仅限于孤立定理和短片段，无法扩展到教科书和研究论文级别，因为需要处理跨文件依赖、解析导入和确保整个项目端到端编译。

Method: M2F采用两阶段框架：1) 语句编译阶段：将文档分割为原子块，通过推断依赖关系排序，修复声明骨架直到项目编译；2) 证明修复阶段：在固定签名下使用目标导向的局部编辑填补证明空缺。整个过程保持验证器在循环中，只有工具链反馈确认改进时才提交编辑。

Result: 在三周内将479页实分析和凸分析教材转化为包含153,853行代码的Lean库，证明成功率在FATE-H基准上达到96%（基线为80%），实现了教科书级形式化，速度远超传统专家手动工作。

Conclusion: M2F框架证明大规模自动化数学文献形式化是可行的，将通常需要数月或数年专家工作的任务压缩到数周内完成，为数学形式化的实际应用开辟了新途径。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [37] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 微软Dynamics 365 Sales中的Sales Research Agent是一个AI驱动的销售研究应用，通过连接实时CRM数据、推理复杂模式，生成文本和图表洞察。该研究提出了Sales Research Bench基准，在8个维度评估系统质量，该代理在定制企业模式上显著优于Claude Sonnet 4.5和ChatGPT-5。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够基于实时定制CRM数据回答销售领导问题的AI系统，但现有模型缺乏透明、可重复的质量证据。需要开发一个能够连接实时数据、推理复杂模式并提供决策就绪洞察的AI应用，同时建立可观察的质量评估方法。

Method: 开发Sales Research Agent应用，连接实时CRM和相关数据，推理复杂模式，生成文本和图表输出。引入Sales Research Bench基准，在8个客户加权的维度上评估系统：文本和图表的基础性、相关性、可解释性、模式准确性和图表质量。

Result: 在2025年10月19日对定制企业模式的200个问题测试中，Sales Research Agent在100分综合得分上比Claude Sonnet 4.5高出13分，比ChatGPT-5高出24.1分，为客户提供了可重复比较AI解决方案的方法。

Conclusion: Sales Research Agent成功解决了企业需要基于实时CRM数据的AI决策支持问题，并通过Sales Research Bench基准提供了透明、可重复的质量评估方法，显著优于现有主流AI模型，为企业AI解决方案比较提供了可靠框架。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [38] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 提出PA-MoE方法，通过相位感知的路由机制解决传统MoE在RL中token级路由导致的模式碎片化问题，使专家能专注于特定任务阶段。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络会导致"简单性偏差"，即简单任务占据大部分参数并主导梯度更新，复杂任务得不到足够容量。传统MoE的token级路由会将相位一致的模式分散到不同专家，破坏专家专业化。

Method: 提出相位感知专家混合(PA-MoE)：1) 轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2) 相位路由器为相同专家分配时间一致的分配，使专家能保持相位特定专业知识。

Result: 实验结果表明PA-MoE的有效性。

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，使专家能更好地专注于特定任务阶段，提高了RL代理解决复杂任务的能力。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [39] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 提出Instruction-Tool Retrieval (ITR)方法，通过检索式方法动态选择最小系统提示片段和必要工具子集，大幅减少LLM代理每步的上下文token使用，降低成本并提升工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在运行时通常需要反复加载长系统指令和大型工具目录，这增加了成本、延迟、代理偏离概率和工具选择错误。需要一种方法来减少每步的上下文负担。

Method: 提出Instruction-Tool Retrieval (ITR)，这是一种RAG变体，每步检索最小系统提示片段和必要工具子集。ITR组合动态运行时系统提示，并暴露经过缩小的工具集，带有置信度门控回退机制。

Result: 在受控基准测试中，ITR将每步上下文token减少95%，正确工具路由相对提升32%，端到端成本降低70%。这些节省使代理能在上下文限制内运行2-20倍更多循环。

Conclusion: ITR特别适用于长时间运行的自主代理，随着代理步骤增加，节省效果会累积。论文详细描述了方法、评估协议、消融研究和实际部署操作指南。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [40] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA：一个通过意图对齐计划记忆来稳定长视野计算机使用任务的多智能体框架，显著提升任务成功率和执行效率


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理在长视野、噪声感知、多窗口环境下容易偏离用户意图，重复解决常规子问题，导致错误累积和效率低下

Method: 提出多智能体框架IntentCUA，包含规划器、计划优化器和批评器，通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能，运行时通过意图原型检索子群对齐技能并注入部分计划

Result: 在端到端评估中，IntentCUA达到74.83%的任务成功率，步骤效率比为0.91，优于基于强化学习和轨迹检索的基线方法

Conclusion: 系统级意图抽象和基于记忆的协调是大型动态环境中实现可靠高效桌面自动化的关键，多智能体协作循环在长视野任务中提供最大增益

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [41] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 论文提出评估大推理模型忠实性的框架RFEval，发现49.7%的输出存在不忠实问题，准确率不能作为忠实性的可靠代理指标


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能强大，但经常产生看似合理却无法反映真实决策过程的推理，这损害了模型的可靠性和可信度。需要建立正式框架来评估推理的忠实性。

Method: 提出基于两个可测试条件的忠实性框架：立场一致性（推理与答案间的连贯立场）和因果影响（在输出级干预下，所述推理能因果驱动答案）。开发RFEval基准，包含7,186个实例，通过受控的输出级反事实干预来探测忠实性。

Result: 评估12个开源大推理模型，发现49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱收敛领域。与规模相比，后训练机制与不忠实性更相关：添加当前RL风格目标会降低推理忠实性，即使准确率保持不变。准确率既不是忠实性的充分条件，也不是可靠代理指标。

Conclusion: 建立了审计大推理模型可靠性的严谨方法，表明可信AI不仅需要优化正确结果，还需要确保推理过程的结构完整性。准确率不能替代对忠实性的直接评估。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [42] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 提出S2Q方法，通过多个子价值函数保留替代高价值动作，增强多智能体强化学习的适应性和探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，在训练中价值函数变化时难以适应，容易收敛到次优策略。

Method: 提出S2Q方法，学习多个子价值函数来保留替代高价值动作，结合基于Softmax的行为策略，促进持续探索和快速适应。

Result: 在挑战性MARL基准测试中，S2Q持续优于多种MARL算法，显示出改进的适应性和整体性能。

Conclusion: S2Q通过多子价值函数方法有效解决了价值分解中的适应性问题，提升了多智能体强化学习的性能。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [43] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态标记级特征估计样本难度，相比传统方法计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法需要预定义难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。需要一种既有效又计算开销小的训练优化方法。

Method: 提出预测性批量调度（PBS），使用在线训练的轻量级线性预测器，仅基于四个静态标记级特征（标记频率、序列长度、词汇多样性、稀有标记比例）来估计样本难度，动态构建优先处理高损失样本的批次。

Result: 在130M参数transformer上的实验显示，PBS实现了6-13%的收敛加速（通过评估损失测量），预测器相关性在10,000训练步中从0.14提升到0.44，仅使用四个简单特征就达到0.44的实际损失相关性。

Conclusion: 标记频率统计编码了有关样本难度的有意义信息，能够实现有效的课程学习且计算开销可忽略，为训练优化提供了高效实用的解决方案。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [44] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: 对5个AI编程代理在GitHub上创建的PR进行实证分析，发现不同代理的PR描述风格差异显著，这些差异影响了评审者的参与度、响应时间和合并结果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI编程代理在GitHub上自主创建PR的现象日益普遍，但不同代理在PR描述特征上的差异以及人类评审者如何响应这些PR仍缺乏深入研究。

Method: 使用AIDev数据集，对5个AI编程代理创建的PR进行实证分析，研究PR描述的结构特征，并考察评审者的评审活动、响应时间、情感倾向和合并结果。

Result: 发现AI编程代理具有不同的PR描述风格，这些风格差异与评审者参与度、响应时间和合并结果相关。不同代理在评审互动指标和合并率上存在显著差异。

Conclusion: PR呈现方式和评审者互动动态在人类-AI协作软件开发中扮演重要角色，AI代理的PR描述风格会影响人类评审者的响应行为。

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [45] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 本文探讨了6G无线系统中从基于规则的控制向意图驱动自主智能的转变，提出利用大语言模型构建意图感知网络代理，重点关注物理层的代理AI实现路径，并通过AgenCom案例展示意图驱动链路决策代理的应用。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线系统发展，功能复杂性增加和服务需求多样化推动从基于规则控制转向意图驱动自主智能。用户需求不再由单一指标（如吞吐量或可靠性）捕获，而是由多维度目标（如延迟敏感性、能耗偏好、计算约束和服务级别要求）构成，这些目标可能因环境动态和用户-网络交互而变化。因此，准确理解通信环境和用户意图对于6G通信的自主和可持续演进至关重要。

Method: 本文采用意图感知、自主决策和网络执行的闭环管道框架，研究6G物理层的代理AI及其实现路径。方法包括：回顾代表性物理层任务及其在支持意图感知和自主性方面的局限性；识别代理AI具有优势的应用场景；讨论多模态感知、跨层决策和可持续优化等关键挑战和使能技术；最后通过AgenCom案例研究展示意图驱动链路决策代理的实现。

Result: 本文提出了利用大语言模型构建意图感知网络代理的方法，大语言模型凭借强大的上下文理解和跨模态推理能力，能够整合异构信息并将自然语言意图转化为可执行的控制和配置决策。通过AgenCom案例展示了意图驱动链路决策代理在不同用户偏好和信道条件下的自适应通信链路构建能力。

Conclusion: 代理AI为6G物理层提供了从基于规则控制向意图驱动自主智能转变的有效途径。大语言模型作为意图感知网络代理的基础，能够处理多维度用户需求和环境动态变化。未来的研究需要解决多模态感知、跨层决策和可持续优化等关键挑战，以实现6G通信系统的自主和可持续演进。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [46] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出STRIDE和SR-Delta框架，通过人机协作生成可信的基准数据集，以解决不同ESG评级机构评分差异大的问题，提高可持续性评级的可比性和可信度。


<details>
  <summary>Details</summary>
Motivation: 不同可持续性/ESG评级机构对同一公司的评分差异很大，这限制了评级的可比性、可信度和决策相关性，需要一种方法来协调和评估这些评级方法。

Method: 提出一个通用的人机协作框架，包含两个部分：STRIDE（提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集）和SR-Delta（差异分析程序框架，揭示潜在调整的见解）。

Result: 该框架能够实现可持续性评级方法的可扩展和可比评估，为评估和改进ESG评级方法提供了系统化的工具。

Conclusion: 呼吁AI社区采用AI驱动的方法来加强和推进可持续性评级方法，以支持和执行紧迫的可持续性议程，通过人机协作生成可信基准数据集来解决评级不一致问题。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [47] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 论文提出O-Shap方法，通过满足T特性的语义分割改进Owen值，在特征依赖的视觉任务中提升SHAP归因的准确性、语义一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法假设特征独立，但在视觉任务中像素特征存在空间和语义依赖。现有Owen值实现依赖的分割方法（如轴对齐或SLIC）违反一致性属性，导致归因不准确。

Method: 提出新的分割方法满足T特性，确保层次结构中的语义对齐。该方法支持计算剪枝，同时改进归因准确性和可解释性。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下。

Conclusion: 通过满足T特性的语义分割改进Owen值，能够有效处理特征依赖问题，提升SHAP归因方法的实际应用效果。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [48] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG：基于课程材料自动构建教师对齐的知识图谱框架，用于捕捉学习依赖关系


<details>
  <summary>Details</summary>
Motivation: 在大规模课程中，教师难以诊断个别学生的知识缺口和确定需要强化的概念。现有知识图谱方法要么过于表面（关注课程级概念），要么忽略了教学材料中丰富的教学信号。

Method: 从课程讲座材料（幻灯片、笔记等）中提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系）。结合教育材料特有的时间语义信号和大语言模型的泛化能力。

Result: 通过多个真实世界课程的多样化讲座材料实验和人工评估，证明InstructKG能够捕捉丰富、教师对齐的学习进展。

Conclusion: InstructKG框架能够自动构建教师对齐的知识图谱，有效捕捉课程预期的学习进展，为个性化学习提供支持。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [49] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出高维空间的索引认识论，将生成式AI视为在语义空间中导航，而非传统符号推理或统计重组，为理解生成式AI的知识生产提供新框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的认知机制尚未被充分理解，其知识生产方式与传统技术不同。缺乏这种理解将阻碍AI在科学、教育和制度中的负责任整合。需要突破传统图灵-香农-冯·诺依曼范式，建立新的认识论框架。

Method: 提出高维空间的索引认识论，基于高维几何的四个结构特性：测度集中、近正交性、指数方向容量和流形正则性。结合皮尔士符号学和帕珀特建构主义，将生成模型重新概念化为学习流形的导航器。

Result: 提出导航知识作为第三种知识生产方式，区别于符号推理和统计重组。生成式AI通过在语义空间中的位置操作产生意义，这种高维几何结构塑造了其生成过程。

Conclusion: 需要范式转变来理解生成式AI的知识生产。高维空间的索引认识论为AI的负责任整合提供了原则基础，导航知识作为新知识模式对科学和教育有重要意义。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [50] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种用于分解困难CircuitSAT实例的新型并行算法，通过专用约束将原始SAT实例划分为弱化公式族，参数化设计允许高效识别高质量分解


<details>
  <summary>Details</summary>
Motivation: CircuitSAT问题在形式验证和密码分析中至关重要，但困难实例难以求解。现有方法在处理复杂电路实例时效率有限，需要更有效的分解策略来应对挑战性实例

Method: 采用并行算法框架，使用专用约束将原始SAT实例划分为弱化公式族。算法参数化设计，通过并行计算的硬度估计指导高质量分解的识别，调整参数可优化分解质量

Result: 在挑战性CircuitSAT实例上验证了算法的实际效果，包括布尔电路逻辑等价性检查和密码哈希函数的原像攻击编码实例，展示了算法的实用价值

Conclusion: 提出的并行分解算法为处理困难CircuitSAT实例提供了有效解决方案，在形式验证和密码分析领域具有应用潜力，参数化设计增强了算法的适应性和效率

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [51] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: 提出Combine框架，这是一个基于准则的剪枝解决方案，用于高效迭代剪枝CNN，减少模型大小和计算量，同时保持或提升精度。


<details>
  <summary>Details</summary>
Motivation: 随着CNN对精度和性能要求的提高，模型规模、执行时间、内存占用和功耗也随之增加。现有剪枝解决方案缺乏统一实现标准，难以实施和比较。

Method: 提出Combine框架，这是一个基于准则的剪枝解决方案，支持迭代剪枝。创建了比较准则函数的标准语言，并提出了几种新颖的准则函数。

Result: 在VGG启发模型上，最多剪除79%的滤波器，同时保持或提高精度，网络计算量最多减少68%。

Conclusion: Combine是一个快速有效的迭代剪枝框架，不同准则函数对不同模型有不同效果，为剪枝准则比较提供了标准化语言。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [52] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个结合联合嵌入预测架构（JEPA）与生成目标的基因组基础模型预训练框架，通过潜在空间预测提升全局功能理解能力


<details>
  <summary>Details</summary>
Motivation: 现有的基因组基础模型主要依赖掩码语言建模（MLM）或下一个标记预测（NTP），这些方法擅长捕捉局部基因组语法和精细基序模式，但往往无法捕捉更广泛的功能上下文，导致表示缺乏全局生物学视角

Method: 提出JEPA-DNA框架，将联合嵌入预测架构（JEPA）与传统生成目标结合，通过潜在接地将标记级恢复与潜在空间预测目标耦合，监督CLS标记，迫使模型预测掩码基因组片段的高级功能嵌入而非仅关注单个核苷酸

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上始终优于纯生成基线，提供更稳健和生物学接地的表示

Conclusion: JEPA-DNA为理解基因组字母和序列底层功能逻辑的基础模型提供了可扩展的路径，能够同时捕捉局部语法和全局功能上下文

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [53] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的高性能公式识别模型，通过精心设计、蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少80%/65%，支持消费级硬件实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型通常参数量大，需要高性能硬件，限制了在消费级设备和浏览器中的实时部署。需要开发一个轻量级但高性能的公式识别模型，使其能在普通硬件上运行。

Method: 采用注意力机制设计、知识蒸馏技术，以及词汇表和分词器的迁移策略，构建了一个仅含2000万参数的极简模型架构。

Result: Texo在性能上可与UniMERNet-T和PPFormulaNet-S等SOTA模型相媲美，同时模型大小分别减少了80%和65%，实现了在消费级硬件上的实时推理和浏览器内部署。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别任务在资源受限环境中的部署提供了可行方案。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [54] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 提出一个在线构建符号因果世界模型的框架，通过元解释学习和谓词发明实现高效、可扩展的因果推理，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有世界建模方法存在样本效率低、缺乏透明性和可扩展性差的问题，特别是在复杂关系动态环境中，命题方法面临组合爆炸的挑战。

Method: 集成连续模型学习和修复到智能体决策循环中，利用元解释学习和谓词发明发现语义上有意义且可重用的抽象概念，构建解耦的高质量概念层次结构。

Result: 该方法能够扩展到具有复杂关系动态的领域，避免了命题方法的组合爆炸问题，样本效率比PPO神经网络基线高出数个数量级。

Conclusion: 提出的框架能够在线构建符号因果世界模型，实现高效、可扩展的因果推理，为智能体在复杂环境中的导航提供了更优的解决方案。

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [55] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出并验证了一个基于AI Agent的人文社科研究协作工作流，使用台湾Claude.ai使用数据作为实证案例，探索人机协作的三种操作模式。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI研究主要集中在软件工程和自然科学领域，人文社科领域的方法论探索有限。本研究旨在填补这一空白，为人文社科研究者提供一个可复制的AI协作框架。

Method: 提出一个七阶段模块化工作流（Agentic Workflow），基于任务模块化、人机分工和可验证性三原则。使用台湾Anthropic Economic Index（AEI）的7,729个对话数据作为实证案例，展示工作流在二手数据分析中的应用。

Result: 成功验证了工作流的可行性，并识别出人机协作的三种操作模式：直接执行、迭代优化和人类主导。揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思中的不可替代性。

Conclusion: 本研究为人文社科研究者提供了一个可复制的AI协作方法论框架，强调了人类判断在研究中的核心地位，同时承认了单平台数据、横截面设计和AI可靠性风险等局限性。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [56] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出大型行为模型(LBM)，通过行为嵌入而非即时提示来预测个体战略决策，利用结构化心理特征档案提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在预测高风险环境下的人类决策时存在局限，特别是难以生成一致、个体特定的行为，且基于提示的方法在处理心理特征与情境约束的复杂交互时表现脆弱

Method: 开发大型行为模型(LBM)，通过微调将结构化高维特征档案（来自综合心理测量工具）作为行为嵌入条件，训练数据链接稳定倾向、动机状态和情境约束到观察到的选择

Result: LBM在保留场景评估中相比未适应的Llama-3.1-8B-Instruct骨干模型改进了行为预测，与前沿基线在Big Five特征条件下表现相当；LBM能从更密集的特征档案中持续获益，而提示方法存在复杂性上限

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [57] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布鲁姆分类法分析LLM内部神经表征，发现不同认知复杂度层级（从记忆到创造）在模型残差流中线性可分，线性分类器准确率达95%，表明认知难度在模型前向传播早期就被编码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的黑箱特性需要超越表面性能指标的新评估框架。本研究旨在探究LLM内部神经表征如何编码认知复杂性，使用布鲁姆分类法作为分层视角来分析模型是否能在内部表征中区分不同认知层级。

Method: 通过分析不同LLM的高维激活向量，研究不同认知层级（从基础记忆到抽象创造）是否在模型残差流中线性可分。使用线性分类器对布鲁姆分类法的各个层级进行分类，并观察表征在不同层中的可分离性变化。

Result: 线性分类器在所有布鲁姆层级上平均准确率达到约95%，提供了强有力的证据表明认知层级被编码在模型表征的线性可访问子空间中。研究还发现模型在前向传播早期就解析了提示的认知难度，且表征在不同层中变得越来越可分离。

Conclusion: 该研究证明LLM内部表征能够编码认知复杂性，且这种编码是线性可访问的。这为理解LLM如何处理不同认知难度的任务提供了新视角，也为开发更精细的模型评估框架奠定了基础。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [58] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLMs在回溯测试中的时间知识泄漏，并开发TimeSPEC方法通过声明验证主动过滤污染信息


<details>
  <summary>Details</summary>
Motivation: 评估LLMs预测未来事件需要回溯测试，但模型可能在训练中编码了截止日期后的知识，导致时间知识泄漏，影响评估有效性

Method: 1) 提出声明级框架：将模型推理分解为原子声明并按时间可验证性分类；2) 使用Shapley值测量每个声明对预测的贡献，得到Shapley-DCLR指标；3) 开发TimeSPEC方法：在生成过程中穿插声明验证和再生，主动过滤时间污染信息

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上实验显示标准提示方法存在显著泄漏，TimeSPEC能降低Shapley-DCLR同时保持任务性能

Conclusion: 显式的、可解释的声明级验证优于基于提示的时间约束，TimeSPEC方法能更可靠地进行回溯测试，减少时间知识泄漏

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [59] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 提出Web Verbs作为网络代理的语义层，通过类型化、语义化的函数统一API和浏览器操作，提高可靠性、效率和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理大多基于点击、按键等低级操作，这些操作脆弱、低效且难以验证。随着LLM的发展，需要为代理式网络建立语义化的动作层。

Method: 提出Web Verbs概念，这是一个网络规模的类型化、语义化函数集合，通过统一接口暴露网站功能，无论通过API还是客户端工作流实现。这些动词包含前置条件、后置条件、策略标签和日志支持。

Result: 通过概念验证实现和案例研究，展示了相比现有代理更简洁、更稳健的执行。Web Verbs通过稳定接口提高可靠性，将数十步操作简化为几个函数调用提高效率，并通过类型化合约和可检查跟踪提高可验证性。

Conclusion: Web Verbs为网络代理提供了语义动作层，统一了API和浏览器范式，使LLM能够合成可靠、可审计的工作流。提出了标准化路线图以实现网络规模的部署和可信度。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [60] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了在有限计算资源下（2xA100 GPU）从arXiv LaTeX源文件训练1.36B参数科学语言模型的完整工程流程，分析了预处理、分词和基础设施瓶颈对训练的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但如何从原始科学文献源文件训练领域专用语言模型的实践过程缺乏详细文档。本研究旨在为中等计算预算的研究者提供训练领域专用模型的工程指导。

Method: 构建端到端训练流程：包括元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词，以及在2xA100 GPU约束下的密集Transformer训练。通过24次实验运行分析训练稳定性、扩展行为、数据损失和基础设施瓶颈。

Result: 研究发现预处理决策显著影响可用token数量，分词影响符号稳定性，存储和I/O约束可能成为与计算同等重要的限制因素。在数据丰富的52B预训练token下展现出稳定的训练行为。

Conclusion: 本研究提供了从零开始训练小型科学语言模型的工程实践记录，而非提出新架构。希望这些见解能帮助中等计算预算的研究者构建领域专用模型。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [61] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify是一个用于医学诊断的AI代理，通过生成后续问题进行迭代推理，减少诊断不确定性，相比单次LLM基线减少约27%的诊断错误。


<details>
  <summary>Details</summary>
Motivation: 当前医学LLMs在诊断任务中存在局限性：真实临床诊断需要系统性的病史采集和迭代推理，考虑鉴别诊断并排除紧急情况，而现有LLMs生成信息性后续问题并进行鉴别诊断推理的能力尚未充分探索。

Method: MedClarify采用信息论推理方法：1) 计算类似鉴别诊断的候选诊断列表；2) 主动生成旨在减少诊断不确定性的后续问题；3) 选择预期信息增益最高的问题进行针对性、不确定性感知的推理。

Result: 实验显示：1) 当前LLMs在医学推理中存在局限性，特别是在病例不完整或相关信息缺失时会产生多个相似可能性的诊断；2) MedClarify的信息论推理方法能生成有效的后续提问，相比标准单次LLM基线减少约27个百分点的诊断错误。

Conclusion: MedClarify通过代理式信息寻求改进了医学LLMs，促进了反映真实世界临床推理迭代性和不确定性的有效医学LLM对话。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [62] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据的正则化方法，通过曲率矩阵近似解决任务向量组合中的表示漂移问题，实现任务算术的模块化扩展


<details>
  <summary>Details</summary>
Motivation: 任务算术提供模块化扩展基础模型的方法，但多个任务向量组合会导致跨任务干扰和表示漂移。现有正则化方法需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突

Method: 将表示漂移正则化框架化为曲率矩阵近似问题，采用Kronecker分解近似曲率技术，获得实用的正则化器。该方法具有任务数量恒定的复杂度，无需外部数据

Result: 在任务添加和否定任务上达到最先进结果，对任务向量缩放具有鲁棒性，无需保留调优

Conclusion: 提出的无数据正则化方法有效解决了任务算术中的表示漂移问题，保持了模块化优势，在任务组合中表现出色

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [63] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一种结合形式验证与深度学习的图像检索框架，通过图验证和神经代码生成处理复杂查询，提供可验证的透明结果


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言检索在处理涉及复杂关系、对象组合或精确约束（如身份、计数、比例）的查询时仍不可靠，需要更可信和可验证的检索方法

Method: 整合形式验证到深度学习图像检索中，结合基于图的验证方法和神经代码生成，对用户查询中的每个原子事实进行形式化验证

Result: 框架不仅返回匹配结果，还能识别标记哪些具体约束被满足或未满足，提供更透明和可追溯的检索过程，同时提升了主流嵌入方法的性能

Conclusion: 通过将检索结果建立在形式推理系统上，超越了向量表示的模糊性和近似性，实现了对开放词汇自然语言查询的可信且可验证的检索

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [64] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈和门控机制处理多模态数据缺失问题，结合生存预测、重建和跨模态对比损失，在肺癌生存预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌生存预测因个体差异而困难，多模态数据（病理图像、转录组、甲基化）能提供互补信息，但临床数据常存在模态缺失，现有方法对严重缺失情况不够鲁棒。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特定变分编码器捕捉各数据源不确定性；融合瓶颈和门控机制归一化现有模态贡献；结合生存损失、重建损失和跨模态对比损失的多任务目标；训练时使用随机模态掩蔽提升鲁棒性。

Result: 在TCGA-LUAD（475例）和TCGA-LUSC（446例）数据集上评估，MCVAE在疾病特异性生存预测上优于两种SOTA模型，对严重缺失情况更鲁棒；发现多模态集成并非总是有益。

Conclusion: MCVAE能有效处理多模态数据缺失问题，提升肺癌生存预测性能，同时揭示了多模态集成并非总是有益的，为临床决策提供更可靠的预测工具。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [65] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 提出基于隐私设计原则的框架，指导儿童AI应用开发者在LLM全生命周期中实施隐私保护措施，确保符合法规要求。


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但面临隐私风险。现有隐私法规要求实施保护措施，但在实践中难以执行，需要实用框架来指导开发。

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等法规原则，映射到LLM应用的数据收集、模型训练、运营监控和持续验证等阶段，并结合UNCRC、AADC等儿童设计指南。

Result: 框架提供了各阶段的操作控制措施和设计指南，通过13岁以下儿童LLM教育导师的案例研究展示了实际应用效果。

Conclusion: 通过技术组织控制和适龄设计决策，可以在LLM全生命周期中开发出既提供隐私保护又符合法律要求的儿童AI应用。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [66] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构解决了学术研究与工业部署之间的鸿沟，支持50+算法、40指标和19种策略的本地到分布式无缝切换，集成能耗追踪，并面向Agentic AI演进。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统生态存在分裂问题：研究人员需要在易于内存实验和需要复杂重写以适应分布式工业引擎之间做出选择。这种分裂阻碍了创新，需要一种能无缝连接学术研究和工业部署的解决方案。

Method: 提出WarpRec框架，采用新颖的后端无关架构，包含50多种最先进算法、40个评估指标和19种过滤与分割策略。该框架支持从本地执行到分布式训练和优化的无缝过渡，并集成CodeCarbon进行实时能耗追踪。

Result: WarpRec成功消除了学术实验与工业部署之间的权衡，展示了可扩展性不必以科学完整性或可持续性为代价。框架支持推荐系统向Agentic AI演进，从静态排名引擎转变为生成式AI生态系统中的交互工具。

Conclusion: WarpRec不仅弥合了学术界与工业界之间的鸿沟，还可以作为下一代可持续、支持智能体的推荐系统的架构基础，为推荐系统创新提供了统一的高性能平台。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [67] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文提出了一个针对ARM Cortex处理器（M0+、M4、M7）的AI模型优化基准测试框架，专注于嵌入式系统中的能效、精度和资源利用率，通过自动化测试平台评估关键性能指标，帮助开发者设计高性能、高能效的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI在嵌入式系统中的广泛应用，需要在资源受限的ARM Cortex处理器上优化AI模型性能，平衡能效、精度和计算资源，但目前缺乏系统化的评估框架来指导开发者选择最优的处理器和模型组合。

Method: 设计自动化测试平台，系统评估ARM Cortex M0+、M4、M7处理器上的AI模型性能；使用关键性能指标（KPIs）分析；发现浮点运算（FLOPs）与推理时间之间的近线性相关性；采用帕累托分析平衡能效与精度之间的权衡。

Result: 发现FLOPs与推理时间存在近线性关系，可作为计算需求估计的可靠指标；M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更高，M0+处理器适合简单AI任务；通过帕累托分析实现了能效与精度的最佳平衡。

Conclusion: 该基准测试框架为嵌入式AI系统开发提供了实用指导，帮助开发者根据具体应用需求选择最优的处理器和模型配置，在满足性能要求的同时实现能效优化，推动可持续的嵌入式AI应用发展。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [68] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架通过结合知识图谱和检索增强生成，提升LLM在电信领域的准确性和可靠性，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域应用困难，因为该领域具有复杂性、标准不断演进和专用术语，导致模型输出不准确、幻觉增多，实用性降低。

Method: 提出KG-RAG框架，将知识图谱（提供电信标准和文档的结构化知识表示）与检索增强生成（动态检索相关事实）相结合，增强LLM的电信任务能力。

Result: 在基准数据集上的实验表明，KG-RAG优于纯LLM和标准RAG基线，平均准确率分别提升21.6%和14.3%。

Conclusion: KG-RAG能有效在复杂电信场景中产生准确、可靠且可解释的输出，解决了领域特定挑战。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [69] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 该论文提出了评估思维链（CoT）质量的两个新指标：可重用性和可验证性，发现它们与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体IR管道中，LLM智能体通过思维链（CoT）交换中间推理过程，但现有的CoT评估仅关注目标任务准确率，无法评估推理过程本身的质量或效用。

Method: 采用Thinker-Executor框架将CoT生成与执行解耦，提出两个新指标：可重用性（衡量Executor重用Thinker的CoT的容易程度）和可验证性（衡量Executor使用CoT匹配Thinker答案的频率）。在五个基准测试中评估了四个Thinker模型与十个Executor模型组成的委员会。

Result: 研究发现可重用性和可验证性与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。令人惊讶的是，专门推理模型生成的CoT并不比Llama和Gemma等通用LLM生成的CoT更具可重用性或可验证性。

Conclusion: 需要超越准确率的评估指标来全面评估LLM的推理能力，可重用性和可验证性提供了评估思维链质量的重要维度，有助于更全面地理解多智能体系统中的推理过程。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [70] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决超长视野任务，在多个基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理超长视野任务时能力有限，需要开发专门针对长轨迹任务的训练方法和智能体架构。

Method: 1. 使用轨迹分割SFT冷启动模型：保留早期上下文，逐步截断后期上下文，保持子轨迹重叠；2. 通过Research-Factory自动管道生成高质量训练数据；3. 提出渐进式RL训练，分阶段逐步延长超时时间。

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，在SWE-bench Verified和MLE-bench等编码基准上也表现出泛化优势。

Conclusion: KLong通过创新的轨迹分割SFT和渐进式RL训练方法，有效解决了超长视野任务，展示了在学术研究和编码任务中的强大泛化能力。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [71] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于常微分方程的统一理论框架ODESteer，用于大语言模型激活导向对齐，通过屏障函数和多步自适应导向提升性能


<details>
  <summary>Details</summary>
Motivation: 当前激活导向方法存在两个关键限制：缺乏统一的理论框架指导导向方向设计，以及过度依赖单步导向无法捕捉激活分布的复杂模式

Method: 提出基于常微分方程的理论框架，将传统激活加法解释为ODE的一阶近似，通过控制理论中的屏障函数定义导向方向，实现多步自适应导向

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%

Conclusion: 通过ODE统一激活导向的理论基础，并通过ODESteer方法进行实证验证，为LLM对齐提供了原则性的新视角

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [72] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 提出一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN，用于从X光片中检测COVID-19和肺炎，确保医疗数据安全和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的显著提升，人工智能在医疗健康领域应用前景广阔。医疗专家和医院需要共享数据空间，但医疗数据处理需要安全可靠的分布式系统。通过结合人工智能和联邦学习，可以构建一个安全、高效的医疗数据处理系统。

Method: 采用联邦学习框架下的混合模型方法，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和Transformer模型（SWIN Transformer）。使用TensorFlow、Keras和微软开发的Vision Transformer技术，实现实时持续学习。

Result: 该方法能够检测COVID-19和肺炎，为医生提供可靠的辅助诊断工具。联邦学习的集成确保了混合模型的安全性，保持了信息的真实性，同时提高了疾病诊断和严重程度预测的准确性。

Conclusion: 基于联邦学习的混合AI模型能够有效提高肺部疾病诊断的准确性和安全性，为全球抗击疫情提供技术支持，作为医生的有力助手在医疗领域发挥作用。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [73] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 提出AI GameStore平台，通过生成和评估AI在所有人类游戏中的表现来全面衡量机器智能，发现当前前沿模型在多数游戏中表现远低于人类水平


<details>
  <summary>Details</summary>
Motivation: 现有AI基准测试通常只评估狭窄能力，且容易饱和，需要更全面评估人类般通用智能的方法

Method: 提出AI GameStore平台，使用LLM和人类参与合成代表性人类游戏，基于App Store和Steam热门游戏生成100个游戏环境，评估7个前沿视觉语言模型

Result: 最佳模型在多数游戏中得分低于人类平均水平的10%，在需要世界模型学习、记忆和规划能力的游戏中表现尤其差

Conclusion: AI GameStore可作为衡量和推动机器实现人类般通用智能的实用方法，需要进一步扩展平台功能

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [74] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT：基于分层离散扩散模型的新型分子图生成框架，在MOSES数据集上实现近乎完美的化学有效性，首次在图形扩散中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在化学有效性和满足目标属性方面存在不足，相比1D建模表现较差，需要克服这些长期存在的性能限制

Method: 基于分层离散扩散模型，将离散扩散推广到编码化学先验的附加类别，并采用解耦原子编码技术，根据化学角色分割原子类型

Result: 在MOSES数据集上实现新的SOTA性能，首次在图扩散中达到近乎完美的化学有效性，在多个指标上超越强大的1D基线模型

Conclusion: MolHIT克服了现有方法的性能限制，在分子生成任务中表现出色，并在多属性引导生成和骨架扩展等下游任务中展现强大性能

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [75] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics是一个多智能体框架，能够从自然语言描述自主设计、实现、调试和验证通用PDE的数值求解器，生成基于经典数值分析的透明求解器而非黑盒神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器设计需要大量数学专业知识和手动调参，而现有神经网络方法虽然灵活但计算成本高且可解释性差，需要一种更自动化和可解释的PDE求解方法。

Method: 提出多智能体框架AutoNumerics，采用从粗到细的执行策略和基于残差的自验证机制，能够从自然语言描述生成基于经典数值分析的透明数值求解器。

Result: 在24个经典和实际PDE问题上，AutoNumerics相比现有神经和LLM基线达到竞争性或更优的精度，并能根据PDE结构特性正确选择数值方案。

Conclusion: AutoNumerics展示了作为自动化PDE求解可访问范式的可行性，能够生成透明、可解释的数值求解器，为科学计算提供了新的自动化途径。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [76] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了之前的工作，增加了语义关系提取任务，并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 历史文本中的人物-地点关系提取对于知识图谱构建、历史传记重建和数字人文空间分析至关重要，但现有方法在处理多语言、噪声历史文本时面临挑战。

Method: 采用多语言、多时期的历史文本数据集，要求系统识别两种关系类型(at和isAt)，并引入三重评估框架：准确性、计算效率和领域泛化能力。

Result: 建立了专门的历史人物-地点关系提取评估基准，为系统提供了统一的测试平台，支持数字人文领域的大规模历史数据处理应用。

Conclusion: HIPE-2026通过扩展关系提取任务和引入综合评估框架，推动了历史文本处理技术的发展，为数字人文研究提供了重要工具。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [77] [Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling](https://arxiv.org/abs/2602.16961)
*Rahul Thomas,Arka Pal*

Main category: cs.IT

TL;DR: 该论文提出了一种贪心多路径块验证（GBV）方法，通过扩展块验证到多路径设置，显著提高了推测解码的效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法中，块验证（BV）虽然比标准方法更优，但仍存在改进空间。作者希望探索在多路径设置下，如何进一步优化验证算法以提高解码效率。

Method: 首先构建信息无关的线性规划（LP）证明块验证的最优性，然后扩展到多路径设置。虽然计算最优算法不可行，但通过限制在贪心算法类中，提出了高效的贪心多路径块验证（GBV）方法。

Result: GBV相比BV能提高超过30%的块效率和超过15%的解码时间减少。在Llama-3 70B模型上，GBV相比最先进的多路径验证方法能提高超过15%的端到端解码吞吐量。

Conclusion: 贪心多路径块验证（GBV）是一种高效的多路径验证算法，能显著提升推测解码性能，为大规模语言模型推理加速提供了有效的解决方案。

Abstract: The goal of $L$-step speculative decoding is to accelerate autoregressive decoding of a target model by using a cheaper draft model to generate a candidate path of $L$ tokens. Based on a verification algorithm involving target and draft model probabilities, a prefix of the candidate sequence is accepted, and an additional correction token is sampled from a residual distribution to ensure that the final output adheres to the target distribution. While standard speculative decoding uses a verification algorithm which is independent at each token on the path, a recent extension called block verification uses a joint condition involving all sampled on-path probabilities. Block verification (BV) was shown to be optimal over all verification algorithms which use only on-path probabilities, improving on standard speculative decoding. In this work, we first show that block verification is optimal even over verification algorithms that use off-path probabilities, by constructing an information-agnostic linear program (LP). Further, we can extend our LP to the setting where the draft model samples multiple candidate paths, and use it to construct a natural class of multi-path block verification generalizations. While computing the optimal algorithm in this class is not tractable, by considering a stricter class of greedy algorithms, we can formulate an efficient method called greedy multi-path block verification (GBV). Empirically, GBV can improve block efficiency by over 30% and reduce decoding walltimes by over 15% relative to BV. On Llama-3 70B, GBV can improve the end-to-end decoding throughput over SOTA multi-path verification methods by more than 15%.

</details>


### [78] [Resource Allocation for STAR-RIS-enhanced Metaverse Systems with Augmented Reality](https://arxiv.org/abs/2602.17123)
*Sun Mao,Lei Liu,Kun Yang,F. Richard Yu,Duist Niyato,Chau Yuen*

Main category: cs.IT

TL;DR: 提出一种基于STAR-RIS辅助的AR增强元宇宙系统资源管理框架，通过联合优化基站计算资源分配、STAR-RIS系数矩阵、AR用户CPU频率和发射功率，最小化服务延迟。


<details>
  <summary>Details</summary>
Motivation: AR增强的元宇宙系统面临网络资源有限和无线传播环境不可预测的瓶颈，需要提高AR用户与元宇宙服务器之间的通信效率。

Method: 使用STAR-RIS辅助通信，构建服务延迟最小化问题，采用近似方法转化为可处理形式，通过交替优化方法解耦多维变量，包括：基于惩罚函数法获取最优系数矩阵，闭式解推导AR用户CPU频率，拉格朗日对偶法和凸优化理论求解AR用户发射功率和基站计算资源分配。

Result: 仿真结果表明，所提方法相比多个基准方法实现了显著的延迟降低。

Conclusion: 提出的STAR-RIS辅助AR增强元宇宙资源管理框架能有效降低服务延迟，提升系统性能。

Abstract: Augmented reality (AR)-enabled Metaverse is a promising technique to provide immersive service experience for mobile users. However, the limited network resources and unpredictable wireless propagation environments are key design bottlenecks of AR-enabled Metaverse systems. Therefore, this paper presents a resource management framework for simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted AR-enabled Metaverse, where the STAR-RIS is configured to improve the communication efficiency between AR users and the Metaverse server located at the base station (BS). Moreover, we formulate a service latency minimization problem via jointly optimizing the computation resource allocation of the BS, coefficient matrix of the STAR-RIS, central processing unit (CPU) frequency and transmit power of the AR users. To tackle the non-convex problem, we utilize an approximate method to transform it to a tractable form, and decouple the multi-dimensional variables via the alternating optimization method. Particularly, the optimal coefficient matrix is obtained by a penalty function-based method with proved convergence, the CPU frequencies of AR users are derived as the closed-form solution, and the transmit power of AR users and computation resource allocation of the BS are obtained by the Lagrange duality method and convex optimization theory. Finally, simulation results demonstrates that the proposed method achieves remarkable latency reduction than several benchmark methods.

</details>


### [79] [Isometric Invariant Quantification of Gaussian Divergence over Poincare Disc](https://arxiv.org/abs/2602.17159)
*Levent Ali Mengütürk*

Main category: cs.IT

TL;DR: 论文建立了球面平方Hellinger距离与庞加莱圆盘在一般莫比乌斯群作用下的双曲等距不变量之间的几何对偶关系，并提出使用L2嵌入的双曲等距不变量作为高斯测度间散度的替代量化方法。


<details>
  <summary>Details</summary>
Motivation: 受几何对偶关系的启发，作者希望利用这种几何连接，为信息论中高斯测度间的散度度量提供一种新的替代方法。

Method: 通过建立球面平方Hellinger距离与庞加莱圆盘在一般莫比乌斯群作用下的双曲等距不变量之间的几何对偶关系，提出使用L2嵌入的双曲等距不变量来量化高斯测度间的散度。

Result: 成功建立了球面平方Hellinger距离与双曲等距不变量之间的几何对偶关系，并提出了基于L2嵌入双曲等距不变量的高斯测度散度量化新方法。

Conclusion: 该几何对偶关系为信息论中高斯测度散度度量提供了新的视角和工具，L2嵌入的双曲等距不变量可作为传统散度度量的有效替代。

Abstract: The paper presents a geometric duality between the spherical squared-Hellinger distance and a hyperbolic isometric invariant of the Poincare disc under the action of the general Mobius group. Motivated by the geometric connection, we propose the usage of the L2-embedded hyperbolic isometric invariant as an alternative way to quantify divergence between Gaussian measures as a contribution to information theory.

</details>


### [80] [Federated Latent Space Alignment for Multi-user Semantic Communications](https://arxiv.org/abs/2602.17271)
*Giuseppe Di Poce,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.IT

TL;DR: 该论文提出了一种联邦优化方法，通过共享语义预均衡器和本地语义均衡器来解决AI原生设备间的潜在空间不对齐问题，实现任务导向的语义通信。


<details>
  <summary>Details</summary>
Motivation: AI原生设备间不同的潜在表示会导致语义不匹配，阻碍相互理解，影响任务执行效果。需要解决多智能体AI原生语义通信中的潜在空间不对齐问题。

Method: 在接入点共享语义预均衡器，在用户设备部署本地语义均衡器，采用联邦优化方法进行去中心化训练，同时考虑功率和复杂度约束。

Result: 数值结果验证了该方法在目标导向语义通信中的有效性，揭示了准确性、通信开销、复杂度和AI原生通信设备语义邻近度之间的关键权衡。

Conclusion: 提出的联邦优化方法能有效缓解多智能体AI原生语义通信中的潜在空间不对齐问题，促进相互理解和任务导向通信。

Abstract: Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that shares a semantic pre-equalizer at the AP and local semantic equalizers at user devices, fostering mutual understanding and task-oriented communication while considering power and complexity constraints. To achieve this, we employ a federated optimization for the decentralized training of the semantic equalizers at the AP and user sides. Numerical results validate the proposed approach in goal-oriented semantic communication, revealing key trade-offs among accuracy, com- munication overhead, complexity, and the semantic proximity of AI-native communication devices.

</details>
