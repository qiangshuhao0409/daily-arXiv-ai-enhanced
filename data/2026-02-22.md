<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [HyRA: A Hybrid Resource Allocation Framework for RAN Slicing](https://arxiv.org/abs/2602.16952)
*Mohammad Zangooei,Bo Sun,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: HyRA是一个用于RAN切片的混合资源分配框架，结合了专用分配和共享资源池，在保证性能隔离的同时提高资源效率，相比纯专用或纯共享方案可节省50-75%频谱资源。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络需要灵活高效的RAN资源管理来满足多样化SLA要求。现有RAN切片框架主要依赖每切片资源预留，虽然能保证性能隔离，但在突发流量下会导致资源利用率低下。

Method: 提出HyRA混合资源分配框架，结合专用分配和跨切片共享资源池。将设计建模为双层随机优化问题：外层确定专用和共享资源预算，内层采用新颖的注水算法进行每用户调度。通过样本平均近似、KKT条件和Big-M编码，将问题转化为可求解的混合整数规划。

Result: 在不同需求模式、SLA配置和流量突发性下的广泛仿真表明，HyRA相比纯专用和纯共享基线方案，可实现高达50-75%的频谱节省。

Conclusion: HyRA是未来移动网络中实现资源高效、SLA合规的RAN切片的可行方法，在保持性能隔离的同时显著提高资源利用率。

Abstract: The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks.

</details>


### [2] [Robust and Extensible Measurement of Broadband Plans with BQT+](https://arxiv.org/abs/2602.16969)
*Laasya Koduru,Sylee Beltiukov,Alexander Nguyen,Eugene Vuong,Jaber Daneshamooz,Tejas Narechania,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: BQT+ 是一个宽带计划测量框架，使用声明式状态/动作规范替代传统工作流，支持对64家ISP的纵向监控，用于宽带基础设施投资评估和政策研究。


<details>
  <summary>Details</summary>
Motivation: 评估宽带基础设施投资（如420亿美元的BEAD计划）需要地址级别的宽带数据，包括可用性、质量和可负担性的纵向可见性。现有系统无法满足三个关键需求：对频繁界面演变的鲁棒性、跨数百家提供商的扩展性，以及非专家用户的低技术开销。

Method: BQT+ 将查询意图建模为交互状态空间，形式化为抽象非确定性有限自动机（NFA），在运行时选择执行路径以适应替代交互流程和本地化界面变化。它用声明式状态/动作规范替代传统单体工作流。

Result: BQT+ 能够持续监控64家ISP，支持查询超过100家ISP。应用于两个政策研究：构建BEAD拨款前基准线，以及在四个州超过124,000个地址上评估宽带可负担性。

Conclusion: BQT+ 框架成功解决了宽带数据收集的三个关键系统需求，为宽带基础设施投资评估提供了有效的测量工具，支持重要的政策研究。

Abstract: Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.
  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states.

</details>


### [3] [RIS Control through the Lens of Stochastic Network Calculus: An O-RAN Framework for Delay-Sensitive 6G Applications](https://arxiv.org/abs/2602.17198)
*Oscar Adamuz-Hinojosa,Lanfranco Zanzi,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.NI

TL;DR: 提出DARIO框架，通过动态分配RIS设备给用户，在6G网络中实现上行延迟最小化，满足异构用户的延迟和可靠性需求


<details>
  <summary>Details</summary>
Motivation: 现有RIS控制方案对快速变化的网络条件响应不足，限制了其在超可靠低延迟通信中的应用，特别是在多RIS场景下需要满足异构用户的延迟和可靠性需求

Method: 提出DARIO框架，基于随机网络演算模型分析估计延迟边界，通过非线性整数规划公式化问题，使用在线启发式算法实现近优性能

Result: 仿真和真实流量追踪评估显示，在高负载或RIS可用性条件下，延迟减少高达95.7%

Conclusion: DARIO框架能有效动态分配RIS资源，显著降低上行延迟，满足6G网络中异构用户的延迟和可靠性需求

Abstract: Reconfigurable Intelligent Surfaces (RIS) enable dynamic electromagnetic control for 6G networks, but existing control schemes lack responsiveness to fast-varying network conditions, limiting their applicability for ultra-reliable low latency communications. This work addresses uplink delay minimization in multi-RIS scenarios with heterogeneous per-user latency and reliability demands. We propose Delay-Aware RIS Orchestrator (DARIO), an O-RAN-compliant framework that dynamically assigns RIS devices to users within short time windows, adapting to traffic fluctuations to meet per-user delay and reliability targets. DARIO relies on a novel Stochastic Network Calculus (SNC) model to analytically estimate the delay bound for each possible user-RIS assignment under specific traffic and service dynamics. These estimations are used by DARIO to formulate a Nonlinear Integer Program (NIP), for which an online heuristic provides near-optimal performance with low computational overhead. Extensive evaluations with simulations and real traffic traces show consistent delay reductions up to 95.7% under high load or RIS availability.

</details>


### [4] [Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare](https://arxiv.org/abs/2602.17209)
*Alejandro Flores,Danial Shafaie,Konstantinos Ntontin,Elli Kartsakli,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 研究分层非地面网络作为边缘云平台，用于远程医疗设施或医疗物联网设备的任务计算，通过HAPS提供本地MEC服务，LEO卫星连接远程云服务器，各层自私地最大化自身效用。


<details>
  <summary>Details</summary>
Motivation: 为远程医疗设施和医疗物联网设备提供可靠的计算平台，解决偏远地区医疗设备计算资源不足的问题，通过分层架构结合边缘计算和云计算的优势。

Method: 采用分层非地面网络架构：HAPS提供本地MEC服务，LEO卫星作为桥梁连接远程云服务器；建立三层自私效用最大化模型，考虑本地延迟成本，制定最优任务成本和带宽分配策略。

Result: 提出了分层网络中各层的最优任务成本公式，并找到了相应的最优带宽分配方案，使各层在自私行为下仍能有效协作完成任务计算。

Conclusion: 分层非地面网络架构能有效支持远程医疗计算需求，通过优化任务成本和带宽分配，可以在各层自私最大化效用的前提下实现系统整体性能优化。

Abstract: In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.

</details>


### [5] [End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2602.17381)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.NI

TL;DR: 本文提出了一种测量远程操作自动驾驶车辆端到端延迟的框架，包括运动到运动延迟和玻璃到玻璃延迟，通过实验发现总延迟约500毫秒，其中M2M延迟占比高达60%。


<details>
  <summary>Details</summary>
Motivation: 现有延迟评估方法主要关注玻璃到玻璃延迟，但这只是远程驾驶员体验的总延迟的一部分。需要同时测量运动到运动延迟和玻璃到玻璃延迟，才能全面评估端到端延迟。

Method: 使用陀螺仪、光电晶体管和两个GPS同步的Raspberry Pi 5单元构建测量框架。通过低通滤波和阈值检测识别远程操作端和车辆端的转向盘运动，光电晶体管检测摄像头视野内LED激活来生成中断。

Result: 在商用4G和5G网络上对远程操作原型车辆进行初始测量，平均端到端延迟约为500毫秒（测量精度+/-4毫秒），其中M2M延迟贡献了高达60%的总延迟值。

Conclusion: 提出的测量框架能够全面量化远程操作CAV的端到端延迟，揭示了M2M延迟在总延迟中的显著贡献，这对系统性能优化具有重要意义。

Abstract: Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera's Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value.

</details>


### [6] [Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks](https://arxiv.org/abs/2602.17394)
*Nuno Saavedra,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: SIREN是一个AI驱动的框架，通过语音识别、大语言模型语义提取和NLP验证，将紧急语音通信转换为结构化信息，实现无人机辅助网络中基于语音的感知能力。


<details>
  <summary>Details</summary>
Motivation: 在紧急响应场景中，无人机辅助网络虽然能提供快速灵活的通信，但第一响应者依赖的语音无线电通信具有非结构化特性，无法直接集成到自动化的无人机网络管理中，需要解决语音驱动的感知问题。

Method: 集成自动语音识别(ASR)、基于大语言模型(LLM)的语义提取和自然语言处理(NLP)验证，将紧急语音流量转换为包含响应单位、位置参考、紧急严重性和QoS需求的结构化机器可读信息。

Result: 在包含语言变化、说话者数量、背景噪声和消息复杂度的合成紧急场景中评估，结果显示在不同操作条件下具有稳健的转录和可靠的语义提取能力，主要限制因素是说话者分离和地理歧义。

Conclusion: 该研究证实了语音驱动的态势感知在无人机辅助网络中的可行性，为紧急响应操作中的人机协同决策支持和自适应网络管理奠定了实践基础。

Abstract: Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.

</details>


### [7] [ACOS: Arrays of Cheap Optical Switches](https://arxiv.org/abs/2602.17449)
*Daniel Amir,Ori Cohen,Jakob Krebs,Mark Silberstein*

Main category: cs.NI

TL;DR: ACOS使用低成本低端口数的光路交换机阵列替代昂贵的高端口数光路交换机，通过应用协同设计实现可重构网络，在保持大规模LLM训练性能的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练对集群网络需求巨大，现有光路交换机方案依赖昂贵的高端口数OCS且重配置慢，限制了可扩展性和性能。需要更经济高效的网络架构。

Method: 提出ACOS架构，使用低成本低端口数的光路交换机作为构建模块，通过应用协同设计支持拓扑选择、工作负载适应和故障恢复等重配置需求。

Result: ACOS在模拟中匹配全配置包交换网络的性能，训练最先进的大语言模型时显著降低成本，使用现成商用OCS实现强带宽扩展和未来更高成本节省。

Conclusion: ACOS通过应用协同设计和低成本低端口数OCS阵列，突破了现有专用ML网络的可扩展性限制，在保持性能的同时实现显著成本节约，具有良好未来扩展性。

Abstract: Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.
  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future.

</details>


### [8] [HAP Networks for the Future: Applications in Sensing, Computing, and Communication](https://arxiv.org/abs/2602.17534)
*Sultan Çoğay,T. Tolga Sari,Muhammad Nadeem Ali,Byung-Seo Kim,Gökhan Seçinti*

Main category: cs.NI

TL;DR: 本文综述了高空平台(HAPs)在非地面网络中的应用，包括先进空中通信、集成感知和空中信息学，分析了数据处理、网络性能、计算存储需求、经济可行性和监管挑战。


<details>
  <summary>Details</summary>
Motivation: 高空平台(HAPs)作为非地面网络的重大进步，提供了广泛的覆盖范围和独特能力，在卫星系统与地面网络之间形成关键连接，并在下一代通信技术中发挥重要作用。

Method: 通过文献综述方法，评估HAP中心应用现状，重点考察数据处理、网络性能、计算存储需求、经济可行性和监管挑战等方面。

Result: 分析突出了HAPs在全球通信中不断演变的角色，识别了支持其部署的未来研究方向。

Conclusion: HAPs在下一代通信网络中具有重要地位，但需要解决技术、经济和监管方面的挑战才能实现全面部署。

Abstract: High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment.

</details>


### [9] [EDRP: Enhanced Dynamic Relay Point Protocol for Data Dissemination in Multi-hop Wireless IoT Networks](https://arxiv.org/abs/2602.17619)
*Jothi Prasanna Shanmuga Sundaram,Magzhan Gabidolla,Luis Fujarte,Shawn Duong,Jianlin Guo,Toshiaki Koike-Akino,Pu,Wang,Kieran Parsons,Philip V. Orlik,Takenori Sumi,Yukimasa Nagai,Miguel A. Carreira-Perpinan,Alberto E. Cerpa*

Main category: cs.NI

TL;DR: EDRP协议通过LQ-CSMA和ML-BSS算法改进DRP，应对现实链路质量波动，提升物联网数据传输效率


<details>
  <summary>Details</summary>
Motivation: 现有DRP协议在现实环境中无法适应链路质量波动，导致多个发送者传输重叠，触发CSMA随机退避延迟，降低有效吞吐量

Method: 1. 理论分析链路质量波动和被动确认的设计要求；2. 设计LQ-CSMA，根据实时链路质量估计动态限制退避延迟范围；3. 开发ML-BSS算法预测未来链路质量并优化无速率编码块大小

Result: 现场评估显示EDRP相比竞争协议平均提升39.43%的有效吞吐量

Conclusion: EDRP通过结合链路质量感知的CSMA和机器学习块大小选择，有效解决了现实链路波动问题，显著提升了物联网数据传输性能

Abstract: Emerging IoT applications are transitioning from battery-powered to grid-powered nodes. DRP, a contention-based data dissemination protocol, was developed for these applications. Traditional contention-based protocols resolve collisions through control packet exchanges, significantly reducing goodput. DRP mitigates this issue by employing a distributed delay timer mechanism that assigns transmission-start delays based on the average link quality between a sender and its children, prioritizing highly connected nodes for early transmission. However, our in-field experiments reveal that DRP is unable to accommodate real-world link quality fluctuations, leading to overlapping transmissions from multiple senders. This overlap triggers CSMA's random back-off delays, ultimately degrading the goodput performance.
  To address these shortcomings, we first conduct a theoretical analysis that characterizes the design requirements induced by real-world link quality fluctuations and DRP's passive acknowledgments. Guided by this analysis, we design EDRP, which integrates two novel components: (i) Link-Quality Aware CSMA (LQ-CSMA) and (ii) a Machine Learning-based Block Size Selection (ML-BSS) algorithm for rateless codes. LQ-CSMA dynamically restricts the back-off delay range based on real-time link quality estimates, ensuring that nodes with stronger connectivity experience shorter delays. ML-BSS algorithm predicts future link quality conditions and optimally adjusts the block size for rateless coding, reducing overhead and enhancing goodput. In-field evaluations of EDRP demonstrate an average goodput improvement of 39.43\% than the competing protocols.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 探索LLM、RAG和GraphRAG在生成设计结构矩阵(DSM)方面的潜力，通过两个实际用例评估性能


<details>
  <summary>Details</summary>
Motivation: 探索自动化生成设计结构矩阵(DSM)的可能性，以解决传统手动创建DSM的效率和准确性挑战

Method: 使用大型语言模型(LLM)、检索增强生成(RAG)和图基RAG(GraphRAG)方法，在电动螺丝刀和CubeSat两个已知架构参考的用例上进行测试

Result: 尽管存在设计和计算挑战，但识别出了自动化DSM生成的机会，所有代码已公开供复现和领域专家进一步反馈

Conclusion: LLM、RAG和GraphRAG在DSM生成方面具有潜力，特别是在确定预定义组件关系和识别组件及其关系等任务上

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [11] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE本体论为法医牙科年龄评估提供了一个标准化的语义框架，整合人工和AI辅助工作流程，提高透明度、可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 年龄评估在法医和司法决策中至关重要，尤其涉及无证件个人和无人陪伴未成年人时。当前牙科年龄评估方法存在异质性、数据碎片化和系统互操作性差等问题，AI方法的采用进一步加剧了透明度和可重复性挑战。

Method: 开发领域特定的AIdentifyAGE本体论，基于上层和已建立的生物医学、牙科及机器学习本体论，与领域专家合作开发。该本体论建模完整的法医-法律工作流程，整合司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和AI估计方法。

Result: AIdentifyAGE本体论提供了一个标准化的语义一致框架，能够追踪观察、方法、参考数据和报告结果之间的关联，确保互操作性、可扩展性和符合FAIR原则。

Conclusion: 该本体论是提高一致性和透明度的基础步骤，为法医-法律和司法环境中的本体驱动决策支持系统奠定坚实基础，增强年龄评估的可解释性。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [12] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 单状态复用必然导致语境性，这是经典概率表示中的基本限制，而非量子力学特有现象


<details>
  <summary>Details</summary>
Motivation: 自适应系统常在多个语境中复用固定内部状态空间，这种单状态复用普遍存在于自然和人工智能中，但其基本表示后果尚不清楚

Method: 将语境建模为作用于共享内部状态的干预，证明任何再现语境性结果统计的经典模型都必须承担不可约的信息论成本

Result: 证明了语境性不是量子力学的特性，而是经典概率表示中单状态复用的必然结果；提供了最小构造示例来具体实现这种成本

Conclusion: 语境性是自适应智能的一般表示约束，与物理实现无关；非经典概率框架通过放松单一全局联合概率空间假设来避免这种障碍

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [13] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache：一个利用可重构缓存进行高效大规模人类移动模拟的框架，通过潜在空间推理重用和轻量解码器提升效率，同时保持与最先进LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模人类移动模拟对于城市规划、流行病学和交通分析等应用至关重要。现有方法使用大语言模型作为人类代理进行模拟，但计算成本过高限制了可扩展性。

Method: 设计MobCache框架，包含：1) 推理组件：将推理步骤编码为潜在空间嵌入，使用潜在空间评估器实现推理步骤的重用和重组；2) 解码组件：采用移动规律约束蒸馏训练的轻量解码器，将潜在空间推理链转换为自然语言。

Result: 实验表明，MobCache在多个维度上显著提高了效率，同时保持了与最先进LLM方法相当的性能。

Conclusion: MobCache通过可重构缓存框架解决了LLM模拟人类移动的高计算成本问题，实现了高效的大规模模拟，为实际应用提供了可行的解决方案。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [14] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准已饱和，且饱和率随基准使用时间增长而增加。研究识别了影响饱和的关键设计因素，为创建更持久的评估基准提供指导。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳模型，降低了其长期价值。需要了解哪些设计因素导致饱和，以创建更持久的评估基准。

Method: 研究分析了60个LLM基准测试，从主要模型开发商的技术报告中选取。通过14个属性（任务设计、数据构建、评估格式）对基准进行特征化。测试了5个假设，检验每个属性如何影响饱和率。

Result: 近半数基准测试表现出饱和，饱和率随基准使用时间增长而增加。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试的寿命，为创建更持久的评估策略提供信息。专家策划的基准设计比数据保密更能有效抵抗饱和。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [15] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 论文发现简单的代码进化基线方法在数学边界优化、智能体脚手架设计和机器学习竞赛三个领域中都匹配或超越了更复杂的方法，指出当前代码进化研究存在评估不足和过度复杂化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前代码进化技术虽然表现出色，但缺乏与简单基线的系统比较。作者旨在验证简单方法是否真的不如复杂方法，并分析代码进化研究中的潜在问题。

Method: 在三个领域测试简单基线方法：1) 数学边界优化，2) 智能体脚手架设计，3) 机器学习竞赛。通过对比分析复杂方法与简单基线的表现，识别代码进化研究中的问题。

Result: 简单基线在所有三个领域都匹配或超越了更复杂的代码进化方法。研究发现：数学边界优化的关键是好搜索空间设计而非搜索算法；智能体脚手架设计因高方差和小数据集导致次优选择；需要更好的评估方法减少随机性。

Conclusion: 代码进化研究需要更严谨的评估，应关注搜索空间设计、减少评估随机性，并提出未来工作的最佳实践建议。复杂方法不一定优于简单方法。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [16] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文改进了n维超立方体边切片所需的最小超平面数上界，从之前的⌈5n/6⌉改进到⌈4n/5⌉（除n为5的奇数倍时需加1），并利用AI工具CPro1发现了Q₁₀的8个超平面切片构造。


<details>
  <summary>Details</summary>
Motivation: 研究n维超立方体Q_n的边切片问题：需要多少超平面才能与超立方体的每条边在其内部相交？这个问题在组合几何和离散几何中有重要意义，自1971年Paterson给出上界⌈5n/6⌉后一直未改进。

Method: 1. 理论分析改进上界：证明S(n) ≤ ⌈4n/5⌉（n不是5的奇数倍时），或S(n) ≤ 4n/5 + 1（n是5的奇数倍时）
2. 使用AI工具CPro1：结合推理大语言模型和自动超参数调优，搜索数学构造
3. 具体构造：找到Q₁₀的8个超平面切片方案，作为关键实例

Result: 1. 改进了S(n)的上界：从Paterson的⌈5n/6⌉改进到⌈4n/5⌉（或4n/5 + 1）
2. 获得了使用k < n个超平面时能切片的最大边数的新下界
3. 成功构造了Q₁₀的8个超平面切片方案，验证了改进上界的可行性

Conclusion: 该论文显著改进了超立方体边切片问题的上界，展示了AI辅助数学发现的有效性。CPro1工具成功帮助找到了关键构造，为组合几何问题的解决提供了新方法。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [17] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流，用于加速中子散射设施中的晶体结构分析，将手动时间从435分钟减少到约90分钟（4.6-5.0倍加速），同时生成经过验证的CIF文件。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临分析和报告延迟问题，特别是对于结构复杂的样品，传统的手动分析流程耗时且效率低下，限制了科学产出速度。

Method: 开发了受治理的工具使用AI工作流（NeuDiff Agent），通过白名单工具限制、关键工作流边界的故障关闭验证门以及完整溯源捕获，自动执行数据还原、积分、精修和验证的完整晶体结构分析流程。

Result: 在基准测试中，NeuDiff Agent将分析时间从435分钟（手动）减少到86.5-94.4分钟（4.6-5.0倍加速），生成无checkCIF A/B级警报的验证CIF文件，同时保持完全可追溯性。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了一条实用路径，在保持可追溯性和出版验证要求的同时，显著提高了分析效率和科学产出速度。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [18] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning范式：将智能部署在边缘节点，通过选择性对等交互扩展学习，避免集中式AI的成本和脆弱性问题


<details>
  <summary>Details</summary>
Motivation: AI向边缘扩展时，集中式智能面临数据传输、延迟、能耗和对大型数据中心的依赖等瓶颈，在异构、移动和资源受限环境中扩展性差

Method: 节点从本地数据持续学习，维护自身模型状态，在有益时通过机会性对等交互交换知识。学习通过重叠和扩散传播，而非全局同步或中心聚合

Result: 概念论文建立了该范式的理论基础，对比了现有去中心化方法，并探讨了对通信、硬件、信任和治理的影响

Conclusion: Node Learning不抛弃现有范式，而是将其置于更广泛的去中心化视角中，统一了自主和协作行为，适应数据、硬件、目标和连接的异构性

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [19] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出一个基于序理论的犹豫模糊集统一评分框架，证明经典序不构成格结构，而对称序满足评分函数的关键规范准则，并引入优势函数用于犹豫模糊元素排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论基础，需要建立形式化的评分框架，使评分相对于给定序明确定义，实现更灵活一致的评分机制。

Method: 1. 提出序导向的统一评分框架；2. 分析犹豫模糊元素上的经典序，证明其不构成格结构；3. 证明对称序定义的评分满足强单调性和Gärdenfors条件；4. 引入优势函数类，包含离散优势函数和相对优势函数两个具体实例。

Result: 1. 经典序不诱导格结构；2. 对称序定义的评分满足评分函数的关键规范准则；3. 优势函数可用于构建犹豫模糊集上的模糊偏好关系，支持群体决策。

Conclusion: 本文建立了基于序理论的犹豫模糊集评分统一框架，证明了对称序的优越性，提出的优势函数为犹豫模糊元素排序提供了有效工具，可应用于群体决策支持。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [20] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR是一个针对12种印度和南亚语言的安全对齐基准测试，揭示了多语言漏洞：合同约束会提高拒绝率但无法阻止越狱攻击，英语到印度语言的攻击转移性强，罗马化输入会降低越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐评估主要集中于英语和合同约束，忽略了多语言漏洞，特别是南亚地区用户经常进行代码切换和罗马化输入，需要专门的多语言安全基准测试。

Method: 提出Indic Jailbreak Robustness (IJR)基准，覆盖12种印度和南亚语言，包含45216个提示，分为JSON（合同约束）和Free（自然语言）两个轨道，采用无裁判评估方法。

Result: 发现三个关键模式：1）合同约束提高拒绝率但无法阻止越狱；2）英语到印度语言的攻击转移性强，格式包装优于指令包装；3）罗马化或混合输入降低越狱成功率，与罗马化比例和分词相关。

Conclusion: IJR揭示了仅关注英语和合同约束的评估会隐藏多语言安全风险，特别是对南亚用户，需要更全面的多语言压力测试来确保模型安全。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [21] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是一个多平台GUI代理模型，在20+基准测试中取得SOTA结果，采用混合数据飞轮、统一能力增强和多平台环境RL扩展等创新技术。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在多种平台（桌面、移动、浏览器等）上实现云边协作和实时交互的GUI代理模型，解决现有模型在多平台GUI任务中的性能限制。

Method: 1. 混合数据飞轮：结合模拟环境和云沙箱环境构建UI理解和轨迹生成数据管道；2. 统一能力增强：使用统一思维合成管道增强推理能力，重点提升工具调用、记忆和多代理适应能力；3. 多平台环境RL扩展：提出MRPO算法解决多平台冲突和长时程任务训练效率问题。

Result: 在20+GUI基准测试中取得SOTA结果：GUI自动化任务（OSWorld 56.5，AndroidWorld 71.6，WebArena 48.4），基础任务（ScreenSpotPro 80.3），工具调用任务（OSWorld-MCP 47.6，MobileWorld 46.8），记忆和知识任务（GUI-Knowledge Bench 75.5）。

Conclusion: GUI-Owl-1.5通过创新的数据收集、能力增强和训练方法，在多平台GUI任务中实现了最先进的性能，为云边协作和实时交互提供了有效的解决方案，并已开源。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [22] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个能让LLM自动创建具有自生成拓扑和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 现有智能体开发套件要么功能支持不足，要么依赖人工手动设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建和管理子智能体及工具集，采用分层图结构内存系统，并提供专门针对软件工程任务的工具包

Result: 在三个最先进基准测试中，使用不同骨干模型的实验表明OpenSage优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage能为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的范式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [23] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有防御措施无法有效缓解长视野威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长视野复杂环境中的部署增加，它们面临利用多轮用户-智能体-环境交互的长视野攻击风险，这些攻击在单轮设置中无法实现。目前缺乏专门评估智能体对此类攻击脆弱性的基准测试。

Method: 提出AgentLAB基准测试框架，支持5种新型攻击类型：意图劫持、工具链攻击、任务注入、目标漂移和内存污染，覆盖28个真实智能体环境和644个安全测试用例。使用该基准评估代表性LLM智能体。

Result: 评估发现LLM智能体对长视野攻击仍然高度脆弱；为单轮交互设计的防御措施无法可靠缓解长视野威胁。AgentLAB可作为跟踪实际环境中LLM智能体安全进展的基准。

Conclusion: AgentLAB是首个专门评估LLM智能体长视野攻击脆弱性的基准测试，揭示了现有智能体的安全漏洞和防御措施的不足，为实际环境中LLM智能体安全研究提供了重要工具。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [24] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面逐步导航到目标页面。前沿模型在简单级别表现优异甚至超越人类，但在困难级别性能急剧下降，最佳模型成功率仅23%，揭示了当前推理系统在规划、长视野推理和失败后重新规划方面的明显局限。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是它们在真实世界概念连接和前瞻规划方面的表现。现有基准可能无法充分测试这些复杂能力，因此需要创建一个简单但具有挑战性的任务来揭示当前模型的局限性。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接从给定源页面逐步导航到目标页面。评估了包括Gemini-3、GPT-5和Claude Opus 4.5在内的广泛开源和闭源模型，设置了不同难度级别（简单和困难），并进行轨迹级分析来理解模型的行为模式。

Result: 前沿模型在简单级别表现优异，甚至超越人类水平。但在困难级别，性能急剧下降：最佳模型Gemini-3的成功率仅为23%。分析表明世界知识是成功的必要条件，但达到一定阈值后，规划和长视野推理能力成为主导因素。轨迹分析显示即使最强模型在失败后也难以重新规划，经常陷入循环。

Conclusion: LLM-Wikirace是一个简单但有效的基准测试，揭示了当前推理系统在规划、长视野推理和失败恢复方面的明显局限性。尽管前沿模型在某些方面表现优异，但在复杂规划任务中仍有很大提升空间。该基准为规划能力强的LLMs提供了一个开放的竞技场，代码和排行榜已公开。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [25] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 在视觉语言模型上进行窄域有害数据微调会导致严重的跨任务和跨模态的泛化性错位，即使只有10%有害数据也会显著降低对齐性，且多模态评估比文本评估揭示更严重的错位问题。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力和保持安全对齐之间产生了根本性矛盾。研究者希望探究在已对齐的视觉语言模型上进行窄域有害数据微调会如何影响其安全对齐性。

Method: 使用Gemma3-4B模型进行实验，通过LoRA（低秩适应）在不同秩参数下进行微调，评估文本和多模态两种评估方式下的错位程度。进行几何分析以理解有害行为的低维子空间特性，并评估两种缓解策略：良性窄域微调和基于激活的引导。

Result: 微调导致严重的泛化性错位，错位程度随LoRA秩单调增加；多模态评估显示错位率（70.71±1.22，r=128）远高于文本评估（41.19±2.51）；即使训练数据中只有10%有害数据也会导致显著对齐退化；几何分析显示有害行为占据极低维子空间（10个主成分即可捕获大部分错位信息）；两种缓解策略虽能显著减少错位，但无法完全消除已学习的有害行为。

Conclusion: 当前的后训练范式在部署后环境中可能无法充分保持对齐性，需要开发更鲁棒的持续学习框架来应对终身多模态智能体在适应新任务时面临的安全对齐挑战。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [26] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的监控框架，使用RNN架构追踪多轮对话中的用户意图演变，显著提升越狱攻击检测性能，同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护大多是状态无关的，将多轮对话视为独立事件处理，导致无法检测跨轮次的渐进式恶意意图积累（如Crescendo和ActorAttack攻击），存在"安全漏洞"。

Method: 提出DeepContext状态监控框架，采用RNN架构处理序列化的细粒度轮次嵌入，通过隐藏状态在对话中传播，捕捉风险增量积累，替代孤立评估模型。

Result: 在多轮越狱检测中显著优于现有基线，达到SOTA的F1分数0.84，优于云提供商防护和开源模型（Llama-Prompt-Guard-2和Granite-Guardian均为0.67），在T4 GPU上保持低于20ms的推理开销。

Conclusion: 建模意图的序列演化比部署大规模状态无关模型更有效且计算效率更高，为实时应用提供了可行的状态感知安全防护方案。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [27] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench是一个评估LLM引用网页源质量的基准测试，涵盖100个真实查询和8个质量指标，揭示了AI搜索工具在引用源质量方面的关键问题


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注答案正确性，而忽视了引用源的质量。随着LLM越来越多地通过引用网页源来回答问题，需要系统评估这些引用源的质量

Method: 开发了SourceBench基准测试，包含100个真实世界查询（涵盖信息、事实、论证、社交、购物等意图），使用8个指标框架评估内容质量和页面级信号，并构建了人工标注数据集和匹配专家判断的LLM评估器

Result: 评估了8个LLM、Google搜索和3个AI搜索工具，分析了3996个引用源。研究揭示了四个关键新见解，可为生成式AI和网络搜索的未来研究提供指导

Conclusion: SourceBench填补了LLM引用源质量评估的空白，揭示了当前AI搜索工具在引用源质量方面的问题，为未来研究提供了重要方向和基准

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [28] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现LLM的文本安全性与工具调用安全性存在显著差异，即使文本拒绝有害请求，工具调用仍可能执行危险操作，需要专门的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前安全评估主要关注文本层面的拒绝行为，但LLM作为智能体通过工具调用与外部系统交互时，文本安全是否等同于工具调用安全尚不明确，存在评估盲区。

Method: 提出GAP基准测试框架，在6个监管领域、7种越狱场景、3种系统提示条件下测试6个前沿模型，产生17,420个数据点，量化文本安全与工具调用安全之间的差异。

Result: 文本安全无法迁移到工具调用安全，所有模型都存在文本拒绝但工具执行危险操作的情况；系统提示对工具调用行为影响显著；运行时治理合约能减少信息泄露但无法阻止工具调用尝试。

Conclusion: 仅依赖文本安全评估不足以评估智能体行为，工具调用安全需要专门的测量和缓解措施，现有对齐方法在工具调用层面存在安全漏洞。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [29] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov：一种离线代理学习框架，通过执行验证数据筛选、策略感知数据合成和最差状态优先采样，在硬件验证任务中实现高效学习，4B参数模型达到69.2%覆盖率通过率。


<details>
  <summary>Details</summary>
Motivation: 基于执行的LLM代理学习需要昂贵的工具反馈，特别是在硬件验证等依赖工业模拟器和非可微执行信号的任务中，在线强化学习不切实际。

Method: 将验证建模为由确定性评估器引导的无记忆状态转移，提出执行验证数据筛选、策略感知代理数据合成和最差状态优先采样，构建离线学习框架。

Result: 4B参数模型在代理评估下达到69.2%覆盖率通过率，比其教师模型提升5.3%，性能可与大一个数量级的模型竞争。

Conclusion: LLM4Cov框架通过离线学习方法有效解决了执行反馈昂贵的问题，在硬件验证任务中实现了高效学习，为类似约束环境下的代理学习提供了可行方案。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [30] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom是一个自动化代理劫持框架，通过结构化模板注入攻击LLM代理的架构机制，利用优化的结构化模板诱导角色混淆，显著提升攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有代理劫持攻击主要依赖手动制作、语义驱动的提示操纵，攻击成功率低且对闭源商业模型的可转移性有限。需要一种更有效的攻击方法来揭示LLM代理系统的安全漏洞。

Method: 基于结构化模板注入，通过多级模板增强增加结构多样性，训练模板自编码器将离散模板嵌入连续潜在空间，使用贝叶斯优化高效识别最优对抗向量并解码为高效结构化模板。

Result: 在Qwen、GPT和Gemini上的实验显示，Phantom在攻击成功率和查询效率上显著优于现有基线。在真实商业产品中发现了70多个已获厂商确认的漏洞。

Conclusion: 结构化模板劫持具有实际严重性，为保护下一代代理系统提供了实证基础，揭示了LLM代理架构层面的安全风险。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [31] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一个结合量子变分电路预测、量子退火优化和后量子签名的金融风险系统，在预测精度、投资组合表现和求解速度上均优于经典基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险系统采用预测-优化两步流程，但在实际应用中面临市场变化、离散约束、求解速度慢和审计追溯困难等问题，需要一体化解决方案。

Method: 1) 使用变分量子电路(VQC)预测收益和波动率；2) 将风险-收益目标和约束转化为QUBO问题，采用量子退火求解（经典求解器作为备选）；3) 使用后量子签名对调仓结果进行数字签名，确保可审计性。

Result: 在收益预测误差上降低7.8%，波动率预测误差降低6.1%；样本外夏普比率提升9.4%，最大回撤降低11.7%；QUBO求解时间比混合整数基线减少28%，同时生成完全可追溯的签名分配记录。

Conclusion: HQFS成功将预测、离散风险优化和审计追溯整合到单一流程中，在预测精度、投资组合表现和求解效率方面均显著优于经典方法，为实际金融风险系统提供了可行的量子混合解决方案。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [32] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 黑盒安全评估存在根本限制：当模型行为依赖于部署中常见但评估中罕见的潜在上下文时，任何黑盒评估器都无法可靠估计部署风险。


<details>
  <summary>Details</summary>
Motivation: 挑战AI系统黑盒安全评估的基本假设——测试分布上的模型行为能可靠预测部署性能。当模型输出依赖于评估中罕见但部署中常见的未观测内部变量时，这一假设可能失效。

Method: 通过潜在上下文条件策略的形式化分析，使用Le Cam方法证明被动评估的下界，基于哈希的触发构造和Yao极小极大原理分析自适应评估，在陷门单向函数假设下展示计算分离，并提供白盒探测的样本复杂度分析。

Result: 证明黑盒评估存在根本限制：(1)被动评估的期望绝对误差≥0.208δL；(2)自适应评估的最坏情况误差≥δL/16，检测需要Θ(1/ε)查询；(3)计算分离表明拥有特权信息的部署环境可激活不安全行为，而多项式时间评估器无法区分；(4)白盒探测需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定安全性，为最坏情况安全保证需要额外保障措施：架构约束、训练时保证、可解释性和部署监控。研究量化了黑盒评估的局限性并提供了何时需要额外安全措施的明确标准。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [33] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越行为模仿，评估LLMs的决策质量，区分描述性行为和基于投资者风险偏好的规范性效用。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准主要评估模型模仿用户行为的能力，但在金融咨询领域，观察到的用户行为可能因市场波动而存在噪声或短视，可能与用户的长期目标冲突。将用户选择作为唯一真实标签会混淆行为模仿与决策质量。

Method: 构建Conv-FinRe基准：使用真实市场数据和人类决策轨迹，创建包含入职访谈、逐步市场情境和咨询对话的对话式纵向数据集。模型需要在固定投资期限内生成股票排名。基准提供多视角参考，区分描述性行为与基于投资者风险偏好的规范性效用。

Result: 评估多个最先进的LLMs发现，理性决策质量与行为对齐之间存在持续张力：在基于效用的排名上表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪声。

Conclusion: Conv-FinRe基准能够诊断LLMs是遵循理性分析、模仿用户噪声还是受市场动量驱动，为金融咨询中的推荐系统评估提供了超越行为模仿的新视角。数据集和代码已公开。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [34] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过搜索-验证流程解决时间序列数据库的自然语言查询问题，并提出了首个大规模基准测试NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要新的解决方案来处理时间序列数据库的自然语言查询。

Method: 提出Sonar-TS神经符号框架，采用类似主动声纳的搜索-验证流程：首先通过特征索引和SQL查询候选窗口，然后生成Python程序对原始信号进行验证锁定。

Result: 实验表明Sonar-TS能有效处理传统方法无法解决的复杂时间查询，并揭示了该领域的独特挑战。

Conclusion: 这是首个针对时间序列数据库自然语言查询的系统性研究，提供了通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [35] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似度指数快速筛选，再用基于反正态分布技能桶的Kantorovich距离计算"制裁分数"，实现公平快速的团队匹配。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏中，公平快速的匹配系统对玩家留存和满意度至关重要。然而，为技能水平异质的预组团队创建公平匹配具有挑战性，仅基于平均团队技能指标（如平均或中位数评分）通常会导致不平衡的比赛。

Method: Cinder采用两阶段方法：第一阶段使用Ruzicka相似度指数比较团队的"非异常值"技能范围进行快速初步筛选；第二阶段将玩家排名映射到基于反正态分布生成的技能桶中，然后使用Kantorovich距离计算团队排序桶索引之间的"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟团队配对的制裁分数分布，证明了该系统的可行性，为公平匹配阈值提供了稳健基础。

Conclusion: Cinder系统能够为技能水平异质的预组团队提供公平快速的匹配，解决了传统基于平均技能指标匹配方法的不平衡问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [36] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个用于端到端、项目规模数学自动形式化的智能体框架，能在三周内将数百页数学教材转换为15万行Lean代码，实现96%的证明成功率。


<details>
  <summary>Details</summary>
Motivation: 当前数学自动形式化主要局限于孤立定理和简短片段，难以扩展到教科书和研究论文规模，因为需要处理跨文件依赖、解析导入并确保整个项目能够端到端编译。

Method: M2F采用两阶段框架：1) 语句编译阶段将文档分割为原子块，通过推断依赖关系排序，修复声明骨架直到项目编译；2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺。整个过程保持验证器在循环中，只在工具链反馈确认改进后才提交编辑。

Result: 在三周内将479页实分析和凸分析教材转换为包含153,853行代码的Lean库，在FATE-H基准上实现96%的证明成功率（相比基线80%），实现了教科书规模的自动形式化。

Conclusion: M2F框架证明大规模数学文献的自动形式化是可行的，能以通常需要专家数月或数年努力的速度完成教科书级形式化，为数学自动形式化的实际应用开辟了新途径。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [37] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 微软Dynamics 365 Sales中的销售研究代理AI系统，通过专门的Sales Research Bench基准测试，在8个客户加权维度上显著优于Claude Sonnet 4.5和ChatGPT-5，为企业提供可重复的质量评估方法。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够基于实时定制CRM数据回答销售领导问题的AI系统，但现有模型缺乏透明、可重复的质量证据。

Method: 开发Sales Research Agent AI应用，连接实时CRM和相关数据，在复杂模式上进行推理，通过文本和图表输出决策就绪的洞察。引入Sales Research Bench基准测试，在8个客户加权维度上评估系统质量。

Result: 在2025年10月19日针对定制企业模式的200个问题测试中，Sales Research Agent在100分综合得分上比Claude Sonnet 4.5高出13分，比ChatGPT-5高出24.1分。

Conclusion: Sales Research Agent通过专门的基准测试提供了可重复的质量评估方法，使企业能够透明地比较AI解决方案的质量。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [38] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: PA-MoE：针对RL中单策略网络导致的简单任务偏差问题，提出相位感知的专家混合架构，通过相位路由器实现时间一致的专家分配，提升复杂任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单策略网络导致"简单性偏差"——简单任务占据大部分参数和梯度更新，复杂任务得不到足够容量。传统MoE的token级路由会分散相位一致模式，破坏专家专业化。

Method: 提出相位感知专家混合（PA-MoE）：1）轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2）相位路由器为相同专家分配时间一致的分配，保持相位特定专业知识。

Result: 实验结果证明了PA-MoE的有效性。

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，实现了更好的专家专业化，提升了复杂任务的处理能力。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [39] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR方法通过动态检索最小化系统提示和工具子集，大幅减少LLM代理的上下文长度和成本


<details>
  <summary>Details</summary>
Motivation: LLM代理在运行时需要反复加载长系统指令和大型工具目录，导致成本高、延迟大、工具选择错误率高、代理偏离概率增加

Method: 提出Instruction-Tool Retrieval (ITR)，一种RAG变体，在每一步只检索必要的系统提示片段和最小工具子集，动态组合运行时系统提示并暴露缩小的工具集，带有置信度门控回退机制

Result: 在受控基准测试中，ITR将每步上下文token减少95%，工具路由正确率相对提高32%，端到端成本降低70%，使代理能在上下文限制内运行2-20倍更多循环

Conclusion: ITR特别适用于长时间运行的自主代理，随着代理步数增加，节省效果会累积，论文提供了方法细节、评估协议、消融研究和实际部署操作指南

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [40] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA：通过意图对齐计划记忆的多智能体计算机使用框架，提升长时程任务执行的稳定性和效率


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理在长时程、噪声感知、多窗口环境下存在意图漂移、重复解决常规子问题、错误累积和效率低下等问题

Method: 提出多智能体框架（规划器、优化器、批评器），通过共享记忆将原始交互轨迹抽象为多视图意图表示和可重用技能，运行时通过意图原型检索子目标对齐技能并注入部分计划

Result: 端到端评估中达到74.83%任务成功率，步骤效率比0.91，优于RL和轨迹中心基线；多视图意图抽象和共享计划记忆共同提升执行稳定性

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [41] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 本文提出了一个评估大型推理模型忠实性的正式框架，发现49.7%的输出存在不忠实问题，准确率不能作为忠实性的可靠代理指标。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能强大，但常常产生听起来合理却未能反映真实决策过程的推理，这削弱了可靠性和信任度。需要建立正式框架来评估推理的忠实性。

Method: 提出了一个正式的推理忠实性框架，定义了两个可测试条件：立场一致性和因果影响。开发了RFEval基准测试，包含7,186个实例，通过受控的输出级反事实干预来探测忠实性。评估了12个开源大型推理模型。

Result: 发现49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱的收敛领域。与规模相比，后训练机制与不忠实性更相关：在监督微调基础上添加当前RL风格目标会降低推理忠实性，即使准确率保持不变。准确率既不是忠实性的充分条件，也不是可靠代理指标。

Conclusion: 建立了一个严谨的方法论来审计大型推理模型的可靠性，表明可信AI不仅需要优化正确结果，还需要优化推理过程的结构完整性。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [42] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 提出S2Q方法，通过多个子价值函数保留替代高价值动作，增强多智能体强化学习的探索能力和适应性


<details>
  <summary>Details</summary>
Motivation: 现有基于价值分解的多智能体强化学习方法依赖单一最优动作，在价值函数变化时难以适应，容易收敛到次优策略

Method: 提出连续子价值Q学习(S2Q)，学习多个子价值函数来保留替代高价值动作，结合基于Softmax的行为策略促进持续探索

Result: 在挑战性MARL基准测试中，S2Q持续优于多种MARL算法，展现出更好的适应性和整体性能

Conclusion: S2Q通过多个子价值函数有效解决了价值函数变化时的适应性问题，提升了多智能体强化学习的探索能力和性能

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [43] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术，使用轻量级线性预测器从静态标记特征估计样本难度，相比传统方法计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有的课程学习方法需要预定义的难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在通过更高效的方式识别困难样本来加速模型收敛。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的标记级特征（标记频率、序列长度、词汇多样性和稀有标记比例）来估计样本难度，动态构建优先处理高损失样本的批次。

Result: 在130M参数transformer上的实验显示，PBS实现了6-13%的收敛加速（通过训练检查点的评估损失衡量），预测器与真实损失的相关系数从0.14提升到0.44（超过10,000训练步）。

Conclusion: 标记频率统计编码了关于样本难度的有意义信息，使得能够以可忽略的计算开销实现有效的课程学习，为训练优化提供了高效的新方法。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [44] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI编程代理在GitHub上创建的PR在描述特征上存在差异，这些差异会影响人类评审员的参与度、响应时间和合并结果


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI编程代理在GitHub上自主创建PR，但不同代理在PR描述特征上的差异以及人类评审员如何响应这些差异尚未得到充分研究

Method: 使用AIDev数据集对五个AI编程代理创建的PR进行实证分析，分析PR描述的结构特征，并检查人类评审员在评审活动、响应时间、情感和合并结果方面的响应

Result: AI编程代理展现出不同的PR描述风格，这些风格与评审员参与度、响应时间和合并结果的差异相关；不同代理在评审互动指标和合并率上存在显著差异

Conclusion: PR呈现方式和评审员互动动态在人类-AI协作软件开发中起着重要作用，AI代理的PR描述特征会影响人类评审员的响应和最终结果

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [45] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 该论文探讨了在6G物理层中应用基于大语言模型的智能代理实现意图驱动的自主通信，提出了AgenCom案例研究来展示意图感知的链路决策


<details>
  <summary>Details</summary>
Motivation: 6G系统功能复杂性增加，用户需求从单一指标转向多维目标（时延敏感性、能耗偏好、计算约束等），且这些目标会随时间变化。传统基于规则的控制无法满足这些动态、多维的意图需求，需要转向意图驱动的自主智能系统。

Method: 采用基于大语言模型的智能代理框架，构建意图感知、自主决策和网络执行的闭环管道。研究多模态感知、跨层决策和可持续优化等关键技术，并通过AgenCom案例展示意图驱动的链路决策代理如何根据用户偏好和信道条件自适应构建通信链路。

Result: 论文提出了在6G物理层实现代理式AI的路径，识别了意图感知和自主性的应用场景，讨论了关键挑战和技术使能因素。AgenCom案例展示了智能代理能够有效整合异构信息，将自然语言意图转化为可执行的控制和配置决策。

Conclusion: 基于大语言模型的智能代理为6G系统提供了实现意图驱动自主通信的有前景的解决方案，能够更好地理解通信环境和用户意图，支持可持续演进的6G通信。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [46] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出STRIDE和SR-Delta框架，通过人机协作生成可信的基准数据集，以解决不同ESG评级机构评分差异大、缺乏可比性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前不同ESG评级机构对同一公司的评分差异很大，限制了评级的可比性、可信度和决策相关性，需要一种方法来协调和评估这些评级方法。

Method: 提出包含两个互补部分的框架：STRIDE（提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集）和SR-Delta（差异分析程序框架，揭示潜在调整的见解）。

Result: 该框架能够实现可持续性评级方法的可扩展和可比评估，为评估不同评级方法提供了系统化工具。

Conclusion: 呼吁AI社区采用AI驱动的方法来加强和推进可持续性评级方法，以支持和执行紧迫的可持续性议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [47] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 本文提出O-Shap方法，通过满足T特性的语义分割改进Owen值在视觉任务中的特征归因，解决传统SHAP方法中特征独立性假设失效的问题。


<details>
  <summary>Details</summary>
Motivation: 在视觉任务中，SHAP方法基于特征独立性的假设失效，因为像素之间存在空间和语义依赖关系。虽然现代SHAP实现引入了支持分组归因的Owen值，但其效果严重依赖于特征分组的定义方式，而常用的分割方法（如轴对齐或SLIC）违反了关键的一致性特性。

Method: 提出新的分割方法，满足T特性以确保层次结构间的语义对齐。该方法通过语义对齐的分层结构实现计算剪枝，同时提高归因准确性和可解释性。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的场景中表现更佳。

Conclusion: 通过满足T特性的语义分割改进Owen值的分组定义，能够有效解决视觉任务中特征依赖性问题，提升SHAP方法的实际应用效果。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [48] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG：基于课程讲义自动构建教师对齐知识图谱的框架，用于捕捉学习依赖关系


<details>
  <summary>Details</summary>
Motivation: 大规模课程中，教师难以诊断个别学生的知识缺口和确定需要强化的概念。现有知识图谱方法要么停留在表层概念，要么忽略了教学材料中丰富的教学信号。

Method: 从课程讲义（幻灯片、笔记等）中提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系）。结合教育材料特有的时间与语义信号（如"递归"在"归并排序"之前教授）与大语言模型的泛化能力。

Result: 通过在多门课程的真实讲义上进行实验和人工评估，证明InstructKG能够捕捉丰富且与教师意图一致的学习进展关系。

Conclusion: InstructKG框架能够自动构建教师对齐的知识图谱，有效捕捉课程的学习进展，为个性化学习提供支持。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [49] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出"高维空间的索引认识论"，认为生成式AI通过高维语义空间中的导航操作产生知识，这代表了不同于符号推理和统计重组的第三种知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的认知机制尚未被充分理解，其工作原理与传统计算范式（图灵-香农-冯·诺依曼传统）存在根本断裂。缺乏这种理解，就无法在科学、教育和制度生活中负责任地整合生成式AI。

Method: 基于高维几何的四个结构特性（测度集中、近正交性、指数方向容量、流形正则性），结合皮尔士符号学和帕珀特建构主义，发展"高维空间的索引认识论"，将生成模型重新概念化为学习流形的导航器。

Result: 提出了"导航知识"作为第三种知识生产模式，区别于符号推理和统计重组。这种认识论框架为理解生成式AI的认知机制提供了新视角。

Conclusion: 生成式AI代表了一种根本性的认知范式转变，需要通过高维空间的索引认识论来理解其知识生产方式，这为负责任地整合AI技术提供了理论基础。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [50] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种新颖的并行算法，用于分解困难的CircuitSAT实例，通过专用约束将原始SAT实例分割为弱化公式族，参数可调以高效识别高质量分解


<details>
  <summary>Details</summary>
Motivation: CircuitSAT（电路可满足性问题）是SAT问题的特例，在形式验证和密码分析中具有重要应用。传统的SAT求解器在处理复杂电路实例时效率有限，需要新的分解技术来应对这些挑战性实例

Method: 采用并行算法框架，使用专用约束将原始SAT实例分割为弱化公式族。算法参数化设计，通过调整参数并结合并行计算的硬度估计来指导高质量分解的识别

Result: 在具有挑战性的CircuitSAT实例上验证了算法的实际效果，包括布尔电路逻辑等价性检查和密码哈希函数原像攻击的编码实例

Conclusion: 提出的并行分解算法为处理困难的CircuitSAT实例提供了有效解决方案，在形式验证和密码分析领域具有实际应用价值

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [51] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: Combine是一个基于准则的剪枝框架，提供快速有效的迭代剪枝方法，建立了准则函数比较标准，并提出新准则函数，在VGG模型上实现高达79%的滤波器剪枝和68%的计算量减少。


<details>
  <summary>Details</summary>
Motivation: 随着CNN模型对精度和性能要求的提高，模型规模、执行时间、内存占用和功耗也随之增加。现有剪枝解决方案缺乏统一实现标准，难以实施和比较。

Method: 提出Combine框架，这是一个基于准则的剪枝解决方案，支持迭代剪枝，建立了准则函数比较的标准语言，并提出了几种新颖的准则函数。

Result: 在VGG启发式模型上，实现了高达79%的滤波器剪枝，同时保持或提高准确率，并将网络所需计算量减少高达68%。

Conclusion: Combine是一个快速有效的剪枝框架，能够统一不同剪枝准则的实现和比较，展示了不同准则对不同模型的不同影响，为剪枝研究提供了标准化工具。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [52] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA 是一个结合联合嵌入预测架构（JEPA）与传统生成目标的基因组基础模型预训练框架，通过潜在空间预测提升对基因组功能上下文的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型主要依赖掩码语言建模（MLM）或下一词预测（NTP），这些方法擅长捕捉局部基因组语法和精细的基序模式，但往往无法捕获更广泛的功能上下文，导致表示缺乏全局生物学视角。

Method: JEPA-DNA 整合了联合嵌入预测架构（JEPA）与传统生成目标，引入潜在接地机制，通过监督CLS标记将词元级恢复与潜在空间预测目标相结合，迫使模型预测掩码基因组片段的高层功能嵌入，而非仅关注单个核苷酸。

Result: 在多样化的基因组基准测试中，JEPA-DNA 在监督和零样本任务上始终优于纯生成基线，提供了更稳健和生物学接地的表示。

Conclusion: JEPA-DNA 为基因组基础模型提供了可扩展的路径，使其不仅能理解基因组字母表，还能理解序列底层的功能逻辑。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [53] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅含2000万参数的轻量级公式识别模型，通过精心设计、蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少65-80%，支持实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 当前公式识别模型通常参数庞大，难以在消费级硬件上实时运行或浏览器中部署。需要开发一个既保持高性能又足够轻量的模型，以实现实际应用中的实时推理和便捷部署。

Method: 采用最小化设计，通过注意力机制优化、知识蒸馏技术以及词汇表和分词器的迁移策略，将模型参数控制在2000万以内，同时保持识别性能。

Result: Texo在性能上可与UniMERNet-T和PPFormulaNet-S等SOTA模型相媲美，同时模型大小分别减少了80%和65%，实现了在消费级硬件上的实时推理和浏览器内部署。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别任务的实际应用部署提供了可行的轻量级解决方案。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [54] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 提出在线构建符号因果世界模型的框架，通过元解释学习和谓词发明实现高效、可扩展的因果推理，显著优于PPO神经网络基线


<details>
  <summary>Details</summary>
Motivation: 传统世界建模方法存在样本效率低、缺乏透明性和扩展性差的问题，需要一种能够在线构建符号化因果模型的方法来提升智能体在复杂环境中的导航能力

Method: 集成连续模型学习和修复到智能体决策循环中，利用元解释学习和谓词发明技术发现语义上有意义且可重用的抽象概念，构建解耦的高质量概念层次结构

Result: 该方法能够扩展到具有复杂关系动态的领域，避免了命题方法的组合爆炸问题，样本效率比PPO神经网络基线高出数量级

Conclusion: 提出的框架能够在线构建符号因果世界模型，通过提升推理方法在复杂关系动态领域中实现高效、可扩展的学习，显著优于传统神经网络方法

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [55] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 提出AI Agent协作研究流程，应用于人文社科研究，以台湾Claude.ai使用数据验证方法可行性


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑知识工作，但现有研究主要关注软件工程和自然科学，人文社科领域的方法论探索有限，需要专门的研究工作流程

Method: 设计七阶段模块化工作流程，基于任务模块化、人机分工和可验证性三原则，以台湾Claude.ai使用数据（N=7,729对话）作为实证验证

Result: 提出可复制的人机协作框架，识别三种协作模式（直接执行、迭代优化、人类主导），展示人类在研究问题制定、理论解释、情境推理和伦理反思中的不可替代性

Conclusion: 为人文社科研究者提供可复制的AI协作框架，强调人类判断在研究中的核心作用，同时承认单平台数据、横截面设计和AI可靠性风险等局限性

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [56] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出大型行为模型(LBM)，通过行为嵌入而非提示工程来预测个体战略决策，利用结构化心理特征档案提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测人类高风险决策时存在局限：难以生成一致、个体特定的行为，且对心理特质与情境约束的复杂交互处理不足。基于提示的方法易出现身份漂移，无法有效利用详细人物描述。

Method: 引入大型行为模型(LBM)，这是一种行为基础模型，通过微调来预测个体战略选择。LBM从临时人物提示转向行为嵌入，基于从综合心理测量电池中提取的结构化高维特质档案进行条件生成。模型在专有数据集上训练，该数据集将稳定倾向、动机状态和情境约束与观察到的选择相关联。

Result: 在保留场景评估中，LBM微调相比未适应的Llama-3.1-8B-Instruct骨干模型提升了行为预测能力，在基于大五人格特质条件下与前沿基线表现相当。研究发现，基于提示的基线存在复杂度上限，而LBM能从更密集的特质档案中持续获益，随着提供更多特质维度，性能持续提升。

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等领域具有应用潜力，建立了从临时提示转向稳定行为嵌入的新范式。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [57] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 该研究使用布鲁姆分类法分析LLM内部神经表征，发现认知复杂度在线性可分离的子空间中被编码，线性分类器准确率达95%


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑箱特性需要超越表面性能指标的新评估框架，本研究旨在探索LLM内部如何表示不同认知复杂度的信息

Method: 使用布鲁姆分类法作为层次化视角，分析不同LLM的高维激活向量，探究从基础回忆到抽象合成的不同认知水平是否在模型残差流中线性可分

Result: 线性分类器在所有布鲁姆认知水平上平均准确率达到约95%，证明认知水平在模型表征的线性可访问子空间中被编码，模型在前向传播早期就解析了提示的认知难度

Conclusion: 研究提供了证据表明LLM内部确实编码了认知复杂度信息，且这些信息在线性可分离的子空间中，为理解模型内部工作机制提供了新视角

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [58] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄露，并开发TimeSPEC方法通过声明验证来减少泄露，保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 在回溯测试LLM预测未来事件能力时，模型可能无意中使用训练中编码的未来信息（时间知识泄露），这会破坏评估的有效性。需要检测和量化这种泄露。

Method: 1) 提出Shapley-DCLR指标：将模型推理分解为原子声明，按时间可验证性分类，用Shapley值衡量每个声明对预测的贡献，计算决策关键泄露率；2) 开发TimeSPEC方法：在生成过程中插入声明验证和重新生成，主动过滤时间污染，确保所有支持声明都能追溯到截止日期前的来源。

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上的实验显示，标准提示方法存在显著泄露。TimeSPEC能降低Shapley-DCLR，同时保持任务性能，表明显式的声明级验证优于基于提示的时间约束。

Conclusion: 提出的Shapley-DCLR框架能有效检测和量化LLM中的时间知识泄露，TimeSPEC方法通过声明级验证减少泄露，为可靠的LLM回溯测试提供了可解释的解决方案。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [59] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 提出Web Verbs作为网络动作的语义层，将网站功能通过类型化、语义化的函数暴露，为AI代理提供稳定、可组合的操作单元，统一API和浏览器操作范式。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理主要使用点击、键盘输入等低级操作，这些操作脆弱、低效且难以验证。随着大语言模型的发展，自然语言成为目标导向任务的实用接口，但缺乏对网络动作的语义抽象层。

Method: 提出Web Verbs概念，作为网络规模的类型化、语义化函数集合，通过统一接口暴露网站功能（无论是通过API还是客户端工作流实现）。这些动词包含前置条件、后置条件、策略标签和日志支持，作为稳定可组合的单元供代理发现、选择和组合成简洁程序。

Result: 通过概念验证实现和代表性案例研究，展示了与现有代理相比更简洁和鲁棒的执行效果。Web Verbs通过提供稳定接口提高可靠性，将数十个步骤减少为几个函数调用提高效率，通过类型化合约和可检查跟踪提高可验证性。

Conclusion: Web Verbs为网络代理动作提供了必要的语义层，统一了API和浏览器范式，使大语言模型能够合成可靠、可审计的工作流。提出了标准化路线图，以实现网络规模的可部署和可信赖的动词系统。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [60] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 该研究详细记录了在有限计算资源下（2xA100 GPU）从arXiv LaTeX原始数据训练1.36B参数科学语言模型的完整工程流程，分析了预处理、分词和基础设施瓶颈对训练的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但如何从原始科学文献训练领域专用语言模型的实践过程缺乏详细文档。本研究旨在为中等计算预算的研究者提供训练领域专用模型的工程指导。

Method: 构建端到端训练流程：元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词，在2xA100 GPU上训练1.36B参数密集Transformer模型，使用数学、计算机科学和理论物理领域的arXiv LaTeX数据。

Result: 通过24次实验运行，发现预处理决策显著影响可用token数量，分词影响符号稳定性，存储和I/O约束与计算资源同等重要。在数据丰富（52B预训练token）的情况下训练稳定收敛。

Conclusion: 本研究提供了从零开始训练小型科学语言模型的工程实践记录，而非提出新架构。希望这些见解能帮助中等计算预算的研究者构建领域专用模型。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [61] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify是一个用于医学诊断的AI代理，通过生成后续问题来模拟临床医生的迭代推理过程，减少诊断不确定性，相比单次LLM基线减少约27个百分点的诊断错误。


<details>
  <summary>Details</summary>
Motivation: 当前医学大语言模型在诊断任务中存在局限性，无法像临床医生那样通过系统性的病史采集和迭代提问来推理鉴别诊断。真实临床实践中，诊断往往需要多次信息收集来排除紧急情况和不确定性，而现有LLM缺乏生成有效后续问题的能力。

Method: MedClarify首先计算类似鉴别诊断的候选诊断列表，然后主动生成旨在减少诊断不确定性的后续问题。通过选择预期信息增益最高的问题，实现有针对性的、不确定性感知的推理。

Result: 实验显示当前LLM在医学推理中存在局限性，特别是在病例不完整或相关信息缺失时会产生多个相似可能性的诊断。MedClarify的信息论推理方法能生成有效的后续提问，相比标准单次LLM基线减少约27个百分点的诊断错误。

Conclusion: MedClarify通过代理式信息寻求改进了医学LLM，促进了反映真实世界临床推理迭代性和不确定性的有效对话，为医学LLM的发展提供了新路径。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [62] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据的正则化方法，通过曲率矩阵近似解决任务向量组合中的表示漂移问题，实现任务算术的模块化扩展


<details>
  <summary>Details</summary>
Motivation: 任务算术为调整基础模型提供了模块化、可扩展的方法，但组合多个任务向量会导致跨任务干扰，引起表示漂移和性能下降。现有表示漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 将表示漂移正则化框架化为曲率矩阵近似问题，采用Kronecker分解近似曲率技术，开发出实用的正则化器。该方法具有任务数量恒定的复杂度，无需外部数据。

Result: 在任务添加和任务否定方面取得了最先进的结果，方法对任务向量缩放具有鲁棒性，无需保留调优。

Conclusion: 提出的无数据正则化方法有效解决了任务算术中的表示漂移问题，实现了模块化、可扩展的任务组合，同时满足数据隐私和可用性约束。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [63] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一种结合形式验证与深度学习的图像检索框架，通过图验证和神经代码生成支持开放词汇自然语言查询，提供可信且可验证的结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合或精确约束（如身份、数量和比例）时仍存在不可靠问题，需要更透明和可验证的检索方法。

Method: 整合形式验证到深度学习图像检索中，结合基于图的验证方法和神经代码生成，对用户查询中的每个原子真值进行形式化验证。

Result: 不仅能返回匹配结果，还能识别并标记哪些具体约束被满足、哪些未满足，提供更透明和可追溯的检索过程，同时提升了最流行的基于嵌入方法的性能。

Conclusion: 通过将检索结果建立在形式推理系统上，超越了向量表示常见的模糊性和近似性，为复杂自然语言查询提供了可信且可验证的解决方案。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [64] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特异性变分编码器、融合瓶颈和多任务目标，解决非小细胞肺癌生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌生存预测因个体预后特征差异而具挑战性，多模态数据（全切片图像、转录组学、DNA甲基化）可提供互补信息，但临床数据常存在模态缺失问题，现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特异性变分编码器捕捉各数据源不确定性；融合瓶颈与学习门控机制归一化现有模态贡献；多任务目标结合生存损失和重建损失正则化患者表示；跨模态对比损失增强潜在空间对齐；训练中应用随机模态掩码提高对任意缺失模式的鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上评估，模型在疾病特异性生存预测方面表现优异，且在严重缺失场景下相比两种最先进模型更具鲁棒性。测试所有模态子集发现，多模态整合并非总是对任务有益。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，提高非小细胞肺癌生存预测的鲁棒性。研究还澄清了多模态整合的局限性，表明并非所有模态组合都能改善预测性能。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [65] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 提出一个基于隐私设计原则的框架，指导AI应用开发者减少儿童隐私风险，特别是针对LLM应用，通过案例研究展示如何实施


<details>
  <summary>Details</summary>
Motivation: 儿童越来越多地使用AI技术，但存在隐私风险担忧。现有隐私法规要求保护措施，但在实践中实施困难，需要系统性的指导框架

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等法规原则，映射到LLM应用的数据收集、模型训练、运营监控等阶段，结合UNCRC、AADC等制定儿童设计指南，并通过LLM教育辅导案例研究验证

Result: 框架展示了如何通过技术和组织控制以及适龄设计决策，在LLM生命周期中实施数据保护策略，支持开发符合法律要求的儿童AI应用

Conclusion: 通过主动的风险规避方法和系统化的隐私设计框架，可以在AI应用中为儿童提供隐私保护并满足法律合规要求

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [66] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构解决了学术界与工业界之间的鸿沟，支持50+算法、40指标和19种策略，能够无缝从本地执行扩展到分布式训练，同时集成了能耗追踪功能，为可持续、面向智能体的下一代推荐系统提供架构基础。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统生态系统存在分裂问题：研究人员需要在易于内存实验和需要重写代码以适应分布式工业引擎之间做出选择，这阻碍了推荐系统的创新。学术界与工业界之间存在鸿沟，需要一种能够无缝过渡的解决方案。

Method: 提出了WarpRec框架，采用新颖的后端无关架构，包含50多种最先进算法、40个评估指标和19种过滤与分割策略。框架集成了CodeCarbon进行实时能耗追踪，支持从本地执行到分布式训练和优化的无缝过渡。

Result: WarpRec消除了学术界与工业界之间的权衡，展示了可扩展性不必以科学完整性或可持续性为代价。框架能够作为可持续、面向智能体的下一代推荐系统的架构基础，代码已开源。

Conclusion: WarpRec不仅弥合了学术界与工业界之间的鸿沟，还可以作为下一代可持续、面向智能体的推荐系统的架构基础，为推荐系统从静态排名引擎向生成式AI生态系统中的交互工具演进提供支持。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [67] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文提出了一个针对ARM Cortex处理器（M0+, M4, M7）的AI模型优化基准测试框架，通过自动化测试平台系统评估能耗、精度和资源利用率，发现FLOPs与推理时间呈近线性关系，并通过帕累托分析平衡能耗与精度权衡。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统中AI模型部署面临能耗、精度和资源利用率的平衡挑战，需要系统化的评估框架来指导开发者选择最优的处理器和AI模型组合，以实现高性能且可持续的AI应用。

Method: 设计自动化测试平台，系统评估ARM Cortex处理器（M0+, M4, M7）上的AI模型性能，分析关键性能指标（KPIs），研究FLOPs与推理时间的相关性，并采用帕累托分析平衡能耗与精度的权衡。

Result: 发现FLOPs与推理时间呈近线性关系，可作为计算需求估计的可靠指标；M7处理器适合短推理周期任务，M4处理器在长推理任务中能效更优，M0+处理器适合简单AI任务；通过帕累托分析成功平衡能耗与精度权衡。

Conclusion: 该基准测试框架为开发者提供了设计高能效AI系统的实用指导，帮助在嵌入式系统中实现高性能AI应用，同时确保可持续性，不同ARM Cortex处理器在不同应用场景下各有优势。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [68] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架通过结合知识图谱和检索增强生成，提升LLM在电信领域的准确性和可靠性，减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域面临挑战：领域复杂性高、标准不断演进、专业术语多，导致输出不准确、幻觉增多、实用性降低

Method: 提出KG-RAG框架：结合知识图谱（结构化表示电信标准和文档知识）和检索增强生成（动态检索相关事实），以增强LLM在电信特定任务中的表现

Result: 在基准数据集上，KG-RAG优于纯LLM和标准RAG基线：平均准确率比RAG提高14.3%，比纯LLM模型提高21.6%

Conclusion: KG-RAG能有效在复杂电信场景中产生准确、可靠且可解释的输出，解决了电信领域LLM应用的挑战

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [69] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文提出两种新指标（可重用性和可验证性）来评估多智能体IR系统中思维链的质量，发现这些指标与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前思维链评估仅关注目标任务准确率，无法评估推理过程本身的质量或效用。在多智能体IR系统中，智能体之间交换思维链进行中间推理，需要更全面的评估指标。

Method: 采用Thinker-Executor框架将思维链生成与执行解耦，提出可重用性（衡量Executor重用Thinker思维链的容易程度）和可验证性（衡量Executor使用思维链匹配Thinker答案的频率）两个新指标。在五个基准测试上评估了四个Thinker模型与十个Executor模型组成的委员会。

Result: 可重用性和可验证性与标准准确率不相关，暴露了当前基于准确率的排行榜在评估推理能力方面的盲点。令人惊讶的是，专门推理模型生成的思维链并不比通用LLM（如Llama和Gemma）生成的思维链更可重用或可验证。

Conclusion: 需要超越准确率的评估指标来全面评估思维链的质量，可重用性和可验证性提供了评估推理过程本身的新视角，有助于更准确地评估多智能体系统中的推理能力。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [70] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决超长视野任务，在多个基准测试中超越现有模型


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在处理超长视野任务时存在能力不足的问题，需要开发专门针对长轨迹任务训练的模型

Method: 1. 使用轨迹分割SFT冷启动模型：保留早期上下文，渐进截断后期上下文，保持子轨迹重叠；2. 通过Research-Factory自动管道生成高质量训练数据；3. 采用渐进式RL训练：分多个阶段逐步延长超时时间

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，在SWE-bench Verified和MLE-bench等其他编码基准测试中也表现出良好的泛化性能

Conclusion: 提出的轨迹分割SFT和渐进式RL训练方法有效提升了LLM智能体解决超长视野任务的能力，KLong在多个基准测试中表现出色

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [71] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于常微分方程的统一理论框架ODESteer，用于大语言模型激活引导对齐，通过屏障函数设计实现多步自适应引导，在多个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前激活引导方法存在两个关键限制：缺乏统一的理论框架指导引导方向设计，以及过度依赖单步引导无法捕捉激活分布的复杂模式。需要建立理论基础并改进引导效果。

Method: 提出基于常微分方程的统一理论框架，将传统激活加法解释为ODE的一阶近似。通过控制理论中的屏障函数设计引导方向，具体实现为ODESteer方法，使用正负激活的对数密度比作为屏障函数，构建多步自适应引导的ODE。

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%。相比现有激活引导方法表现更优。

Conclusion: 通过ODE框架为激活引导建立了新的理论视角，提出的ODESteer方法在理论和实证上都验证了其有效性，为LLM对齐提供了原则性的新方法。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [72] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 提出一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN，用于从X光片诊断COVID-19和肺炎，确保医疗数据安全的同时提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 利用计算能力的显著提升，结合人工智能和联邦学习技术，为医疗领域创建安全、分布式的数据处理系统，帮助医生更准确地诊断肺部疾病。

Method: 采用混合联邦学习方法，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和SWIN Transformer视觉Transformer模型，使用TensorFlow和Keras框架构建系统。

Result: 该研究提出了一种能够提高疾病诊断准确性和严重程度预测的混合AI模型，通过联邦学习确保数据安全性和信息真实性。

Conclusion: 联邦学习与混合AI模型的结合为医疗领域提供了可靠、安全的解决方案，能够帮助医生更有效地诊断COVID-19和肺炎，对抗全球疫情。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [73] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出用"人类游戏多宇宙"来评估AI的类人通用智能，并开发了AI GameStore平台，通过LLM生成代表性人类游戏来测试AI性能


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试只评估狭窄能力且容易饱和，需要更全面的方法来评估AI的类人通用智能

Method: 提出"人类游戏多宇宙"概念，开发AI GameStore平台，使用LLM和人类参与从App Store和Steam等平台自动生成标准化游戏环境

Result: 生成100个游戏测试7个前沿视觉语言模型，最佳模型在多数游戏中得分不到人类平均分的10%，尤其在需要世界模型学习、记忆和规划的游戏中表现不佳

Conclusion: AI GameStore是评估和推动机器实现类人通用智能的实用方法，需要进一步扩展和完善

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [74] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一种基于分层离散扩散模型的分子图生成框架，通过引入编码化学先验的额外类别和解耦原子编码，在MOSES数据集上实现了接近完美的化学有效性，超越了现有图扩散模型和1D基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散模型在分子生成中存在化学有效性低、难以满足目标属性要求的问题，相比1D建模方法表现不佳。需要开发能够克服这些长期性能限制的新方法。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码技术，根据原子的化学角色分割原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到接近完美的化学有效性，在多个指标上超越了强大的1D基线方法。在下游任务如多属性引导生成和骨架扩展中也表现出色。

Conclusion: MolHIT成功克服了现有图扩散模型的性能限制，为AI驱动的药物发现和材料科学提供了强大的分子生成框架，在化学有效性和属性控制方面取得了突破性进展。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [75] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics是一个多智能体框架，能够从自然语言描述自动设计、实现、调试和验证通用偏微分方程（PDE）的数值求解器，生成基于经典数值分析的透明求解器，而非黑盒神经求解器。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器设计需要大量数学专业知识和手动调优，而现有神经网络方法虽然灵活但计算成本高且可解释性有限。需要一种既能自动生成求解器又保持透明度和可解释性的方法。

Method: 提出AutoNumerics多智能体框架，采用从粗到细的执行策略和基于残差的自验证机制，能够从自然语言描述自主设计、实现、调试和验证PDE数值求解器，生成基于经典数值分析的透明求解器。

Result: 在24个经典和实际PDE问题上进行实验，AutoNumerics相比现有神经和基于LLM的基线方法达到竞争性或更优的精度，并能根据PDE结构特性正确选择数值方案。

Conclusion: AutoNumerics展示了作为自动化PDE求解可访问范式的可行性，能够生成透明且基于经典数值分析的求解器，为科学和工程建模提供更易访问的PDE求解方法。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [76] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了前两届活动，增加了语义关系提取任务，要求系统对两种关系类型进行分类，并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 历史文本中的人物-地点关系提取对于数字人文研究至关重要，但现有方法在处理多语言、跨时期、噪声文本方面存在挑战。HIPE-2026旨在推动关系提取技术发展，支持知识图谱构建、历史传记重建和空间分析等下游应用。

Method: 通过CLEF评估实验室形式，提供多语言历史文本数据集，要求系统识别两种关系类型：$at$（人物是否曾到过该地点）和$isAt$（人物在出版时是否位于该地点）。引入三重评估框架：准确性、计算效率和领域泛化能力。

Result: 该论文描述了一个评估框架和任务设计，而非具体实验结果。HIPE-2026作为评估实验室，将为参与者提供基准数据集和评估指标，以比较不同系统在历史文本关系提取任务上的性能。

Conclusion: HIPE-2026将历史文本关系提取与大规模历史数据处理相结合，通过标准化评估推动该领域技术进步，为数字人文研究提供重要工具支持，特别是在知识图谱构建和历史空间分析方面。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [77] [Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling](https://arxiv.org/abs/2602.16961)
*Rahul Thomas,Arka Pal*

Main category: cs.IT

TL;DR: 本文提出了一种新的贪心多路径块验证（GBV）方法，通过构建信息无关线性规划证明块验证的最优性，并扩展为多路径版本，显著提升解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法中，标准验证算法在每个token上独立验证，块验证（BV）虽然使用联合条件但仅利用单路径概率。需要探索更优的验证算法，特别是能够利用多路径候选和离路径概率的方法，以进一步提升解码效率。

Method: 1. 构建信息无关线性规划（LP）证明块验证在所有验证算法中的最优性，包括使用离路径概率的算法。2. 将LP扩展到多候选路径设置，构建多路径块验证泛化类。3. 提出贪心多路径块验证（GBV）作为该类的有效实现，通过贪心策略在计算效率和性能间取得平衡。

Result: GBV相比BV提升块效率超过30%，减少解码时间超过15%。在Llama-3 70B模型上，GBV相比最先进的多路径验证方法提升端到端解码吞吐量超过15%。

Conclusion: 块验证在理论上是最优的验证算法，而贪心多路径块验证（GBV）作为其实用扩展，能够显著提升推测解码的效率，为大规模语言模型的高效推理提供了有效解决方案。

Abstract: The goal of $L$-step speculative decoding is to accelerate autoregressive decoding of a target model by using a cheaper draft model to generate a candidate path of $L$ tokens. Based on a verification algorithm involving target and draft model probabilities, a prefix of the candidate sequence is accepted, and an additional correction token is sampled from a residual distribution to ensure that the final output adheres to the target distribution. While standard speculative decoding uses a verification algorithm which is independent at each token on the path, a recent extension called block verification uses a joint condition involving all sampled on-path probabilities. Block verification (BV) was shown to be optimal over all verification algorithms which use only on-path probabilities, improving on standard speculative decoding. In this work, we first show that block verification is optimal even over verification algorithms that use off-path probabilities, by constructing an information-agnostic linear program (LP). Further, we can extend our LP to the setting where the draft model samples multiple candidate paths, and use it to construct a natural class of multi-path block verification generalizations. While computing the optimal algorithm in this class is not tractable, by considering a stricter class of greedy algorithms, we can formulate an efficient method called greedy multi-path block verification (GBV). Empirically, GBV can improve block efficiency by over 30% and reduce decoding walltimes by over 15% relative to BV. On Llama-3 70B, GBV can improve the end-to-end decoding throughput over SOTA multi-path verification methods by more than 15%.

</details>


### [78] [Resource Allocation for STAR-RIS-enhanced Metaverse Systems with Augmented Reality](https://arxiv.org/abs/2602.17123)
*Sun Mao,Lei Liu,Kun Yang,F. Richard Yu,Duist Niyato,Chau Yuen*

Main category: cs.IT

TL;DR: 本文提出了一种用于同时传输和反射RIS（STAR-RIS）辅助AR元宇宙的资源管理框架，通过联合优化BS计算资源分配、STAR-RIS系数矩阵、AR用户CPU频率和发射功率，最小化服务延迟。


<details>
  <summary>Details</summary>
Motivation: AR元宇宙需要提供沉浸式服务体验，但有限的网络资源和不可预测的无线传播环境成为关键设计瓶颈。STAR-RIS可以改善AR用户与元宇宙服务器之间的通信效率，但需要有效的资源管理来最小化服务延迟。

Method: 提出STAR-RIS辅助AR元宇宙的资源管理框架，将服务延迟最小化问题转化为可处理形式，通过交替优化方法解耦多维变量。具体包括：使用基于惩罚函数的方法获得最优系数矩阵，推导AR用户CPU频率的闭式解，通过拉格朗日对偶方法和凸优化理论获得AR用户发射功率和BS计算资源分配。

Result: 仿真结果表明，所提出的方法相比多个基准方法实现了显著的延迟降低。

Conclusion: STAR-RIS辅助AR元宇宙的资源管理框架能有效优化系统性能，显著降低服务延迟，为AR元宇宙系统提供了可行的资源管理解决方案。

Abstract: Augmented reality (AR)-enabled Metaverse is a promising technique to provide immersive service experience for mobile users. However, the limited network resources and unpredictable wireless propagation environments are key design bottlenecks of AR-enabled Metaverse systems. Therefore, this paper presents a resource management framework for simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted AR-enabled Metaverse, where the STAR-RIS is configured to improve the communication efficiency between AR users and the Metaverse server located at the base station (BS). Moreover, we formulate a service latency minimization problem via jointly optimizing the computation resource allocation of the BS, coefficient matrix of the STAR-RIS, central processing unit (CPU) frequency and transmit power of the AR users. To tackle the non-convex problem, we utilize an approximate method to transform it to a tractable form, and decouple the multi-dimensional variables via the alternating optimization method. Particularly, the optimal coefficient matrix is obtained by a penalty function-based method with proved convergence, the CPU frequencies of AR users are derived as the closed-form solution, and the transmit power of AR users and computation resource allocation of the BS are obtained by the Lagrange duality method and convex optimization theory. Finally, simulation results demonstrates that the proposed method achieves remarkable latency reduction than several benchmark methods.

</details>


### [79] [Isometric Invariant Quantification of Gaussian Divergence over Poincare Disc](https://arxiv.org/abs/2602.17159)
*Levent Ali Mengütürk*

Main category: cs.IT

TL;DR: 论文建立了球面平方Hellinger距离与庞加莱圆盘在一般Möbius群作用下的双曲等距不变量之间的几何对偶关系，并提出使用L2嵌入的双曲等距不变量作为量化高斯测度间散度的新方法。


<details>
  <summary>Details</summary>
Motivation: 受几何对偶关系的启发，作者希望利用双曲等距不变量为信息论提供一种新的量化高斯测度间散度的方法。

Method: 通过建立球面平方Hellinger距离与庞加莱圆盘在一般Möbius群作用下的双曲等距不变量之间的几何对偶关系，提出使用L2嵌入的双曲等距不变量作为散度度量。

Result: 建立了球面平方Hellinger距离与双曲等距不变量之间的几何对偶关系，并提出了基于双曲几何的新散度度量方法。

Conclusion: 该几何对偶关系为信息论提供了新的视角，基于双曲等距不变量的L2嵌入方法可作为量化高斯测度间散度的有效替代方案。

Abstract: The paper presents a geometric duality between the spherical squared-Hellinger distance and a hyperbolic isometric invariant of the Poincare disc under the action of the general Mobius group. Motivated by the geometric connection, we propose the usage of the L2-embedded hyperbolic isometric invariant as an alternative way to quantify divergence between Gaussian measures as a contribution to information theory.

</details>


### [80] [Federated Latent Space Alignment for Multi-user Semantic Communications](https://arxiv.org/abs/2602.17271)
*Giuseppe Di Poce,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.IT

TL;DR: 提出一种联邦优化方法，通过共享语义预均衡器和本地语义均衡器来解决多智能体AI原生语义通信中的潜在空间失配问题，实现任务导向通信。


<details>
  <summary>Details</summary>
Motivation: AI原生设备的不同潜在表示会导致语义不匹配，阻碍多智能体语义通信中的相互理解和有效任务执行。

Method: 在接入点共享语义预均衡器，在用户设备部署本地语义均衡器，采用联邦优化进行去中心化训练，考虑功率和复杂度约束。

Result: 数值结果验证了该方法在目标导向语义通信中的有效性，揭示了准确性、通信开销、复杂度和语义邻近度之间的关键权衡。

Conclusion: 提出的协议能有效缓解潜在空间失配，促进多智能体AI原生语义通信中的相互理解和任务导向通信。

Abstract: Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that shares a semantic pre-equalizer at the AP and local semantic equalizers at user devices, fostering mutual understanding and task-oriented communication while considering power and complexity constraints. To achieve this, we employ a federated optimization for the decentralized training of the semantic equalizers at the AP and user sides. Numerical results validate the proposed approach in goal-oriented semantic communication, revealing key trade-offs among accuracy, com- munication overhead, complexity, and the semantic proximity of AI-native communication devices.

</details>
