{"id": "2508.19696", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19696", "abs": "https://arxiv.org/abs/2508.19696", "authors": ["Diego Lentner", "Thomas Wiegart", "Richard D. Wesel"], "title": "Efficient Probabilistic Parity Shaping for Irregular Repeat-Accumulate LDPC Codes", "comment": "Presented at the 2025 International Symposium on Topics in Coding", "summary": "Algorithms are presented that efficiently shape the parity bits of systematic\nirregular repeat-accumulate (IRA) low-density parity-check (LDPC) codes by\nfollowing the sequential encoding order of the accumulator. Simulations over\nadditive white Gaussian noise (AWGN) channels with on-off keying show a gain of\nup to 0.9 dB over uniform signaling."}
{"id": "2508.19858", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19858", "abs": "https://arxiv.org/abs/2508.19858", "authors": ["Massimo Battaglioni", "Kenneth Andrews", "Rebecca Giuliani", "Fabrizio Marinelli", "Franco Chiaraluce", "Marco Baldi"], "title": "Design and Analysis of the Tail Sequence for Short LDPC-Coded Space Communications", "comment": "The article will be published in IEEE Transactions on Aerospace and\n  Electronic Systems. 15 pages", "summary": "According to some standards for satellite communications, the transmitted\nstream is divided into transmission units with variable length, for which\ndetecting the termination is particularly relevant. This is the case of space\nTeleCommands (TCs), where coded data are usually preceded by a start sequence,\nand optionally followed by a tail sequence, forming the Communication Link\nTransmission Unit (CLTU). Regarding the choice of schemes for error correction,\nthe Consultative Committee for Space Data Systems recommendations for TC\nsynchronization and coding suggests to use, among others, two Low-Density\nParity-Check (LDPC) codes: one (relatively) long and one short. Adopting the\nlong LDPC code eliminates the need for a tail sequence, as the LDPC decoder\nalways fails when overrunning the end of the CLTU, thus causing the decoding\nand detection process to stop. This, however, is not true when the short LDPC\ncode is adopted, since its decoding might converge on a codeword even when the\ndecoder input is not a noisy codeword. This makes it necessary to use a tail\nsequence that causes the decoder to fail regardless of its input. In this\npaper, we study the features required for such a sequence and propose some\nmethods for its design. Our numerical results, obtained considering various\ndetection approaches for the tail sequence, show that the overall TC rejection\nprobability improves significantly when the proposed tail sequence is employed.\nOur simulations also show that, for moderate values of the Signal-to-Noise\nRatio (SNR), with a properly designed tail sequence it is possible to obtain\nthe same performance in terms of TC rejection probability using decoder-based\ndetection and likelihood ratio test-based detection, with the former approach\nbeing less complex than the latter."}
{"id": "2508.19951", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19951", "abs": "https://arxiv.org/abs/2508.19951", "authors": ["Christoph Hirche"], "title": "Renyi partial orders for BISO channels", "comment": "9 pages", "summary": "A fundamental question in information theory is to quantify the loss of\ninformation under a noisy channel. Partial orders are typical tools to that\nend, however, they are often also challenging to evaluate. For the special\nclass of binary input symmetric output (BISO) channels, Geng et al. showed that\namong channels with the same capacity, the binary symmetric channel (BSC) and\nbinary erasure channel (BEC) are extremal with respect to the more capable\norder. Here we extend on this result by considering partial orders based on\nRenyi mutual information. We establish the extremality of the BSC and BEC in\nthis setting with respect to the generalized Renyi capacity. In the process, we\nalso generalize the needed tools and introduce $\\alpha$-Lorenz curves."}
{"id": "2508.20003", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20003", "abs": "https://arxiv.org/abs/2508.20003", "authors": ["Ayten Gürbüz", "Giuseppe Caire", "Alexander Steingass"], "title": "On the Outage Probability of Multiuser Multiple Antenna Systems with Non-Orthogonal Multiple Access for Air-Ground Communications", "comment": "14 pages, 7 figures", "summary": "This paper explores multiuser multiple antenna systems as a means to enhance\nthe spectral efficiency of aeronautical communications systems. To this end,\nthe outage regime for a multiuser multiple antenna system is studied within a\nrealistic geometry-based stochastic air-ground (AG) channel model. In this\napplication, users (aircraft) transmit air traffic management data to the\nground station at a predefined target rate. Due to the nature of the AG\npropagation, we argue that the relevant performance metric in this context is\nthe information outage probability. We consider the outage probability under\nthree decoding approaches. The first is based on successive interference\ncancellation (SIC). The second extends the first approach by considering joint\ngroup decoding. The third is a version of the second that limits the size of\nthe jointly decoded user groups in order to lower the decoding complexity. The\nresults show that joint group decoding, even in groups of only two, can\nsignificantly increase the spectral efficiency in the AG channel by allowing a\nlarge number of aircraft to transmit over a non-orthogonal channel with very\nlow outage probabilities."}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences."}
{"id": "2508.19350", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19350", "abs": "https://arxiv.org/abs/2508.19350", "authors": ["Kaiqiang Lin", "Mohamed-Slim Alouini"], "title": "Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC", "comment": "13 pages, 10 figures, 5 tables, submitted to IEEE IoTJ", "summary": "Wireless underground sensor networks (WUSNs) offer significant social and\neconomic benefits by enabling the monitoring of subterranean entities. However,\nthe communication reliability of WUSNs diminishes in harsh environments where\nterrestrial network infrastructure is either unavailable or unreliable. To\naddress this challenge, we explore the feasibility of integrating buried\nmassive machine-type communication (mMTC) sensors with non-terrestrial networks\n(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms\n(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN\nconnectivity for various large-scale underground monitoring applications. To\nassess the effectiveness of underground-to-NTN connectivity, we develop a Monte\nCarlo simulator that incorporates a multi-layer underground attenuation model,\nthe 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN\nmodulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum\n(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for\nshort-range UAV communication in rural environments, while LR-FHSS modulation\nproves to be a promising option for HAP and LEO satellite platforms in massive\nWUSNs scenarios thanks to its adequate link budget and robustness to the\ninterference. Finally, we demonstrate that the success probability of\nunderground-to-NTN connectivity using LoRa and LR-FHSS is significantly\naffected by factors such as the monitoring environment, the number of devices,\nburial depth, and the soil's volumetric water content."}
{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs."}
{"id": "2508.19736", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19736", "abs": "https://arxiv.org/abs/2508.19736", "authors": ["Mohsen Ahadi", "Adeel Malik", "Omid Esrafilian", "Florian Kaltenberger", "Cedric Thienot"], "title": "Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions", "comment": "8 pages", "summary": "5G New Radio (NR) is a key enabler of accurate positioning in smart cities\nand smart factories. This paper presents the experimental results from three 5G\npositioning testbeds running open-source OpenAirInterface (OAI) gNB and Core\nNetwork (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly\nintegrated Location Management Function (LMF). The testbeds are deployed across\nboth indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),\nfollowing a 3GPP-compliant system model. The experiments highlight the impact\nof synchronization impairments, multipath propagation, and deployment geometry\non positioning accuracy. To address these challenges, we propose tailored ToA\nand TDoA filtering as well as a novel position estimation method based on\nParticle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a\nbeyond-5G framework that leverages non-conventional measurements such as\nChannel Impulse Response (CIR) to train and test Artificial Intelligence and\nMachine Learning (AI/ML) models for data-driven positioning. The results\ndemonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%\nof cases in different testbeds, offering practical insights for the design of\nrobust 5G positioning systems. Moreover, we publicly release the datasets\ncollected in this work to support the research within the 5G positioning\ncommunity."}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences."}
{"id": "2508.19870", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19870", "abs": "https://arxiv.org/abs/2508.19870", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "comment": "35 pages", "summary": "Agentification serves as a critical enabler of Edge General Intelligence\n(EGI), transforming massive edge devices into cognitive agents through\nintegrating Large Language Models (LLMs) and perception, reasoning, and acting\nmodules. These agents collaborate across heterogeneous edge infrastructures,\nforming multi-LLM agentic AI systems that leverage collective intelligence and\nspecialized capabilities to tackle complex, multi-step tasks. However, the\ncollaborative nature of multi-LLM systems introduces critical security\nvulnerabilities, including insecure inter-LLM communications, expanded attack\nsurfaces, and cross-domain data leakage that traditional perimeter-based\nsecurity cannot adequately address. To this end, this survey introduces\nzero-trust security of multi-LLM in EGI, a paradigmatic shift following the\n``never trust, always verify'' principle. We begin by systematically analyzing\nthe security risks in multi-LLM systems within EGI contexts. Subsequently, we\npresent the vision of a zero-trust multi-LLM framework in EGI. We then survey\nkey technical progress to facilitate zero-trust multi-LLM systems in EGI.\nParticularly, we categorize zero-trust security mechanisms into model- and\nsystem-level approaches. The former and latter include strong identification,\ncontext-aware access control, etc., and proactive maintenance, blockchain-based\nmanagement, etc., respectively. Finally, we identify critical research\ndirections. This survey serves as the first systematic treatment of zero-trust\napplied to multi-LLM systems, providing both theoretical foundations and\npractical strategies."}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions."}
{"id": "2508.20044", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20044", "abs": "https://arxiv.org/abs/2508.20044", "authors": ["Kfir Toledo", "Isaac Keslassy"], "title": "2SYN: Congestion-Aware Multihoming", "comment": "Accepted at IEEE/IFIP NOMS", "summary": "When sending flows to arbitrary destinations, current multihoming routers\nadopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid\ncongested paths.\n  In this paper, we introduce 2SYN, the first congestion-aware multihoming\nalgorithm that works for any destination. We explain how it dynamically selects\na preferred path for new connections, even given previously-unseen\ndestinations. We further demonstrate that it can be easily implemented in\nLinux. Finally, in a real-world experiment with either LTE or a wired link, we\nshow how 2SYN dynamically adapts to the quality of the connection and\noutperforms alternative approaches. Thus, 2SYN helps companies better manage\ntheir networks by leveraging their multihoming capabilities."}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch."}
{"id": "2508.20060", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20060", "abs": "https://arxiv.org/abs/2508.20060", "authors": ["Daqian Ding", "Yibo Pi", "Cailian Chen"], "title": "A First Look at Inter-Cell Interference in the Wild", "comment": null, "summary": "In cellular networks, inter-cell interference management has been studied for\ndecades, yet its real-world effectiveness remains under-explored. To bridge\nthis gap, we conduct a first measurement study of inter-cell interference for\noperational 4G/5G networks. Our findings reveal the prevalence of inter-cell\ninterference and a surprising absence of interference coordination among\noperational base stations. As a result, user equipments experience unnecessary\ninterference, which causes significant signal quality degradation, especially\nunder frequency-selective channel fading. We examine the inter-cell\ninterference issues from four major perspectives: network deployment, channel\nassignment, time-frequency resource allocation, and network configuration. In\nnone of these dimensions is inter-cell interference effectively managed.\nNotably, even when spectrum resources are underutilized and simple strategies\ncould effectively mitigate inter-cell interference, base stations consistently\nprioritize using the same set of time-frequency resources, causing interference\nacross cells. Our measurements reveal substantial opportunities for improving\nsignal quality by inter-cell interference management."}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits."}
{"id": "2508.20077", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20077", "abs": "https://arxiv.org/abs/2508.20077", "authors": ["Tao Xiuyuan", "Milena Radenkovic"], "title": "ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication", "comment": null, "summary": "In disaster-stricken and large-scale urban emergency scenarios, ensuring\nreliable communication remains a formidable challenge, as collapsed\ninfrastructure, unpredictable mobility, and severely constrained resources\ndisrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient\nthrough their store-carry-forward paradigm, reveal the fundamental weaknesses\nof classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when\nconfronted with sparse encounters, buffer shortages, and volatile connectivity.\nTo address these obstacles, this study proposes ML-MaxProp, a hybrid routing\nprotocol that strengthens MaxProp with supervised machine learning. By\nleveraging contextual features such as encounter frequency, hop count, buffer\noccupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay\nsuitability in real time, transforming rigid heuristics into adaptive\nintelligence. Extensive simulations in the ONE environment using the Helsinki\nSPMBM mobility model show that ML-MaxProp consistently surpasses baseline\nprotocols, achieving higher delivery probability, lower latency, and reduced\noverhead. Statistical validation further shows that these improvements are both\nsignificant and robust, even under highly resource-constrained and unstable\nconditions. Overall, this work shows that ML-MaxProp is not just an incremental\nrefinement but a lightweight, adaptive, and practical solution to one of the\nhardest challenges in DTNs: sustaining mission-critical communication when\ninfrastructure collapses and every forwarding decision becomes critical."}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models."}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities."}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems."}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL."}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry."}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned."}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments."}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains."}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course."}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems."}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems."}
