<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [FLARE: Flying Learning Agents for Resource Efficiency in Next-Gen UAV Networks](https://arxiv.org/abs/2509.12307)
*Xuli Cai,Poonam Lohan,Burak Kantarci*

Main category: cs.NI

TL;DR: FLARE框架通过混合强化学习策略优化无人机资源分配，在动态环境中显著提升用户覆盖率和服务效率


<details>
  <summary>Details</summary>
Motivation: 解决6G及未来无线网络中无人机在动态环境下与移动地面用户设备的功率和带宽资源联合优化挑战

Method: 采用基于轮廓的K-Means聚类动态分组用户，使用MADDPG和DQN混合强化学习策略优化无人机位置、高度、发射功率和带宽分配

Result: 在5Mbps数据速率约束下，服务用户数量提升73.45%，显著优于MADDPG基线

Conclusion: FLARE框架能有效适应动态环境，实现高效的无人机资源分配和用户服务

Abstract: This letter addresses a critical challenge in the context of 6G and beyond
wireless networks, the joint optimization of power and bandwidth resource
allocation for aerial intelligent platforms, specifically uncrewed aerial
vehicles (UAVs), operating in highly dynamic environments with mobile ground
user equipment (UEs). We introduce FLARE (Flying Learning Agents for Resource
Efficiency), a learning-enabled aerial intelligence framework that jointly
optimizes UAV positioning, altitude, transmit power, and bandwidth allocation
in real-time. To adapt to UE mobility, we employ Silhouette-based K-Means
clustering, enabling dynamic grouping of users and UAVs' deployment at cluster
centroids for efficient service delivery. The problem is modeled as a
multi-agent control task, with bandwidth discretized into resource blocks and
power treated as a continuous variable. To solve this, our proposed framework,
FLARE, employs a hybrid reinforcement learning strategy that combines
Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Deep Q-Network
(DQN) to enhance learning efficiency. Simulation results demonstrate that our
method significantly enhances user coverage, achieving a 73.45% improvement in
the number of served users under a 5 Mbps data rate constraint, outperforming
MADDPG baseline.

</details>


### [2] [Automatic Network Planning with Digital Radio Twin](https://arxiv.org/abs/2509.12441)
*Xiaomeng Li,Yuru Zhang,Qiang Liu,Mehmet Can Vuran,Nathan Huynh,Li Zhao,Mizan Rahman,Eren Erman Ozguven*

Main category: cs.NI

TL;DR: AutoPlan是一个基于数字无线电孪生技术的自动网络规划框架，通过贝叶斯优化算法优化基站部署参数，在保证性能的同时大幅减少计算时间


<details>
  <summary>Details</summary>
Motivation: 解决蜂窝网络规划中由于部署场景多样性和仿真与现实差异导致的优化挑战

Method: 利用众包真实用户数据微调建筑材料参数来构建数字无线电孪生以减小仿真与现实差异，然后设计基于贝叶斯优化的算法来高效优化基站部署参数

Result: 在Husker-Net现场测量数据上的评估显示，AutoPlan能灵活适应不同场景，性能与穷举搜索相当，但计算时间仅需后者的2%

Conclusion: AutoPlan框架通过数字无线电孪生技术和贝叶斯优化，有效解决了网络规划中的仿真与现实差异问题，实现了高效且性能优异的自动网络规划

Abstract: Network planning seeks to determine base station parameters that maximize
coverage and capacity in cellular networks. However, achieving optimal planning
remains challenging due to the diversity of deployment scenarios and the
significant simulation-to-reality discrepancy. In this paper, we propose
\emph{AutoPlan}, a new automatic network planning framework by leveraging
digital radio twin (DRT) techniques. We derive the DRT by finetuning the
parameters of building materials to reduce the sim-to-real discrepancy based on
crowdsource real-world user data. Leveraging the DRT, we design a Bayesian
optimization based algorithm to optimize the deployment parameters of base
stations efficiently. Using the field measurement from Husker-Net, we
extensively evaluate \emph{AutoPlan} under various deployment scenarios, in
terms of both coverage and capacity. The evaluation results show that
\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance
comparable to exhaustive search, while requiring less than 2\% of its
computation time.

</details>


### [3] [Digital Twin-Assisted Resilient Planning for mmWave IAB Networks via Graph Attention Networks](https://arxiv.org/abs/2509.12499)
*Jie Zhang,Mostafa Rahmani Ghourtani,Swarna Bindu Chetty,Paul Daniel Mitchell,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 基于GATv2回罕学习的新题IAB部署优化方法，在mmWave 5G/6G网络中实现了更高覆盖率和故障容耐性


<details>
  <summary>Details</summary>
Motivation: 传统IAB部署优化方法在复杂城市环境中面临组合复杂性挑战，导致次优解和网络故障弱点

Method: 使用Graph Attention Network v2 (GATv2)回罕学习方法，将部署问题形式化为马尔可夫决策过程，通过关注机制捕捉异构节点类型间的空间依赖关系

Result: 在三个城市场景中实现了98.5-98.7%覆盖率，节省14.3-26.7%节点数量，在30%链路故障下仍保持87.1%覆盖保持率，故障容耐性提升11.3-15.4%

Conclusion: 该方法为动态城市环境中的mmWave网络提供了高效、强容锐的IAB部署解决方案，显著提升了网络性能和可靠性

Abstract: Digital Twin (DT) technology enables real-time monitoring and optimization of
complex network infrastructures by creating accurate virtual replicas of
physical systems. In millimeter-wave (mmWave) 5G/6G networks, the deployment of
Integrated Access and Backhaul (IAB) nodes faces highly dynamic urban
environments, necessitating intelligent DT-enabled optimization frameworks.
Traditional IAB deployment optimization approaches struggle with the
combinatorial complexity of jointly optimizing coverage, connectivity, and
resilience, often leading to suboptimal solutions that are vulnerable to
network disruptions. With this consideration, we propose a novel Graph
Attention Network v2 (GATv2)-based reinforcement learning approach for
resilient IAB deployment in urban mmWave networks. Specifically, we formulate
the deployment problem as a Markov Decision Process (MDP) with explicit
resilience constraints and employ edge-conditioned GATv2 to capture complex
spatial dependencies between heterogeneous node types and dynamic connectivity
patterns. The attention mechanism enables the model to focus on critical
deployment locations to maximize coverage and ensure fault tolerance through
redundant backhaul connections. To address the inherent vulnerability of mmWave
links, we train the GATv2 policy using Proximal Policy Optimization (PPO) with
a carefully designed balance between coverage, cost, and resilience.
Comprehensive simulations across three urban scenarios demonstrate that our
method achieves 98.5-98.7 percent coverage with 14.3-26.7 percent fewer nodes
than baseline approaches, while maintaining 87.1 percent coverage retention
under 30 percent link failures, representing 11.3-15.4 percent improvement in
fault tolerance compared to state-of-the-art methods.

</details>


### [4] [A Unified Learning-based Optimization Framework for 0-1 Mixed Problems in Wireless Networks](https://arxiv.org/abs/2509.12664)
*Kairong Ma,Yao Sun,Shuheng Hua,Muhammad Ali Imran,Walid Saad*

Main category: cs.NI

TL;DR: 这篇论文提出了一种统一框架，结合强化学习和优化理论来解决无线网络中的0-1混合优化问题，在大规模场景下显著提高了效果和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决无线网络中的0-1混合优化问题在大规模、多维资源和多样化服务要求下面临极大挑战，传统优化方法和强化学习算法都遇到困难。

Method: 使用RL技术将二进制变量求解模拟为序列决策任务，在决策过程中克服二进制变量并求解松弛问题，将松弛解作为导向RL搜索策略的先验信息，最后基于决策结果更新搜索策略。

Result: 在小规模问题中比B&B算法减少30%的收敛时间，目标值略高；在大规模场景下比RL算法提高20%的标准化目标值，且收敛更快。

Conclusion: 该框架有劳的性能界限和收敛保证，并能扩展到非凸目标函数和约束的问题，为解决复杂无线网络优化问题提供了有效方法。

Abstract: Several wireless networking problems are often posed as 0-1 mixed
optimization problems, which involve binary variables (e.g., selection of
access points, channels, and tasks) and continuous variables (e.g., allocation
of bandwidth, power, and computing resources). Traditional optimization methods
as well as reinforcement learning (RL) algorithms have been widely exploited to
solve these problems under different network scenarios. However, solving such
problems becomes more challenging when dealing with a large network scale,
multi-dimensional radio resources, and diversified service requirements. To
this end, in this paper, a unified framework that combines RL and optimization
theory is proposed to solve 0-1 mixed optimization problems in wireless
networks. First, RL is used to capture the process of solving binary variables
as a sequential decision-making task. During the decision-making steps, the
binary (0-1) variables are relaxed and, then, a relaxed problem is solved to
obtain a relaxed solution, which serves as prior information to guide RL
searching policy. Then, at the end of decision-making process, the search
policy is updated via suboptimal objective value based on decisions made. The
performance bound and convergence guarantees of the proposed framework are then
proven theoretically. An extension of this approach is provided to solve
problems with a non-convex objective function and/or non-convex constraints.
Numerical results show that the proposed approach reduces the convergence time
by about 30% over B&B in small-scale problems with slightly higher objective
values. In large-scale scenarios, it can improve the normalized objective
values by 20% over RL with a shorter convergence time.

</details>


### [5] [Joint AoI and Handover Optimization in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2509.12716)
*Zifan Lang,Guixia Liu,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Dusit Niyato,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出一种基于温度模型增强的深度强化学习算法，通过空中高度平台作为智能中继，解决低轨道卫星覆盖不连续性问题，同时优化信息年龄和卫星切换频率。


<details>
  <summary>Details</summary>
Motivation: 虽然低轨道卫星组网提供了全球覆盖能力，但由于轨道动力学导致的间歇性覆盖和有限通信窗口，影响了通信服务的可靠性。特别是在远离地区和紧急情况下，维持连接性仍然面临挑战。

Method: 提出三层空天-空中-地面集成网络架构，利用高空平台作为智能中继。采用混合光学光空间链路实现卫星到HAP的高容量通信，以及无线电频率链路保证HAP到地面的可靠传输。设计了基于温度模型增强的DD3QN-AS算法，包含Transformer时序特征提取和潜在提示生成模块。

Result: 模拟结果显示，所提方法在优化信息年龄和降低卫星切换频率方面表现出艶工胜优性，较传统策略基方法和其他深度强化学习基准方法都有更好的性能。

Conclusion: 该研究成功地解决了LEO卫星组网的间歇性覆盖问题，通过智能的空中中继和混合链路技术，实现了更可靠的全球通信服务。提出的温度模型增强强化学习算法有效处理了高度动态非凸优化问题的计算挑战。

Abstract: Despite the widespread deployment of terrestrial networks, providing reliable
communication services to remote areas and maintaining connectivity during
emergencies remains challenging. Low Earth orbit (LEO) satellite constellations
offer promising solutions with their global coverage capabilities and reduced
latency, yet struggle with intermittent coverage and limited communication
windows due to orbital dynamics. This paper introduces an age of information
(AoI)-aware space-air-ground integrated network (SAGIN) architecture that
leverages a high-altitude platform (HAP) as intelligent relay between the LEO
satellites and ground terminals. Our three-layer design employs hybrid
free-space optical (FSO) links for high-capacity satellite-to-HAP communication
and reliable radio frequency (RF) links for HAP-to-ground transmission, and
thus addressing the temporal discontinuity in LEO satellite coverage while
serving diverse user priorities. Specifically, we formulate a joint
optimization problem to simultaneously minimize the AoI and satellite handover
frequency through optimal transmit power distribution and satellite selection
decisions. This highly dynamic, non-convex problem with time-coupled
constraints presents significant computational challenges for traditional
approaches. To address these difficulties, we propose a novel diffusion model
(DM)-enhanced dueling double deep Q-network with action decomposition and state
transformer encoder (DD3QN-AS) algorithm that incorporates transformer-based
temporal feature extraction and employs a DM-based latent prompt generative
module to refine state-action representations through conditional denoising.
Simulation results highlight the superior performance of the proposed approach
compared with policy-based methods and some other deep reinforcement learning
(DRL) benchmarks.

</details>


### [6] [State Aware Traffic Generation for Real-Time Network Digital Twins](https://arxiv.org/abs/2509.12860)
*Enes Koktas,Peter Rost*

Main category: cs.NI

TL;DR: 基于隐马尔可夫模型和混合密度网络的紧凑流量生成器，能够在数秒内训练并生成实际的网络数据包，为数字双生体提供持续的数据供应而不影响实际网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决数字双生体对实时真实世界数据的依赖性问题，避免收集和传输完整跟踪数据带来的重大挑战。

Method: 结合隐马尔可夫模型（捕捉缓冲、流媒体和空闲期的广泛节奏）和小型前向混合密度网络（生成实际有效负货大小和刻间刻到达时间）的紧凑流量生成器。

Result: 该流量生成器能够在服务器GPU上秒级训练，实时运行，生成的数据包跟踪在分布保真度、多样性和时间相关性等各种指标上都显示出高实际性。

Conclusion: 该方法允许运营商在不影响操作网络的情况下保持数字双生体的最新状态，为智能化自动优化移动网络提供可靠的数据支持。

Abstract: Digital twins (DTs) enable smarter, self-optimizing mobile networks, but they
rely on a steady supply of real world data. Collecting and transferring
complete traces in real time is a significant challenge. We present a compact
traffic generator that combines hidden Markov model, capturing the broad
rhythms of buffering, streaming and idle periods, with a small feed forward
mixture density network that generates realistic payload sizes and
inter-arrival times to be fed to the DT. This traffic generator trains in
seconds on a server GPU, runs in real time and can be fine tuned inside the DT
whenever the statistics of the generated data do not match the actual traffic.
This enables operators to keep their DT up to date without causing overhead to
the operational network. The results show that the traffic generator presented
is able to derive realistic packet traces of payload length and inter-arrival
time across various metrics that assess distributional fidelity, diversity, and
temporal correlation of the synthetic trace.

</details>


### [7] [It Takes a Village: Bridging the Gaps between Current and Formal Specifications for Protocols](https://arxiv.org/abs/2509.13208)
*David Basin,Nate Foster,Kenneth L. McMillan,Kedar S. Namjoshi,Cristina Nita-Rotaru,Jonathan M. Smith,Pamela Zave,Lenore D. Zuck*

Main category: cs.NI

TL;DR: 这篇论文讨论了形式规范在网络协议设计中的重要性以及当前广泛使用非形式文档的现状，并提出了渗透两个社区洞差的建议。


<details>
  <summary>Details</summary>
Motivation: 形式规范虽然有明确、无歧义、可证明安全性等优势，但在网络协议设计中并未得到广泛采用，而是依赖RFC等非形式文档。论文强调了形式规范的价值并寻求解决两个社区间的理解差异。

Method: 论文首先分析了规范在网络和形式方法社区中的不同角色，然后通过几个最近的成功案例展示形式规范的潜在优势，最后识别并建议渗透两个社区洞差的策略。

Result: 论文展示了形式规范在网络协议设计中的重要价值，包括提供清晰文档、发现歧义、支持形式推理和安全性证明。同时指出了形式方法社区与网络社区在规范理解上的关键差异。

Conclusion: 形式规范对网络协议设计有重要价值，但需要渗透形式方法社区和网络社区之间的洞差。论文提供了有效的成功案例和建议策略，以促进形式规范在网络协议开发中的更广泛采用。

Abstract: Formal specifications have numerous benefits for both designers and users of
network protocols. They provide clear, unambiguous representations, which are
useful as documentation and for testing. They can help reveal disagreements
about what a protocol "is" and identify areas where further work is needed to
resolve ambiguities or internal inconsistencies. They also provide a foundation
for formal reasoning, making it possible to establish important security and
correctness guarantees on all inputs and in every environment. Despite these
advantages, formal methods are not widely used to design, implement, and
validate network protocols today. Instead, Internet protocols are usually
described in informal documents, such as IETF Requests for Comments (RFCs) or
IEEE standards. These documents primarily consist of lengthy prose
descriptions, accompanied by pseudocode, header descriptions, state machine
diagrams, and reference implementations which are used for interoperability
testing. So, while RFCs and reference implementations were only intended to
help guide the social process used by protocol designers, they have evolved
into the closest things to formal specifications the Internet community has. In
this paper, we discuss the different roles that specifications play in the
networking and formal methods communities. We then illustrate the potential
benefits of specifying protocols formally, presenting highlights from several
recent success stories. Finally, we identify key differences between how formal
specifications are understood by the two communities and suggest possible
strategies to bridge the gaps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math是一个面向越南高中数学考试的自主代理框架，集成了题目生成、解题解释和个性化辅导三个AI代理，支持学生自主练习和教师出题工作。


<details>
  <summary>Details</summary>
Motivation: 帮助越南高中生准备国家高中数学毕业考试，减轻教师手动出题负担，提供符合国家标准的个性化数学学习支持。

Method: 采用三代理架构：规范矩阵条件题目生成器、分步解题解释器、个性化辅导代理，支持学生练习模式和教师题目生成功能。

Result: 初步评估显示系统能生成符合矩阵要求的考题，解题准确率高，解释连贯，丰富了练习材料的多样性。

Conclusion: V-Math有潜力支持规模化、公平的数学备考，符合国家标准，同时通过AI辅助出题赋能教师。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [9] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 提出了DISPLIB标准格式和数据集，用于火车调度问题的标准化研究和算法性能比较


<details>
  <summary>Details</summary>
Motivation: 现有火车调度优化研究缺乏统一标准，代码和数据不公开，导致难以复现和比较不同算法的性能

Method: 定义统一的火车重路由和重调度问题格式DISPLIB，收集多个真实工业案例的问题实例，提供参考求解器实现

Result: 建立了开放的火车调度问题标准库，包含多个真实工业实例，为研究社区提供了统一的测试平台

Conclusion: DISPLIB标准促进了火车调度问题的可复现研究，使研究人员无需工业合作就能开展相关研究，并支持算法间的实证比较

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [10] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: 提出了InPhyRe基准测试来评估大型多模态模型的归纳物理推理能力，发现现有模型在违反训练时物理定律的场景中表现不佳，存在语言偏见和视觉输入忽略问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型仅编码训练时观察到的物理定律作为参数知识，无法处理违反这些定律的推理场景，而人类具备从少量视觉示例中适应新物理环境的归纳推理能力。

Method: 创建InPhyRe视觉问答基准，使用算法生成的合成碰撞视频来评估模型在碰撞事件结果预测中的归纳物理推理能力。

Result: 对13个大型多模态模型的评估显示：1）模型难以将有限的参数知识应用于推理；2）在违反物理定律的演示样本中归纳推理能力弱；3）存在语言偏见且忽略视觉输入。

Conclusion: 大型多模态模型在归纳物理推理方面存在显著缺陷，质疑其在安全关键应用中替代人类代理的可信度，需要改进对视觉输入的利用和推理能力。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [11] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 提出了LLMAP系统，结合LLM解析自然语言偏好和图搜索算法，解决多约束条件下的最优路径规划问题


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：直接使用LLM处理地图数据能力不足，图搜索方法理解自然语言偏好能力有限，且用户时空分布高度异构不可预测

Method: 使用LLM-as-Parser解析自然语言、识别任务、提取用户偏好和任务依赖关系，结合多步图构建迭代搜索算法(MSGS)进行最优路径求解，采用多目标优化自适应调整权重

Result: 在14个国家27个城市的1000个路由提示上进行实验，结果显示该方法在多重约束下实现了优越性能

Conclusion: LLMAP系统能够有效处理自然语言驱动的路径规划问题，在保证多约束条件的同时实现最优路径规划

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [12] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 研究开发了一个智能汽培温室系统，结合IoT和AI技术监控玫瑰植物状态和环境条件，并通过AI算法识别植物疾病。


<details>
  <summary>Details</summary>
Motivation: 精准控制温室环境条件和监测植物状态，以便及时做出合理的管理决策，提高作物生产效率。

Method: 开发IoT基础平台监控环境参数，使用VGG-19、InceptionResNetV2和InceptionV3算法构建AI疾病检测框架，分析定期拍摄的植物图像。

Result: IoT系统能够实时发布温度、湿度、水流等数据，调整环境参数以优化植物生长。VGG-19算法在识别干旱压力和锌病叶片方面达到最高92%的准确率。

Conclusion: 研究成功开发了一个整合IoT和AI技术的智能温室系统，能够有效监控植物生长环境和识别疾病，为精准农业提供技术支撑。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [13] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant是一个开源的人类-AI协作框架，用于简化端到端的科研工作流创建，在机器学习的综述和观点论文写作中表现出色，但仍需要人类监督来确保准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助研究工具存在碎片化问题，缺乏以人为中心的工作流程，需要开发集成化的协作框架来提升科研效率。

Method: 开发了模块化的AIssistant系统，包含文献合成、分段实验、引用管理和LaTeX自动生成等工具，采用三层评估体系：独立人工评审、自动化LLM评审和程序主席监督。

Result: AIssistant提高了起草效率和主题一致性，但存在引用幻觉、动态论文结构适应性差和多模态内容整合不完整等局限性。

Conclusion: 人类-AI协作对于保持事实准确性、方法严谨性和伦理合规性仍然至关重要，AIssistant为科研工作流提供了有前景的解决方案但需要进一步完善。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [14] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 通过分解方法，先对用户交互进行结构化摘要，再使用精调模型进行意图提取，在资源受限模型中实现了更好的意图理解效果


<details>
  <summary>Details</summary>
Motivation: 解决小型设备端模型在用户交互轨迹意图推理上的性能不足，同时保障隐私、低成本和低延迟的体验

Method: 两步分解方法：1.结构化交互摘要，捐捕每个用户动作的关键信息；2.使用精调模型对聚合摘要进行意图提取

Result: 在资源受限模型中显著提升了意图理解能力，甚至超越了大型多模态语言模型的基础性能

Conclusion: 通过结构化的分解方法，小型模型也能在设备端实现高效的用户意图推理，为智能代理发展提供了新的解决方案

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [15] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 提出了一个名为\sys的熵增强框架，用于在多轮工具辅助的代码生成任务中增强偏好优化算法，通过保持策略熵和多轮交互优化来提升测试时扩展的效果，在SWE-bench基准测试中取得了开源模型的最先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化算法（如DPO和KTO）虽然能有效对齐模型输出与人类偏好，但会降低输出多样性，限制了测试时扩展的效果。同时，这些算法主要针对单轮任务设计，无法充分处理交互式编程代理所需的多轮推理和工具集成复杂性。

Method: \sys框架通过增强偏好目标来显式保持策略熵，并将学习推广到优化多轮交互而非单轮响应。还提出了混合最佳轨迹选择方案，结合学习验证器模型和无模型方法。在不同家族和大小的模型（最大106B参数）上进行了微调验证。

Result: 在SWE-bench排行榜上，该方法在开源权重模型中建立了新的最先进结果。使用\sys训练的30B参数模型在\lite上排名第1，在\verified上排名第4，仅被参数量超过10倍（>350B）的模型超越。

Conclusion: \sys框架成功地将偏好优化算法适应到多轮工具辅助设置中，通过保持输出多样性和优化多轮交互，显著提升了LLM在复杂软件工程任务中的性能，特别是在测试时扩展方面取得了显著效果。

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [16] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: PIWM是一个紧凑的鸟瞰图世界模型，通过Soft Mask训练和Warm Start推理技术，在保持小参数量的同时显著提升物理动态建模和预测性能


<details>
  <summary>Details</summary>
Motivation: 解决世界模型在部署时面临的大小与性能之间的权衡问题：大模型计算资源需求高，小模型物理建模精度不足

Method: 提出Physics-Informed BEV World Model (PIWM)，使用Soft Mask训练改善动态物体建模，引入Warm Start推理技术提升预测质量

Result: 在相同参数量(400M)下比基线模型提升60.6%的加权总分；最小PIWM(130M)比最大基线模型(400M)得分高7.4%，推理速度快28%

Conclusion: PIWM通过创新的训练和推理技术，成功实现了小参数模型的高性能物理建模，为边缘设备部署提供了有效解决方案

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [17] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 通过在剪枝过程中同时重构输入激活和模型的链式思绪追踪，提出思绪感知压缩(RAC)方法，解决了标准LLM剪枝方法在推理任务中性能下降和速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 识别到DeepSeek-R1等推理语言模型在推理时产生长链式思绪追踪，部署成本高；标准LLM剪枝方法在推理任务上导致更大性能损失和更多思考标记但性能更差的问题。

Method: 提出思绪感知压缩(RAC)方法：在剪枝过程中同时重构输入激活和模型的链式思绪追踪，无缝集成到SparseGPT等现有剪枝流程中。

Result: RAC方法显著提升了现有剪枝方法的性能，解决了标准剪枝在推理任务中的性能下降问题。

Conclusion: 通过考虑推理任务的特殊性，RAC方法提供了一种简单有效的解决方案，能够在保持模型性能的同时降低部署成本。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [18] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT是一个AI驱动的系统，通过整合大语言模型和统计引擎，自动化临床试验设计中的检验选择和样本量计算，显著提高了完成率、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 临床研究和试验设计中的样本量计算对统计能力分析至关重要，但其复杂性和对统计专业知识的依赖给许多研究人员造成了障碍。

Method: 开发了PowerGPT系统，整合大语言模型(LLMs)与统计引擎，自动化测试选择和样本量估计过程。通过随机试验评估其有效性。

Result: PowerGPT显著提高了任务完成率(检验选择99.3% vs 88.9%，样本量计算99.3% vs 77.8%)和准确性(样本量估计94.1% vs 55.4%，p<0.001)，同时减少了平均完成时间(4.0 vs 9.3分钟，p<0.001)。这些优势在各种统计检验中保持一致，对统计学家和非统计学家都有益。

Conclusion: PowerGPT代表了一种可扩展的AI驱动方法，提高了临床研究中统计能力分析的可及性、效率和准确性，目前已在多个机构部署使用。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [19] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 该研究通过分析Soma Cube物理拼图的计算复杂度，将计算机科学概念映射到认知问题解决策略，提出了"物质性原则"，展示了如何通过分层策略（预处理、排序、剪枝等）降低任务难度，并建立了智力作为心智与物质能力算法库的模型。


<details>
  <summary>Details</summary>
Motivation: 认知科学和理论计算机科学都致力于分类和解释任务难度，本研究旨在通过物理拼图的分析来理解智能机制如何通过利用物质约束来降低任务复杂度。

Method: 通过分析Soma Cube拼图的分支因子（搜索树出度）来量化任务难度，系统研究不同策略如何修改复杂度。采用分层方法逐步改进试错搜索，包括预处理（认知组块）、值排序（认知自由排序）、变量排序（认知支架）和剪枝（认知推理）。

Result: 研究表明，通过合理使用人工制品可以利用物理约束有效降低时间复杂度，不同认知策略能够显著影响问题解决的复杂度和效率。

Conclusion: 提出了智力作为算法库的模型，该模型能够同时利用心智和物质的能力，"物质性原则"为理解智能机制如何通过物理约束降低任务难度提供了理论框架。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [20] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 本研究通过可解释的两步工作流程分析俄亥俄州环岛事故，识别出四种事故模式，并使用SHAP解释树模型量化伤害驱动因素，发现黑暗、湿滑路面和高速与固定物体/角度事件同时发生时严重性更高。


<details>
  <summary>Details</summary>
Motivation: 环岛虽然减少了严重事故，但风险模式因条件而异，需要可解释的分析方法来理解不同条件下的风险机制。

Method: 使用聚类对应分析(CCA)识别共现因素和四种事故模式，然后基于树的严重性模型通过SHAP进行解释，量化模式内和跨模式的伤害驱动因素。

Result: 研究发现黑暗、湿滑路面和较高限速与固定物体或角度事件同时发生时事故严重性更高，而在清晰、低速环境下严重性较低。特定模式的解释揭示了入口处(未能让行、间隙接受)、多车道循环内(不当操作)和减速时(追尾)的机制。

Conclusion: 该工作流程将模式发现与案例级解释联系起来，支持站点筛选、对策选择和审计就绪报告，为公共安全分析提供了实用的可解释AI模板。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [21] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: zELO是一种新颖的训练方法，通过将排序任务等同于Thurstone模型来优化检索性能，使用无监督数据训练出了zerank-1系列重排序模型，在多个领域达到最先进的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有的重排序模型往往依赖大量标注数据，训练成本高且泛化能力有限。zELO方法旨在通过无监督学习方式，利用Thurstone模型的统计等价性来优化检索性能，降低训练成本同时提升模型效果。

Method: 基于Thurstone模型的统计等价性分析，使用112,000个查询和每个查询100个文档的无标注数据，在不到10,000 H100小时的训练时间内进行端到端训练，开发了zerank-1和zerank-1-small两个重排序模型。

Result: zerank-1系列模型在金融、法律、代码和STEM等多个领域取得了最高的检索分数，在NDCG@10和Recall指标上均优于闭源专有重排序器，同时在域外和私有客户数据集上保持了优秀的零样本性能。

Conclusion: zELO方法证明了通过无监督学习和Thurstone模型等价性分析可以有效训练高性能重排序模型，在多个专业领域超越现有技术，展示了强大的泛化能力和实用性。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [22] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 这是首个结合场景文本检测、图像修复、机器翻译和文本重新排版的框架，用于加速多语言广告本地化评估流程。


<details>
  <summary>Details</summary>
Motivation: 多语言广告本地化需要保持视觉一致性、空间对齐和风格完整性，而不仅仅是简单的文本翻译。

Method: 提出结构化框架，结合自动化组件与人工监督，集成场景文本检测、图像修复、机器翻译和文本重新排版技术。

Result: 在六个语言区域的定性评估显示，该方法能够产生语义准确且视觉一致的本地化广告，适用于实际工作流程。

Conclusion: 该框架有效解决了广告本地化的复杂性问题，为实际应用提供了可行的解决方案。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [23] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: 这篇论文介绍了Agentic AI技术，通过Minerva CQ实时助理协助系统提升客服效率和客户体验，解决了传统AI助理工具的困扰。


<details>
  <summary>Details</summary>
Motivation: 客户服务领域存在处理时间长、一通电话解决率低、客户满意度差等问题，主要原因是人工助理面临的认知负荷和系统分割挑战。现有AI助理工具多为反应式设计，缺乏深度上下文理解能力。

Method: 提出Agentic AI目标驱动的自主工具使用系统，通过Minerva CQ实现实时转写、意图和情感检测、实体识别、上下文检索、动态客户画像和部分对话摘要等功能，支持主动工作流程和持续上下文构建。

Result: Minerva CQ在生产环境中部署，作为AI副驾驶为助理提供支持，在多个部署中实现了助理效率和客户体验的可衡量改善。

Conclusion: Agentic AI通过主动性、自主性和动态适应能力，有效解决了传统客服领域的挑战，为实时助理支持系统开辟了新路径。

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [24] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat是一个基于智能代理的实时网球比赛助手，结合GenAI和GenComp技术，在温网和美网为100万用户提供比赛相关查询的即时准确回答，准确率达92.83%，平均响应时间6.25秒。


<details>
  <summary>Details</summary>
Motivation: 提升网球球迷的观赛体验，通过自然语言查询为球迷提供实时、准确的比赛相关信息和数据分析。

Method: 采用面向代理架构(AOA)，结合规则引擎、预测模型和智能代理预处理和优化用户查询，再传递给GenAI组件，使用交互式提示设计引导96.08%的查询。

Result: 系统在120 RPS负载下达到92.83%的答案准确率，平均响应时间6.25秒，支持近100万独立用户，保持100%正常运行时间。

Conclusion: 该工作提出了实时消费级AI系统的关键设计模式，强调了速度、精度和可用性，为在动态环境中部署高性能代理系统提供了实用路径。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [25] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: DaSAThco是一个基于大语言模型的框架，通过学习从实例特征到定制启发式集合的可泛化映射，解决了SAT求解器配置通用性问题，实现了训练一次、广泛适应的模型。


<details>
  <summary>Details</summary>
Motivation: 传统CDCL求解器的内部启发式配置难以在异构SAT问题上实现通用最优，而现有的数据集特定优化方法缺乏泛化能力且需要昂贵的重新优化成本。

Method: 使用大语言模型，在系统定义的问题原型指导下生成多样化的专用启发式集合组合，然后学习自适应选择机制来形成最终的映射关系。

Result: 实验表明DaSAThco实现了优越性能，特别是在非自适应方法表现有限的领域外泛化方面表现出强大的鲁棒性。

Conclusion: 这项工作为复杂可配置系统的自动化算法设计建立了更可扩展和实用的路径。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [26] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: AD-FCoT是一个基于类比推理和思维链提示的金融新闻情感分析框架，无需额外训练即可提升情感分类准确性和市场回报相关性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以捕捉复杂经济背景且缺乏透明推理，影响可靠性。大语言模型虽具备强大文本理解能力，但需要更好的方法来整合历史经济情境。

Method: 提出类比驱动的金融思维链(AD-FCoT)提示框架，通过引导LLM在新事件与已知结果的历史场景间建立类比，并将这些类比嵌入结构化逐步推理链中。

Result: 在数千篇新闻文章上的实验显示，AD-FCoT在情感分类准确性上优于强基线，与市场回报的相关性显著更高，生成的解释与领域专业知识一致。

Conclusion: AD-FCoT是首批在金融领域明确结合类比示例与思维链推理的方法之一，通过纯提示方式提供可解释的金融分析见解，适合实际应用。

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [27] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: GBV-SQL是一个多智能体框架，通过SQL2Text反向翻译验证来解决Text2SQL中的语义鸿沟问题，同时揭示了基准数据中的系统性错误问题。


<details>
  <summary>Details</summary>
Motivation: 当前Text2SQL生成中存在语义鸿沟问题，即语法有效的查询经常误解用户意图，同时基准数据本身存在质量问题，掩盖了模型的真实性能。

Method: 提出GBV-SQL多智能体框架，引入引导生成和SQL2Text反向翻译验证机制，使用专门智能体将生成的SQL翻译回自然语言以验证逻辑一致性。

Result: 在BIRD基准上达到63.23%执行准确率（提升5.8%），在去除错误样本后，在Spider基准上达到96.5%（开发集）和97.6%（测试集）执行准确率。

Conclusion: 该工作不仅提供了语义验证的鲁棒框架，还揭示了基准完整性的关键问题，强调了更严格数据集管理的必要性。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [28] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于随机森林模型的新方法，用于预测群体生牛一个月后的体重增长，精准度较高，并开发了自动化数据处理工具。


<details>
  <summary>Details</summary>
Motivation: 预测生牛体重增长可以帮助农场主优化喂养策略、做出更好的繁育选择，并降低气候变化和市场波动带来的风险。

Method: 研究采用随机森林模型，与支持向量回归和长短期记忆模型进行对比。使用了包含108头生牛756个样本数据，以及雨量和温度等气象因素。

Result: 随机森林模型在所有数据集上表现最佳，当包含气象和年龄因素时，R²达到0.973，RMSE为0.040，MAE为0.033。结果显示包含这些因素显著提高了预测准确性。

Conclusion: 随机森林模型是预测生牛体重增长的稳健工具，年龄和气候因素对群体体重趋势有显著影响。研究还开发了自动化数据处理工具并公开提供，以便未来研究使用。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [29] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: ECG-aBcDe是一种新颖的心电图编码方法，将ECG信号转换为通用ECG语言，使任何大语言模型都能直接分析心电图，解决了可迁移性、时间尺度信息学习和可解释性三大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在心电图分析中存在三个主要问题：模型特定的ECG编码器阻碍跨模型迁移、Transformer难以捕捉ECG关键时间尺度信息、黑盒特性限制临床应用。

Method: 通过构建ECG语言和自然语言的混合数据集，ECG-aBcDe将ECG信号转换为通用ECG语言，无需修改LLMs架构即可直接微调预训练模型，实现双向可转换性并提取注意力热图。

Result: 在ROUGE-L和METEOR上达到竞争性性能，在BLEU-4指标上显著提升，数据集内评估提升2.8倍(42.58分)，跨数据集评估提升3.9倍(30.76分)。

Conclusion: ECG-aBcDe为ECG分析与LLMs集成提供了新范式，证明了这种方法的可行性，显著提升了性能指标和可解释性。

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [30] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: AutoCO是首个端到端的自动化约束优化方法，利用LLM生成约束松弛策略，通过双向协同进化机制结合进化算法和蒙特卡洛树搜索，在非线性组合优化问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统约束松弛方法依赖专家经验且缺乏自动化，现有LLM优化方法主要作为被动约束验证器而非主动策略设计者，无法处理NCOPs中复杂的约束交互

Method: 利用结构化LLM推理生成约束松弛策略，通过统一的三重表示方案动态演化算法原理和可执行代码；建立双向（全局-局部）协同进化机制，结合进化算法进行局部优化和蒙特卡洛树搜索进行全局策略探索

Result: 在三个具有挑战性的NCOP基准测试上的综合实验验证了AutoCO的一致有效性和优于基线的性能表现

Conclusion: AutoCO通过LLM学习松弛约束的方法革新了NCOPs的解决方式，在碎片化解空间中实现了强化和多样化的最优平衡

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [31] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 对前沿大语言模型在18个月期间的推理能力进行纵向研究，发现通过思维链提示和思维模型的引入，模型性能显著提升，并提出了一种神经符号架构来降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型随时间推移的推理能力变化，并探索更高效的推理方法以减少计算成本。

Method: 使用PrOntoQA数据集的真假问题测试三个领先模型，采用隐藏思维链提示和思维模型，并开发神经符号架构将问题转化为标准化形式后用Z3求解器解决。

Result: 从2023年到2025年模型性能持续提升，神经符号方法在保持近乎完美性能的同时显著降低了计算成本，FLOPs估算准确率在10%以内。

Conclusion: 大语言模型的推理能力随时间显著提升，神经符号架构提供了一种计算效率更高的替代方案，FLOPs估算公式在实践中准确可靠。

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [32] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: GRRAF是一种无需训练的图推理方法，利用检索增强生成和LLM代码生成能力，通过生成可执行查询代码从图数据库中检索信息，在多种图推理任务上达到100%准确率，并能扩展到万节点大图。


<details>
  <summary>Details</summary>
Motivation: 现有图推理方法需要大量微调或依赖预定义算法，存在局限性。GRRAF旨在开发一种无需训练、灵活且可扩展的图推理解决方案。

Method: 将目标图存储在图形数据库中，提示LLM生成可执行代码查询来检索必要信息，包含错误反馈循环和超时机制确保正确性和效率。

Result: 在GraphInstruct数据集上，大多数图推理任务达到100%准确率（包括环检测、二分图检查、最短路径、最大流等），子图匹配性能也很高，能有效扩展到10,000节点的大图，且token成本与图大小无关。

Conclusion: GRRAF提供了一种高效、准确且可扩展的图推理方法，无需训练即可处理复杂图任务，展现了检索增强生成与代码生成LLM结合的强大潜力。

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [33] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 通过分层记忆架构H^2R实现细粒度知识转移，提升LLM组件在多任务场景中的氛园性和决策性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将经验和知识作为整体单元处理，导致知识转移效率低下和粗粒度问题

Method: 提出分层记忆架构，将高级规划记忆与低级执行记忆解耦，通过分层后见反思(H^2R)机制从历史交互中精炼可重用知识

Result: 在两个测试集上证明H^2R能够提升氛园性和决策性能，性能超过Expel等基线方法

Conclusion: 分层记忆架构和H^2R机制为LLM组件提供了更有效的细粒度知识转移方案

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [34] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: LTA-Thinker是一个潜在思维增强训练框架，通过增加潜在思维分布的方差和引入基于分布的定向优化范式，提升大语言模型的复杂推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Coconut和SoftCoT在连续潜在空间推理中有效，但高质量潜在思维的生成和利用仍是核心瓶颈。基于SoftCoT++理论，更大的潜在思维分布方差能更好逼近真实分布。

Method: 1. 基于可学习先验构建潜在思维生成架构，增加生成向量的方差分布；2. 引入基于分布的定向优化范式，结合SFT损失、语义对齐损失（KL散度）和推理焦点损失（对比学习）进行多目标协同训练。

Result: 实验显示LTA-Thinker在各种基线中达到最先进性能，表现出更高的性能上限和更好的扩展效果。

Conclusion: LTA-Thinker通过增强潜在思维分布的方差和优化分布特性，有效提升了复杂推理任务的性能，为大规模语言模型的推理优化提供了新思路。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [35] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: LLMs无法可靠生成欧洲城市随机街道地址


<details>
  <summary>Details</summary>
Motivation: 测试大型语言模型在生成看似简单但需要精确地理知识的随机街道地址方面的能力

Method: 通过让LLMs生成欧洲城市的随机街道地址来评估其表现

Result: LLMs在生成随机街道地址方面表现不佳，尽管它们能解决复杂数学问题和回答困难问题

Conclusion: LLMs在需要精确地理信息的简单任务上存在局限性，显示出知识覆盖的不均衡性

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [36] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [37] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC是一个基于星际争霸II的新多智能体强化学习基准测试，包含12个基于三十六计设计的场景，专注于评估高级战略决策能力，超越了传统微操测试的局限。


<details>
  <summary>Details</summary>
Motivation: 现有MARL基准测试（如SMAC）主要关注微观操作，缺乏对高级战略智能的全面评估。需要一个新的基准来测试多智能体在复杂战略决策方面的能力。

Method: 基于三十六计设计了12个星际争霸II场景，每个场景对应一个特定计策，包含战术机动、时机协调和欺骗等战略元素。提出了超越传统胜率的多维度新指标。

Result: HLSMAC为评估多智能体战略决策能力提供了强大的测试平台，实验结果表明该基准能够有效评估不同MARL算法和基于LLM的智能体的战略表现。

Conclusion: HLSMAC填补了MARL基准测试在高级战略评估方面的空白，为多智能体战略决策研究提供了新的评估标准和测试环境。

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [38] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: FSRL是一种透明的对齐框架，通过训练轻量级适配器来调节稀疏自编码器的可解释特征，实现与RLHF相当的效果，但更易于分析对齐机制。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF方法导致参数变化分散且不透明，难以理解模型内化了什么内容，需要更透明的对齐方法。

Method: 使用特征引导强化学习(FSRL)，训练轻量级适配器来调节稀疏自编码器(SAE)提取的可解释特征。

Result: FSRL在偏好优化方面与当前RLHF方法效果相当，机制分析发现适配器策略更倾向于提升风格特征而非明确的对其概念。

Conclusion: FSRL为可解释的模型控制和诊断对齐内部机制提供了工具，发现偏好优化过程将风格呈现作为质量的代理进行奖励。

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [39] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 通过衍化算法实现黑盒大语言模型合并，仅需API查询即可结合多个模型优势


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型作为黑盒服务时无法获取模型参数的合并挑战

Method: 基于衍化算法的无导数优化框架，包括稀疏性去噪和符号效应缩放两个核心组件

Result: 在多个任务上达到最先进水平，显著超过现有基线方法

Conclusion: Evo-Merging方法为黑盒大模型合并提供了高效解决方案，仅需推理API即可实现多模型能力结合

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [40] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 提出了隐私增强持续学习框架PeCL，通过动态差分隐私和隐私引导记忆雕刻，在保护敏感信息的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法对所有数据统一保护，导致模型性能严重下降，阻碍了持续学习在隐私敏感领域的应用

Method: 1. 基于语义敏感性的token级动态差分隐私策略；2. 隐私引导的记忆雕刻模块，智能遗忘敏感信息并保留任务不变知识

Result: PeCL在隐私保护和模型效用之间取得了优越的平衡，在保持先前任务高精度的同时确保强大的隐私保护

Conclusion: 该框架为隐私敏感的持续学习应用提供了有效的解决方案，通过选择性保护机制实现了隐私与性能的最佳权衡

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [41] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: Planning Copilot是一个集成多种规划工具的聊天机器人，通过自然语言指令调用工具，显著提升LLMs在规划任务上的性能，甚至优于GPT-5


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏可靠的长期规划能力，需要外部工具支持来完成复杂的规划任务

Method: 基于Model Context Protocol标准，将LLMs与外部规划工具集成，支持语法检查、规划器选择、计划验证和执行模拟等功能

Result: 实验表明Planning Copilot显著优于未使用规划工具的相同LLMs，并且在有限定性比较中明显优于GPT-5

Conclusion: 专用规划工具是使LLMs能够有效执行规划任务的有效方式

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [42] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 研究测试安娜·卡列尼娜原则在不同媒介文本中的适用性，发现结构模式因媒介而异：小说遵循反向AKP顺序，维基百科结合AKP和有序模式，学术论文顺序上反向AKP但位置杂乱，电影按类型分化


<details>
  <summary>Details</summary>
Motivation: 验证安娜·卡列尼娜原则（成功需要满足少量必要条件，失败形式多样）及其变体在不同文本媒介中的适用性，探索不同领域成功与失败的结构模式差异

Method: 将文本表示为功能块序列，通过转换顺序和位置评估收敛性，分析小说、在线百科全书、研究论文和电影四种媒介

Result: 不同媒介呈现不同结构模式：小说顺序上遵循反向AKP，维基百科结合AKP和有序模式，学术论文顺序反向AKP但位置杂乱，电影类型间存在分化

Conclusion: 成功依赖于特定媒介的结构约束，而失败在不同领域呈现不同形态，结构原则因媒介而异

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [43] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: MiniAgentPro是一个可视化平台，用于评估LLM代理在物理环境中的事件组织能力，包含地图编辑器和模拟播放器，通过8个不同事件场景的测试集发现基础场景表现良好但复杂协调存在挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理社会模拟框架缺乏系统的事件组织评估和物理环境可视化集成，限制了代理在空间中导航和与现实物品交互的能力

Method: 开发MiniAgentPro可视化平台，包含直观的地图编辑器用于定制环境，以及带有平滑动画的模拟播放器；构建包含8个不同事件场景（基础版和困难版变体）的综合测试集

Result: 使用GPT-4o进行评估，在基础设置中表现出色，但在困难变体中突显了协调挑战

Conclusion: MiniAgentPro平台有效解决了现有框架在物理环境集成和系统评估方面的不足，为LLM代理的社会模拟提供了更真实的测试环境，但复杂协调任务仍需改进

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [44] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新的大学招生问题基准测试集，用于评估大语言模型在匹配问题中的性能，发现虽然LLMs能满足某些约束，但在合格性、稳定性和最优性方面仍有困难，且不同提示策略的效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学推理和组合优化任务上表现出艰，但在需要在偏好和结构约束下进行推理的匹配问题上仍然研究不足。需要开发专门的测试基准来评估LLMs在这类问题中的性能。

Method: 建立了一个包含369个实例的大学招生问题基准测试集，从合格性、稳定性和最优性三个维度评估多个开源LLMs的性能。测试了不同的提示策略，包括思维链、上下文学习和角色基于提示，以及迭代提示与自动生成反馈的结合。

Result: 结果显示LLMs虽然能满足某些约束，但无法一致满足所有评估标准。具有推理能力的模型（如QwQ和GPT-oss）显著超过传统模型（如Llama、Qwen或Mistral）。不同提示策略的效果差异显著，没有一种提示能一致提供最佳性能。迭代提示的性能并非单调增长，可能在早期达到峰值后减退。

Conclusion: 这项工作为模型推理性能和提示策略在带有偏好约束的组合优化问题中的效果提供了新的视角，强调了在这些复杂任务中对推理能力和提示策略进行进一步研究的必要性。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [45] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于自治代理的AI系统，用于数字化金融平台的金融犯罪合规管理，重点强调可解释性、可追踪性和设计合规性。


<details>
  <summary>Details</summary>
Motivation: 金融犯罪合规管理成本高、复杂度增加，但效果提升不明显；当前AI解决方案不透明且与监管要求对齐不佳。

Method: 采用行动设计研究(ADR)方法，与金融科技公司和监管利益相关者合作开发，通过工件中心模型赋予自治代理明确身份角色，支持任务特定模型路由和审计日志记录。

Result: 开发了一个实际原型系统，能够自动化客户入档、监控、调查和报告等合规流程，并提供了参考架构。

Conclusion: 研究表明，在可负责治理结构下嵌入的自动化AI系统能够在高风险监管环境中支持透明度和制度信任，为信息系统领域的AI合规研究做出了贡献。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [46] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 提出基于图的冲突集提取算法(G-CSEA)，用于高效识别伪布尔约束模型中的不可行子集，解决传统方法在调度问题中计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 劳动力调度中存在大量基于规则的约束，这些约束可能相互冲突导致模型不可行。传统IIS提取方法需要大量可行性检查，计算成本高；对偶射线分析方法在伪布尔模型中可能失效。

Method: 基于冲突驱动子句学习(CDCL)思想，在约束传播过程中构建蕴含图，检测到冲突时追踪所有决策分支中的贡献约束，形成冲突集，可选使用QuickXplain进一步最小化为IIS。

Result: 开发了G-CSEA算法，能够有效提取伪布尔约束模型中的冲突集，减少求解器调用次数。

Conclusion: G-CSEA为处理复杂调度规则约束的不可行性分析提供了更高效的解决方案，特别适用于伪布尔约束模型。

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [47] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 这篇论文研究多模态大语言模型在糖尿病视网膜病检测中的应用，比较了GPT-4o和MedGemma两款模型的性能，并探索了不同输出格式对临床AI协作的影响。


<details>
  <summary>Details</summary>
Motivation: 目前FDA批准的糖尿病视网膜病AI筛查系统主要提供二进制转诊输出，这种最小化输出可能限制了临床信任和实用性。需要找到最有效的输出格式来提升医生与AI的协同性能。

Method: 使用IDRiD和Messidor-2数据集评估GPT-4o和MedGemma两款MLLM模型。实验包括：(1)基准评估 (2)使用合成预测模拟AI协助 (3)实际AI之间协作（GPT-4o整合MedGemma输出）

Result: MedGemma在基准测试中表现更好，效应性和AUROC更高；GPT-4o特异性近于完美但效应性低。在模拟AI协助中，GPT-4o的性能在错误输入时崩溃，而MedGemma更稳定。在实际协作中，GPT-4o在MedGemma描述性输出指导下取得了强劲结果（AUROC达0.96）。

Conclusion: MLLM模型可以改善糖尿病视网膜病筛查流程，并作为可扩展的模拟器研究临床AI协助。过渐、轻量级模型如MedGemma在资源稀缺环境中特别有价值，描述性输出可以增强可解释性和临床信任。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [48] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 提出了COLMA认知分层记忆架构，通过场景驱动方法解决AI记忆系统的适应性、多模态整合和持续学习问题，为AGI发展提供结构化基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI向AGI发展，现有记忆架构存在适应性有限、多模态整合不足、无法支持持续学习等问题，需要更健壮和类人的记忆系统。

Method: 采用场景驱动方法，从代表性认知场景中提取核心功能需求，提出统一的设计原则，并构建COLMA认知分层记忆架构，将认知场景、记忆过程和存储机制整合为统一设计。

Result: 开发了COLMA框架，为AI系统提供了结构化基础，使其能够实现终身学习和类人推理。

Conclusion: COLMA架构为下一代AI记忆系统提供了实用设计方法，有助于推动AGI的实际发展。

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [49] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: RepIt是一个简单高效的概念表示分离框架，能够在大型语言模型中精确隔离特定概念向量，实现针对性干预，仅需少量数据和计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法往往产生比预期更广泛的影响，需要更纯净的概念向量来实现精确干预和更细粒度地理解LLM行为。

Method: 提出RepIt框架，通过数据高效的方式分离概念特定的表示，能够从少量示例中提取鲁棒的目标表示，并将修正信号定位到100-200个神经元。

Result: 在五个前沿LLM上，RepIt能够选择性抑制特定概念的拒绝行为，同时保持其他地方的拒绝能力，使模型能够回答WMD相关问题但在标准基准测试中仍保持安全评分。

Conclusion: 通过RepIt分离拒绝向量，证明了针对性干预可以抵消过度泛化，为更细粒度的模型行为控制奠定了基础，但也引发了关于计算资源需求较低可能带来的安全担忧。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [50] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: Shapes of cognition是一种新的计算认知建模范式，通过记忆化的知识星座来简化复杂现实，让智能体像人类一样通过模式识别、类比推理等方式最小化认知负荷。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言赋能智能体(LEIAs)在复杂现实环境中的认知建模问题，提出一种能够模拟人类认知方式的新范式，使智能体能够像人类一样通过典型性预期、模式识别和习惯行为来应对复杂情境。

Method: 采用基于形状的建模方法，包括特定目标、假设、建模策略、知识库和实际模型，所有实现都在特定的认知架构内完成。异常结果通过基于形状的恢复方法处理，如即时学习、寻求人类帮助或获得可操作的情境理解。

Result: 提出了一个具体可行的shapes-based建模框架，该框架不仅可用于LEIAs的特定实例，其原则还可以更广泛地应用于知识型和混合型AI，为这些领域注入新的活力。

Conclusion: Shapes of cognition为计算认知建模提供了新的理论范式和实践框架，能够构建可解释、可扩展且值得信赖的智能体系统，即使在关键领域也能发挥作用。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [51] [Exploring the entropic region](https://arxiv.org/abs/2509.12439)
*Laszlo Csirmaz*

Main category: cs.IT

TL;DR: 论文探讨了获取新的熵低不等式的三种方法：Copy Lemma及其迭代版本、最大熵原则方法和Ahlswede-Körner方法，并分析了它们的变体、限制和关联性。


<details>
  <summary>Details</summary>
Motivation: 演续张磅-叶阳构造的Copy Lemma产生了第一个非香农不等式，需要系统性地研究不同方法在生成新熵低不等式方面的效果和限制。

Method: 分析了三种主要方法：1）Copy Lemma及其迭代版本、对称化处理和多面体顶点枚举关联 2）基于最大熵原则的方法 3）Ahlswede-Körner方法及其隐藏的Copy Lemma应用

Result: 发现Copy Lemma是最强的方法，最大熵方法的两种变体都不能生成比迭代Copy Lemma更多的不等式，Ahlswede-Körner方法因依赖隐藏的Copy Lemma应用而比Copy Lemma更弱。

Conclusion: 论文以教程式结构展现了熵低不等式生成方法的系统研究，指出了开放问题和研究挑战，为进一步探索非香农不等式提供了基础。

Abstract: The paper explores three known methods, their variants and limitations, that
can be used to obtain new entropy inequalities. The Copy Lemma was distilled
from the original Zhang-Yeung construction which produced the first non-Shannon
inequality. Its iterated version, effects of symmetrizations, and connections
with polyhedral vertex enumeration are discussed. Another method, derived from
the principle of maximum entropy, has the Copy Lemma as a special case.
Nevertheless, none of the two presented variants is known to generate more
inequalities than the iterated Copy Lemma. Finally, the Ahlswede-K\"orner
method is shown to employ a hidden application of the Copy Lemma - the
underlying lemma alone cannot generate new inequalities -, which makes this
method strictly weaker than the Copy Lemma. The paper is written in a tutorial
style and concludes with a list of open questions and research problems.

</details>


### [52] [Channel Estimation for Rydberg Atomic Quantum Receivers](https://arxiv.org/abs/2509.12586)
*Jian Xiao,Ji Wang,Ming Zeng,Hongbo Xu,Xingwang Li,Arumugam Nallanathan*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The advent of Rydberg atomic quantum receivers (RAQRs) offers a new solution
for the evolution of wireless transceiver architecture, promising unprecedented
sensitivity and immunity to thermal noise. However, RAQRs introduce a unique
non-linear signal model based on biased phase retrieval, which complicates
fundamental channel estimation tasks. Traditional iterative algorithms often
struggle in low signal-to-noise regimes and fail to capture complex and
non-ideal system characteristics. To address this, we propose a novel
model-driven deep learning framework for channel estimation in RAQRs.
Specifically, we propose a Transformer-based unrolling architecture, termed
URformer, which is derived by unrolling a stabilized variant of the
expectation-maximization Gerchberg-Saxton (EM-GS) algorithm. Specifically, each
layer of the proposed URformer incorporates three trainable modules: 1) a
learnable filter implemented by a neural network that replaces the fixed Bessel
function ratio in the classic EM-GS algorithm; 2) a trainable gating mechanism
that adaptively combines classic and model-based updates to ensure training
stability; and 3) a efficient channel Transformer block that learns to correct
residual errors by capturing non-local dependencies across the channel matrix.
Numerical results demonstrate that the proposed URformer significantly
outperforms classic iterative algorithms and conventional black-box neural
networks with less pilot overhead.

</details>


### [53] [Three Classes of Twisted Gabidulin Codes with Different Twists](https://arxiv.org/abs/2509.12693)
*Ran Li,Fang-Wei Fu,Weijun Fang*

Main category: cs.IT

TL;DR: 本文研究了三种不同类型的扭曲Gabidulin码，建立了它们成为最大秩距离(MRD)码的充要条件，确定了非MRD码的条件，并通过扭曲Gabidulin码构造了多类MRD码。此外，在汉明度量下分析了这些码的最大距离可分(MDS)、几乎MDS和近MDS性质，并研究了覆盖半径和深洞问题。


<details>
  <summary>Details</summary>
Motivation: 扭曲Gabidulin码作为Gabidulin码的扩展，近年来受到广泛关注。研究这些码的MRD性质、MDS性质以及覆盖特性，有助于推动编码理论的发展和应用。

Method: 通过数学分析和理论推导，建立了三类不同扭曲的Gabidulin码的MRD和MDS性质的充要条件，并构造了多类MRD码实例。同时研究了这些码的覆盖半径和深洞特性。

Result: 确定了扭曲Gabidulin码成为MRD码的完整条件，构造了多个新的MRD码类。在汉明度量下，给出了这些码成为MDS、几乎MDS或近MDS码的充要条件，并获得了覆盖半径和深洞的相关结果。

Conclusion: 本文系统研究了扭曲Gabidulin码的多重性质，为这类重要编码的结构分析和应用提供了理论基础，特别是在MRD码构造和覆盖特性方面取得了重要进展。

Abstract: Twisted Gabidulin codes are an extension of Gabidulin codes and have recently
attracted great attention. In this paper, we study three classes of twisted
Gabidulin codes with different twists. Moreover, we establish necessary and
sufficient conditions for them to be maximum rank distance (MRD) codes,
determine the conditions under which they are not MRD codes, and construct
several classes of MRD codes via twisted Gabidulin codes. In addition,
considering these codes in the Hamming metric, we provide necessary and
sufficient conditions for them to be maximum distance separable (MDS), almost
MDS, or near MDS. Finally, we investigate the covering radii and deep holes of
twisted Gabidulin codes.

</details>


### [54] [Linear Complexity Computation of Code Distance and Minimum Size of Trapping Sets for LDPC Codes with Bounded Treewidth](https://arxiv.org/abs/2509.13040)
*Qingqing Peng,Ke Liu,Guiying Yan,Guanghui Wang*

Main category: cs.IT

TL;DR: 本文针对二进制线性码中寻找最小a的(a,b)-trapping set问题，证明了对于有界树宽度的码，该问题具有线性复杂度，并提供了具体算法。


<details>
  <summary>Details</summary>
Motivation: 已知在二进制线性码中寻找最小a的(a,b)-trapping set是NP难问题，本文旨在为有界树宽度的码提供高效解决方案。

Method: 利用树分解技术，针对树宽度有界的二进制线性码，设计了一个线性复杂度的算法来计算最小a值及对应的(a,b)-trapping set数量。

Result: 理论证明该问题对于有界树宽度的码具有线性复杂度，仿真实验验证了所提算法的正确性。

Conclusion: 对于树宽度有界的二进制线性码，寻找最小a的(a,b)-trapping set问题可以在线性时间内解决，为解决这类NP难问题提供了有效途径。

Abstract: It is well known that, given \(b\ge 0\), finding an $(a,b)$-trapping set with
the minimum \(a\) in a binary linear code is NP-hard. In this paper, we
demonstrate that this problem can be solved with linear complexity with respect
to the code length for codes with bounded treewidth. Furthermore, suppose a
tree decomposition corresponding to the treewidth of the binary linear code is
known. In that case, we also provide a specific algorithm to compute the
minimum \(a\) and the number of the corresponding \((a, b)\)-trapping sets for
a given \(b\) with linear complexity. Simulation experiments are presented to
verify the correctness of the proposed algorithm.

</details>
